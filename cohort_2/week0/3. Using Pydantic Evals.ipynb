{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pydantic Evals\n",
    "\n",
    "> Prerequisites: Make sure you have your Logfire API token configured as LOGFIRE_TOKEN in your environment variables. If you don't have a Logfire API Token, you can sign up for an account on [Logfire's Website](http://logfire.pydantic.dev), setup a project and then create a token with WRITE access.\n",
    "\n",
    "In this notebook, we'll learn how to use Pydantic Evals to run evaluation testcases and track results with Logfire.\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Using the same tools for evaluations, production systems, and monitoring dashboards creates a powerful unified workflow. Logfire provides native integration with various tracing libraries, allowing you to keep all your metrics in one place with minimal code changes. This integration between Pydantic Evals and Logfire means you can:\n",
    "\n",
    "1. Track model performance consistently across development and production\n",
    "2. Build dashboards that monitor critical aspects of your RAG system\n",
    "3. Identify performance regressions immediately when they occur\n",
    "4. Share evaluation results easily with your entire team\n",
    "\n",
    "Instead of maintaining separate evaluation scripts, production monitoring, and reporting systems, this unified approach streamlines your workflow and ensures nothing falls through the cracks.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "Through this hands-on tutorial, you'll discover how to:\n",
    "\n",
    "1. Set Up Evaluation Infrastructure\n",
    "- Configure Logfire for tracking results\n",
    "- Create test cases and datasets\n",
    "- Define custom evaluators\n",
    "\n",
    "2. Run Basic Evaluations\n",
    "- Test model outputs against expected results\n",
    "- Calculate performance scores\n",
    "- Track results in Logfire\n",
    "\n",
    "3. Build Custom Evaluators\n",
    "- Create specialized evaluation metrics\n",
    "- Customize scoring logic\n",
    "- Combine multiple evaluators\n",
    "\n",
    "\n",
    "By the end of this notebook, you'll have a foundation for systematically evaluating model performance and tracking results. We'll be using Pydantic Evals heavily in this course so make sure that you're able to run the code here in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Logfire\n",
    "\n",
    "Before running this notebook, you'll need to configure your Logfire Project locally. You can do so by running the following command\n",
    "\n",
    "```\n",
    "logfire auth\n",
    "```\n",
    "\n",
    "Follow the instructions and then make sure you can run the code below. This will ensure that all of the evaluations that we run will be automatically sent to logfire in an easy way for us to track.\n",
    "\n",
    "If you're facing some issues and being asked to authenticate, you can manually set the env variable by doing\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "os.environ[\"LOGFIRE_TOKEN\"] = <your logfire token>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanleo/Documents/coding/systematically-improving-rag/cohort_2/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<logfire._internal.main.Logfire at 0x1074677c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mLogfire\u001b[0m project URL: \u001b]8;id=983671;https://logfire-us.pydantic.dev/ivanleomk/logfire-demo\u001b\\\u001b[4;36mhttps://logfire-us.pydantic.dev/ivanleomk/logfire-demo\u001b[0m\u001b]8;;\u001b\\\n"
     ]
    }
   ],
   "source": [
    "import logfire\n",
    "\n",
    "logfire.configure(\n",
    "    send_to_logfire=True,  \n",
    "    environment='experimentation',  \n",
    "    service_name='evals',  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Logfire configured, we're ready to build our first evaluation using Pydantic Evals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Your First Evaluation\n",
    "\n",
    "When running evaluations on AI systems, we typically need three key elements: test cases to evaluate, a way to run those test cases against our model, and metrics to grade the outputs. Pydantic Evals formalizes this process with three main components that work together to create a complete evaluation pipeline.\n",
    "\n",
    "\n",
    "1. Cases: Individual test scenarios with specific inputs and expected outputs\n",
    "2. Datasets: Collections of test cases that can be run together\n",
    "3. Evaluators: Functions that assess model outputs and calculate performance metrics\n",
    "\n",
    "This structure allows you to organize your test cases logically, run them efficiently, and apply consistent evaluation metrics across different models or versions. Let's build a simple example to see how these components work together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04:19:54.438 evaluate guess_city\n",
      "04:19:54.439   case: simple_case\n",
      "04:19:54.439     execute guess_city\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">         Evaluation Summary: guess_city         </span>\n",
       "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Case ID     </span>┃<span style=\"font-weight: bold\"> Scores              </span>┃<span style=\"font-weight: bold\"> Duration </span>┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"font-weight: bold\"> simple_case </span>│ IsExactMatch: 0.000 │    506µs │\n",
       "├─────────────┼─────────────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> </span><span style=\"font-weight: bold; font-style: italic\">Averages</span><span style=\"font-weight: bold\">    </span>│ IsExactMatch: 0.000 │    506µs │\n",
       "└─────────────┴─────────────────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m         Evaluation Summary: guess_city         \u001b[0m\n",
       "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mCase ID    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mScores             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDuration\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001b[1m \u001b[0m\u001b[1msimple_case\u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 0.000 │    506µs │\n",
       "├─────────────┼─────────────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1;3mAverages\u001b[0m\u001b[1m   \u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 0.000 │    506µs │\n",
       "└─────────────┴─────────────────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from pydantic_evals.evaluators import Evaluator, EvaluatorContext\n",
    "\n",
    "from pydantic_evals import Case, Dataset\n",
    "\n",
    "# Create a single test case\n",
    "case1 = Case(\n",
    "    name='simple_case',\n",
    "    inputs='What is the capital of France?',\n",
    "    expected_output='paris',\n",
    "    metadata={'difficulty': 'easy'},\n",
    ")\n",
    "\n",
    "# Create a dataset from our case\n",
    "dataset = Dataset(cases=[case1])\n",
    "\n",
    "# Create a custom evaluator that checks for exact matches\n",
    "@dataclass\n",
    "class IsExactMatch(Evaluator):\n",
    "    async def evaluate(self, ctx: EvaluatorContext[str, str]) -> float:  \n",
    "        if ctx.output == ctx.expected_output:\n",
    "            return 1.0  # Perfect match\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# Add our custom evaluators to the dataset\n",
    "dataset.add_evaluator(IsExactMatch())\n",
    "\n",
    "# Then we define a function that we want to evaluate\n",
    "\n",
    "async def guess_city(question: str) -> str:  \n",
    "    return 'Paris'\n",
    "\n",
    "\n",
    "report = await dataset.evaluate(guess_city)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation summary above shows the results of our first test case. Our function `guess_city` returned \"Paris\" (with a capital P) while our expected output was \"paris\" (lowercase). The `IsExactMatch` evaluator returned a score of 0.0 because the case doesn't match exactly. This demonstrates how strict the `IsExactMatch` evaluator is - even a difference in capitalization results in a failed test.\n",
    "\n",
    "If we change the `guess_city` function above to instead return a string `paris`, we'll see a change in the score to be `1` instead\n",
    "\n",
    "\n",
    "And with that we've just ran our first set of evaluations with Logfire. If you navigate to [the Logfire website](https://logfire.pydantic.dev), you should be able to see your trace show up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating LLM Classification\n",
    "\n",
    "Moving beyond simple examples, let's explore how Pydantic Evals can be used to evaluate more complex tasks like classification. We'll evaluate a customer support function that generates responses to common questions about product returns and refunds - a scenario directly relevant to e-commerce RAG systems.\n",
    "\n",
    "We'll first define a function that takes in a user question and then classifies as either Refunds, Informational, Shipping or Account related queries. Once we've done so, we'll then run an evaluation on our test cases to see if our model is able to predict the right value a majority of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Refunds'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "from openai import AsyncOpenAI\n",
    "import instructor\n",
    "\n",
    "class QueryType(BaseModel):\n",
    "    query_type: Literal['Refunds', 'Informational', 'Shipping', 'Account']\n",
    "\n",
    "client = instructor.from_openai(AsyncOpenAI())\n",
    "\n",
    "async def classify_query(question: str) -> str:\n",
    "    resp =await client.chat.completions.create(\n",
    "        model='gpt-4.1-nano',\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': 'You are a customer support agent that can classify user queries into one of the following categories: Refunds, Informational, Shipping, Account.'},\n",
    "            {'role': 'user', 'content': question}\n",
    "        ],\n",
    "        response_model=QueryType\n",
    "    )\n",
    "    return resp.query_type\n",
    "\n",
    "await classify_query('How do I return a product?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined our task with instructor, let's now define some simple test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04:07:27.840 evaluate classify_query\n",
      "04:07:27.841   case: refund_query\n",
      "04:07:27.841     execute classify_query\n",
      "04:07:27.846   case: informational_query\n",
      "04:07:27.846     execute classify_query\n",
      "04:07:27.851   case: shipping_query\n",
      "04:07:27.851     execute classify_query\n",
      "04:07:27.856   case: account_query\n",
      "04:07:27.856     execute classify_query\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">          Evaluation Summary: classify_query           </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Case ID             </span>┃<span style=\"font-weight: bold\"> Scores             </span>┃<span style=\"font-weight: bold\"> Duration </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"font-weight: bold\"> refund_query        </span>│ IsExactMatch: 1.00 │  543.6ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> informational_query </span>│ IsExactMatch: 1.00 │  962.4ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> shipping_query      </span>│ IsExactMatch: 1.00 │  534.5ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> account_query       </span>│ IsExactMatch: 1.00 │  525.6ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> </span><span style=\"font-weight: bold; font-style: italic\">Averages</span><span style=\"font-weight: bold\">            </span>│ IsExactMatch: 1.00 │  641.5ms │\n",
       "└─────────────────────┴────────────────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m          Evaluation Summary: classify_query           \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mCase ID            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mScores            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDuration\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001b[1m \u001b[0m\u001b[1mrefund_query       \u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 1.00 │  543.6ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1minformational_query\u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 1.00 │  962.4ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mshipping_query     \u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 1.00 │  534.5ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1maccount_query      \u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 1.00 │  525.6ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1;3mAverages\u001b[0m\u001b[1m           \u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 1.00 │  641.5ms │\n",
       "└─────────────────────┴────────────────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cases = [\n",
    "    Case(\n",
    "        name='refund_query',\n",
    "        inputs='How do I return a product?',\n",
    "        expected_output='Refunds'\n",
    "    ),\n",
    "    Case(\n",
    "        name='informational_query',\n",
    "        inputs='What is the return policy?',\n",
    "        expected_output='Informational'\n",
    "    ),\n",
    "    Case(\n",
    "        name='shipping_query',\n",
    "        inputs='How do I track my order?',\n",
    "        expected_output='Shipping'\n",
    "    ),\n",
    "    Case(\n",
    "        name='account_query',\n",
    "        inputs='How do I change my password?',\n",
    "        expected_output='Account'\n",
    "    )\n",
    "]\n",
    "\n",
    "dataset = Dataset(cases=cases)\n",
    "\n",
    "# Now we'll add our evaluator to the dataset\n",
    "dataset.add_evaluator(IsExactMatch())\n",
    "\n",
    "# Now we'll run our evaluation\n",
    "report = await dataset.evaluate(classify_query)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save this dataset to a `.yaml` file for versioning and tracking. Note here that we need to provide the custom types of all of the evaluators that we used which don't ship out of the box with Pydantic Evals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_file(\"dataset.yaml\", custom_evaluator_types=[IsExactMatch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then load it in again from the yaml file and run our evaluations again using the same function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04:07:32.216 evaluate classify_query\n",
      "04:07:32.217   case: refund_query\n",
      "04:07:32.217     execute classify_query\n",
      "04:07:32.222   case: informational_query\n",
      "04:07:32.222     execute classify_query\n",
      "04:07:32.227   case: shipping_query\n",
      "04:07:32.228     execute classify_query\n",
      "04:07:32.232   case: account_query\n",
      "04:07:32.233     execute classify_query\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">          Evaluation Summary: classify_query           </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Case ID             </span>┃<span style=\"font-weight: bold\"> Scores             </span>┃<span style=\"font-weight: bold\"> Duration </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"font-weight: bold\"> refund_query        </span>│ IsExactMatch: 1.00 │  461.4ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> informational_query </span>│ IsExactMatch: 1.00 │  474.7ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> shipping_query      </span>│ IsExactMatch: 1.00 │  693.2ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> account_query       </span>│ IsExactMatch: 1.00 │  449.0ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> </span><span style=\"font-weight: bold; font-style: italic\">Averages</span><span style=\"font-weight: bold\">            </span>│ IsExactMatch: 1.00 │  519.6ms │\n",
       "└─────────────────────┴────────────────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m          Evaluation Summary: classify_query           \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mCase ID            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mScores            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDuration\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001b[1m \u001b[0m\u001b[1mrefund_query       \u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 1.00 │  461.4ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1minformational_query\u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 1.00 │  474.7ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mshipping_query     \u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 1.00 │  693.2ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1maccount_query      \u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 1.00 │  449.0ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1;3mAverages\u001b[0m\u001b[1m           \u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 1.00 │  519.6ms │\n",
       "└─────────────────────┴────────────────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.from_file(\"dataset.yaml\", custom_evaluator_types=[IsExactMatch])\n",
    "\n",
    "report = await dataset.evaluate(classify_query)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored how to use Pydantic Evals to create systematic evaluation pipelines for AI systems. By combining Pydantic Evals with Logfire, we've built a foundation for tracking and improving model performance over time.\n",
    "\n",
    "Key takeaways from this introduction include:\n",
    "\n",
    "1. **Structured Evaluation Framework**: Pydantic Evals provides a clear structure for organizing test cases, running evaluations, and measuring performance with custom metrics.\n",
    "\n",
    "2. **Integration with Production Tools**: The same evaluation system can connect directly to your monitoring infrastructure, creating a unified workflow from development to production.\n",
    "\n",
    "3. **Scalable Approach**: This framework scales from simple exact-match evaluations to more complex assessments of model outputs.\n",
    "\n",
    "In the following notebooks, we'll build on these concepts to evaluate more complex RAG behaviors, like retrieval quality, answer correctness, and citation accuracy. You'll see how these evaluation techniques integrate with the embedding fine-tuning from Week 2, the query understanding from Week 4, and the structured data handling from Week 5.\n",
    "\n",
    "Make sure you have Logfire properly configured before proceeding, as we'll continue to use this evaluation framework to measure our progress throughout the rest of the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
