{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pydantic Evals\n",
    "\n",
    "> **Prerequisites**: Make sure that you've signed up for an account with [Logfire](http://logfire.pydantic.dev) and created your Read and Write tokens.\n",
    "> 1. [Creating your Write Tokens](https://logfire.pydantic.dev/docs/how-to-guides/create-write-tokens/)\n",
    "> 2. [Creating your Read Tokens](https://logfire.pydantic.dev/docs/how-to-guides/query-api/)\n",
    "> \n",
    "> Once you've done so, you should then set the write token as an environment variable called `LOGFIRE_TOKEN` and your read token as `LOGFIRE_READ_TOKEN`\n",
    "\n",
    "In this notebook, we'll learn how to use Pydantic Evals to run evaluation testcases and track results with Logfire.\n",
    "\n",
    "You'll need a Read token to export your data and a Write token to be able to save the generated logs to Logfire.\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Using the same tools for evaluations, production systems, and monitoring dashboards creates a powerful unified workflow. Logfire provides native integration with various tracing libraries, allowing you to keep all your metrics in one place with minimal code changes. This integration between Pydantic Evals and Logfire means you can:\n",
    "\n",
    "1. Track model performance consistently across development and production\n",
    "2. Build dashboards that monitor critical aspects of your RAG system\n",
    "3. Identify performance regressions immediately when they occur\n",
    "4. Share evaluation results easily with your entire team\n",
    "\n",
    "Instead of maintaining separate evaluation scripts, production monitoring, and reporting systems, this unified approach streamlines your workflow and ensures nothing falls through the cracks.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "Through this hands-on tutorial, you'll discover how to:\n",
    "\n",
    "1. Set Up Evaluation Infrastructure\n",
    "- Configure Logfire for tracking results\n",
    "- Create test cases and datasets\n",
    "- Define custom evaluators\n",
    "\n",
    "2. Run Basic Evaluations\n",
    "- Test model outputs against expected results\n",
    "- Calculate performance scores\n",
    "- Track results in Logfire\n",
    "\n",
    "3. Build Custom Evaluators\n",
    "- Create specialized evaluation metrics\n",
    "- Customize scoring logic\n",
    "- Combine multiple evaluators\n",
    "\n",
    "\n",
    "By the end of this notebook, you'll have a foundation for systematically evaluating model performance and tracking results. We'll be using Pydantic Evals heavily in this course so make sure that you're able to run the code here in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Logfire\n",
    "\n",
    "Before running this notebook, you'll need to configure your logfire environment variables. If you're facing some issues and being asked to authenticate, you can manually set the env variables by doing\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "os.environ[\"LOGFIRE_TOKEN\"] = <your logfire write token>\n",
    "os.environ[\"LOGFIRE_READ_TOKEN] = <your logfire read token >\n",
    "```\n",
    "\n",
    "We recommend setting it so these variables are present in your shell instead or with `python-dotenv` as outlined in the README instead.\n",
    "\n",
    "Alternatively, you can run the following command\n",
    "\n",
    "```\n",
    "logfire auth\n",
    "```\n",
    "\n",
    "Follow the instructions and link your project to Logfire to get it set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanleo/Documents/coding/systematically-improving-rag/cohort_2/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<logfire._internal.main.Logfire at 0x107844280>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logfire\n",
    "\n",
    "logfire.configure(\n",
    "    send_to_logfire=True,\n",
    "    environment=\"experimentation\",\n",
    "    service_name=\"evals\",\n",
    "    console=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Logfire configured, we're ready to build our first evaluation using Pydantic Evals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Your First Evaluation\n",
    "\n",
    "When running evaluations on AI systems, we typically need three key elements: test cases to evaluate, a way to run those test cases against our model, and metrics to grade the outputs. Pydantic Evals formalizes this process with three main components that work together to create a complete evaluation pipeline.\n",
    "\n",
    "\n",
    "1. Cases: Individual test scenarios with specific inputs and expected outputs\n",
    "2. Datasets: Collections of test cases that can be run together\n",
    "3. Evaluators: Functions that assess model outputs and calculate performance metrics\n",
    "\n",
    "This structure allows you to organize your test cases logically, run them efficiently, and apply consistent evaluation metrics across different models or versions. Let's build a simple example to see how these components work together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">         Evaluation Summary: guess_city         </span>\n",
       "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Case ID     </span>┃<span style=\"font-weight: bold\"> Scores              </span>┃<span style=\"font-weight: bold\"> Duration </span>┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"font-weight: bold\"> simple_case </span>│ IsExactMatch: 0.000 │     82µs │\n",
       "├─────────────┼─────────────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> </span><span style=\"font-weight: bold; font-style: italic\">Averages</span><span style=\"font-weight: bold\">    </span>│ IsExactMatch: 0.000 │     82µs │\n",
       "└─────────────┴─────────────────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m         Evaluation Summary: guess_city         \u001b[0m\n",
       "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mCase ID    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mScores             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDuration\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001b[1m \u001b[0m\u001b[1msimple_case\u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 0.000 │     82µs │\n",
       "├─────────────┼─────────────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1;3mAverages\u001b[0m\u001b[1m   \u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 0.000 │     82µs │\n",
       "└─────────────┴─────────────────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from pydantic_evals.evaluators import Evaluator, EvaluatorContext\n",
    "\n",
    "from pydantic_evals import Case, Dataset\n",
    "\n",
    "# Create a single test case\n",
    "case1 = Case(\n",
    "    name=\"simple_case\",\n",
    "    inputs=\"What is the capital of France?\",\n",
    "    expected_output=\"paris\",\n",
    "    metadata={\"difficulty\": \"easy\"},\n",
    ")\n",
    "\n",
    "# Create a dataset from our case\n",
    "dataset = Dataset(cases=[case1])\n",
    "\n",
    "\n",
    "# Create a custom evaluator that checks for exact matches\n",
    "@dataclass\n",
    "class IsExactMatch(Evaluator):\n",
    "    async def evaluate(self, ctx: EvaluatorContext[str, str]) -> float:\n",
    "        if ctx.output == ctx.expected_output:\n",
    "            return 1.0  # Perfect match\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# Add our custom evaluators to the dataset\n",
    "dataset.add_evaluator(IsExactMatch())\n",
    "\n",
    "# Then we define a function that we want to evaluate\n",
    "\n",
    "\n",
    "async def guess_city(question: str) -> str:\n",
    "    return \"Paris\"\n",
    "\n",
    "\n",
    "report = await dataset.evaluate(guess_city)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation summary above shows the results of our first test case. Our function `guess_city` returned \"Paris\" (with a capital P) while our expected output was \"paris\" (lowercase). The `IsExactMatch` evaluator returned a score of 0.0 because the case doesn't match exactly. This demonstrates how strict the `IsExactMatch` evaluator is - even a difference in capitalization results in a failed test.\n",
    "\n",
    "If we change the `guess_city` function above to instead return a string `paris`, we'll see a change in the score to be `1` instead\n",
    "\n",
    "\n",
    "And with that we've just ran our first set of evaluations with Logfire. If you navigate to [the Logfire website](https://logfire.pydantic.dev), you should be able to see your trace show up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating LLM Classification\n",
    "\n",
    "Moving beyond simple examples, let's explore how Pydantic Evals can be used to evaluate more complex tasks like classification. We'll evaluate a customer support function that generates responses to common questions about product returns and refunds - a scenario directly relevant to e-commerce RAG systems.\n",
    "\n",
    "We'll first define a function that takes in a user question and then classifies as either Refunds, Informational, Shipping or Account related queries. Once we've done so, we'll then run an evaluation on our test cases to see if our model is able to predict the right value a majority of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Refunds'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "from openai import AsyncOpenAI\n",
    "import instructor\n",
    "\n",
    "\n",
    "class QueryType(BaseModel):\n",
    "    query_type: Literal[\"Refunds\", \"Informational\", \"Shipping\", \"Account\"]\n",
    "\n",
    "\n",
    "client = instructor.from_openai(AsyncOpenAI())\n",
    "\n",
    "\n",
    "async def classify_query(question: str) -> str:\n",
    "    resp = await client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a customer support agent that can classify user queries into one of the following categories: Refunds, Informational, Shipping, Account.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        response_model=QueryType,\n",
    "    )\n",
    "    return resp.query_type\n",
    "\n",
    "\n",
    "await classify_query(\"How do I return a product?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined our task with instructor, let's now define some simple test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">          Evaluation Summary: classify_query           </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Case ID             </span>┃<span style=\"font-weight: bold\"> Scores             </span>┃<span style=\"font-weight: bold\"> Duration </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"font-weight: bold\"> refund_query        </span>│ IsExactMatch: 1.00 │  402.1ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> informational_query </span>│ IsExactMatch: 1.00 │  470.4ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> shipping_query      </span>│ IsExactMatch: 1.00 │  469.3ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> account_query       </span>│ IsExactMatch: 1.00 │  462.2ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> </span><span style=\"font-weight: bold; font-style: italic\">Averages</span><span style=\"font-weight: bold\">            </span>│ IsExactMatch: 1.00 │  451.0ms │\n",
       "└─────────────────────┴────────────────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m          Evaluation Summary: classify_query           \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mCase ID            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mScores            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDuration\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001b[1m \u001b[0m\u001b[1mrefund_query       \u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 1.00 │  402.1ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1minformational_query\u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 1.00 │  470.4ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mshipping_query     \u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 1.00 │  469.3ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1maccount_query      \u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 1.00 │  462.2ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1;3mAverages\u001b[0m\u001b[1m           \u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 1.00 │  451.0ms │\n",
       "└─────────────────────┴────────────────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cases = [\n",
    "    Case(\n",
    "        name=\"refund_query\",\n",
    "        inputs=\"How do I return a product?\",\n",
    "        expected_output=\"Refunds\",\n",
    "    ),\n",
    "    Case(\n",
    "        name=\"informational_query\",\n",
    "        inputs=\"What is the return policy?\",\n",
    "        expected_output=\"Informational\",\n",
    "    ),\n",
    "    Case(\n",
    "        name=\"shipping_query\",\n",
    "        inputs=\"How do I track my order?\",\n",
    "        expected_output=\"Shipping\",\n",
    "    ),\n",
    "    Case(\n",
    "        name=\"account_query\",\n",
    "        inputs=\"How do I change my password?\",\n",
    "        expected_output=\"Account\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "dataset = Dataset(cases=cases)\n",
    "\n",
    "# Now we'll add our evaluator to the dataset\n",
    "dataset.add_evaluator(IsExactMatch())\n",
    "\n",
    "# Now we'll run our evaluation\n",
    "report = await dataset.evaluate(classify_query)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save this dataset to a `.yaml` file for versioning and tracking. Note here that we need to provide the custom types of all of the evaluators that we used which don't ship out of the box with Pydantic Evals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanleo/Documents/coding/systematically-improving-rag/cohort_2/.venv/lib/python3.9/site-packages/pydantic_evals/dataset.py:390: UserWarning: Could not determine the generic parameters for <class 'pydantic_evals.dataset.Dataset'>; using `Any` for each. You should explicitly set the generic parameters via `Dataset[MyInputs, MyOutput, MyMetadata]` when serializing or deserializing.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset.to_file(\"dataset.yaml\", custom_evaluator_types=[IsExactMatch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then load it in again from the yaml file and run our evaluations again using the same function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">          Evaluation Summary: classify_query           </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Case ID             </span>┃<span style=\"font-weight: bold\"> Scores             </span>┃<span style=\"font-weight: bold\"> Duration </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"font-weight: bold\"> refund_query        </span>│ IsExactMatch: 1.00 │  466.1ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> informational_query </span>│ IsExactMatch: 1.00 │  448.7ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> shipping_query      </span>│ IsExactMatch: 1.00 │  410.4ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> account_query       </span>│ IsExactMatch: 1.00 │  477.7ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> </span><span style=\"font-weight: bold; font-style: italic\">Averages</span><span style=\"font-weight: bold\">            </span>│ IsExactMatch: 1.00 │  450.7ms │\n",
       "└─────────────────────┴────────────────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m          Evaluation Summary: classify_query           \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mCase ID            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mScores            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDuration\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001b[1m \u001b[0m\u001b[1mrefund_query       \u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 1.00 │  466.1ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1minformational_query\u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 1.00 │  448.7ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mshipping_query     \u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 1.00 │  410.4ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1maccount_query      \u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 1.00 │  477.7ms │\n",
       "├─────────────────────┼────────────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1;3mAverages\u001b[0m\u001b[1m           \u001b[0m\u001b[1m \u001b[0m│ IsExactMatch: 1.00 │  450.7ms │\n",
       "└─────────────────────┴────────────────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.from_file(\"dataset.yaml\", custom_evaluator_types=[IsExactMatch])\n",
    "\n",
    "report = await dataset.evaluate(classify_query)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting your Logfire spans\n",
    "\n",
    "Working with Logfire is easy because you can query your spans using simple SQL.\n",
    "\n",
    "This means that you have an incredible amount of flexibility to create custom views and group by(s) that suit your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'assertions'</span>: <span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'attributes'</span>: <span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'case_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'simple_case'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'expected_output'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'paris'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'inputs'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What is the capital of France?'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'labels'</span>: <span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'logfire.msg_template'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'case: {case_name}'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'difficulty'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'easy'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'metrics'</span>: <span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'output'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Paris'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'scores'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'IsExactMatch'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'IsExactMatch'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'value'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'reason'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'IsExactMatch'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'arguments'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'task_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.2e-05</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'task_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'guess_city'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'assertions'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'attributes'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'case_name'\u001b[0m: \u001b[32m'simple_case'\u001b[0m,\n",
       "    \u001b[32m'expected_output'\u001b[0m: \u001b[32m'paris'\u001b[0m,\n",
       "    \u001b[32m'inputs'\u001b[0m: \u001b[32m'What is the capital of France?'\u001b[0m,\n",
       "    \u001b[32m'labels'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'logfire.msg_template'\u001b[0m: \u001b[32m'case: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mcase_name\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "    \u001b[32m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'difficulty'\u001b[0m: \u001b[32m'easy'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'metrics'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'output'\u001b[0m: \u001b[32m'Paris'\u001b[0m,\n",
       "    \u001b[32m'scores'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'IsExactMatch'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'name'\u001b[0m: \u001b[32m'IsExactMatch'\u001b[0m,\n",
       "            \u001b[32m'value'\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "            \u001b[32m'reason'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'name'\u001b[0m: \u001b[32m'IsExactMatch'\u001b[0m, \u001b[32m'arguments'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'task_duration'\u001b[0m: \u001b[1;36m8.2e-05\u001b[0m,\n",
       "    \u001b[32m'task_name'\u001b[0m: \u001b[32m'guess_city'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from logfire.experimental.query_client import LogfireQueryClient\n",
    "import os\n",
    "from rich import print\n",
    "\n",
    "client = LogfireQueryClient(os.environ[\"LOGFIRE_READ_TOKEN\"])\n",
    "\n",
    "results = client.query_json_rows(\n",
    "    \"\"\"\n",
    "    SELECT * FROM records\n",
    "    WHERE service_name = 'evals'\n",
    "    LIMIT 1;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "for row in results[\"rows\"]:\n",
    "    print(row[\"attributes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logfire exposes the full span and information that was captured during the course of the eval run, making it easy to customise and compute custom metrics for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored how to use Pydantic Evals to create systematic evaluation pipelines for AI systems. By combining Pydantic Evals with Logfire, we've built a foundation for tracking and improving model performance over time.\n",
    "\n",
    "Key takeaways from this introduction include:\n",
    "\n",
    "1. **Structured Evaluation Framework**: Pydantic Evals provides a clear structure for organizing test cases, running evaluations, and measuring performance with custom metrics.\n",
    "\n",
    "2. **Integration with Production Tools**: The same evaluation system can connect directly to your monitoring infrastructure, creating a unified workflow from development to production.\n",
    "\n",
    "3. **Scalable Approach**: This framework scales from simple exact-match evaluations to more complex assessments of model outputs.\n",
    "\n",
    "In the following notebooks, we'll build on these concepts to evaluate more complex RAG behaviors, like retrieval quality, answer correctness, and citation accuracy. You'll see how these evaluation techniques integrate with the embedding fine-tuning from Week 2, the query understanding from Week 4, and the structured data handling from Week 5.\n",
    "\n",
    "Make sure you have Logfire properly configured before proceeding, as we'll continue to use this evaluation framework to measure our progress throughout the rest of the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
