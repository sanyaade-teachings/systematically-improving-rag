{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Building Your RAG Evaluation Foundation\n",
    "\n",
    "Evaluating RAG systems is challenging, especially when you're just starting out. This notebook introduces a practical approach to assessment using synthetic data generation - a crucial first step before diving into more complex metrics.\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Traditional RAG evaluation focuses on generated content quality, but this approach has significant drawbacks:\n",
    "\n",
    "| Aspect | Content Generation | Retrieval Metrics |\n",
    "|--------|-------------------|-------------------|\n",
    "| Speed | 1-10s per test | 10-800ms per test |\n",
    "| Cost | $100s per run | Negligible |\n",
    "| Objectivity | Subjective | Quantitative |\n",
    "| Iteration Speed | Hours | Minutes |\n",
    "| Scale | Limited | Automated |\n",
    "\n",
    "Instead, we'll focus on retrieval metrics that are:\n",
    "- Fast to compute (milliseconds vs seconds)\n",
    "- Objective and reproducible\n",
    "- Easy to automate\n",
    "- Cost-effective at scale\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "Through this hands-on tutorial, you'll learn to build a comprehensive evaluation framework:\n",
    "\n",
    "1. **Synthetic Data Generation**\n",
    "   - Create diverse, realistic test questions\n",
    "   - Generate comprehensive datasets without real user data\n",
    "   - Learn techniques for systematic question generation\n",
    "\n",
    "2. **Maintain Query Diversity**\n",
    "   - Identify different types of questions to test\n",
    "   - Cover various query patterns and edge cases\n",
    "   - Build representative test scenarios\n",
    "\n",
    "3. **Evaluation Setup**\n",
    "   - Establish measurement foundations\n",
    "   - Set up automated testing pipelines\n",
    "   - Create reproducible evaluation workflows\n",
    "\n",
    "By the end of this notebook, you'll have a good understanding of what retrieval metrics are, how we can generate synthetic questions to benchmark our retrieval system and how we can use these questions to evaluate our retrieval system.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluating Retrieval\n",
    "\n",
    "Before looking at our case study, let's first understand some of the metrics that we'll be using to evaluate our retrieval system. These metrics will form the basis of our evaluation framework throughout this course.\n",
    "\n",
    "### Key Retrieval Metrics\n",
    "\n",
    "**Precision** measures how many of our retrieved items are actually relevant:\n",
    "\n",
    "$$ \\text{Precision} = \\frac{\\text{Number of Relevant Items Retrieved}}{\\text{Total Number of Retrieved Items}} $$ \n",
    "\n",
    "For example, if your system retrieves 10 documents but only 5 are relevant, that's 50% precision. Low precision indicates your system is wasting resources processing irrelevant content.\n",
    "\n",
    "**Recall** measures how many of the total relevant items we managed to find:\n",
    "\n",
    "$$ \\text{Recall} = \\frac{\\text{Number of Relevant Items Retrieved}}{\\text{Total Number of Relevant Items}} $$ \n",
    "\n",
    "If there are 20 relevant documents in your database but you only retrieve 10 of them, that's 50% recall. Low recall suggests you're missing important information.\n",
    "\n",
    "In practice, we often measure these metrics at specific cutoff points (like top-5 or top-10 results), denoted as Precision@K or Recall@K. This reflects real-world usage where users typically only look at the first few results.\n",
    "\n",
    "Consider an example where we're trying to build a Text-2-SQL application. This is an application where we take in a user query and output a SQL query which can be used to retrieve the relevant information as seen below.\n",
    "\n",
    "```\n",
    "Text : Hey could you help me find the top 5 most popular items in our store?\n",
    "Query: SELECT item_name, COUNT(*) as popularity FROM items GROUP BY item_name ORDER BY popularity DESC LIMIT 5\n",
    "```\n",
    "\n",
    "This feels like a generation task but ultimately can be greatly improved by including few-shot examples. This is very similar to how we might look for relevant text chunks in a classic Question Answer RAG application.\n",
    "\n",
    "However,by framing this as a retrieval task, we can start by looking at precision and recall of our retrieval system before we even start looking at the generated SQL queries. This has two main benefits.\n",
    "\n",
    "1. When we do evaluate the generated SQL queries, we can identify edge cases early on and add them to our list of snippets. We can then verify that these few shot examples are retrieved when we encounter these questions to help generate better SQL snippets.\n",
    "2. Different companies have unique business logic or calculation methods. Being able to retrieve the relevant snippets when these specific measurements are required is crucial. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study : Bird-Bench\n",
    "\n",
    "For this case-study, we'll be using the Bird-Bench dataset. This is a huge Text-2-SQL dataset which contains a collection of text questions to a corresponding sql query. \n",
    "\n",
    "We'll be using the dev split of this dataset for this case study that provides ~1500+ sql snippets that involves ~95 different tables that we can use. \n",
    "\n",
    "We've cleaned the dataset ahead of time and uploaded it to `567-labs/bird-rag`. Each example in our dataset contains three things\n",
    "\n",
    "- `id` : This is a unique identifier for each query\n",
    "- `query` : This is a sample SQL query \n",
    "- `difficulty` : This is a label that indicates how difficult the query is to generate. It can be either `simple`, `moderate` or `challenging`. \n",
    "\n",
    "For this case study, we'll only be using the `challenging` queries so that we can generate more difficult questions. This allows us to test our retrieval system under more demanding conditions, ensuring that it performs well even with complex queries\n",
    "\n",
    "With that in mind, let's take a look at our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanleo/Documents/coding/systematically-improving-rag/cohort_2/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'query'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"SELECT `Free Meal Count (K-12)` / `Enrollment (K-12)` FROM frpm WHERE `County Name` = 'Alameda' ORDER</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">BY (CAST(`Free Meal Count (K-12)` AS REAL) / `Enrollment (K-12)`) DESC LIMIT 1\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'difficulty'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'simple'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'id'\u001b[0m: \u001b[32m'0'\u001b[0m,\n",
       "    \u001b[32m'query'\u001b[0m: \u001b[32m\"SELECT `Free Meal Count \u001b[0m\u001b[32m(\u001b[0m\u001b[32mK-12\u001b[0m\u001b[32m)\u001b[0m\u001b[32m` / `Enrollment \u001b[0m\u001b[32m(\u001b[0m\u001b[32mK-12\u001b[0m\u001b[32m)\u001b[0m\u001b[32m` FROM frpm WHERE `County Name` = 'Alameda' ORDER\u001b[0m\n",
       "\u001b[32mBY \u001b[0m\u001b[32m(\u001b[0m\u001b[32mCAST\u001b[0m\u001b[32m(\u001b[0m\u001b[32m`Free Meal Count \u001b[0m\u001b[32m(\u001b[0m\u001b[32mK-12\u001b[0m\u001b[32m)\u001b[0m\u001b[32m` AS REAL\u001b[0m\u001b[32m)\u001b[0m\u001b[32m / `Enrollment \u001b[0m\u001b[32m(\u001b[0m\u001b[32mK-12\u001b[0m\u001b[32m)\u001b[0m\u001b[32m`\u001b[0m\u001b[32m)\u001b[0m\u001b[32m DESC LIMIT 1\"\u001b[0m,\n",
       "    \u001b[32m'difficulty'\u001b[0m: \u001b[32m'simple'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE T2.FundingType =\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'Locally funded'</span> AND <span style=\"font-weight: bold\">(</span>T1.`Enrollment <span style=\"font-weight: bold\">(</span>K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">)</span>` - T1.`Enrollment <span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>`<span style=\"font-weight: bold\">)</span> &gt; <span style=\"font-weight: bold\">(</span>SELECT <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AVG</span><span style=\"font-weight: bold\">(</span>T3.`Enrollment <span style=\"font-weight: bold\">(</span>K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">)</span>` - \n",
       "T3.`Enrollment <span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>`<span style=\"font-weight: bold\">)</span> FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = T4.CDSCode WHERE \n",
       "T4.FundingType = <span style=\"color: #008000; text-decoration-color: #008000\">'Locally funded'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE T2.FundingType =\n",
       "\u001b[32m'Locally funded'\u001b[0m AND \u001b[1m(\u001b[0mT1.`Enrollment \u001b[1m(\u001b[0mK-\u001b[1;36m12\u001b[0m\u001b[1m)\u001b[0m` - T1.`Enrollment \u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m`\u001b[1m)\u001b[0m > \u001b[1m(\u001b[0mSELECT \u001b[1;35mAVG\u001b[0m\u001b[1m(\u001b[0mT3.`Enrollment \u001b[1m(\u001b[0mK-\u001b[1;36m12\u001b[0m\u001b[1m)\u001b[0m` - \n",
       "T3.`Enrollment \u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m`\u001b[1m)\u001b[0m FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = T4.CDSCode WHERE \n",
       "T4.FundingType = \u001b[32m'Locally funded'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "from rich import print\n",
    "\n",
    "dataset = datasets.load_dataset(\"567-labs/bird-rag\")[\"train\"]\n",
    "\n",
    "print(dataset[0])\n",
    "for item in dataset:\n",
    "    if item[\"difficulty\"] == \"challenging\":\n",
    "        print(item[\"query\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze a sample SQL query to understand what kind of synthetic questions we can generate:\n",
    "\n",
    "> SELECT `Free Meal Count (K-12)` / `Enrollment (K-12)` FROM frpm WHERE `County Name` = 'Alameda' ORDER BY (CAST(`Free Meal Count (K-12)` AS REAL) / `Enrollment (K-12)`) DESC LIMIT 1\n",
    "\n",
    "This query:\n",
    "1. Calculates the percentage of students receiving free meals\n",
    "2. Filters to Alameda County schools only\n",
    "3. Returns the school with the highest percentage\n",
    "\n",
    "Some relevant natural language questions could be:\n",
    "\n",
    "- \"What school in Alameda County has the highest proportion of students on free meal programs?\"\n",
    "- \"Which school has the highest free meal participation rate in Alameda County?\"\n",
    "\n",
    "By generating a dataset of similar questions, we can evaluate how well our retrieval system matches user queries to the appropriate SQL snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Synthetic Questions\n",
    "\n",
    "Now let's start generating our synthetic questions. We're going to begin by defining some Pydantic models that represent the format of the data that we're working with.\n",
    "\n",
    "We're doing so because of the following reasons\n",
    "\n",
    "1. It helps us to be explicit about the data we're working with \n",
    "2. We can use these models with the `instructor` library to obtain structured outputs from our LLM calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "# This represents how we're representing our data from the dataset\n",
    "class Chunk(BaseModel):\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "\n",
    "\n",
    "# This is the synthetic question that we want our model to generate\n",
    "class Question(BaseModel):\n",
    "    chain_of_thought: str\n",
    "    question: str\n",
    "\n",
    "\n",
    "# This is a single question-chunk pair that we'll be uploading to Braintrust as a dataset later on to be used for benchmarking in `benchmark_retrieval.py`\n",
    "class ChunkEval(BaseModel):\n",
    "    chunk_id: str\n",
    "    question: str\n",
    "    chunk: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using `instructor` because it handles prompt templating with `jinja` for us and provides validated structured outputs. \n",
    "\n",
    "All we need to do is to define a Pydantic model that represents a desired output and the library will handle the rest. \n",
    "\n",
    "Remember that we want to generate a question that should either be answerable by the data returned by the SQL snippet directly or with some small tweaks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What is the county with the highest ratio of free meal counts to enrollment figures over the past year in a \n",
       "specified geographic area?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "What is the county with the highest ratio of free meal counts to enrollment figures over the past year in a \n",
       "specified geographic area?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import openai\n",
    "import instructor\n",
    "from asyncio import Semaphore\n",
    "from tqdm.asyncio import tqdm_asyncio as asyncio\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "from rich import print\n",
    "\n",
    "client = instructor.from_openai(openai.OpenAI())\n",
    "\n",
    "sql_snippet = \"\"\"\\\n",
    "SELECT `Free Meal Count (K-12)` / `Enrollment (K-12)` \n",
    "FROM frpm \n",
    "WHERE `County Name` = 'Alameda' \n",
    "ORDER BY (CAST(`Free Meal Count (K-12)` AS REAL) / `Enrollment (K-12)`) DESC \n",
    "LIMIT 1\"\"\"\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"\n",
    "        Generate a hypothetical question that can be answered using the following SQL snippet. \n",
    "\n",
    "        SQL Snippet:\n",
    "        {{ snippet }}\n",
    "\n",
    "        Rules\n",
    "        - If there are specific values in the snippet, do not use them directly in the question if possible. \n",
    "        - The question should be at most 2 sentences long\n",
    "        - if necessary, consider making the question more challenging using the following constraint - If there's a time period mentioned in the snippet, modify it slightly (Eg. if the snippet is looking at the entire year, change it to 6 months or 1.5 years)\n",
    "        - The question must be answerable using the SQL snippet or at most with a small tweak\n",
    "        \"\"\",\n",
    "        }\n",
    "    ],\n",
    "    response_model=Question,\n",
    "    context={\"snippet\": sql_snippet}, # This is the context that we're passing to the model\n",
    ")\n",
    "\n",
    "print(resp.question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ''What is the highest ratio of free meal counts to total enrollments in K-12 schools for a specific county over a recent semester?'\n",
    "\n",
    "\n",
    "This is a question which the SQL snippet would be highly relevant for. In order to answer this query, we just need to make two changes\n",
    "\n",
    "1. add in a new time filter of a recent semester\n",
    "2. change the county to a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Diversity Problem\n",
    "\n",
    "We cannot use the same prompt and expect a diverse set of questions. Therefore we need to introduce slight variations in the prompt to generate questions that are different in wording, intent and content. This is crucial in identifying blindspots in our retrieval system. \n",
    "\n",
    "In the example below, we're using the same prompt but introducing randomly chosen constraints at each point. This forces the model to write and generate different questions each time, allowing us to collect a more diverse set of questions. The key here is to really introduce different sources of variation when doing these generations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Up Our Questions\n",
    "\n",
    "With those points in mind, let's scale our question generation up.\n",
    "\n",
    "We'll do so by generating a question for each SQL snippet marked as challenging. Since this will be a large number of requests, we're going to be doing so asynchronously with the `asyncio` library.\n",
    "\n",
    "Additionally, to make sure we stay within our rate limits , we'll be using a semaphore to limit the number of concurrent requests.\n",
    "\n",
    "We're also making sure that we have a good diversity of questions by randomly selecting a constraint from a set of constraints to make the question more challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:02<00:00,  4.62it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import asyncio\n",
    "\n",
    "# Define Instructor Client\n",
    "client = instructor.from_openai(openai.AsyncOpenAI())\n",
    "\n",
    "# Define some constraints to make the question more challenging\n",
    "constraints = [\n",
    "    \"If there's a time period mentioned in the snippet, modify it slightly (Eg. if the snippet is looking at the entire year, change it to 6 months or 1.5 years)\",\n",
    "    \"Add in some irrelevant context (Eg. Add information about the weather, a random event or a backstory that isn't mentioned in the snippet)\",\n",
    "    \"Changing the value of the filter (Eg. if the snippet is looking at the results in Canada, change the question to ask about another country or city instead)\",\n",
    "]\n",
    "\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(10))\n",
    "async def generate_questions(chunk: Chunk, sem: Semaphore) -> ChunkEval:\n",
    "    async with sem:\n",
    "        coro = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"\"\"\n",
    "                Generate a hypothetical question that can be answered using the following SQL snippet. \n",
    "\n",
    "                SQL Snippet:\n",
    "                {{ snippet }}\n",
    "\n",
    "                Rules\n",
    "                - If there are specific values in the snippet, do not use them directly in the question if possible. \n",
    "                - The question should be at most 2 sentences long\n",
    "                - if necessary, consider making the question more challenging using the following constraint of {{ constraint }}\n",
    "                - The question must be answerable using the SQL snippet or at most with a small tweak\n",
    "                \"\"\",\n",
    "                }\n",
    "            ],\n",
    "            response_model=Question,\n",
    "            context={\"snippet\": chunk.text, \"constraint\": random.choice(constraints)},\n",
    "        )\n",
    "        resp = await asyncio.wait_for(coro, timeout=30)\n",
    "\n",
    "        return ChunkEval(\n",
    "            chunk_id=chunk.chunk_id,\n",
    "            question=resp.question,\n",
    "            chunk=chunk.text,\n",
    "        )\n",
    "\n",
    "\n",
    "sem = Semaphore(10)\n",
    "dataset = [\n",
    "    item\n",
    "    for item in datasets.load_dataset(\"567-labs/bird-rag\")[\"train\"]\n",
    "    if item[\"difficulty\"] == \"challenging\"\n",
    "]\n",
    "dataset = [Chunk(chunk_id=item[\"id\"], text=item[\"query\"]) for item in dataset]\n",
    "\n",
    "coros = []\n",
    "\n",
    "num_samples = 2\n",
    "for chunk in dataset:\n",
    "    for _ in range(num_samples):\n",
    "        coros.append(generate_questions(chunk, sem))\n",
    "\n",
    "questions: list[ChunkEval] = await tqdm_asyncio.gather(*coros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've generated our questions, let's take a look at what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "    Question: In the past year, which locally funded schools have an enrollment difference between K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> and ages \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span> that exceeds the average difference among their peers?\n",
       "\n",
       "    SQL Snippet: SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE\n",
       "T2.FundingType = <span style=\"color: #008000; text-decoration-color: #008000\">'Locally funded'</span> AND <span style=\"font-weight: bold\">(</span>T1.`Enrollment <span style=\"font-weight: bold\">(</span>K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">)</span>` - T1.`Enrollment <span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>`<span style=\"font-weight: bold\">)</span> &gt; <span style=\"font-weight: bold\">(</span>SELECT \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AVG</span><span style=\"font-weight: bold\">(</span>T3.`Enrollment <span style=\"font-weight: bold\">(</span>K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">)</span>` - T3.`Enrollment <span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>`<span style=\"font-weight: bold\">)</span> FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = \n",
       "T4.CDSCode WHERE T4.FundingType = <span style=\"color: #008000; text-decoration-color: #008000\">'Locally funded'</span><span style=\"font-weight: bold\">)</span>\n",
       "    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "    Question: In the past year, which locally funded schools have an enrollment difference between K-\u001b[1;36m12\u001b[0m and ages \n",
       "\u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m that exceeds the average difference among their peers?\n",
       "\n",
       "    SQL Snippet: SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE\n",
       "T2.FundingType = \u001b[32m'Locally funded'\u001b[0m AND \u001b[1m(\u001b[0mT1.`Enrollment \u001b[1m(\u001b[0mK-\u001b[1;36m12\u001b[0m\u001b[1m)\u001b[0m` - T1.`Enrollment \u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m`\u001b[1m)\u001b[0m > \u001b[1m(\u001b[0mSELECT \n",
       "\u001b[1;35mAVG\u001b[0m\u001b[1m(\u001b[0mT3.`Enrollment \u001b[1m(\u001b[0mK-\u001b[1;36m12\u001b[0m\u001b[1m)\u001b[0m` - T3.`Enrollment \u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m`\u001b[1m)\u001b[0m FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = \n",
       "T4.CDSCode WHERE T4.FundingType = \u001b[32m'Locally funded'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "    Question: Which schools in France are locally funded and have a greater difference between their total K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> \n",
       "enrollment and enrollment for ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span> compared to the average difference for locally funded schools?\n",
       "\n",
       "    SQL Snippet: SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE\n",
       "T2.FundingType = <span style=\"color: #008000; text-decoration-color: #008000\">'Locally funded'</span> AND <span style=\"font-weight: bold\">(</span>T1.`Enrollment <span style=\"font-weight: bold\">(</span>K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">)</span>` - T1.`Enrollment <span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>`<span style=\"font-weight: bold\">)</span> &gt; <span style=\"font-weight: bold\">(</span>SELECT \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AVG</span><span style=\"font-weight: bold\">(</span>T3.`Enrollment <span style=\"font-weight: bold\">(</span>K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">)</span>` - T3.`Enrollment <span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>`<span style=\"font-weight: bold\">)</span> FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = \n",
       "T4.CDSCode WHERE T4.FundingType = <span style=\"color: #008000; text-decoration-color: #008000\">'Locally funded'</span><span style=\"font-weight: bold\">)</span>\n",
       "    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "    Question: Which schools in France are locally funded and have a greater difference between their total K-\u001b[1;36m12\u001b[0m \n",
       "enrollment and enrollment for ages \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m compared to the average difference for locally funded schools?\n",
       "\n",
       "    SQL Snippet: SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE\n",
       "T2.FundingType = \u001b[32m'Locally funded'\u001b[0m AND \u001b[1m(\u001b[0mT1.`Enrollment \u001b[1m(\u001b[0mK-\u001b[1;36m12\u001b[0m\u001b[1m)\u001b[0m` - T1.`Enrollment \u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m`\u001b[1m)\u001b[0m > \u001b[1m(\u001b[0mSELECT \n",
       "\u001b[1;35mAVG\u001b[0m\u001b[1m(\u001b[0mT3.`Enrollment \u001b[1m(\u001b[0mK-\u001b[1;36m12\u001b[0m\u001b[1m)\u001b[0m` - T3.`Enrollment \u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m`\u001b[1m)\u001b[0m FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = \n",
       "T4.CDSCode WHERE T4.FundingType = \u001b[32m'Locally funded'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    print(f\"\"\"\n",
    "    Question: {questions[i].question}\n",
    "\n",
    "    SQL Snippet: {questions[i].chunk}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at both of the generated questions, they're essentially asking about the same thing - that is the schools that are locally funded and have an enrollment difference that's above average. However, the questions are slightly different in wording and intent. The first has a time frame of 1 year while the second one is looking at schools in France specifically. This is a small change but it's enough to create diversity in our questions.\n",
    "\n",
    "We can scale this up further by adding more constraints and generating more questions. This is crucial in uncovering blindspots in our retrieval system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing with the team\n",
    "\n",
    "If you're working with a team, it's incredibly important to be able to share the results of the experiments with them. In this course, we'll be using [Braintrust](https://www.braintrust.dev/) because it provides an easy way to store private datasets that we can use for benchmarking our retrieval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DatasetSummary</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">project_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Text-2-SQL'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">dataset_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Bird-Bench-Questions'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">project_url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'https://www.braintrust.dev/app/567/p/Text-2-SQL'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">dataset_url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'https://www.braintrust.dev/app/567/p/Text-2-SQL/datasets/Bird-Bench-Questions'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">data_summary</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DataSummary</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">new_records</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>, <span style=\"color: #808000; text-decoration-color: #808000\">total_records</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDatasetSummary\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mproject_name\u001b[0m=\u001b[32m'Text-2-SQL'\u001b[0m,\n",
       "    \u001b[33mdataset_name\u001b[0m=\u001b[32m'Bird-Bench-Questions'\u001b[0m,\n",
       "    \u001b[33mproject_url\u001b[0m=\u001b[32m'https://www.braintrust.dev/app/567/p/Text-2-SQL'\u001b[0m,\n",
       "    \u001b[33mdataset_url\u001b[0m=\u001b[32m'https://www.braintrust.dev/app/567/p/Text-2-SQL/datasets/Bird-Bench-Questions'\u001b[0m,\n",
       "    \u001b[33mdata_summary\u001b[0m=\u001b[1;35mDataSummary\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnew_records\u001b[0m=\u001b[1;36m290\u001b[0m, \u001b[33mtotal_records\u001b[0m=\u001b[1;36m290\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import braintrust\n",
    "\n",
    "# Initialise Braintrust Dataset\n",
    "dataset = braintrust.init_dataset(project=\"Text-2-SQL\", name=\"Bird-Bench-Questions\")\n",
    "\n",
    "# Insert Individual Questions row by row\n",
    "for question in questions:\n",
    "    dataset.insert(\n",
    "        input=question.question,\n",
    "        expected=[question.chunk],\n",
    "        metadata={\"chunk_id\": question.chunk_id, \"chunk\": question.chunk},\n",
    "    )\n",
    "\n",
    "print(dataset.summarize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we covered key metrics like precision and recall, and demonstrated how to generate diverse synthetic questions to benchmark our retrieval system using a Text-2-SQL retrieval system as our example.\n",
    "\n",
    "In the next notebook, we'll be using these same questions to benchmark different retrieval strategies. This will be followed by Notebook 3 where we'll learn to validate our improvements using bootstrapping and confidence intervals.\n",
    "\n",
    "Looking ahead to Week 2, we'll leverage these concepts to fine-tune our retrieval models. Using both Cohere's managed re-ranker and the open-source BGE embedding model, we'll see how fine-tuning can improve our recall and MRR metrics on domain-specific queries. \n",
    "\n",
    "It's important to note that while synthetic data is valuable for development, it should eventually be augmented with real user queries in production."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
