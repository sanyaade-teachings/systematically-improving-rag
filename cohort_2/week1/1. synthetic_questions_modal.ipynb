{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Building Your RAG Evaluation Foundation\n",
    "\n",
    "Evaluating RAG systems is challenging, especially when you're just starting out. This notebook introduces a practical approach to assessment using synthetic data generation - a crucial first step before diving into more complex metrics.\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Traditional RAG evaluation focuses on generated content quality, but this approach has significant drawbacks:\n",
    "\n",
    "| Aspect | Content Generation | Retrieval Metrics |\n",
    "|--------|-------------------|-------------------|\n",
    "| Speed | 1-10s per test | 10-800ms per test |\n",
    "| Cost | $100s per run | Negligible |\n",
    "| Objectivity | Subjective | Quantitative |\n",
    "| Iteration Speed | Hours | Minutes |\n",
    "| Scale | Limited | Automated |\n",
    "\n",
    "Instead, we'll focus on retrieval metrics that are:\n",
    "- Fast to compute (milliseconds vs seconds)\n",
    "- Objective and reproducible\n",
    "- Easy to automate\n",
    "- Cost-effective at scale\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "Through this hands-on tutorial, you'll learn to build a comprehensive evaluation framework:\n",
    "\n",
    "1. **Synthetic Data Generation**\n",
    "   - Create diverse, realistic test questions\n",
    "   - Generate comprehensive datasets without real user data\n",
    "   - Learn techniques for systematic question generation\n",
    "\n",
    "2. **Maintain Query Diversity**\n",
    "   - Identify different types of questions to test\n",
    "   - Cover various query patterns and edge cases\n",
    "   - Build representative test scenarios\n",
    "\n",
    "3. **Evaluation Setup**\n",
    "   - Establish measurement foundations\n",
    "   - Set up automated testing pipelines\n",
    "   - Create reproducible evaluation workflows\n",
    "\n",
    "By the end of this notebook, you'll have a good understanding of what retrieval metrics are, how we can generate synthetic questions to benchmark our retrieval system and how we can use these questions to evaluate our retrieval system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Source Models\n",
    "\n",
    "> Make sure that you've deployed your vLLM server to Modal before continuing with this notebook. You can do so by running the following commands from the `week1` folder in your terminal\n",
    "> ```\n",
    "> modal run modal_vllm.py\n",
    "> modal deploy modal_vllm.py\n",
    "> ```\n",
    "> \n",
    "> The initial download of the model should be relatively fast. Expect startup to take ~3-5 minutes\n",
    "\n",
    "For this notebook, we'll use `Qwen-2.5-7B-Instruct`, a powerful open-source model that's cost-effective for generating synthetic evaluation data. While commercial APIs like OpenAI provide excellent quality, using open-source models offers several advantages:\n",
    "\n",
    "Cost control: Generate thousands of test questions without expensive API fees\n",
    "Data privacy: Keep all your data within your infrastructure\n",
    "Customizability: Fine-tune models for your specific domain if needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">tool_call</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"name\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"User\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"arguments\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"name\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Ivan\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"age\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"28\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">tool_call</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mtool_call\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[1;39m{\u001b[0m\u001b[32m\"name\"\u001b[0m\u001b[39m: \u001b[0m\u001b[32m\"User\"\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"arguments\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m{\u001b[0m\u001b[32m\"name\"\u001b[0m\u001b[39m: \u001b[0m\u001b[32m\"Ivan\"\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"age\"\u001b[0m\u001b[39m: \u001b[0m\u001b[32m\"28\"\u001b[0m\u001b[1;39m}\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mtool_call\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from rich import print as rprint\n",
    "\n",
    "# Modal will give you the unique URL, add a /v1 behind it so that we're using the correct endpoint on your server\n",
    "BASE_URL = \"https://ivanleomk--vllm-server-serve-dev.modal.run/v1\"\n",
    "API_KEY = \"super-secret-key\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY,\n",
    "    base_url=BASE_URL,\n",
    ")\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Ivan is 28 years old?\"}],\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"User\",\n",
    "                \"description\": \"Correctly extracted `User` with all the required parameters with correct types\",\n",
    "                \"parameters\": {\n",
    "                    \"properties\": {\n",
    "                        \"name\": {\"title\": \"Name\", \"type\": \"string\"},\n",
    "                        \"age\": {\"title\": \"Age\", \"type\": \"string\"},\n",
    "                    },\n",
    "                    \"required\": [\"age\", \"name\"],\n",
    "                    \"type\": \"object\",\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    tool_choice={\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"User\",\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "rprint(resp.choices[0].message.tool_calls[0].function.arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the vLLM server returns to us the function arguments in a `<tool_call>` XML tag. Therefore, in order to parse it into a Pydantic object, we can override the BaseModel class so that we can remove these XML tags before validating the JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">User</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Ivan'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">age</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mUser\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname\u001b[0m=\u001b[32m'Ivan'\u001b[0m, \u001b[33mage\u001b[0m=\u001b[1;36m28\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Any, Type\n",
    "import json\n",
    "\n",
    "\n",
    "class XMLBaseModel(BaseModel):\n",
    "    @classmethod\n",
    "    def model_validate_json(\n",
    "        cls: Type[BaseModel], json_data: str, **kwargs: Any\n",
    "    ) -> BaseModel:\n",
    "        # Custom logic before validation\n",
    "        json_data = json_data.replace(\"<tool_call>\\n\", \"\").replace(\"\\n</tool_call>\", \"\")\n",
    "        tool_call_payload = json.dumps(json.loads(json_data)[\"arguments\"])\n",
    "\n",
    "        # Call the original model_validate_json for standard validation\n",
    "        instance = super().model_validate_json(tool_call_payload, **kwargs)\n",
    "        return instance\n",
    "\n",
    "\n",
    "class User(XMLBaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "user = User.model_validate_json(\n",
    "    resp.choices[0].message.tool_calls[0].function.arguments\n",
    ")\n",
    "rprint(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've figured out how to parse the returned values from our vLLM server, let's see how we can use `instructor` with our new OpenAI Client in order to get a validated User object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Ivan' age=28\n"
     ]
    }
   ],
   "source": [
    "import instructor\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = instructor.from_openai(\n",
    "    AsyncOpenAI(\n",
    "        api_key=API_KEY,\n",
    "        base_url=BASE_URL\n",
    "    )\n",
    ")\n",
    "\n",
    "resp = await client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Ivan is 28 years old?\"}],\n",
    "    response_model=User,\n",
    ")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluating Retrieval\n",
    "\n",
    "Before looking at our case study, let's first understand some of the metrics that we'll be using to evaluate our retrieval system. These metrics will form the basis of our evaluation framework throughout this course.\n",
    "\n",
    "### Key Retrieval Metrics\n",
    "\n",
    "**Precision** measures how many of our retrieved items are actually relevant:\n",
    "\n",
    "$$ \\text{Precision} = \\frac{\\text{Number of Relevant Items Retrieved}}{\\text{Total Number of Retrieved Items}} $$ \n",
    "\n",
    "For example, if your system retrieves 10 documents but only 5 are relevant, that's 50% precision. Low precision indicates your system is wasting resources processing irrelevant content.\n",
    "\n",
    "**Recall** measures how many of the total relevant items we managed to find:\n",
    "\n",
    "$$ \\text{Recall} = \\frac{\\text{Number of Relevant Items Retrieved}}{\\text{Total Number of Relevant Items}} $$ \n",
    "\n",
    "If there are 20 relevant documents in your database but you only retrieve 10 of them, that's 50% recall. Low recall suggests you're missing important information.\n",
    "\n",
    "In practice, we often measure these metrics at specific cutoff points (like top-5 or top-10 results), denoted as Precision@K or Recall@K. This reflects real-world usage where users typically only look at the first few results.\n",
    "\n",
    "Consider an example where we're trying to build a Text-2-SQL application. This is an application where we take in a user query and output a SQL query which can be used to retrieve the relevant information as seen below.\n",
    "\n",
    "```\n",
    "Text : Hey could you help me find the top 5 most popular items in our store?\n",
    "Query: SELECT item_name, COUNT(*) as popularity FROM items GROUP BY item_name ORDER BY popularity DESC LIMIT 5\n",
    "```\n",
    "\n",
    "This feels like a generation task but ultimately can be greatly improved by including few-shot examples. This is very similar to how we might look for relevant text chunks in a classic Question Answer RAG application.\n",
    "\n",
    "However,by framing this as a retrieval task, we can start by looking at precision and recall of our retrieval system before we even start looking at the generated SQL queries. This has two main benefits.\n",
    "\n",
    "1. When we do evaluate the generated SQL queries, we can identify edge cases early on and add them to our list of snippets. We can then verify that these few shot examples are retrieved when we encounter these questions to help generate better SQL snippets.\n",
    "2. Different companies have unique business logic or calculation methods. Being able to retrieve the relevant snippets when these specific measurements are required is crucial. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study : Bird-Bench\n",
    "\n",
    "For this case-study, we'll be using the Bird-Bench dataset. This is a huge Text-2-SQL dataset which contains a collection of text questions to a corresponding sql query. \n",
    "\n",
    "We'll be using the dev split of this dataset for this case study that provides ~1500+ sql snippets that involves ~95 different tables that we can use. \n",
    "\n",
    "We've cleaned the dataset ahead of time and uploaded it to `567-labs/bird-rag`. Each example in our dataset contains three things\n",
    "\n",
    "- `id` : This is a unique identifier for each query\n",
    "- `query` : This is a sample SQL query \n",
    "- `difficulty` : This is a label that indicates how difficult the query is to generate. It can be either `simple`, `moderate` or `challenging`. \n",
    "\n",
    "For this case study, we'll only be using the `challenging` queries so that we can generate more difficult questions. This allows us to test our retrieval system under more demanding conditions, ensuring that it performs well even with complex queries\n",
    "\n",
    "With that in mind, let's take a look at our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanleo/Documents/coding/systematically-improving-rag/cohort_2/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'query'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"SELECT `Free Meal Count (K-12)` / `Enrollment (K-12)` FROM frpm WHERE `County Name` = 'Alameda' ORDER</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">BY (CAST(`Free Meal Count (K-12)` AS REAL) / `Enrollment (K-12)`) DESC LIMIT 1\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'difficulty'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'simple'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'id'\u001b[0m: \u001b[32m'0'\u001b[0m,\n",
       "    \u001b[32m'query'\u001b[0m: \u001b[32m\"SELECT `Free Meal Count \u001b[0m\u001b[32m(\u001b[0m\u001b[32mK-12\u001b[0m\u001b[32m)\u001b[0m\u001b[32m` / `Enrollment \u001b[0m\u001b[32m(\u001b[0m\u001b[32mK-12\u001b[0m\u001b[32m)\u001b[0m\u001b[32m` FROM frpm WHERE `County Name` = 'Alameda' ORDER\u001b[0m\n",
       "\u001b[32mBY \u001b[0m\u001b[32m(\u001b[0m\u001b[32mCAST\u001b[0m\u001b[32m(\u001b[0m\u001b[32m`Free Meal Count \u001b[0m\u001b[32m(\u001b[0m\u001b[32mK-12\u001b[0m\u001b[32m)\u001b[0m\u001b[32m` AS REAL\u001b[0m\u001b[32m)\u001b[0m\u001b[32m / `Enrollment \u001b[0m\u001b[32m(\u001b[0m\u001b[32mK-12\u001b[0m\u001b[32m)\u001b[0m\u001b[32m`\u001b[0m\u001b[32m)\u001b[0m\u001b[32m DESC LIMIT 1\"\u001b[0m,\n",
       "    \u001b[32m'difficulty'\u001b[0m: \u001b[32m'simple'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE T2.FundingType =\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'Locally funded'</span> AND <span style=\"font-weight: bold\">(</span>T1.`Enrollment <span style=\"font-weight: bold\">(</span>K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">)</span>` - T1.`Enrollment <span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>`<span style=\"font-weight: bold\">)</span> &gt; <span style=\"font-weight: bold\">(</span>SELECT <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AVG</span><span style=\"font-weight: bold\">(</span>T3.`Enrollment <span style=\"font-weight: bold\">(</span>K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">)</span>` - \n",
       "T3.`Enrollment <span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>`<span style=\"font-weight: bold\">)</span> FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = T4.CDSCode WHERE \n",
       "T4.FundingType = <span style=\"color: #008000; text-decoration-color: #008000\">'Locally funded'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE T2.FundingType =\n",
       "\u001b[32m'Locally funded'\u001b[0m AND \u001b[1m(\u001b[0mT1.`Enrollment \u001b[1m(\u001b[0mK-\u001b[1;36m12\u001b[0m\u001b[1m)\u001b[0m` - T1.`Enrollment \u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m`\u001b[1m)\u001b[0m > \u001b[1m(\u001b[0mSELECT \u001b[1;35mAVG\u001b[0m\u001b[1m(\u001b[0mT3.`Enrollment \u001b[1m(\u001b[0mK-\u001b[1;36m12\u001b[0m\u001b[1m)\u001b[0m` - \n",
       "T3.`Enrollment \u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m`\u001b[1m)\u001b[0m FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = T4.CDSCode WHERE \n",
       "T4.FundingType = \u001b[32m'Locally funded'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "from rich import print\n",
    "\n",
    "dataset = datasets.load_dataset(\"567-labs/bird-rag\")[\"train\"]\n",
    "\n",
    "print(dataset[0])\n",
    "for item in dataset:\n",
    "    if item[\"difficulty\"] == \"challenging\":\n",
    "        print(item[\"query\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze a sample SQL query to understand what kind of synthetic questions we can generate:\n",
    "\n",
    "> SELECT `Free Meal Count (K-12)` / `Enrollment (K-12)` FROM frpm WHERE `County Name` = 'Alameda' ORDER BY (CAST(`Free Meal Count (K-12)` AS REAL) / `Enrollment (K-12)`) DESC LIMIT 1\n",
    "\n",
    "This query:\n",
    "1. Calculates the percentage of students receiving free meals\n",
    "2. Filters to Alameda County schools only\n",
    "3. Returns the school with the highest percentage\n",
    "\n",
    "Some relevant natural language questions could be:\n",
    "\n",
    "- \"What school in Alameda County has the highest proportion of students on free meal programs?\"\n",
    "- \"Which school has the highest free meal participation rate in Alameda County?\"\n",
    "\n",
    "By generating a dataset of similar questions, we can evaluate how well our retrieval system matches user queries to the appropriate SQL snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Synthetic Questions\n",
    "\n",
    "Now let's start generating our synthetic questions. We're going to begin by defining some Pydantic models that represent the format of the data that we're working with.\n",
    "\n",
    "We're doing so because of the following reasons\n",
    "\n",
    "1. It helps us to be explicit about the data we're working with \n",
    "2. We can use these models with the `instructor` library to obtain structured outputs from our LLM calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This represents how we're representing our data from the dataset\n",
    "class Chunk(BaseModel):\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "\n",
    "\n",
    "# This is the synthetic question that we want our model to generate\n",
    "class Question(XMLBaseModel):\n",
    "    chain_of_thought: str\n",
    "    question: str\n",
    "\n",
    "\n",
    "# This represents a single question-chunk pair that we'll be using for our evaluation later on\n",
    "class ChunkEval(BaseModel):\n",
    "    chunk_id: str\n",
    "    question: str\n",
    "    chunk: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using `instructor` because it handles prompt templating with `jinja` for us and provides validated structured outputs. \n",
    "\n",
    "All we need to do is to define a Pydantic model that represents a desired output and the library will handle the rest. \n",
    "\n",
    "Remember that we want to generate a question that should either be answerable by the data returned by the SQL snippet directly or with some small tweaks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Which county had the highest rate of free meals per student in the second half of the year?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Which county had the highest rate of free meals per student in the second half of the year?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import openai\n",
    "import instructor\n",
    "from asyncio import Semaphore\n",
    "from tqdm.asyncio import tqdm_asyncio as asyncio\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "from rich import print\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=API_KEY,\n",
    "    base_url=BASE_URL,\n",
    ")\n",
    "client = instructor.from_openai(client)\n",
    "\n",
    "sql_snippet = \"\"\"\\\n",
    "SELECT `Free Meal Count (K-12)` / `Enrollment (K-12)` \n",
    "FROM frpm \n",
    "WHERE `County Name` = 'Alameda' \n",
    "ORDER BY (CAST(`Free Meal Count (K-12)` AS REAL) / `Enrollment (K-12)`) DESC \n",
    "LIMIT 1\"\"\"\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"\n",
    "        Generate a hypothetical question that can be answered using the following SQL snippet. \n",
    "\n",
    "        SQL Snippet:\n",
    "        {{ snippet }}\n",
    "\n",
    "        Rules\n",
    "        - If there are specific values in the snippet, do not use them directly in the question if possible. \n",
    "        - The question should be at most 2 sentences long\n",
    "        - if necessary, consider making the question more challenging using the following constraint - If there's a time period mentioned in the snippet, modify it slightly (Eg. if the snippet is looking at the entire year, change it to 6 months or 1.5 years)\n",
    "        - The question must be answerable using the SQL snippet or at most with a small tweak\n",
    "        \"\"\",\n",
    "        }\n",
    "    ],\n",
    "    response_model=Question,\n",
    "    context={\n",
    "        \"snippet\": sql_snippet\n",
    "    },  # This is the context that we're passing to the model\n",
    ")\n",
    "\n",
    "print(resp.question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `Which county had the highest rate of free meals per student in the second half of the year?`\n",
    "\n",
    "This is a question which the SQL snippet would be highly relevant for. In order to answer this query, we would just need to add a groupBy instead of using a filter on the `county` value as well as include a new time based filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Diversity Problem\n",
    "\n",
    "We cannot use the same prompt and expect a diverse set of questions. Therefore we need to introduce slight variations in the prompt to generate questions that are different in wording, intent and content. This is crucial in identifying blindspots in our retrieval system. \n",
    "\n",
    "In the example below, we're using the same prompt but introducing randomly chosen constraints at each point. This forces the model to write and generate different questions each time, allowing us to collect a more diverse set of questions. The key here is to really introduce different sources of variation when doing these generations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Up Our Questions\n",
    "\n",
    "With those points in mind, let's scale our question generation up.\n",
    "\n",
    "We'll do so by generating a question for each SQL snippet marked as challenging. Since this will be a large number of requests, we're going to be doing so asynchronously with the `asyncio` library.\n",
    "\n",
    "Additionally, to make sure we stay within our rate limits , we'll be using a semaphore to limit the number of concurrent requests.\n",
    "\n",
    "We're also making sure that we have a good diversity of questions by randomly selecting a constraint from a set of constraints to make the question more challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:11<00:00,  4.07it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import asyncio\n",
    "\n",
    "# Define Instructor Client\n",
    "client = instructor.from_openai(\n",
    "    openai.AsyncOpenAI(\n",
    "        api_key=API_KEY,\n",
    "        base_url=BASE_URL,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define some constraints to make the question more challenging\n",
    "constraints = [\n",
    "    \"If there's a time period mentioned in the snippet, modify it slightly (Eg. if the snippet is looking at the entire year, change it to 6 months or 1.5 years)\",\n",
    "    \"Add in some irrelevant context (Eg. Add information about the weather, a random event or a backstory that isn't mentioned in the snippet)\",\n",
    "    \"Changing the value of the filter (Eg. if the snippet is looking at the results in Canada, change the question to ask about another country or city instead)\",\n",
    "]\n",
    "\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(10))\n",
    "async def generate_questions(chunk: Chunk, sem: Semaphore) -> ChunkEval:\n",
    "    async with sem:\n",
    "        coro = client.chat.completions.create(\n",
    "            model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"\"\"\n",
    "                Generate a hypothetical question that can be answered using the following SQL snippet. \n",
    "\n",
    "                SQL Snippet:\n",
    "                {{ snippet }}\n",
    "\n",
    "                Rules\n",
    "                - If there are specific values in the snippet, do not use them directly in the question if possible. \n",
    "                - The question should be at most 2 sentences long\n",
    "                - if necessary, consider making the question more challenging using the following constraint of {{ constraint }}\n",
    "                - The question must be answerable using the SQL snippet or at most with a small tweak\n",
    "                \"\"\",\n",
    "                }\n",
    "            ],\n",
    "            response_model=Question,\n",
    "            context={\"snippet\": chunk.text, \"constraint\": random.choice(constraints)},\n",
    "        )\n",
    "        resp = await asyncio.wait_for(coro, timeout=30)\n",
    "\n",
    "        return ChunkEval(\n",
    "            chunk_id=chunk.chunk_id,\n",
    "            question=resp.question,\n",
    "            chunk=chunk.text,\n",
    "        )\n",
    "\n",
    "\n",
    "sem = Semaphore(10)\n",
    "dataset = [\n",
    "    item\n",
    "    for item in datasets.load_dataset(\"567-labs/bird-rag\")[\"train\"]\n",
    "    if item[\"difficulty\"] == \"challenging\"\n",
    "]\n",
    "dataset = [Chunk(chunk_id=item[\"id\"], text=item[\"query\"]) for item in dataset]\n",
    "\n",
    "coros = []\n",
    "\n",
    "num_samples = 2\n",
    "for chunk in dataset:\n",
    "    for _ in range(num_samples):\n",
    "        coros.append(generate_questions(chunk, sem))\n",
    "\n",
    "questions: list[ChunkEval] = await tqdm_asyncio.gather(*coros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've generated our questions, let's take a look at what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "    Question: In a region with diverse weather conditions, which schools, that are locally funded and have more \n",
       "students in K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> than in Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span> by a significant margin compared to the average, located in sunny areas, would \n",
       "you consider for a summer program?\n",
       "\n",
       "    SQL Snippet: SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE\n",
       "T2.FundingType = <span style=\"color: #008000; text-decoration-color: #008000\">'Locally funded'</span> AND <span style=\"font-weight: bold\">(</span>T1.`Enrollment <span style=\"font-weight: bold\">(</span>K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">)</span>` - T1.`Enrollment <span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>`<span style=\"font-weight: bold\">)</span> &gt; <span style=\"font-weight: bold\">(</span>SELECT \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AVG</span><span style=\"font-weight: bold\">(</span>T3.`Enrollment <span style=\"font-weight: bold\">(</span>K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">)</span>` - T3.`Enrollment <span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>`<span style=\"font-weight: bold\">)</span> FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = \n",
       "T4.CDSCode WHERE T4.FundingType = <span style=\"color: #008000; text-decoration-color: #008000\">'Locally funded'</span><span style=\"font-weight: bold\">)</span>\n",
       "    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "    Question: In a region with diverse weather conditions, which schools, that are locally funded and have more \n",
       "students in K-\u001b[1;36m12\u001b[0m than in Ages \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m by a significant margin compared to the average, located in sunny areas, would \n",
       "you consider for a summer program?\n",
       "\n",
       "    SQL Snippet: SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE\n",
       "T2.FundingType = \u001b[32m'Locally funded'\u001b[0m AND \u001b[1m(\u001b[0mT1.`Enrollment \u001b[1m(\u001b[0mK-\u001b[1;36m12\u001b[0m\u001b[1m)\u001b[0m` - T1.`Enrollment \u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m`\u001b[1m)\u001b[0m > \u001b[1m(\u001b[0mSELECT \n",
       "\u001b[1;35mAVG\u001b[0m\u001b[1m(\u001b[0mT3.`Enrollment \u001b[1m(\u001b[0mK-\u001b[1;36m12\u001b[0m\u001b[1m)\u001b[0m` - T3.`Enrollment \u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m`\u001b[1m)\u001b[0m FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = \n",
       "T4.CDSCode WHERE T4.FundingType = \u001b[32m'Locally funded'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "    Question: Which schools, among those locally funded, have a higher difference between their K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> enrollment and\n",
       "Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span> enrollment compared to the average difference for all locally funded schools?\n",
       "\n",
       "    SQL Snippet: SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE\n",
       "T2.FundingType = <span style=\"color: #008000; text-decoration-color: #008000\">'Locally funded'</span> AND <span style=\"font-weight: bold\">(</span>T1.`Enrollment <span style=\"font-weight: bold\">(</span>K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">)</span>` - T1.`Enrollment <span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>`<span style=\"font-weight: bold\">)</span> &gt; <span style=\"font-weight: bold\">(</span>SELECT \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AVG</span><span style=\"font-weight: bold\">(</span>T3.`Enrollment <span style=\"font-weight: bold\">(</span>K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">)</span>` - T3.`Enrollment <span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>`<span style=\"font-weight: bold\">)</span> FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = \n",
       "T4.CDSCode WHERE T4.FundingType = <span style=\"color: #008000; text-decoration-color: #008000\">'Locally funded'</span><span style=\"font-weight: bold\">)</span>\n",
       "    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "    Question: Which schools, among those locally funded, have a higher difference between their K-\u001b[1;36m12\u001b[0m enrollment and\n",
       "Ages \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m enrollment compared to the average difference for all locally funded schools?\n",
       "\n",
       "    SQL Snippet: SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE\n",
       "T2.FundingType = \u001b[32m'Locally funded'\u001b[0m AND \u001b[1m(\u001b[0mT1.`Enrollment \u001b[1m(\u001b[0mK-\u001b[1;36m12\u001b[0m\u001b[1m)\u001b[0m` - T1.`Enrollment \u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m`\u001b[1m)\u001b[0m > \u001b[1m(\u001b[0mSELECT \n",
       "\u001b[1;35mAVG\u001b[0m\u001b[1m(\u001b[0mT3.`Enrollment \u001b[1m(\u001b[0mK-\u001b[1;36m12\u001b[0m\u001b[1m)\u001b[0m` - T3.`Enrollment \u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m`\u001b[1m)\u001b[0m FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = \n",
       "T4.CDSCode WHERE T4.FundingType = \u001b[32m'Locally funded'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    print(\n",
    "        f\"\"\"\n",
    "    Question: {questions[i].question}\n",
    "\n",
    "    SQL Snippet: {questions[i].chunk}\n",
    "    \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at both questions, they target the same data - locally funded schools with above-average enrollment differences. However, the first question adds contextual elements about weather conditions and summer programs that aren't in the SQL query, while the second question directly matches the query's purpose.\n",
    "\n",
    "Creating such variations with different contexts, time frames, or locations helps test how well our retrieval system handles diverse phrasings of essentially the same core question, which is crucial for uncovering potential blindspots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving our Questions\n",
    "\n",
    "Pydantic AI makes it easy for us to load and save datasets. We'll be taking advantage of that to save our data to a `.yaml` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanleo/Documents/coding/systematically-improving-rag/cohort_2/.venv/lib/python3.9/site-packages/pydantic_evals/dataset.py:390: UserWarning: Could not determine the generic parameters for <class 'pydantic_evals.dataset.Dataset'>; using `Any` for each. You should explicitly set the generic parameters via `Dataset[MyInputs, MyOutput, MyMetadata]` when serializing or deserializing.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pydantic_evals import Case, Dataset\n",
    "\n",
    "cases = [\n",
    "    Case(\n",
    "        name=f\"question_{index}\",\n",
    "        inputs=question.question,\n",
    "        expected_output=[question.chunk_id],\n",
    "        metadata={\"chunk_id\": question.chunk},\n",
    "    )\n",
    "    for index, question in enumerate(questions)\n",
    "]\n",
    "\n",
    "dataset = Dataset(cases=cases)\n",
    "dataset.to_file(\"questions-qwen.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we covered key metrics like precision and recall, and demonstrated how to generate diverse synthetic questions to benchmark our retrieval system using a Text-2-SQL retrieval system as our example.\n",
    "\n",
    "In the next notebook, we'll be using these same questions to benchmark different retrieval strategies. This will be followed by Notebook 3 where we'll learn to validate our improvements using bootstrapping and confidence intervals.\n",
    "\n",
    "Looking ahead to Week 2, we'll leverage these concepts to fine-tune our retrieval models. Using both Cohere's managed re-ranker and the open-source BGE embedding model, we'll see how fine-tuning can improve our recall and MRR metrics on domain-specific queries. \n",
    "\n",
    "It's important to note that while synthetic data is valuable for development, it should eventually be augmented with real user queries in production."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
