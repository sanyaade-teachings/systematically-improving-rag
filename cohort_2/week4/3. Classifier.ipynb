{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Systematically Improving Your Rag Application\n",
    "\n",
    "> **Prerequisites**: Please make sure that you've completed the notebooks - [1. Generate Dataset](1.%20Generate%20Dataset.ipynb) and [2. Topic Modelling](2.%20Topic%20Modelling.ipynb) before proceeding so that you understand the dataset, topics we're working with and why we're doing this.\n",
    "\n",
    "In this notebook, we'll build a classifier that takes the topics that we've identified in the previous notebook and use it to classify new queries into the topics that we've identified. This helps us track performance over time and alert us when specific query types need attention.\n",
    "\n",
    "## Why this matters\n",
    "\n",
    "By having a system that can automatically classify new queries as they come in, we can monitor the distribution of queries over time and determine if a topic is relevant to our overall RAG application. \n",
    "\n",
    "## What you'll learn\n",
    "\n",
    "Through hands-on coding, you'll discover how to:\n",
    "\n",
    "1. Define Query Categories\n",
    "- Convert topic insights into clear categories\n",
    "- Structure classifications in YAML format\n",
    "- Make categories easy for teams to update\n",
    "\n",
    "2. Build a Classifier\n",
    "- Use LLMs for accurate query classification\n",
    "- Validate results against defined categories\n",
    "- Handle edge cases and ambiguous queries\n",
    "\n",
    "3. Monitor Performance\n",
    "- Track classification accuracy over time\n",
    "- Identify problematic query types\n",
    "- Use metrics to guide improvements\n",
    "\n",
    "By the end of this notebook, you'll have a working classifier that can automatically tag incoming queries. This builds on our topic modeling work while preparing us for more advanced query handling in Week 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Query Categories\n",
    "\n",
    "In the previous notebook, we used topic modelling to generate topics from our dataset. Through that we realised that there were a significant number of queries related to refunds that we were performing poorly on. We also learnt that there were broadly four different types of queries when it came to refunds\n",
    "\n",
    "1. Queries about the refund status of their order\n",
    "2. Queries about whether their order was eligible for a refund\n",
    "3. Queries about wanting a refund instead of store credit\n",
    "4. Queries to ask for help because the store didn't respond to their requests\n",
    "\n",
    "We want to take these insights and build out a classifier that can assign these four types of queries to new queries so that we can monitor and improve our performance on them over time. We can do so using a simple `.yaml` file that contains the categories we've identified over time.\n",
    "\n",
    "We've defined a few categories in our `categories.yml` file that we'll use to classify our queries. This makes it easy to add new categories in the future and allow other members of the team to add new categories as they see fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pre-defined categories to classify queries\n",
    "\n",
    "We'll start by loading in our categories from the `categories.yml` file. We'll use instructor and pydantic here to handle the classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Return/Refund Process',\n",
       "  'description': 'User is asking how to initiate a return or refund for their order'},\n",
       " {'title': 'Return/Refund Status',\n",
       "  'description': 'User is inquiring about the status of their return or refund'},\n",
       " {'title': 'Eligibility',\n",
       "  'description': 'User wants to know if their order is eligible for a return or refund'},\n",
       " {'title': 'Policy Details',\n",
       "  'description': \"User is asking about the company's return and refund policies\"}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "from pydantic import BaseModel, ValidationInfo, field_validator\n",
    "\n",
    "\n",
    "class QuestionType(BaseModel):\n",
    "    types: list[str]\n",
    "\n",
    "    @field_validator(\"types\")\n",
    "    def validate_categories(cls, v: list[str], info: ValidationInfo):\n",
    "        context = info.context\n",
    "        if not context:\n",
    "            raise ValueError(\"No context provided\")\n",
    "        question_types = context.get(\"question_types\")\n",
    "        if not question_types:\n",
    "            raise ValueError(\"No question types provided\")\n",
    "\n",
    "        question_type_names = [question_type[\"title\"] for question_type in question_types]\n",
    "\n",
    "        for question_type in v:\n",
    "            if question_type not in question_type_names:\n",
    "                raise ValueError(\n",
    "                    f\"Question type {question_type} not found in original list of question types\"\n",
    "                )\n",
    "        return v\n",
    "\n",
    "\n",
    "yaml_config = yaml.load(open(\"categories.yml\", \"r\"), Loader=yaml.FullLoader)\n",
    "\n",
    "question_types = []\n",
    "for question_type_category in yaml_config[\"question_type\"]:\n",
    "    question_types.extend(yaml_config[\"question_type\"][question_type_category])\n",
    "\n",
    "question_types[:4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to then load in these categories into our prompt. `instructor` allows us to use the same variable in our template formatting and our validation, making it easy to validate that our generated categories are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QuestionType</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">types</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Return/Refund Status'</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQuestionType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtypes\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'Return/Refund Status'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QuestionType</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">types</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Store Response'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Warranty Information'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Others'</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQuestionType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtypes\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'Store Response'\u001b[0m, \u001b[32m'Warranty Information'\u001b[0m, \u001b[32m'Others'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QuestionType</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">types</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Eligibility'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Policy Details'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Shipping Information'</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQuestionType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtypes\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'Eligibility'\u001b[0m, \u001b[32m'Policy Details'\u001b[0m, \u001b[32m'Shipping Information'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QuestionType</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">types</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Others'</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQuestionType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtypes\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'Others'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import openai\n",
    "import instructor\n",
    "from rich import print\n",
    "\n",
    "\n",
    "client = instructor.from_openai(openai.OpenAI())\n",
    "\n",
    "\n",
    "def classify_query(query: str) -> QuestionType:\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\n",
    "            Breakdown the folowing query into the relevant question types. Select all that apply to the query itself.\n",
    "            \n",
    "            Question Types:\n",
    "            {% for question_type in question_types %}\n",
    "            - {{ question_type.title }} : {{ question_type.description }}\n",
    "            {% endfor %}\n",
    "            \n",
    "            Make sure to only return the categories that are in the list of categories.\n",
    "            \"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        response_model=QuestionType,\n",
    "        context={\"question_types\": question_types},\n",
    "    )\n",
    "\n",
    "\n",
    "queries = [\n",
    "    \"What's the status of my refund?\",\n",
    "    \"Nike isn't responding to me at all despite the broken shoebag, does Klarna provide any warranty support?\",\n",
    "    \"Will Klarna cover the cost of returning my order? I'm out of state and I need to ship it to New York\",\n",
    "    \"How many litres of milk does a cow produce in a day?\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(classify_query(query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In Week 4, we built a complete workflow for understanding and improving how we handle user queries:\n",
    "\n",
    "1. We created diverse synthetic queries by varying personas, intents, and question types\n",
    "2. We used topic modeling to discover that refund-related queries were a major pain point\n",
    "3. We built a classifier that can automatically tag incoming queries based on these insights\n",
    "\n",
    "This systematic approach allows us to spot new query patterns as they emerge, track satisfaction scores and volume for each query type, and make sure that we're improving our system over time. This sets us up well for week 5 where we'll add structured metadata to our documents. For example, when we see lots of refund status queries, we might add specific refund processing time metadata to make these answers more accurate. Or even for Week6 for tool selection, where we might use these query categories to select a subset of relevant refund related tools to use.\n",
    "\n",
    "The key insight from Week 4 is that we can't improve what we don't measure. By building systems to track and categorize queries, we create a data-driven foundation for systematically improving our RAG application over time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
