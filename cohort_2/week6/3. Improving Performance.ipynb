{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 - Systematically Improving Your Rag Application\n",
    "\n",
    "> If you haven't already, please make sure that you've completed the previous notebooks `1. Evaluate Tools` and `2. Generate Dataset` before continuing with this notebook. We'll be using the results from the previous notebook to evaluate the effectiveness of our new techniques.\n",
    "\n",
    "In this notebook, we'll be looking at how we can improve the performance of our model by using techniques like few shot prompting and system prompts provided by users. \n",
    "\n",
    "Specifically we'll be using these techniques to compensate for some of the failure points that we covered in the previous notebook. \n",
    "\n",
    "We'll do so in 2 steps\n",
    "\n",
    "1. **System Prompts** - We'll be looking at how we can improve on the prompt that we provide to our model to help it understand how the user uses each tool. \n",
    "\n",
    "2. **Few Shot Prompting** - We'll then look at how we can use fixed few shot examples to understand how to call specific tools in combinations with one another. \n",
    "\n",
    "Similar to the second notebook, we'll use the functions we defined in our previous notebook to evaluate the performance of our model and establish our initial precision and recall baselines. \n",
    "\n",
    "We'll also use `braintrust` here to store and log the performance of our model since it makes it easy to compare the performance of our model across different runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompts\n",
    "\n",
    "By adding a system prompt for users to outline their specific workflow and tool usage, our model can handle a greater variety of users and their specific tool usage patterns.\n",
    "\n",
    "Let's see this in action below where we add the user provided system prompt to our prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from helpers import load_commands, load_queries, Command, SelectedCommands\n",
    "\n",
    "user_system_prompt = \"\"\"\n",
    "I work as a software engineer at a company. When it comes to work, we normally track all outstanding tasks in Jira and handle the code review/discussions in github itself. \n",
    "\n",
    "refer to jira to get ticket information and updates but github is where code reviews, discussions and any other specific code related updates are tracked. Use the recently updated issues command to get the latest updates over search. \n",
    "\n",
    "for todos, i use a single note in apple notes for all my todos unless i say otherwise. Obsidian is where I store diagrams, charts and notes that I've taken down for things that I'm studying up on. Our company uses confluence for documentation, wikis, release reports, meeting notes etc that need to be shared with the rest of the team. Notion I use it for financial planning, tracking expenses and planning for trips. I always use databases in notion.\n",
    "\n",
    "For messaging apps, I tend to just use discord for chatting with my friends when we game, i use microsoft teams for communicating with colleague about spcifically work related matters and iMessage for personal day to day stuff (Eg. coordinate a party, ask about general things in a personal context)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "async def generate_commands_with_system_prompt(\n",
    "    query: str,\n",
    "    client: instructor.AsyncInstructor,\n",
    "    commands: list[Command],\n",
    "    user_system_prompt: str,\n",
    "):\n",
    "    response = await client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\n",
    "                You are a helpful assistant that can execute commands in response to a user query. You have access to the following commands:\n",
    "                \n",
    "                <commands>\n",
    "                {% for command in commands %}\n",
    "                - {{ command.key }} : {{ command.command_description }}\n",
    "                {% endfor %}\n",
    "                </commands>\n",
    "\n",
    "                You must select at least one command to be called.\n",
    "\n",
    "                Here is some information about how the user uses each extension. Remember to find a chat before sending a message.\n",
    "\n",
    "                <user_behaviour>\n",
    "                {{ user_behaviour }}\n",
    "                </user_behaviour>\n",
    "                \"\"\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            },\n",
    "        ],\n",
    "        response_model=SelectedCommands,\n",
    "        context={\"commands\": commands, \"user_behaviour\": user_system_prompt},\n",
    "    )\n",
    "    return response.selected_commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run our evaluations again to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UserCommand(key='discord.searchMessages', arguments=[UserCommandArgument(title='Channel', value='#gaming')])]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import instructor\n",
    "import google.generativeai as genai\n",
    "\n",
    "commands = load_commands(\"raw_commands.json\")\n",
    "queries = load_queries(commands, \"queries.jsonl\")\n",
    "client = instructor.from_gemini(\n",
    "    genai.GenerativeModel(\"models/gemini-1.5-flash-latest\"), use_async=True\n",
    ")\n",
    "await generate_commands_with_system_prompt(\n",
    "    \"Did anyone respond to my message in #gaming about playing it takes two later?\",\n",
    "    client,\n",
    "    commands,\n",
    "    user_system_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736773841.941677  281213 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1736773841.957182  281213 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1736773841.970902  281213 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1736773841.981821  281213 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1736773841.994535  281213 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1736773842.005035  281213 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1736773842.014434  281213 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Experiment week-6-1736773843 is running at https://www.braintrust.dev/app/567/p/function-calling/experiments/week-6-1736773843\n",
      "function-calling (data): 52it [00:00, 19287.57it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5750a90b55724a6fa112568883438e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "function-calling (tasks):   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ws/q_m6c6qs3n553603dk_zvrgc0000gn/T/ipykernel_10276/4002061888.py:34: DeprecationWarning: meta() is deprecated. Use the metadata field directly instead.\n",
      "  hooks.meta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "week-6-1736773843 compared to week-6-1736768887:\n",
      "59.15% (-15.88%) 'recall'    score\t(4 improvements, 21 regressions)\n",
      "67.81% (-03.21%) 'precision' score\t(10 improvements, 11 regressions)\n",
      "\n",
      "1736773843.88s start\n",
      "1736773846.17s end\n",
      "2.28s (+55.91%) 'duration'\t(2 improvements, 50 regressions)\n",
      "\n",
      "See results for week-6-1736773843 at https://www.braintrust.dev/app/567/p/function-calling/experiments/week-6-1736773843\n"
     ]
    }
   ],
   "source": [
    "from braintrust import Score, Eval\n",
    "from helpers import calculate_precision, calculate_recall\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "def evaluate_braintrust(input, output, **kwargs):\n",
    "    return [\n",
    "        Score(\n",
    "            name=\"precision\",\n",
    "            score=calculate_precision(output, kwargs[\"expected\"]),\n",
    "        ),\n",
    "        Score(\n",
    "            name=\"recall\",\n",
    "            score=calculate_recall(output, kwargs[\"expected\"]),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "\n",
    "commands = load_commands(\"raw_commands.json\")\n",
    "queries = load_queries(commands, \"queries.jsonl\")\n",
    "\n",
    "\n",
    "client = instructor.from_gemini(\n",
    "    genai.GenerativeModel(\"models/gemini-1.5-flash-latest\"), use_async=True\n",
    ")\n",
    "commands = load_commands(\"raw_commands.json\")\n",
    "queries = load_queries(commands, \"queries.jsonl\")\n",
    "\n",
    "\n",
    "async def task(query, hooks):\n",
    "    resp = await generate_commands_with_system_prompt(\n",
    "        query, client, commands, user_system_prompt\n",
    "    )\n",
    "    hooks.meta(\n",
    "        input=query,\n",
    "        output=resp,\n",
    "    )\n",
    "    return [item.key for item in resp]\n",
    "\n",
    "\n",
    "results = await Eval(\n",
    "    \"function-calling\",\n",
    "    data=[\n",
    "        {\n",
    "            \"input\": row[\"query\"],\n",
    "            \"expected\": row[\"labels\"],\n",
    "        }\n",
    "        for row in queries\n",
    "    ],\n",
    "    task=task,\n",
    "    scores=[evaluate_braintrust],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By providing a system prompt, we saw a ~55% increase in precision from 0.44 to 0.68 whereas for recall, we saw a ~64% increase from 0.34 to 0.59. \n",
    "\n",
    "This is a significant improvement and shows that providing a system prompt can help the model understand how the user uses each tool. \n",
    "\n",
    "Better yet, using system prompts allow our model to be more flexible and handle a greater variety of users that may have different ways of interacting with the tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing System Prompts with Baseline\n",
    "\n",
    "Now that we've seen a overall improvement across the board, let's look at what specific queries our model is having issues with. Let's do so by computing the same metrics as we did in our previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expected_count</th>\n",
       "      <th>actual_count</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>github.unread-notifications</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confluence-search.go</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confluence-search.new-blog</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>github.notifications</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imessage.findChat</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft-teams.findChat</th>\n",
       "      <td>12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obsidian.searchMedia</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discord.searchMessages</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>github.workflow-runs</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jira.active-sprints</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             expected_count  actual_count  precision  recall\n",
       "github.unread-notifications               3           0.0       0.00    0.00\n",
       "confluence-search.go                      1           0.0       0.00    0.00\n",
       "confluence-search.new-blog                2           0.0       0.00    0.00\n",
       "github.notifications                      2           0.0       0.00    0.00\n",
       "imessage.findChat                         5           0.0       0.00    0.00\n",
       "microsoft-teams.findChat                 12           2.0       0.17    0.17\n",
       "obsidian.searchMedia                      3           1.0       0.33    0.33\n",
       "discord.searchMessages                    2           1.0       0.50    0.50\n",
       "github.workflow-runs                      2           1.0       0.50    0.50\n",
       "jira.active-sprints                       2           1.0       0.50    0.50"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"query\": row.input,\n",
    "            \"expected\": row.expected,\n",
    "            \"output\": row.output,\n",
    "        }\n",
    "        for row in results.results\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Get all unique tools\n",
    "all_tools = set()\n",
    "for tools in df[\"expected\"] + df[\"output\"]:\n",
    "    all_tools.update(tools)\n",
    "\n",
    "# Group by tools and get counts\n",
    "tool_df = pd.DataFrame(\n",
    "    {\n",
    "        \"expected_count\": df[\"expected\"].explode().value_counts(),\n",
    "        \"actual_count\": df[\"output\"]\n",
    "        .explode()[df[\"output\"].explode().isin(df[\"expected\"].explode().unique())]\n",
    "        .value_counts(),\n",
    "    }\n",
    ").fillna(0)\n",
    "\n",
    "# Calculate precision and recall\n",
    "tool_df[\"precision\"] = round(tool_df[\"actual_count\"] / tool_df[\"expected_count\"], 2)\n",
    "tool_df[\"recall\"] = round(tool_df[\"actual_count\"] / tool_df[\"expected_count\"], 2)\n",
    "\n",
    "# Sort by recall ascending\n",
    "tool_df = tool_df.sort_values([\"recall\"], ascending=[True])\n",
    "tool_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above, we can see a few failure points\n",
    "\n",
    "1. `github.unread-notifications` is never called. \n",
    "2. `findChat` is never called. This is experienced in `microsoft-teams`, `imessage` and `discord`\n",
    "3. Our model doesn't use the `obsidian.searchmedia` command, resulting in a low recall and precision of these commands.\n",
    "\n",
    "Let's see what queries expect to use `findChat` and what was called instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Let's create a new release post about our latest deployment, also make sure to link the specific issues that were fixed in the latest sprint and send a message the #engineering channel to let them know about it\n",
      "\n",
      "Expected: ['confluence-search.new-blog', 'jira.active-sprints', 'microsoft-teams.findChat', 'microsoft-teams.sendMessage']\n",
      "\n",
      "Output: ['confluence-search.new-page', 'confluence-search.add-text', 'confluence-search.add-text', 'microsoft-teams.sendMessage']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: Tell mum i'll be back for dinner around 7pm\n",
      "\n",
      "Expected: ['imessage.findChat', 'imessage.sendMessage']\n",
      "\n",
      "Output: ['imessage.sendMessage']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: pull up munich plans, send mike the airbnb link to the accoms on the 22nd\n",
      "\n",
      "Expected: ['notion.search-page', 'imessage.findChat', 'imessage.sendMessage']\n",
      "\n",
      "Output: ['notion.search-page', 'imessage.sendMessage']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter for rows where extension.findChat is in expected tools\n",
    "chat_df = df[df[\"expected\"].apply(lambda x: any(\"findChat\" in item for item in x))]\n",
    "\n",
    "for _, row in chat_df.head(3).iterrows():\n",
    "    print(\n",
    "        f\"Query: {row['query']}\\n\\nExpected: {row['expected']}\\n\\nOutput: {row['output']}\\n\\n{'-' * 80}\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: any security alerts raised since we upgraded our nextjs dependencies over to 14.2.0? \n",
      "\n",
      "Expected: ['github.unread-notifications']\n",
      "\n",
      "Output: ['jira.recently-updated-issues']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: check if there are any dependency vulnerabilities raised recently\n",
      "\n",
      "Expected: ['github.unread-notifications']\n",
      "\n",
      "Output: ['jira.recently-updated-issues']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: pull up those security alerts and ping the security team\n",
      "\n",
      "Expected: ['github.unread-notifications', 'microsoft-teams.findChat', 'microsoft-teams.sendMessage']\n",
      "\n",
      "Output: ['jira.search-issues', 'microsoft-teams.sendMessage']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter for rows where github.unread-notifications is in expected tools\n",
    "gh_df = df[df[\"expected\"].apply(lambda x: \"github.unread-notifications\" in x)]\n",
    "\n",
    "for _, row in gh_df.head(3).iterrows():\n",
    "    print(\n",
    "        f\"Query: {row['query']}\\n\\nExpected: {row['expected']}\\n\\nOutput: {row['output']}\\n\\n{'-' * 80}\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: can you help me find the diagrams I did to show how docker containerisation works in my personal notes?\n",
      "\n",
      "Expected: ['obsidian.searchMedia']\n",
      "\n",
      "Output: ['obsidian.searchMedia']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: can you help me find that sketch I made that explained the docker containerisation process, i need it to explain to the team\n",
      "\n",
      "Expected: ['obsidian.searchMedia']\n",
      "\n",
      "Output: ['obsidian.searchNoteCommand']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: search for my aws architecture drawings\n",
      "\n",
      "Expected: ['obsidian.searchMedia']\n",
      "\n",
      "Output: ['obsidian.searchNoteCommand']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter for rows where obsidian.searchMedia is in expected tools\n",
    "obsidian_df = df[df[\"expected\"].apply(lambda x: \"obsidian.searchMedia\" in x)]\n",
    "\n",
    "for _, row in obsidian_df.head(3).iterrows():\n",
    "    print(\n",
    "        f\"Query: {row['query']}\\n\\nExpected: {row['expected']}\\n\\nOutput: {row['output']}\\n\\n{'-' * 80}\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot Prompting\n",
    "\n",
    "Once we've identified potential problem areas - like the model failing to find findChat - few shot examples can explicitly demonstrate these commands used in context. \n",
    "\n",
    "For instance, we can show a few examples of how to use the `findChat` command with a `sendMessage` command. A natural fit here could be to grab some content from an internal documentation site like `confluence` and then sending it over to a chat.\n",
    "\n",
    "```\n",
    "<query>generate release notes for the tickets closed in our current sprint and send the link over to the #product channel ahead of time so they know what's coming</query>\n",
    "<commands>\n",
    "    confluence-search.new-blog,\n",
    "    confluence-search.add-text,\n",
    "    microsoft-teams.findChat,\n",
    "    microsoft-teams.sendMessage\n",
    "</commands>\n",
    "```\n",
    "\n",
    "We could also be inventive and use the `searchMedia` command alongside a normal `searchNoteCommand` to show the model how each command differs.\n",
    "\n",
    "```\n",
    "<query>Can you grab my notes and sketches which I put together about cross-attention?</query>\n",
    "<commands>\n",
    "    obsidian.searchMedia,\n",
    "    obsidian.searchNote\n",
    "</commands>\n",
    "```\n",
    "\n",
    "Including these concrete examples in the prompt teaches the model the correct sequence of steps and drastically reduces the chances it calls the wrong command.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_commands_with_prompt_and_examples(\n",
    "    query: str,\n",
    "    client: instructor.AsyncInstructor,\n",
    "    commands: list[Command],\n",
    "    user_behaviour: str,\n",
    "):\n",
    "    response = await client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\n",
    "You are a helpful assistant that can execute commands in response to a user query. Only choose from the commands listed below. \n",
    "\n",
    "You have access to the following commands:\n",
    "\n",
    "<commands>\n",
    "{% for command in commands %}\n",
    "<command>\n",
    "    <command key>{{ command.key }}</command key>\n",
    "    <command description>{{ command.command_description }}</command description>\n",
    "</command>\n",
    "{% endfor %}\n",
    "</commands>\n",
    "\n",
    "Select between 1-4 commands to be called in response to the user query.\n",
    "\n",
    "Here is some information about how the user uses each extension. Remember to find a chat before sending a message.\n",
    "\n",
    "<user_behaviour>\n",
    "{{ user_behaviour }}\n",
    "</user_behaviour>\n",
    "\n",
    "Here are some past examples of queries that the user has asked in the past and the keys of the commands that were expected to be called. These provide valuable context and so look at it carefully and understand why each command was called, taking into account the user query below and the user behaviour provided above.\n",
    "\n",
    "<examples>\n",
    "    <example>\n",
    "        <query>generate release notes for the tickets closed in our current sprint and send the link over to the #product channel ahead of time so they know what's coming</query>\n",
    "        <commands>\n",
    "            confluence-search.new-blog,\n",
    "            confluence-search.add-text,\n",
    "            microsoft-teams.findChat,\n",
    "            microsoft-teams.sendMessage\n",
    "        </commands>\n",
    "    </example>\n",
    "    <example>\n",
    "        <query>Tell Alaistar that I'll be late for dinner at Somma bar tonight and that I'm on my way down in a cab already</query>\n",
    "        <commands>\n",
    "            imessage.findChat,\n",
    "            imessage.sendMessage\n",
    "        </commands>\n",
    "    </example>\n",
    "    <example>\n",
    "        <query>Can you grab my notes and sketches which I put together about cross-attention?</query>\n",
    "        <commands>\n",
    "            obsidian.searchMedia,\n",
    "            obsidian.searchNote\n",
    "        </commands>\n",
    "    </example>\n",
    "    <example>\n",
    "        <query>Check if I have any new feedback on my PRs and additionally what's the status of the build on main</query>\n",
    "        <commands>\n",
    "            github.unread-notifications,\n",
    "            github.workflow-runs\n",
    "        </commands>\n",
    "    </example>\n",
    "</examples>\n",
    "                \"\"\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            },\n",
    "        ],\n",
    "        response_model=SelectedCommands,\n",
    "        context={\"commands\": commands, \"user_behaviour\": user_behaviour},\n",
    "    )\n",
    "    return response.selected_commands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736774741.726326  281213 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1736774741.741421  281213 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1736774741.756831  281213 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1736774741.768472  281213 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1736774741.783235  281213 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1736774741.793348  281213 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1736774741.803715  281213 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Experiment week-6-1736774743 is running at https://www.braintrust.dev/app/567/p/function-calling/experiments/week-6-1736774743\n",
      "function-calling (data): 52it [00:00, 22081.99it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b850038661f84959ba5cd7225971b41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "function-calling (tasks):   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ws/q_m6c6qs3n553603dk_zvrgc0000gn/T/ipykernel_10276/3175853573.py:34: DeprecationWarning: meta() is deprecated. Use the metadata field directly instead.\n",
      "  hooks.meta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "week-6-1736774743 compared to week-6-1736774328:\n",
      "74.54% (+00.31%) 'recall'    score\t(2 improvements, 3 regressions)\n",
      "71.65% (-01.12%) 'precision' score\t(4 improvements, 4 regressions)\n",
      "\n",
      "1736774743.37s start\n",
      "1736774745.96s end\n",
      "2.59s (+00.53%) 'duration'\t(28 improvements, 24 regressions)\n",
      "\n",
      "See results for week-6-1736774743 at https://www.braintrust.dev/app/567/p/function-calling/experiments/week-6-1736774743\n"
     ]
    }
   ],
   "source": [
    "from braintrust import Score, Eval\n",
    "from helpers import calculate_precision, calculate_recall\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "def evaluate_braintrust(input, output, **kwargs):\n",
    "    return [\n",
    "        Score(\n",
    "            name=\"precision\",\n",
    "            score=calculate_precision(output, kwargs[\"expected\"]),\n",
    "        ),\n",
    "        Score(\n",
    "            name=\"recall\",\n",
    "            score=calculate_recall(output, kwargs[\"expected\"]),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "\n",
    "commands = load_commands(\"raw_commands.json\")\n",
    "queries = load_queries(commands, \"queries.jsonl\")\n",
    "\n",
    "\n",
    "client = instructor.from_gemini(\n",
    "    genai.GenerativeModel(\"models/gemini-1.5-flash-latest\"), use_async=True\n",
    ")\n",
    "commands = load_commands(\"raw_commands.json\")\n",
    "queries = load_queries(commands, \"queries.jsonl\")\n",
    "\n",
    "\n",
    "async def task(query, hooks):\n",
    "    resp = await generate_commands_with_prompt_and_examples(\n",
    "        query, client, commands, user_system_prompt\n",
    "    )\n",
    "    hooks.meta(\n",
    "        input=query,\n",
    "        output=resp,\n",
    "    )\n",
    "    return [item.key for item in resp]\n",
    "\n",
    "\n",
    "results = await Eval(\n",
    "    \"function-calling\",\n",
    "    data=[\n",
    "        {\n",
    "            \"input\": row[\"query\"],\n",
    "            \"expected\": row[\"labels\"],\n",
    "        }\n",
    "        for row in queries\n",
    "    ],\n",
    "    task=task,\n",
    "    scores=[evaluate_braintrust],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method | Precision | Recall |\n",
    "|--------|-----------|--------|\n",
    "| Baseline | 0.44 | 0.36 |\n",
    "| System Prompt | 0.68 (+55%) | 0.59 (+64%) |\n",
    "| Few Shot Prompting | 0.72 (+64%) | 0.75 (+108%) |\n",
    "\n",
    "These results show that \n",
    "\n",
    "- A system prompt alone can substantially boost precision and recall by supplying the relevant context for the model to understand the specific usage patterns of the user.\n",
    "- Few shot examples can further improve precision and recall by providing explicit examples of correct command usage.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "In this notebook, we've shown how simple techniques like few shot prompting and system prompts can significantly boost model precision and recall for tool calling. We demonstrated how we can identify specific failure patterns to guide the selection of few-shot examples.\n",
    "\n",
    "This can be further extended by adding more detailed commands, collecting additional failure cases, and providing more domain-specific few-shot examples. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
