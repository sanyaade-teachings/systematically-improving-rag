{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Synthetic Question Generation: The Power of Prompts\n",
    "\n",
    "## Why This Notebook Matters\n",
    "\n",
    "When building RAG systems, one of the biggest challenges is generating good synthetic questions for evaluation and training. Most people focus on getting better models or more data, but **the most critical factor is actually your prompt design**.\n",
    "\n",
    "In this notebook, we'll compare two different approaches to synthetic question generation and show you how dramatically different results can be based on your prompt strategy.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **How prompt design shapes everything** - Small changes in prompts lead to completely different outputs\n",
    "2. **The importance of understanding user intent** - What kinds of questions will your users actually ask?\n",
    "3. **How to choose the right approach** - Different prompts for different use cases\n",
    "4. **Practical guidance** - How to design prompts for your specific domain\n",
    "\n",
    "## The Setup\n",
    "\n",
    "We'll use real conversations from the WildChat dataset and generate synthetic queries using two different approaches:\n",
    "- **V1**: Search-focused (helping users find information)\n",
    "- **V2**: Pattern-focused (finding similar conversations)\n",
    "\n",
    "Pay close attention to how different the results are - this will help you understand why prompt engineering is so critical for your RAG system's success.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Comparing Synthetic Question Generation Methods\n",
    "# This notebook loads examples from WildChat dataset and compares v1 vs v2 processors\n",
    "\n",
    "import asyncio\n",
    "import sys\n",
    "\n",
    "# Add the utils directory to path\n",
    "sys.path.append('../utils')\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import our modules\n",
    "from utils.dataloader import WildChatDataLoader\n",
    "\n",
    "# Setup instructor client\n",
    "import instructor\n",
    "\n",
    "# Initialize instructor-patched OpenAI client\n",
    "client = instructor.from_provider(\"openai/gpt-4o-mini\", async_client=True)\n",
    "\n",
    "print(\"Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class SearchQueries(BaseModel):\n",
    "    \"\"\"Generated search queries that could lead to discovering a conversation.\"\"\"\n",
    "    chain_of_thought: str = Field(\n",
    "        description=\"Chain of thought process for generating the search queries\"\n",
    "    )\n",
    "    queries: List[str] = Field(\n",
    "        description=\"4-7 diverse search queries that users might type to find this conversation\",\n",
    "        min_items=3,\n",
    "        max_items=8\n",
    "    )\n",
    "\n",
    "\n",
    "async def synthetic_question_generation_v1(\n",
    "    client,  # instructor-patched client\n",
    "    messages: List[Dict[str, Any]],\n",
    ") -> SearchQueries:\n",
    "    \"\"\"\n",
    "    Generate diverse synthetic search queries from a chat conversation.\n",
    "    \n",
    "    As a product manager analyzing ChatGPT usage patterns, this function creates\n",
    "    search queries that users might have typed to discover similar conversations.\n",
    "    The queries should be diverse and cover different aspects of the conversation.\n",
    "    \n",
    "    Args:\n",
    "        client: instructor-patched client\n",
    "        conversation: Dictionary containing conversation data with 'messages' or 'conversation' key\n",
    "        \n",
    "    Returns:\n",
    "        SearchQueries object with 4-5 diverse search queries and reasoning\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = \"\"\"\n",
    "    You are a product manager analyzing ChatGPT usage patterns. Your goal is to understand \n",
    "    how users might search to find conversations like this one.\n",
    "    \n",
    "    Given this conversation, generate 4-5 diverse search queries that different users might \n",
    "    type when looking for similar help or information. The queries should:\n",
    "    \n",
    "    1. Cover different aspects of the conversation (technical terms, problem description, solution type)\n",
    "    2. Vary in specificity (some broad, some specific)\n",
    "    3. Use different phrasings and vocabulary levels\n",
    "    4. Reflect natural user search behavior\n",
    "    5. Include both question-style and keyword-style queries\n",
    "    \n",
    "    <conversation>\n",
    "    {% for message in messages %}\n",
    "        <message role=\"{{ message.role }}\">\n",
    "            {{ message.content }}\n",
    "        </message>\n",
    "    {% endfor %}\n",
    "    </conversation>\n",
    "    \n",
    "    Generate queries that would realistically lead someone to discover this conversation.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = await client.chat.completions.create(\n",
    "        response_model=SearchQueries,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        context={\n",
    "            \"messages\": messages\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "async def synthetic_question_generation_v2(\n",
    "    client,  # instructor-patched client\n",
    "    messages: List[Dict[str, Any]],\n",
    ") -> SearchQueries:\n",
    "    \"\"\"\n",
    "    Generate search queries for finding conversations with similar patterns and characteristics.\n",
    "    \n",
    "    This version focuses on identifying conversation types, themes, and patterns that would be\n",
    "    useful for researchers, content moderators, or analysts studying human-AI interactions.\n",
    "    \n",
    "    Args:\n",
    "        client: instructor-patched client\n",
    "        messages: List of messages in the conversation\n",
    "        \n",
    "    Returns:\n",
    "        SearchQueries object with pattern-focused search queries\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = \"\"\"\n",
    "    You are a research analyst studying patterns in human-AI conversations from the WildChat dataset.\n",
    "    Your goal is to identify the key characteristics and patterns in this conversation that would help\n",
    "    researchers find similar types of conversations.\n",
    "    \n",
    "    Analyze this conversation and generate search queries that would help find conversations with:\n",
    "    - Similar content themes or domains (medical, creative, technical, etc.)\n",
    "    - Similar user intents (seeking advice, creative collaboration, testing AI limits, etc.)\n",
    "    - Similar interaction patterns (role-playing, Q&A, refusal situations, etc.)\n",
    "    - Similar AI behaviors or response types\n",
    "    \n",
    "    Focus on generating queries that capture the ESSENCE and PATTERNS rather than specific details.\n",
    "    \n",
    "    Examples of good pattern queries:\n",
    "    - \"conversations where users ask about medical diagnoses\"\n",
    "    - \"role-playing scenarios with fictional characters\"\n",
    "    - \"conversations where AI refuses medical advice\"\n",
    "    - \"creative writing collaborations\"\n",
    "    - \"technical troubleshooting discussions\"\n",
    "    - \"conversations testing AI content policies\"\n",
    "    - \"users seeking relationship advice\"\n",
    "    - \"educational Q&A about scientific concepts\"\n",
    "    \n",
    "    <conversation>\n",
    "    {% for message in messages %}\n",
    "        <message role=\"{{ message.role }}\">\n",
    "            {{ message.content }}\n",
    "        </message>\n",
    "    {% endfor %}\n",
    "    </conversation>\n",
    "    \n",
    "    Generate 5-7 search queries that focus on conversation patterns, themes, and characteristics\n",
    "    rather than specific content details. Think about what makes this conversation type distinct\n",
    "    and how researchers would categorize it.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = await client.chat.completions.create(\n",
    "        response_model=SearchQueries,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert conversation analyst specializing in categorizing and understanding patterns in human-AI interactions. Focus on identifying conversation types, themes, and structural patterns rather than specific content details.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        context={\n",
    "            \"messages\": messages\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections \"HTTP/1.1 200 OK\"\n",
      "INFO:utils.dataloader:Loading WildChat dataset: train[:10000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing streaming write with 50 English conversations...\n",
      "Starting streaming write to collection: wildchat_10k\n",
      "Filters: language=English, min_length=30, limit=10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.dataloader:Loaded 10000 conversations\n",
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 50 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 100 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 150 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 200 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 250 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 300 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 350 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 400 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 450 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 500 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 550 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 600 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 650 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 700 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 750 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 800 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 850 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 900 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 950 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 1000 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 1050 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 1100 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 1150 documents so far...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.trychroma.com:8000/api/v2/tenants/13e7be1a-7c4c-4526-a2af-ca891a0031e0/databases/wild-chat-1m/collections/905f3eaf-2220-41ab-95d7-d0709d6e4fe8/add \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote batch: 1200 documents so far...\n",
      "‚ùå Error in streaming write: Expected IDs to be unique, found duplicates of: f5f01c95c79d12c5c72f87f63577b6dc in add.\n"
     ]
    }
   ],
   "source": [
    "def write_streaming_to_chroma(\n",
    "    collection_name=\"wildchat_streaming\", \n",
    "    limit=100, \n",
    "    batch_size=50,\n",
    "    filter_language=\"English\",\n",
    "    min_message_length=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Write conversations to ChromaDB using streaming data loader\n",
    "    \n",
    "    This approach can handle the full 1M dataset efficiently by:\n",
    "    1. Loading data in streaming fashion (no memory issues)\n",
    "    2. Processing in batches\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting streaming write to collection: {collection_name}\")\n",
    "    print(f\"Filters: language={filter_language}, min_length={min_message_length}, limit={limit}\")\n",
    "    \n",
    "    try:\n",
    "        # Create/get collection\n",
    "        collection = client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"description\": \"Streaming WildChat data\"}\n",
    "        )\n",
    "        \n",
    "        # Batch processing\n",
    "        documents = []\n",
    "        metadatas = []\n",
    "        ids = []\n",
    "        total_processed = 0\n",
    "        \n",
    "        for conversation in load_data(limit=limit):\n",
    "            # Prepare document\n",
    "            doc_id = f\"{conversation['conversation_hash']}\"\n",
    "            \n",
    "            metadata = {\n",
    "                \"hash\": conversation['conversation_hash'],\n",
    "                \"timestamp\": str(conversation['timestamp']),\n",
    "                \"lang\": conversation['language'],\n",
    "                \"model\": conversation['model'],\n",
    "                \"length\": conversation['conversation_length'],\n",
    "                \"type\": \"user_query\"\n",
    "            }\n",
    "            \n",
    "            documents.append(conversation['first_message'][:1000])\n",
    "            metadatas.append(metadata)\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Write batch when full\n",
    "            if len(documents) >= batch_size:\n",
    "                collection.add(\n",
    "                    documents=documents,\n",
    "                    metadatas=metadatas,\n",
    "                    ids=ids\n",
    "                )\n",
    "                total_processed += len(documents)\n",
    "                print(f\"  Wrote batch: {total_processed} documents so far...\")\n",
    "                \n",
    "                # Reset batch\n",
    "                documents = []\n",
    "                metadatas = []\n",
    "                ids = []\n",
    "        \n",
    "        # Write final batch\n",
    "        if documents:\n",
    "            collection.add(\n",
    "                documents=documents,\n",
    "                metadatas=metadatas,\n",
    "                ids=ids\n",
    "            )\n",
    "            total_processed += len(documents)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully wrote {total_processed} conversations to {collection_name}\")\n",
    "        print(f\"Collection now contains {collection.count()} total documents\")\n",
    "        \n",
    "        return collection\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in streaming write: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with a moderate dataset\n",
    "print(\"Testing streaming write with 50 English conversations...\")\n",
    "streaming_collection = write_streaming_to_chroma(\n",
    "    collection_name=\"wildchat_10k\",\n",
    "    limit=10000,  # Will get first 50 English conversations from first 100\n",
    "    filter_language=\"English\",\n",
    "    min_message_length=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WildChat examples...\n",
      "Loaded example 1: c9ec5b44...\n",
      "Loaded example 2: cf1267ca...\n",
      "Loaded example 3: e98d3e74...\n",
      "Loaded example 4: 2e8fd255...\n",
      "Loaded example 5: 59c72510...\n"
     ]
    }
   ],
   "source": [
    "# Load examples from the WildChat dataset\n",
    "print(\"Loading WildChat examples...\")\n",
    "\n",
    "# Initialize the dataloader with a reasonable limit for initial loading\n",
    "loader = WildChatDataLoader(limit=5000)  # Load first 5K to get good examples\n",
    "\n",
    "# Stream conversations and collect examples\n",
    "examples = []\n",
    "target_count = 5  # Aim for 5 good examples\n",
    "\n",
    "for conversation in loader.stream_conversations(\n",
    "    limit=target_count,\n",
    "    min_message_length=50,\n",
    "    filter_language='English',\n",
    "    filter_toxic=True\n",
    "):\n",
    "    examples.append(conversation)\n",
    "    print(f\"Loaded example {len(examples)}: {conversation['conversation_hash'][:8]}...\")\n",
    "    \n",
    "    if len(examples) >= target_count:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting async processing...\n",
      "Processing 5 examples with both processors...\n",
      "\n",
      "Preparing example 1/5: c9ec5b44\n",
      "Preparing example 2/5: cf1267ca\n",
      "Preparing example 3/5: e98d3e74\n",
      "Preparing example 4/5: 2e8fd255\n",
      "Preparing example 5/5: 59c72510\n",
      "Executing all processing tasks in parallel...\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "async def process_examples_async(examples_to_process: List[Dict], num_examples: int = 5):\n",
    "    \"\"\"Process examples using both v1 and v2 processors and return results for comparison\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        'v1_results': [],\n",
    "        'v2_results': [],\n",
    "        'conversations': [],\n",
    "        'processing_times': {'v1': [], 'v2': []},\n",
    "        'errors': {'v1': [], 'v2': []}\n",
    "    }\n",
    "    \n",
    "    print(f\"Processing {min(num_examples, len(examples_to_process))} examples with both processors...\\n\")\n",
    "    \n",
    "    # Prepare all tasks for parallel execution\n",
    "    tasks = []\n",
    "    for i, example in enumerate(examples_to_process[:num_examples]):\n",
    "        print(f\"Preparing example {i+1}/{min(num_examples, len(examples_to_process))}: {example['conversation_hash'][:8]}\")\n",
    "        \n",
    "        results['conversations'].append({\n",
    "            'hash': example['conversation_hash'],\n",
    "            'first_message': example['first_message'][:200],\n",
    "            'length': example['conversation_length']\n",
    "        })\n",
    "        \n",
    "        # Create tasks for both v1 and v2 processing\n",
    "        v1_task = synthetic_question_generation_v1(client, example['conversation'])\n",
    "        v2_task = synthetic_question_generation_v2(client, example['conversation'])\n",
    "        tasks.extend([v1_task, v2_task])\n",
    "    \n",
    "    # Execute all tasks in parallel\n",
    "    print(\"Executing all processing tasks in parallel...\")\n",
    "    all_results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Separate v1 and v2 results\n",
    "    for i in range(0, len(all_results), 2):\n",
    "        results['v1_results'].append(all_results[i])\n",
    "        results['v2_results'].append(all_results[i + 1])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the async processing\n",
    "if examples:\n",
    "    print(\"Starting async processing...\")\n",
    "    results = await process_examples_async(examples, num_examples=5)\n",
    "    print(\"Processing complete!\")\n",
    "else:\n",
    "    print(\"No examples to process!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# ‚ö†Ô∏è Before We Generate: Set Your Expectations\n",
    "\n",
    "We're about to process the same 5 conversations with two different prompts. \n",
    "\n",
    "**What you should watch for:**\n",
    "\n",
    "1. **Completely different query types** - V1 and V2 will produce queries that serve entirely different purposes\n",
    "2. **Different vocabulary and phrasing** - Same content, totally different language\n",
    "3. **Different levels of specificity** - Some approaches will be more general, others more specific\n",
    "4. **Different user assumptions** - Each approach assumes different user goals\n",
    "\n",
    "**The key insight**: Neither approach is \"better\" - they're solving different problems. The \"right\" approach depends on understanding what your users are actually trying to accomplish when they search your system.\n",
    "\n",
    "Let's see this in action...\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üéØ Key Learning: Prompts Shape Everything\n",
    "\n",
    "Before we dive into the results, let's set expectations about what we're about to see.\n",
    "\n",
    "## The Critical Decision: What Questions Are You Trying to Answer?\n",
    "\n",
    "The most important factor in synthetic question generation isn't the model you use or the data you have - **it's understanding what kinds of questions your users will actually ask**.\n",
    "\n",
    "### Two Fundamentally Different Approaches\n",
    "\n",
    "We're comparing two completely different philosophies:\n",
    "\n",
    "**V1 (Search-Focused)**: \"What would someone search for to find this content?\"\n",
    "- Generates queries that would lead users TO this conversation\n",
    "- Focuses on information retrieval and search intent\n",
    "- Good for: Building search systems, content discovery, Q&A databases\n",
    "\n",
    "**V2 (Pattern-Focused)**: \"What kind of conversation pattern is this?\"\n",
    "- Generates queries to find SIMILAR conversations \n",
    "- Focuses on interaction patterns and conversation types\n",
    "- Good for: Conversation analysis, chatbot training, user behavior studies\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "As you'll see in the results below, these approaches produce **dramatically different outputs** from the same input data. Neither is \"wrong\" - they're solving different problems.\n",
    "\n",
    "**The key question you need to answer first**: What will people be searching for in your system?\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Synthetic Question Generation Comparison: V1 vs V2\n",
       "\n",
       "## Overview\n",
       "Comparing two different approaches to generating search queries from conversation data:\n",
       "- **V1**: Direct query generation focused on search intent\n",
       "- **V2**: Query generation focused on conversation patterns and user intents\n",
       "\n",
       "---\n",
       "\n",
       "\n",
       "## Example 1: c9ec5b44\n",
       "\n",
       "**Conversation Preview:** Hey there! Are you familiar with reality shifting? So, I‚Äôm refining a foolproof method for reality shifting and want to pick a destination. Want to he...\n",
       "\n",
       "**Length:** 2 messages\n",
       "\n",
       "### V1 Results (Search-Focused)\n",
       "**Chain of Thought:** The conversation discusses reality shifting and personalizes a fictional world based on specific requirements. Users might search for information on reality shifting, personalized destinations, or creative storytelling. Some queries will focus on the practical aspects of reality shifting, while others will emphasize the imaginative details of the created world. Additionally, the ending and specific elements such as quests, adventure, and character types will also inform the search terms. Keywords like 'reality shifting ideas,' 'creating fictional worlds,' and 'adventure storytelling' will be included to cover both broad and specific interests of users.\n",
       "\n",
       "**Generated Queries:**\n",
       "\n",
       "1. What are some ideas for reality shifting destinations?\n",
       "\n",
       "2. How to create a personalized world for reality shifting?\n",
       "\n",
       "3. Unique quest ideas for reality shifting adventures\n",
       "\n",
       "4. What is reality shifting and how can I do it?\n",
       "\n",
       "5. Fictional world building for immersive experiences\n",
       "\n",
       "\n",
       "### V2 Results (Conversation Pattern-Focused)\n",
       "**Chain of Thought:** This conversation features thematic elements of fantasy and adventure, with a focus on personal experiences and creative storytelling. The user expresses a desire for a detailed fictional scenario related to reality shifting, and the AI responds in a highly imaginative and narrative-driven manner. This indicates a collaboration in world-building and creative exploration. The nature of the interaction suggests that users are often looking for engaging, personalized narratives and that AI facilitates these immersive experiences through elaborate descriptions and structured storytelling. Therefore, the conversation can be characterized by themes of role-playing, creative collaboration, and the exploration of fictional realities.\n",
       "\n",
       "**Generated Queries:**\n",
       "\n",
       "1. creative storytelling about fictional adventures\n",
       "\n",
       "2. users seeking personalized fantasy scenarios\n",
       "\n",
       "3. conversations focused on role-playing journey planning\n",
       "\n",
       "4. collaborative world-building in fantasy contexts\n",
       "\n",
       "5. immersive narrative creation with AI\n",
       "\n",
       "6. discussions around reality shifting experiences\n",
       "\n",
       "7. user-driven quest scenarios in imaginative settings\n",
       "\n",
       "\n",
       "---\n",
       "\n",
       "\n",
       "## Example 2: cf1267ca\n",
       "\n",
       "**Conversation Preview:** Old age PT hx of DM, HTN, dyslipidemia His ECG I.II, aVF (MI) what is the highest risk \n",
       "\n",
       "factor for this condition?...\n",
       "\n",
       "**Length:** 2 messages\n",
       "\n",
       "### V1 Results (Search-Focused)\n",
       "**Chain of Thought:** The conversation revolves around a medical scenario focusing on a patient at risk for myocardial infarction due to age and multiple health conditions. Users searching for similar information might be looking for risk factors related to heart attacks, particularly in older adults with specific health issues. Queries can vary by covering technical terms (e.g., myocardial infarction, ECG), the general problem (e.g., heart attack risk factors), and the outcomes (e.g., what increases heart attack risk). Both broad and specific search queries can help capture different users' needs. Additionally, incorporating question-style and keyword-style queries can reflect natural search behavior.\n",
       "\n",
       "**Generated Queries:**\n",
       "\n",
       "1. What are the risk factors for myocardial infarction in elderly patients?\n",
       "\n",
       "2. High risk factors for heart attacks older adults with diabetes and hypertension\n",
       "\n",
       "3. ECG interpretation in elderly patients: what to look for\n",
       "\n",
       "4. How does age affect heart attack risk with diabetes and hypertension?\n",
       "\n",
       "5. Understanding myocardial infarction risk in older patients with chronic conditions\n",
       "\n",
       "\n",
       "### V2 Results (Conversation Pattern-Focused)\n",
       "**Chain of Thought:** This conversation revolves around a medical query, specifically regarding risk factors for myocardial infarction in a patient with multiple health issues. The user is looking for expert advice on a healthcare topic, showcasing a technical exchange. The assistant responds with a targeted, medically-oriented answer. This prompts me to consider searches that focus on medical advice, patient diagnosis, risk assessment, and user interactions that involve health-related inquiries.\n",
       "\n",
       "**Generated Queries:**\n",
       "\n",
       "1. conversations where users seek medical risk assessments\n",
       "\n",
       "2. patient diagnosis discussions in aging populations\n",
       "\n",
       "3. Q&A about cardiovascular health and risk factors\n",
       "\n",
       "4. conversations involving diabetes and heart disease advice\n",
       "\n",
       "5. medical inquiries about hypertension and lifestyle influences\n",
       "\n",
       "6. discussions on elderly patients with multiple health conditions\n",
       "\n",
       "7. educational conversations about the impact of dyslipidemia on heart health\n",
       "\n",
       "\n",
       "---\n",
       "\n",
       "\n",
       "## Example 3: e98d3e74\n",
       "\n",
       "**Conversation Preview:** Hey there! Are you familiar with reality shifting? So, I‚Äôm refining a foolproof method for reality shifting and want to pick a destination. Want to he...\n",
       "\n",
       "**Length:** 2 messages\n",
       "\n",
       "### V1 Results (Search-Focused)\n",
       "**Chain of Thought:** Given the conversation about reality shifting and the detailed scenario created by the assistant, users might look for specific terms related to reality shifting, creative stories, or character and adventure suggestions. The conversation involves aspects like personalized journeys, fantasy elements, enchantresses, and quests, which are all themes that potential users may be interested in. Thus, I‚Äôll create queries that reflect these elements, using both technical terms and casual, question-style phrasing to capture diverse search behaviors.\n",
       "\n",
       "**Generated Queries:**\n",
       "\n",
       "1. what is reality shifting and how to start\n",
       "\n",
       "2. creative fantasy stories for reality shifting\n",
       "\n",
       "3. specific reality shifting scenarios with quests\n",
       "\n",
       "4. adventure ideas for shifting into a fantasy world\n",
       "\n",
       "5. how to create a personalized reality shift destination\n",
       "\n",
       "\n",
       "### V2 Results (Conversation Pattern-Focused)\n",
       "**Chain of Thought:** The conversation involves a user engaging in a creative collaboration with the AI, focused on generating ideas for a fictional reality-shifting scenario. The themes present include fantasy world-building, user-driven narrative creation, and a focus on specific quest details and character interactions. The user expresses clear, detailed preferences and seeks an imaginative response from the AI. The structure of the conversation is interactive and exploratory, with a blend of guiding questions and elaborate responses. Key characteristics include role-playing elements, creative storytelling, and a whimsical adventure narrative. The conversation patterns and themes necessitate search queries that emphasize these elements without diving into the specific content of the dialogue.\n",
       "\n",
       "**Generated Queries:**\n",
       "\n",
       "1. creative writing prompts for fantasy worlds\n",
       "\n",
       "2. user-driven narrative development in AI conversations\n",
       "\n",
       "3. collaborative storytelling scenarios with AI\n",
       "\n",
       "4. role-playing adventures involving quests and characters\n",
       "\n",
       "5. fantasy reality creation discussions with AI\n",
       "\n",
       "6. conversations focused on unique fictional settings\n",
       "\n",
       "7. interactive storytelling where users dictate plot elements\n",
       "\n",
       "\n",
       "---\n",
       "\n",
       "\n",
       "## Example 4: 2e8fd255\n",
       "\n",
       "**Conversation Preview:** Hey there! Are you familiar with reality shifting? So, I‚Äôm refining a foolproof method for reality shifting and want to pick a destination. Want to he...\n",
       "\n",
       "**Length:** 2 messages\n",
       "\n",
       "### V1 Results (Search-Focused)\n",
       "**Chain of Thought:** The conversation covers the niche topics of reality shifting, personalized destination creation, and specific quest ideas. Users may search for advice on reality shifting techniques, ideas for fantasy worlds, or how to create engaging narratives for their journeys. Some users may have specific attributes they‚Äôre curious about, such as 'female characters in reality shifting' or 'unconscious themes in fantasy realms'. So, the queries should reflect these varied interests from broad to specific.\n",
       "\n",
       "**Generated Queries:**\n",
       "\n",
       "1. How to create a personalized reality shifting destination?\n",
       "\n",
       "2. Reality shifting ideas for fantasy worlds\n",
       "\n",
       "3. What are some fun quests for reality shifting?\n",
       "\n",
       "4. Unique female characters in reality shifting narratives\n",
       "\n",
       "5. Methods to enter a different reality and unconsciousness themes\n",
       "\n",
       "\n",
       "### V2 Results (Conversation Pattern-Focused)\n",
       "**Chain of Thought:** This conversation revolves around a creative and imaginative theme where the user is developing a narrative for reality shifting with specific elements they want included. The user seeks detailed and personalized world-building from the AI, indicating an interest in creative collaboration. The conversation features a role-playing scenario where the AI takes on a guiding role in constructing a fictional reality, emphasizing elements of adventure and temptation. The interaction pattern includes the user providing clear requirements and the AI responding in a narrative format, showcasing an engaging and conversational style. Researchers might search for conversations that encompass similar creative storytelling interactions, user intents focused on imagination and personalization, and frameworks where the AI builds upon user-defined concepts.\n",
       "\n",
       "**Generated Queries:**\n",
       "\n",
       "1. creative collaboration in fictional world-building\n",
       "\n",
       "2. users exploring fantasy role-playing scenarios\n",
       "\n",
       "3. conversations about personalized adventure narratives\n",
       "\n",
       "4. interactive storytelling with AI\n",
       "\n",
       "5. users seeking detailed world descriptions for role-playing\n",
       "\n",
       "6. engaging AI responses in creative scenarios\n",
       "\n",
       "7. conversations about reality shifting techniques\n",
       "\n",
       "\n",
       "---\n",
       "\n",
       "\n",
       "## Example 5: 59c72510\n",
       "\n",
       "**Conversation Preview:** i wanna you to write me terms & conditions and policies for my  website...\n",
       "\n",
       "**Length:** 2 messages\n",
       "\n",
       "### V1 Results (Search-Focused)\n",
       "**Chain of Thought:** In this conversation, the user is requesting help with creating terms and conditions for a website, and the assistant advises against it due to legal implications. Thus, the search queries should reflect this need for legal documents, the advice against using AI for such tasks, and possible alternatives for obtaining legal advice or templates. Variations in phrasing, specificity, and user intent should be represented in the queries.\n",
       "\n",
       "**Generated Queries:**\n",
       "\n",
       "1. how to create terms and conditions for a website\n",
       "\n",
       "2. can AI write legal documents like terms and conditions?\n",
       "\n",
       "3. where to find customizable legal document templates\n",
       "\n",
       "4. professional legal advice for website policies needed\n",
       "\n",
       "5. importance of hiring a lawyer for website terms and conditions\n",
       "\n",
       "\n",
       "### V2 Results (Conversation Pattern-Focused)\n",
       "**Chain of Thought:** This conversation illustrates a refusal situation where the AI declines a user's request for generating legal documents. The essence focuses on conversations involving user requests for professional or specialized outputs that require expertise beyond the AI's capabilities. Therefore, search queries should reflect this refusal scenario, the themes of legal advice and document creation, and the interaction pattern of seeking assistance yet receiving a limitation from the AI.\n",
       "\n",
       "**Generated Queries:**\n",
       "\n",
       "1. conversations where AI refuses to generate legal documents\n",
       "\n",
       "2. users asking for professional advice on legal matters\n",
       "\n",
       "3. interactions where users request expert content generation\n",
       "\n",
       "4. conversations about AI limitations in offering legal advice\n",
       "\n",
       "5. users seeking document templates or legal guidance\n",
       "\n",
       "6. role-playing scenarios involving legal document creation\n",
       "\n",
       "7. AI responses focused on advising users to consult professionals\n",
       "\n",
       "\n",
       "---\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from jinja2 import Template\n",
    "\n",
    "# Define the Jinja template\n",
    "template_str = \"\"\"# Synthetic Question Generation Comparison: V1 vs V2\n",
    "\n",
    "## Overview\n",
    "Comparing two different approaches to generating search queries from conversation data:\n",
    "- **V1**: Direct query generation focused on search intent\n",
    "- **V2**: Query generation focused on conversation patterns and user intents\n",
    "\n",
    "---\n",
    "\n",
    "{% for i in range(conversations|length) %}\n",
    "## Example {{ i + 1 }}: {{ conversations[i].hash[:8] }}\n",
    "\n",
    "**Conversation Preview:** {{ conversations[i].first_message[:150] }}...\n",
    "\n",
    "**Length:** {{ conversations[i].length }} messages\n",
    "\n",
    "### V1 Results (Search-Focused)\n",
    "**Chain of Thought:** {{ v1_results[i].chain_of_thought }}\n",
    "\n",
    "**Generated Queries:**\n",
    "{% for query in v1_results[i].queries %}\n",
    "{{ loop.index }}. {{ query }}\n",
    "{% endfor %}\n",
    "\n",
    "### V2 Results (Conversation Pattern-Focused)\n",
    "**Chain of Thought:** {{ v2_results[i].chain_of_thought }}\n",
    "\n",
    "**Generated Queries:**\n",
    "{% for query in v2_results[i].queries %}\n",
    "{{ loop.index }}. {{ query }}\n",
    "{% endfor %}\n",
    "\n",
    "---\n",
    "\n",
    "{% endfor %}\"\"\"\n",
    "\n",
    "# Create and render the template\n",
    "template = Template(template_str)\n",
    "formatted_results = template.render(\n",
    "    conversations=results['conversations'],\n",
    "    v1_results=results['v1_results'],\n",
    "    v2_results=results['v2_results']\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "display(Markdown(formatted_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing streaming write with 50 English conversations...\n",
      "Starting streaming write to collection: wildchat_10k\n",
      "Filters: language=English, min_length=30, limit=10000\n",
      "‚ùå Error in streaming write: 'AsyncOpenAI' object has no attribute 'get_or_create_collection'\n"
     ]
    }
   ],
   "source": [
    "def write_streaming_to_chroma(\n",
    "    collection_name=\"wildchat_streaming\", \n",
    "    limit=100, \n",
    "    batch_size=50,\n",
    "    filter_language=\"English\",\n",
    "    min_message_length=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Write conversations to ChromaDB using streaming data loader\n",
    "    \n",
    "    This approach can handle the full 1M dataset efficiently by:\n",
    "    1. Loading data in streaming fashion (no memory issues)\n",
    "    2. Processing in batches\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting streaming write to collection: {collection_name}\")\n",
    "    print(f\"Filters: language={filter_language}, min_length={min_message_length}, limit={limit}\")\n",
    "    \n",
    "    try:\n",
    "        # Create/get collection\n",
    "        collection = client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"description\": \"Streaming WildChat data\"}\n",
    "        )\n",
    "        \n",
    "        # Batch processing\n",
    "        documents = []\n",
    "        metadatas = []\n",
    "        ids = []\n",
    "        total_processed = 0\n",
    "        \n",
    "        for conversation in load_data(limit=limit):\n",
    "            # Prepare document\n",
    "            doc_id = f\"{conversation['conversation_hash']}\"\n",
    "            \n",
    "            metadata = {\n",
    "                \"hash\": conversation['conversation_hash'],\n",
    "                \"timestamp\": str(conversation['timestamp']),\n",
    "                \"lang\": conversation['language'],\n",
    "                \"model\": conversation['model'],\n",
    "                \"length\": conversation['conversation_length'],\n",
    "                \"type\": \"user_query\"\n",
    "            }\n",
    "            \n",
    "            documents.append(conversation['first_message'][:1000])\n",
    "            metadatas.append(metadata)\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Write batch when full\n",
    "            if len(documents) >= batch_size:\n",
    "                collection.add(\n",
    "                    documents=documents,\n",
    "                    metadatas=metadatas,\n",
    "                    ids=ids\n",
    "                )\n",
    "                total_processed += len(documents)\n",
    "                print(f\"  Wrote batch: {total_processed} documents so far...\")\n",
    "                \n",
    "                # Reset batch\n",
    "                documents = []\n",
    "                metadatas = []\n",
    "                ids = []\n",
    "        \n",
    "        # Write final batch\n",
    "        if documents:\n",
    "            collection.add(\n",
    "                documents=documents,\n",
    "                metadatas=metadatas,\n",
    "                ids=ids\n",
    "            )\n",
    "            total_processed += len(documents)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully wrote {total_processed} conversations to {collection_name}\")\n",
    "        print(f\"Collection now contains {collection.count()} total documents\")\n",
    "        \n",
    "        return collection\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in streaming write: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with a moderate dataset\n",
    "print(\"Testing streaming write with 50 English conversations...\")\n",
    "streaming_collection = write_streaming_to_chroma(\n",
    "    collection_name=\"wildchat_10k\",\n",
    "    limit=10000,  # Will get first 50 English conversations from first 100\n",
    "    filter_language=\"English\",\n",
    "    min_message_length=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing streaming write with 50 English conversations...\n",
      "Starting streaming write to collection: wildchat_10k\n",
      "Filters: language=English, min_length=30, limit=10000\n",
      "‚ùå Error in streaming write: 'AsyncOpenAI' object has no attribute 'get_or_create_collection'\n"
     ]
    }
   ],
   "source": [
    "def write_streaming_to_chroma(\n",
    "    collection_name=\"wildchat_streaming\", \n",
    "    limit=100, \n",
    "    batch_size=50,\n",
    "    filter_language=\"English\",\n",
    "    min_message_length=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Write conversations to ChromaDB using streaming data loader\n",
    "    \n",
    "    This approach can handle the full 1M dataset efficiently by:\n",
    "    1. Loading data in streaming fashion (no memory issues)\n",
    "    2. Processing in batches\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting streaming write to collection: {collection_name}\")\n",
    "    print(f\"Filters: language={filter_language}, min_length={min_message_length}, limit={limit}\")\n",
    "    \n",
    "    try:\n",
    "        # Create/get collection\n",
    "        collection = client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"description\": \"Streaming WildChat data\"}\n",
    "        )\n",
    "        \n",
    "        # Batch processing\n",
    "        documents = []\n",
    "        metadatas = []\n",
    "        ids = []\n",
    "        total_processed = 0\n",
    "        \n",
    "        for conversation in load_data(limit=limit):\n",
    "            # Prepare document\n",
    "            doc_id = f\"{conversation['conversation_hash']}\"\n",
    "            \n",
    "            metadata = {\n",
    "                \"hash\": conversation['conversation_hash'],\n",
    "                \"timestamp\": str(conversation['timestamp']),\n",
    "                \"lang\": conversation['language'],\n",
    "                \"model\": conversation['model'],\n",
    "                \"length\": conversation['conversation_length'],\n",
    "                \"type\": \"user_query\"\n",
    "            }\n",
    "            \n",
    "            documents.append(conversation['first_message'][:1000])\n",
    "            metadatas.append(metadata)\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Write batch when full\n",
    "            if len(documents) >= batch_size:\n",
    "                collection.add(\n",
    "                    documents=documents,\n",
    "                    metadatas=metadatas,\n",
    "                    ids=ids\n",
    "                )\n",
    "                total_processed += len(documents)\n",
    "                print(f\"  Wrote batch: {total_processed} documents so far...\")\n",
    "                \n",
    "                # Reset batch\n",
    "                documents = []\n",
    "                metadatas = []\n",
    "                ids = []\n",
    "        \n",
    "        # Write final batch\n",
    "        if documents:\n",
    "            collection.add(\n",
    "                documents=documents,\n",
    "                metadatas=metadatas,\n",
    "                ids=ids\n",
    "            )\n",
    "            total_processed += len(documents)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully wrote {total_processed} conversations to {collection_name}\")\n",
    "        print(f\"Collection now contains {collection.count()} total documents\")\n",
    "        \n",
    "        return collection\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in streaming write: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with a moderate dataset\n",
    "print(\"Testing streaming write with 50 English conversations...\")\n",
    "streaming_collection = write_streaming_to_chroma(\n",
    "    collection_name=\"wildchat_10k\",\n",
    "    limit=10000,  # Will get first 50 English conversations from first 100\n",
    "    filter_language=\"English\",\n",
    "    min_message_length=30\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
