{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The RAG Flywheel","text":""},{"location":"#data-driven-product-development-for-ai-applications","title":"Data-Driven Product Development for AI Applications","text":"<p>A systematic approach to building self-improving AI systems</p> <p>About This Book</p> <p>This book provides a structured approach to evolving Retrieval-Augmented Generation (RAG) from a technical implementation into a continuously improving product. You'll learn to combine product thinking with data science principles to create AI systems that deliver increasing value over time.</p>"},{"location":"#the-rag-improvement-flywheel","title":"The RAG Improvement Flywheel","text":"<p>At the core of this book is the RAG improvement flywheel - a continuous cycle that transforms user interactions into product enhancements.</p> <pre><code>graph TD\n    A[Synthetic Data &amp; Evaluation] --&gt; B[Learning from Evaluations]\n    B --&gt; C[UX Design &amp; Feedback Collection]\n    C --&gt; D[User Segmentation &amp; Analysis]\n    D --&gt; E[Building Specialized Capabilities]\n    E --&gt; F[Unified Product Architecture]\n    F --&gt; A\n\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style C fill:#bbf,stroke:#333,stroke-width:2px\n    style E fill:#dfd,stroke:#333,stroke-width:2px</code></pre> <p>Beyond Technical Implementation</p> <p>This book goes beyond teaching you how to implement RAG. It shows you how to think about RAG as a product that continuously evolves to meet user needs and deliver business value.</p>"},{"location":"#chapters","title":"Chapters","text":""},{"location":"#introduction-beyond-implementation-to-improvement","title":"Introduction: Beyond Implementation to Improvement","text":"<p>Understand why systematic improvement matters and how to approach RAG as a product rather than just a technical implementation.</p>"},{"location":"#chapter-1-starting-the-flywheel-with-data","title":"Chapter 1: Starting the Flywheel with Data","text":"<p>Learn how to overcome the cold-start problem, establish meaningful metrics, and create a data foundation that drives product decisions.</p>"},{"location":"#chapter-2-from-evaluation-to-product-enhancement","title":"Chapter 2: From Evaluation to Product Enhancement","text":"<p>Transform evaluation insights into concrete product improvements through fine-tuning, re-ranking, and targeted enhancements.</p>"},{"location":"#chapter-3-the-user-experience-of-ai","title":"Chapter 3: The User Experience of AI","text":"<p>Design interfaces that both delight users and gather valuable feedback, creating a virtuous cycle of improvement.</p>"},{"location":"#chapter-4-understanding-your-users","title":"Chapter 4: Understanding Your Users","text":"<p>Segment users and queries to identify high-value opportunities and create targeted improvement strategies.</p>"},{"location":"#chapter-5-building-specialized-capabilities","title":"Chapter 5: Building Specialized Capabilities","text":"<p>Develop purpose-built solutions for different user needs spanning documents, images, tables, and structured data.</p>"},{"location":"#chapter-6-unified-product-architecture","title":"Chapter 6: Unified Product Architecture","text":"<p>Create a cohesive product experience that intelligently routes to specialized components while maintaining a seamless user experience.</p>"},{"location":"#key-takeaways-product-principles-for-ai-applications","title":"Key Takeaways: Product Principles for AI Applications","text":"<p>Core principles that will guide your approach to building AI products regardless of how the technology evolves.</p>"},{"location":"#for-product-leaders-engineers-and-data-scientists","title":"For Product Leaders, Engineers, and Data Scientists","text":"<p>What You'll Learn</p> <p>For Product Leaders - How to establish metrics that align with business outcomes - Frameworks for prioritizing AI product improvements - Approaches to building product roadmaps for RAG applications - Methods for communicating AI improvements to stakeholders</p> <pre><code>**For Engineers**\n- Implementation patterns that facilitate rapid iteration\n- Architectural decisions that enable continuous improvement\n- Techniques for building modular, specialized capabilities\n- Approaches to technical debt management in AI systems\n\n**For Data Scientists**\n- Methods for creating synthetic evaluation datasets\n- Techniques for segmenting and analyzing user queries\n- Frameworks for measuring retrieval effectiveness\n- Approaches to continuous learning from user interactions\n</code></pre>"},{"location":"#about-the-author","title":"About the Author","text":"<p>Jason Liu brings practical experience from his work at Facebook, Stitch Fix, and as a consultant for companies like HubSpot, Zapier, and many others. His background spans computer vision, recommendation systems, and RAG applications across diverse domains.</p> <p>Author's Philosophy</p> <p>\"The most successful AI products aren't the ones with the most sophisticated models, but those built on disciplined processes for understanding users, measuring performance, and systematically improving. This book will show you how to create not just a RAG application, but a product that becomes more valuable with every interaction.\"</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Begin your journey by reading the Introduction or jump directly to Chapter 1 to start building your evaluation framework and data foundation.</p>"},{"location":"misc/introduction/","title":"Introduction to RAG","text":"<p>Retrieval-Augmented Generation (RAG) is a powerful approach that combines the strengths of retrieval-based systems with generative AI models. This hybrid approach helps to address limitations of large language models (LLMs) by providing them with relevant context from external knowledge sources.</p>"},{"location":"misc/introduction/#what-is-rag","title":"What is RAG?","text":"<p>RAG is a technique that:</p> <ol> <li>Retrieves relevant information from a knowledge base in response to a query</li> <li>Augments a prompt with this retrieved information</li> <li>Generates a response using an LLM with the augmented context</li> </ol> <p>This approach helps overcome several limitations of traditional LLMs:</p> <ul> <li>Outdated knowledge: LLMs can only access information they were trained on</li> <li>Hallucinations: LLMs sometimes generate incorrect information</li> <li>Limited context window: LLMs have constraints on how much text they can process at once</li> <li>Lack of specific domain knowledge: LLMs may not have deep expertise in specialized domains</li> </ul>"},{"location":"misc/introduction/#why-improve-rag-systems","title":"Why Improve RAG Systems?","text":"<p>While basic RAG implementations can be effective, there are numerous opportunities to enhance performance:</p> <ul> <li>Better retrieval: Improving how relevant information is found</li> <li>Smarter augmentation: Optimizing how retrieved information is incorporated</li> <li>Enhanced generation: Fine-tuning the response creation process</li> <li>Comprehensive evaluation: Measuring and tracking system performance</li> </ul> <p>This documentation serves as a guide to systematically improving each component of RAG systems, with practical implementation advice and real-world examples.</p>"},{"location":"misc/introduction/#core-components","title":"Core Components","text":"<p>A RAG system typically consists of:</p> <ol> <li>Knowledge Base: The corpus of documents or information sources</li> <li>Embedding Model: Transforms text into vector representations</li> <li>Vector Database: Stores and enables similarity search on embeddings</li> <li>Retriever: Finds relevant information from the knowledge base</li> <li>Prompt Engineering: Constructs effective prompts with retrieved information</li> <li>LLM: Generates the final response</li> </ol> <p>In the following sections, we'll explore each component in detail and provide strategies for optimization.</p>"},{"location":"misc/landingpage/","title":"Systematically Improve Your RAG Applications","text":""},{"location":"misc/landingpage/#stop-guessing-start-building-rag-that-actually-works","title":"Stop Guessing. Start Building RAG That Actually Works.","text":"<p> Top rated AI course on Maven.com (4.7/5 stars, +200 students) </p> <p>Confidently build and refine Retrieval-Augmented Generation (RAG) systems that deliver real-world impact. Our 6-week, hands-on course takes you from the fundamentals of evaluating quality all the way through building stable, production-grade capabilities.</p> <p>Enroll now on Maven (starts Feb 4)</p>"},{"location":"misc/landingpage/#what-people-are-saying","title":"What People Are Saying","text":"Review Name Role \"Practical lessons from every lecture... learning from a community on the vanguard of this emerging field.\" Max Software Engineer, Launch School \"Excellent job of stressing the fundamentals... useful metric tools to measure and improve RAG systems.\" Christopher Senior Data/AI Architect, Procurement Sciences AI \"Jason and Dan help set you on the right path... emphasis on looking at your data and building a metrics-based flywheel.\" Vitor Staff Software Engineer, Zapier \"A game-changer! ... They've got this knack for breaking down complex RAG concepts into a framework that just clicks.\" Moose Founder &amp; CEO, Sociail, Inc. <p>\"Jason helped us break down our vision into actionable steps, providing clear recommendations on the best models for each use case. His guidance gave us a tangible roadmap for our next steps and introduced practical techniques that drive continuous product improvements. Grateful for his expertise and support!\" \u2014 Camu Team (a16z backed)</p>"},{"location":"misc/landingpage/#the-problem-with-rag-today","title":"The Problem With RAG Today","text":"<p>Over the last few years, \"RAG\" has become a buzzword, but making these systems genuinely robust and effective often feels like guesswork. Most teams waste time on:</p> <ul> <li>\u274c Vague metrics like \"make the AI better\"</li> <li>\u274c Random experiments without data</li> <li>\u274c Focusing on generation while ignoring retrieval</li> <li>\u274c Building one-size-fits-all systems that underperform</li> </ul> <p>This course cuts through the confusion by giving you a clear, repeatable process: from collecting the right data and generating synthetic evaluations, to gradually incorporating new retrieval indices, routing strategies, fine-tuned embeddings, and practical UX improvements.</p>"},{"location":"misc/landingpage/#what-youll-get","title":"What You'll Get","text":"<p>In just 6 weeks, you'll learn a proven system to:</p> <ul> <li>\u2705 Build Proper Evaluations - Create synthetic data to measure real improvement</li> <li>\u2705 Find What Matters - Segment queries to identify high-impact opportunities</li> <li>\u2705 Improve Search Quality - Build specialized indices that actually retrieve what users need</li> <li>\u2705 Collect Valuable Feedback - Design UI that generates continuous improvement data</li> <li>\u2705 Optimize Embeddings - Fine-tune models that understand YOUR definition of relevance</li> </ul>"},{"location":"misc/landingpage/#trusted-by-professionals-from-leading-organizations","title":"Trusted by Professionals from Leading Organizations:","text":"Company Industry OpenAI AI Research &amp; Development Anthropic AI Research &amp; Development Google Search Engine, Technology Microsoft Software, Cloud Computing TikTok Social Media Databricks Data Platform Amazon E-commerce, Cloud Computing Airbnb Travel Zapier Automation HubSpot Marketing Software Shopify E-commerce Platform PwC Professional Services Booz Allen Hamilton Consulting Bain &amp; Company Consulting Northrop Grumman Aerospace &amp; Defense Visa Financial Services KPMG Professional Services Company Industry Decagon Technology Anysphere AI GitLab Software Development Intercom Customer Engagement Lincoln Financial Financial Services DataStax Database Technology Timescale Database Technology PostHog Product Analytics Gumroad E-commerce Platform Miro Collaboration Workday Enterprise Software Accenture Consulting, Technology Services Mozilla Non-profit Redhat Software Development Nvidia AI"},{"location":"misc/landingpage/#what-makes-this-course-different","title":"What Makes This Course Different","text":"<p>This isn't theory - it's a practical system used by leading companies to:</p> <ol> <li> <p>Stop treating RAG as an AI problem \"RAG is really just a recommendation system squeezed between two LLMs\"</p> </li> <li> <p>Focus on what you can control Improve search quality first - generation quality follows automatically</p> </li> <li>Build improvement flywheels Create systems that get better with every user interaction</li> </ol> <p>No more fumbling in the dark. This program shows you step-by-step how to:</p> <ol> <li>Set up meaningful evaluations</li> <li>Identify high-impact opportunities</li> <li>Continuously refine retrieval</li> <li>Integrate feedback loops</li> <li>Enhance product experiences</li> </ol> <p>Enroll now on Maven (starts Feb 4)</p>"},{"location":"misc/landingpage/#not-ready-for-a-course-check-out-my-free-rag-playbook","title":"Not ready for a course? Check out my free RAG Playbook","text":"<p>Not ready to invest in a paid course yet? Start with my free RAG Playbook newsletter course. You'll get bite-sized lessons delivered straight to your inbox, covering the fundamentals of RAG systems and practical tips for improvement.</p> <p>Free 6 Day RAG Crash Course</p> <p>Once you're comfortable with the basics and ready to take your RAG skills to the next level, consider enrolling in our comprehensive course in February 2024.</p>"},{"location":"misc/landingpage/#what-youll-learn","title":"What You'll Learn","text":"<p>Our six-week program is designed to take you from RAG basics to advanced implementation strategies. Perfect for those who deployed RAG systems and want to improve them and cover the last mile of RAG. Here's a breakdown of what you can expect:</p>"},{"location":"misc/landingpage/#weeks-1-2-foundations-and-evaluation","title":"Weeks 1-2: Foundations and Evaluation","text":"<ul> <li>Synthetic Data Generation: Learn to create high-quality synthetic data for rapid testing and development. Understand the importance of diversity in your test sets and how to avoid common pitfalls.</li> <li>Fast Evaluation Techniques: Implement quick, iterative improvements using unit test-like evaluations. Focus on basic retrieval metrics like precision and recall to optimize your system efficiently.</li> <li>Query Segmentation: Discover how to categorize and analyze user queries to identify patterns and gaps in your system's performance. Learn to prioritize improvements based on impact, volume, and success likelihood.</li> <li>Metrics That Matter: Understand the difference between leading and lagging metrics. Learn how to set actionable goals that drive real improvements in your RAG system.</li> </ul>"},{"location":"misc/landingpage/#weeks-3-4-advanced-retrieval-and-routing","title":"Weeks 3-4: Advanced Retrieval and Routing","text":"<ul> <li>Specialized Indices: Build targeted indices for different content types (documents, images, tables) to improve retrieval accuracy. Learn advanced techniques for handling multimodal data.</li> <li>Query Routing: Implement sophisticated query routing systems using parallel function calling. Understand how to select the right tools and APIs for different query types.</li> <li>Combining Search Methods: Master the art of blending lexical, semantic, and metadata-based search for optimal results. Learn when and how to use re-rankers effectively.</li> <li>Structured Data Extraction: Explore techniques for extracting and leveraging structured data from various sources to enhance your RAG capabilities.</li> </ul>"},{"location":"misc/landingpage/#week-5-fine-tuning-and-embeddings","title":"Week 5: Fine-tuning and Embeddings","text":"<ul> <li>Embedding Model Optimization: Learn when and how to fine-tune embedding models for your specific use case. Understand the impact of domain-specific data on model performance.</li> <li>Data Collection Strategies: Implement effective feedback mechanisms and logging systems to gather valuable data for future improvements.</li> <li>Re-ranker Implementation: Discover how to fine-tune and implement re-rankers for better search results. Learn about the latest advancements in ranking technologies.</li> <li>Representation Learning: Dive deep into the nuances of creating effective representations for various entities in your system, from user queries to document summaries.</li> </ul>"},{"location":"misc/landingpage/#week-6-product-design-and-user-experience","title":"Week 6: Product Design and User Experience","text":"<ul> <li>Feedback Collection: Design intuitive and effective feedback mechanisms to continuously improve your system. Learn how to incentivize user feedback without disrupting the experience.</li> <li>Streaming Implementations: Implement streaming for improved user experience and perceived performance. Understand the psychological impacts of responsiveness on user satisfaction.</li> <li>Advanced Prompting Techniques: Master the art of crafting effective prompts, including chain-of-thought reasoning and dynamic few-shot learning.</li> <li>UI/UX Best Practices: Explore cutting-edge UI/UX designs for RAG applications, including innovative ways to display citations, confidence levels, and alternative answers.</li> </ul>"},{"location":"misc/landingpage/#why-this-course","title":"Why This Course?","text":"<p>In the rapidly evolving field of AI and machine learning, staying ahead means mastering the fundamentals while keeping pace with the latest advancements. Our course offers:</p> <ul> <li>Practical, Hands-on Learning: Every concept is accompanied by real-world examples and exercises. You'll be implementing and testing ideas from day one.</li> <li>Industry-Relevant Case Studies: Learn from actual scenarios encountered in production environments at leading tech companies.</li> <li>Expert Instruction: Benefit from 12 hours of dedicated time with instructors who have years of experience in building and optimizing RAG systems.</li> <li>Community of Professionals: Connect with a diverse group of peers from companies like Amazon, Adobe, and Zapier. Share insights, challenges, and solutions in a collaborative environment.</li> <li>Cutting-edge Content: Stay updated with the latest trends and technologies in RAG, including advanced embedding techniques, multi-modal retrieval, and emerging evaluation metrics.</li> <li>Personalized Feedback: Receive tailored advice on your specific RAG challenges through interactive Q&amp;A sessions and project reviews.</li> </ul>"},{"location":"misc/landingpage/#more-from-our-students","title":"More From Our Students","text":"Review Name Role \"Practical and grounded in actual industry experience... like getting the inside scoop from folks who've been in the trenches.\" Ashutosh Senior Principal Scientist, Adobe \"System-oriented approach... Highly relevant, directly applicable, and save time in building prototypes.\" Mani Senior Principal Software Engineer, Red Hat \"Pragmatic with lots of advice that you won't find in any course. What I look for in good courses are instructors with strong points of view and Jason has them in abundance. If you follow all the steps given, you are definitely on a fast track to building your AI...\" Naveen SVP of Engineering, BoostUp.ai \"Jason's AI Consultant course brought out lots of new avenues and concepts in the AI Consultant journey which I was previously not aware of - AIDA, what to have in a landing page, contract negotiation and more! It was definitely an eye-opener and helped...\" Laks Independent AI Researcher and Enthusiast \"If you are an expert in the field of AI and want to build a successful business as an independent consultant, this course is for you. Jason teaches you how to build proof and shows how to interact with clients to achieve dream outcomes for everyone involved...\" Philipp AI Consultant, peachstone.ai \"The course completely changed my mindset around communicating value and pricing accordingly. The tips on how to gradually build your audience were super valuable. Highly recommend for anyone starting out or just looking to level up...\" Erikas Senior AI engineer \"Jason's course is packed with actionable insights and advice. It's not a theoretical course on what to do, it's an actual practical guide on real life example and insights that you can start applying right away. Jason is very responsive and approachable...\" Guido Cohort 1 \"Jason's course was packed with actionable insights and advice. It's not a theoretical course on what to do, it's an actual practical guide on real life example and insights that you can start applying right away. Jason is responsive and approachable...\" Dylan AI Consultant, Iwana Labs"},{"location":"misc/landingpage/#risk-free-guarantee","title":"Risk-Free Guarantee","text":"<p>We're so confident in the value of this course that we offer a money-back guarantee. If you don't feel you're making significant progress in improving your RAG applications after 4 weeks, we'll refund your course fee, no questions asked.</p>"},{"location":"misc/landingpage/#bonus-1500-in-free-credits","title":"Bonus: $1,500+ in Free Credits","text":"<p>Get free credits for essential RAG tools including Cohere, LanceDB, Modal Labs, and more!</p>"},{"location":"misc/landingpage/#secure-your-teams-spot-today","title":"Secure Your Team's Spot Today","text":"<p>The field of RAG is evolving quickly. Don't fall behind.</p> <p>Enroll now on Maven (starts Feb 4)</p> <p>How to Get Reimbursed</p> <p>Hey {manager},</p> <p>I've found a course called \"Systematically Improving RAG Applications\" that I believe would be incredibly valuable for our team. Here are the key points:</p> <ul> <li>Expert Instruction: Learn from Jason Liu, who has 8 years of experience in recommendation systems and RAG applications.</li> <li>Comprehensive Curriculum: 6-week course covering everything from synthetic data generation to advanced query routing and embedding optimization.</li> <li>Practical Application: Hands-on sessions for implementing quick testing methods and live data streaming.</li> <li>Strategic Insights: Learn to improve search quality, implement effective feedback loops, and make data-driven decisions.</li> <li>Efficiency Gains: Techniques to increase work speed, user satisfaction, and retention rates.</li> <li>Future-Readiness: Focus on rapid testing and adoption of emerging technologies in the RAG space.</li> <li>Added Value: Over $1,500 in free credits for tools like Cohere, LanceDB, and Modal Labs.</li> <li>Risk-Free: Money-back guarantee if we don't see improvements within 5 weeks.</li> </ul> <p>The course costs $1,650. I plan to share the learnings with our entire team, multiplying the value of this investment. You can find more details here: https://maven.com/applied-llms/rag-playbook</p> <p>What are your thoughts on this opportunity?</p> <p>Thanks,</p> <p>P.S. I've heard that other teams are sending multiple team members to build shared context efficiently. Should we consider a similar approach?</p>"},{"location":"misc/learning-goals/","title":"Learning Goals: Developing AI Product Sense","text":"","tags":["learning-objectives","product-sense","rag","ai-products"]},{"location":"misc/learning-goals/#why-this-book-exists","title":"Why This Book Exists","text":"<p>When I started teaching RAG development, I noticed something important. Most courses teach you the technical parts \u2013 how to connect to an API, how to set up a vector database, or how to write a prompt. But they miss something critical: how to think about your AI product as a whole system that gets better over time.</p> <p>This book exists to fill that gap. My goal isn't just to teach you technical skills, but to help you develop what I call \"AI product sense\" \u2013 the ability to think strategically about AI products, make smart decisions about what to improve, and create systems that get better with use.</p>","tags":["learning-objectives","product-sense","rag","ai-products"]},{"location":"misc/learning-goals/#what-youll-take-away","title":"What You'll Take Away","text":"<p>By the end of this book, you'll be able to:</p> <ol> <li> <p>Build improvement systems, not just features</p> </li> <li> <p>Design RAG applications that collect valuable feedback</p> </li> <li>Create measurement frameworks to track performance</li> <li> <p>Set up processes that turn user interactions into training data</p> </li> <li> <p>Make data-driven decisions</p> </li> <li> <p>Set up proper evaluation metrics before making changes</p> </li> <li>Generate synthetic data to test improvements</li> <li> <p>Analyze patterns in user behavior to identify opportunities</p> </li> <li> <p>Prioritize efforts for maximum impact</p> </li> <li> <p>Identify which improvements will deliver the most value</p> </li> <li>Understand which user segments benefit most from specific features</li> <li> <p>Develop roadmaps based on data rather than hunches</p> </li> <li> <p>Build specialized solutions for specific problems</p> </li> <li> <p>Design purpose-built retrievers for different content types</p> </li> <li>Create query routers that direct questions to the right tools</li> <li> <p>Implement feedback mechanisms tailored to different use cases</p> </li> <li> <p>Think beyond technical implementation</p> </li> <li>Consider user experience in every design decision</li> <li>Understand the business impact of your technical choices</li> <li>Communicate effectively about AI capabilities and limitations</li> </ol>","tags":["learning-objectives","product-sense","rag","ai-products"]},{"location":"misc/learning-goals/#what-makes-this-different-the-product-sense-approach","title":"What Makes This Different: The Product Sense Approach","text":"<p>This isn't just a technical manual \u2013 it's a guide to thinking about AI products in a more holistic way. While other resources might teach you how to implement a specific technique, I'm going to teach you how to:</p> <ul> <li> <p>Ask the right questions: Instead of \"How do I use the latest embedding model?\", you'll learn to ask \"What problem am I really trying to solve for my users?\"</p> </li> <li> <p>See the whole system: You'll understand how each part of your RAG system affects the others, and how small changes can have big impacts.</p> </li> <li> <p>Make strategic choices: You'll learn when to use specialized solutions, when to collect more data, and when to focus on improving user experience.</p> </li> <li> <p>Build with evolution in mind: Everything you build will be designed to get better over time, not just work once and be forgotten.</p> </li> </ul>","tags":["learning-objectives","product-sense","rag","ai-products"]},{"location":"misc/learning-goals/#who-benefits-from-this-approach","title":"Who Benefits from This Approach","text":"<p>The AI product sense you'll develop applies across many roles:</p> <ul> <li> <p>Engineers will learn to build systems that improve with use, not just meet initial requirements.</p> </li> <li> <p>Product managers will understand how to prioritize AI features based on user impact, not just technical interest.</p> </li> <li> <p>Data scientists will see how their models fit into broader systems and user experiences.</p> </li> <li> <p>Founders and leaders will develop a framework for thinking about AI investments and strategic decisions.</p> </li> </ul> <p>No matter your role, you'll finish this book with a clearer understanding of how to build AI products that deliver real value and improve over time.</p>","tags":["learning-objectives","product-sense","rag","ai-products"]},{"location":"misc/learning-goals/#the-true-goal-creating-learning-systems","title":"The True Goal: Creating Learning Systems","text":"<p>The deepest goal of this book isn't just to help you build better RAG applications. It's to change how you think about AI products altogether.</p> <p>By the end, you'll see that the most valuable AI systems aren't the ones with the most advanced models or the fanciest features. They're the ones designed to learn and improve with every interaction \u2013 the ones that create a positive cycle where user engagement leads to better performance, which leads to more user engagement.</p> <p>This mindset \u2013 this product sense \u2013 is what separates truly transformative AI implementations from the many disappointing ones we see in the market today. And it's what I'm most excited to share with you through this book.</p>","tags":["learning-objectives","product-sense","rag","ai-products"]},{"location":"misc/what-i-want-you-to-takeaway/","title":"Product Principles for AI Applications","text":"<p>Hello there! Jason here. After spending these chapters together exploring the world of RAG systems, I want to make sure you walk away with more than just technical knowledge. While the code examples and architectures are valuable, the real lessons I hope you've learned go much deeper.</p>","tags":["product thinking","principles","mindset","improvement"]},{"location":"misc/what-i-want-you-to-takeaway/#the-flywheel-mindset","title":"The Flywheel Mindset","text":"<p>If there's one concept I want permanently etched in your mind, it's the improvement flywheel. Throughout my career\u2014from Facebook to Stitch Fix to my consulting work\u2014I've seen the same pattern: teams that build systems that get better with use succeed, while those that build static implementations eventually fail.</p> <p>Your RAG application should be smarter next month than it is today. If it isn't, something is wrong with your process, not your technology.</p>","tags":["product thinking","principles","mindset","improvement"]},{"location":"misc/what-i-want-you-to-takeaway/#stop-guessing-start-measuring","title":"Stop Guessing, Start Measuring","text":"<p>I've watched too many brilliant engineers waste countless hours debating which embedding model or chunking strategy is \"best\" without ever defining how they'd measure \"best\" in the first place.</p> <p>Don't fall into this trap. Before you change anything in your system, know exactly how you'll measure the impact of that change. Without this discipline, you're just accumulating technical debt while pretending to make improvements.</p>","tags":["product thinking","principles","mindset","improvement"]},{"location":"misc/what-i-want-you-to-takeaway/#users-over-models","title":"Users Over Models","text":"<p>The most sophisticated RAG system that doesn't actually solve user problems is worthless. Period.</p> <p>I've built systems that generated millions in revenue using outdated models because they solved real problems well. And I've seen state-of-the-art implementations fail because they missed the mark on user needs.</p> <p>When in doubt, talk to your users. Read their feedback. Watch them use your system. This will teach you more than any research paper or GitHub repository ever could.</p>","tags":["product thinking","principles","mindset","improvement"]},{"location":"misc/what-i-want-you-to-takeaway/#specialization-beats-generalization","title":"Specialization Beats Generalization","text":"<p>The path to exceptional RAG isn't finding the single best approach\u2014it's identifying the different types of queries your users have and building specialized solutions for each.</p> <p>This principle applies everywhere: specialized embeddings outperform general ones, targeted retrievers beat one-size-fits-all approaches, and segmented generation strategies outshine monolithic prompts.</p>","tags":["product thinking","principles","mindset","improvement"]},{"location":"misc/what-i-want-you-to-takeaway/#data-compounds-like-interest","title":"Data Compounds Like Interest","text":"<p>In the early days of any RAG application, progress feels slow. You're manually creating synthetic queries, writing evaluation examples, and fine-tuning with limited data.</p> <p>Don't get discouraged. Every piece of data you collect now becomes the foundation for automated improvements later. The first hundred examples are the hardest\u2014after that, your flywheel starts spinning faster with each cycle.</p>","tags":["product thinking","principles","mindset","improvement"]},{"location":"misc/what-i-want-you-to-takeaway/#methods-matter-more-than-models","title":"Methods Matter More Than Models","text":"<p>Models will change. What was state-of-the-art when I wrote this will likely be outdated by the time you're reading it.</p> <p>But the methods for systematic improvement are timeless. The processes for collecting feedback, evaluating performance, identifying patterns, and prioritizing improvements will serve you regardless of which models you're using.</p>","tags":["product thinking","principles","mindset","improvement"]},{"location":"misc/what-i-want-you-to-takeaway/#the-hardest-problems-arent-technical","title":"The Hardest Problems Aren't Technical","text":"<p>In my experience, the biggest challenges in building successful RAG applications rarely involve model selection or hyperparameter tuning. They're about:</p> <ul> <li>Convincing stakeholders to invest in measurement infrastructure</li> <li>Getting users to provide meaningful feedback</li> <li>Prioritizing improvements when resources are limited</li> <li>Balancing quick wins against long-term architectural needs</li> </ul> <p>The skills to navigate these challenges are as important as your technical abilities.</p>","tags":["product thinking","principles","mindset","improvement"]},{"location":"misc/what-i-want-you-to-takeaway/#start-small-but-start-now","title":"Start Small, But Start Now","text":"<p>You don't need a perfect RAG implementation to begin this journey. You don't need millions of examples or custom-trained models. You can start with a basic retriever, a few dozen synthetic queries, and simple thumbs-up/down feedback.</p> <p>What matters is establishing the process for improvement from day one. Even a basic system that improves systematically will eventually outperform a sophisticated system that remains static.</p>","tags":["product thinking","principles","mindset","improvement"]},{"location":"misc/what-i-want-you-to-takeaway/#building-a-culture-of-continuous-improvement","title":"Building a Culture of Continuous Improvement","text":"<p>Beyond the technical aspects, successful RAG products require the right organizational culture:</p> <ul> <li> <p>Celebrate learning over correctness: Teams that view failures as learning opportunities improve faster than those focused on being right the first time.</p> </li> <li> <p>Share ownership of metrics: When everyone from engineers to product managers to business stakeholders aligns on key metrics, improvement accelerates.</p> </li> <li> <p>Make feedback visible: Surface user feedback and performance metrics in dashboards, team meetings, and planning sessions to keep improvement central to your work.</p> </li> <li> <p>Budget for refinement: Explicitly allocate resources for post-launch improvement rather than moving the entire team to the next project.</p> </li> <li> <p>Document your journey: Keep records of what you've tried, what worked, and what didn't. This institutional knowledge becomes invaluable as your team grows.</p> </li> </ul> <p>Remember, this field is still young. The techniques we've covered are just the beginning. As you continue your journey, you'll discover new approaches and face unique challenges. But if you take these core principles to heart, you'll have the foundation to adapt and thrive regardless of how the technology evolves.</p> <p>Build systems that learn. Measure before you change. Put users first. Specialize where it matters. Trust the process.</p> <p>I can't wait to see what you build.</p> <p>\u2013 Jason</p>","tags":["product thinking","principles","mindset","improvement"]},{"location":"office-hours/","title":"Office Hours","text":"<p>This section contains summaries and key insights from our weekly office hours sessions, where we discuss challenges, solutions, and best practices for RAG systems.</p>"},{"location":"office-hours/#available-sessions","title":"Available Sessions","text":"<ul> <li>Feb 4 Summary</li> <li>Feb 11 Summary</li> <li>Feb 18 Summary</li> <li>Feb 25 Summary</li> <li>Mar 4 Summary</li> </ul> <p>Each summary captures the questions, answers, and discussions from our sessions, providing additional context and real-world examples to complement the workshop materials.</p>"},{"location":"office-hours/#questions-and-topics","title":"Questions and Topics","text":"<p>Office hours cover a wide range of topics including:</p> <ul> <li>Practical implementation challenges</li> <li>Data collection strategies</li> <li>Evaluation methodologies</li> <li>User experience considerations</li> <li>Architecture decisions</li> <li>Product development approaches</li> </ul> <p>Get the Most from Office Hours</p> <p>Review the workshop materials before office hours sessions to come prepared with specific questions and challenges from your own RAG implementations.</p>"},{"location":"office-hours/week1-summary/","title":"RAG Office Hours Q&amp;A Summary - Week 1","text":""},{"location":"office-hours/week1-summary/#what-is-your-take-on-dspy-should-we-use-it","title":"What is your take on DSpy? Should we use it?","text":"<p>Generally, I think DSpy allows you to do some kind of prompt optimization by synthetically creating a bunch of few-shot examples and then identifying which of these examples could improve the performance of your system.</p> <p>Personally, I feel like most of the time you should be spending a lot of time actually just tweaking those prompts yourself. The most valuable part of looking at data, few-shots, and examples is you building an intuition of what customers are looking for and what mistakes the system is making.</p> <p>Your product isn't just a prompt\u2014it includes how you collect feedback, how you set expectations in the UI, how you think about data extraction, and how you represent chunks in the context. If you spend the time to look at how the model is making mistakes and what users are asking for, you'll make much more progress in improving the product as a whole.</p> <p>DSpy is fine especially when you have very specific evaluations. For example, maybe you have a 35-class classification task where all you care about is accuracy. Then DSpy really works because you can figure out which of the 10 examples you need to maximize your accuracy.</p> <p>But most of the time, that's not the case. If I'm building a model to extract sales insights from a transcript, I don't have a dataset of \"here's all the sales insights.\" The real work might be extracting everything and hand-labeling some stuff. Because these tasks are very hard to hill-climb (when metrics aren't just classification accuracy), tools like DSpy don't work as well.</p> <p>Another good use of DSpy is around using LLMs as judges. If you have a tonality or factuality evaluation you really care about, it makes sense to label a hundred examples yourself and then use prompt optimization tools to create your own judge that aligns with your grades.</p>"},{"location":"office-hours/week1-summary/#is-it-useful-to-prompt-language-models-with-an-understanding-of-structure-and-rationale-for-their-actions","title":"Is it useful to prompt language models with an understanding of structure and rationale for their actions?","text":"<p>Yes, absolutely. Understanding structure and rationale is critical because your product includes the ways you collect feedback, set expectations in the UI, perform data extraction, and represent chunks in the context.</p> <p>It's not just about the prompt\u2014it's a whole system. And if you can spend time looking at how the model makes mistakes and what users are asking for, you'll make much more progress in improving the product holistically.</p> <p>When you build an intuition for what's happening, you can make smarter design decisions across the entire product experience.</p>"},{"location":"office-hours/week1-summary/#how-do-we-introduce-a-concept-of-time-and-vector-search-to-answer-questions-like-whats-the-latest-news-without-needing-to-move-to-a-graph-database","title":"How do we introduce a concept of time and vector search to answer questions like \"What's the latest news?\" without needing to move to a graph database?","text":"<p>The answer is to use a SQL database. If you use something like Timescale or PostgreSQL, there are many ways of doing time filtering.</p> <p>One specific thing to note is the difference between pgvector and pgvector-scale. Pgvector does not do exhaustive search, so there's a chance you don't recall all information because of how the database segments things. With pgvector-scale, it will exhaustively search every single row in your database if required. This small difference means a lot if you're trying to find very specific details.</p> <p>The general idea is to use structured extraction to identify start and end dates, prompt your language model with an understanding of what those dates are, and then use filtering. You would do an embedding search plus a BETWEEN statement in your time query. This works pretty well.</p>"},{"location":"office-hours/week1-summary/#is-knowledge-graph-rag-production-ready-by-now-do-you-recommend-it","title":"Is knowledge graph RAG production ready by now? Do you recommend it?","text":"<p>In my 10 years of doing data science and machine learning, I generally stay away from any kind of graph modeling. The reason is that every time I've seen a company go into this graph-based world, within 4-5 years they decide to move back to a PostgreSQL database.</p> <p>There are several issues with graph databases:</p> <ol> <li>They're really hard to learn - it's much easier to hire talent that knows PostgreSQL than graph databases.</li> <li>Defining schemas in PostgreSQL and joins is well-defined, whereas in graph databases there's often too much debate and not enough best practices.</li> <li>Most cases don't require more than one or two traversals of your graph.</li> </ol> <p>When I was at Facebook, their graph was actually just a very large MySQL database. This makes me cautious about using graph databases unless you have expert users.</p> <p>The only company I really believe could effectively use a graph database is LinkedIn, because they need to compute things like nearest neighbors up to three or five degrees away.</p> <p>Even for cases like Microsoft's approach where you build a document graph with entities and relationships, I'd prefer to use fine-tuned embeddings. A graph can be defined as an adjacency matrix, and fine-tuning your embeddings can get you pretty close to the similarity definition that a graph could maintain.</p> <p>I'd rather start with data and say, \"There are certain kinds of queries that really need a graph structure\" and let that justify the graph structure. Most technology needs to be justified by what the product needs to deliver rather than thinking about technology first.</p>"},{"location":"office-hours/week1-summary/#would-you-recommend-using-colbert-models-or-other-specialized-retrieval-approaches","title":"Would you recommend using Colbert models or other specialized retrieval approaches?","text":"<p>All of these models do similar things at their core. To decide what to use, we should start with a synthetic dataset to measure precision and recall. Then the real question becomes: do any of these interventions (graph RAG, Colbert models, embeddings, re-rankers) beat the baseline in terms of precision and recall?</p> <p>It might be that graph for a certain problem is only 2% better, in which case it might not be worth the complexity. But if you found that, for parsing hospital records, graph RAG is 40% better on recall and precision, then it doesn't matter what I think\u2014the data would speak for itself.</p> <p>For Colbert specifically, it probably does very well for certain tasks. For example, statements like \"I love coffee\" and \"I hate coffee\" would be very similar in embedding space because embeddings don't fully understand negation. With a Colbert model, the cross-attention mechanism can figure out that these statements are different.</p> <p>But you need to tell the model what's important in your context. Without enough tests to guide us, it's hard to know if these interventions work. Usually, it's hard to beat the baseline of embedding search with a good re-ranker. Colbert might do 4-5% better, but you need to justify that improvement against the added complexity.</p>"},{"location":"office-hours/week1-summary/#when-working-with-legal-documents-that-have-multi-level-outlines-and-reference-sections-from-other-documents-what-approach-would-you-recommend","title":"When working with legal documents that have multi-level outlines and reference sections from other documents, what approach would you recommend?","text":"<p>This could be done with a graph, but it could also be done with a simpler pointer system. When you load data, you can pull in other references. For example, in a construction project, whenever we pull up an image, we also pull up the paragraph above and below the image, augmenting the context.</p> <p>We can do the same for legal documents\u2014if it references another page or citation, we pull in that citation. Technically, this is a graph, but it's often easier to build this as a few LEFT JOINs in a PostgreSQL table.</p> <p>When we pull in text chunks, if there are references, we just do another left join back to the original chunk. These systems tend to be much simpler to reason about than dealing with reference types in a graph. Usually, that level of complexity really needs to be earned when building bigger systems.</p>"},{"location":"office-hours/week1-summary/#are-we-going-to-cover-any-fundamentals-of-how-to-systematically-do-generation","title":"Are we going to cover any fundamentals of how to systematically do generation?","text":"<p>In terms of generation, a lot comes down to prompting and using LLMs as judges, which we'll talk about in Week 3 when discussing product experience.</p> <p>If you have specific aspects of generation you want to explore, it's mostly about ensuring formatting is correct and chain of thought is reasonable. The challenge is that you can't systematically improve generation primarily because generation evaluations are much more subjective.</p> <p>If it's just formatting, that can be very explicit. But challenges with generation will mostly be addressed through LLM-as-judge approaches and different levels of regular expressions.</p> <p>For example, we have an evaluation for summarization that simply measures what percentage shorter the summary is relative to the original input. These are very basic evaluations for summarization.</p>"},{"location":"office-hours/week1-summary/#whats-your-take-on-using-rag-for-report-generation-in-response-to-requests-for-proposals","title":"What's your take on using RAG for report generation in response to requests for proposals?","text":"<p>The expert on report generation will talk in Week 4. Look out for a talk from Vantager, who does this for financial due diligence. Companies can give them existing reports, which they parse into a spec, and then when you upload new PDFs, it automatically generates a report for you.</p> <p>There's a lot of economic value that can come from report generation, and it's probably more valuable than just doing generic question answering.</p>"},{"location":"office-hours/week1-summary/#what-is-your-experience-using-reasoning-models-as-the-answer-generator-model","title":"What is your experience using reasoning models as the answer generator model?","text":"<p>Before there were specific reasoning models, I've been pushing everyone to at least have thinking tokens and a reasoning block in the output. This gives language models time to think and allows you to render in a way that minimizes perceived latency.</p> <p>Now that O1 and DeepSeek are available, unless latency is a concern, I would try to use these reasoning models. O3 Mini is fairly affordable, and O1 is very affordable. You can render the product in a way that makes users feel it's faster.</p> <p>DeepSeek's reasoning capability is one reason it stood out to people\u2014they can actually see it think. For many practitioners, we've been asking language models to think step by step for quite a while.</p>"},{"location":"office-hours/week1-summary/#how-do-we-set-user-expectations-on-the-delay-while-using-reasoning-models","title":"How do we set user expectations on the delay while using reasoning models?","text":"<p>The first UI tip is to stream out the thinking part of the model to the customer. Things will feel about 45% faster just because something is moving on the page.</p> <p>The second approach, which DeepSeek does well, is to have a button called \"Think harder\" or \"Reasoning.\" If users don't use it, they get the faster V3 model, but if they press reasoning, it switches to the R1 model. This both tells users you want the model to think (which they know will be slower) and, by rendering the thought tokens, improves the perceived latency.</p>"},{"location":"office-hours/week1-summary/#how-should-we-handle-multiple-rag-sources-with-different-levels-of-information","title":"How should we handle multiple RAG sources with different levels of information?","text":"<p>When you have multiple RAG sources (like a calendar and a news site with more detailed event information), it can slow down the system when you want to use an LLM to act as a judge and provide a holistic answer.</p> <p>One approach is to predict what types of questions are easy versus hard and route them effectively. Another approach is to improve the user experience by rendering sources before rendering the text. Show an animation like \"I am thinking\" and have document 1, 2, and 3 appear, then \"I'm reading,\" and finally the answer.</p> <p>Notion AI's UX does this well\u2014it says \"thinking about your question,\" \"searching documents,\" animates the documents coming in, and then starts talking. The key is to keep the screen moving to make users believe something is happening.</p> <p>Adding a loading screen that moves can make users feel the system is 30% faster, even if the actual processing time is the same.</p>"},{"location":"office-hours/week1-summary/#what-strategies-can-help-when-there-are-negative-consequences-of-thinking-too-hard-with-reasoning-models","title":"What strategies can help when there are negative consequences of \"thinking too hard\" with reasoning models?","text":"<p>One approach is to predict whether a question is easy or hard and decide when to turn on thinking. You could use a model like BERT to classify this.</p> <p>If that's possible, you can make the decision to think on behalf of the user. The objective would be to maximize customer satisfaction while minimizing token costs.</p> <p>Some companies like have their own proprietary model that tells you which is the best model to route to. You could have a model that's trained so that if you ask \"what's 1+1,\" it sends that to a simpler model, but if you ask about reading a legal document, it routes to an R1 model.</p> <p>For evaluation questions specifically, it really depends on the complexity. Some evaluations are simple yes/no decisions, while others involve complex reasoning like assigning the correct speaker to different comments in a transcript. You'll need to test with your specific use case.</p>"},{"location":"office-hours/week1-summary/#what-advice-would-you-give-for-introducing-llms-into-a-healthcare-company-that-may-not-fully-grasp-their-potential","title":"What advice would you give for introducing LLMs into a healthcare company that may not fully grasp their potential?","text":"<p>First, build a demo and let leadership see the results. Then, clearly identify what types of queries you won't attempt to answer, pre-loading all the risk discussions upfront.</p> <p>Instead of saying \"my model is 80% correct,\" say \"I've identified the 20% of questions that don't work at all, but for the 80% of questions we can solve, the success rate is 99%.\"</p> <p>Do the upfront work to know the failure modes and economically valuable opportunities, then present them clearly. Add guardrails to say what the LLM won't attempt to do. Much of this is about setting expectations for leadership.</p>"},{"location":"office-hours/week1-summary/#are-there-open-source-re-ranking-models-that-come-close-to-coheres-re-rankers-in-quality","title":"Are there open source re-ranking models that come close to Cohere's re-rankers in quality?","text":"<p>There are definitely good cross-encoders available, though some of the top models on leaderboards are 7 billion parameters, which may have high latency.</p> <p>Modern BERT (a new BERT-based embedding model with about 8,000 token sequence length compared to the original 512) will likely lead to more powerful BERT-based re-rankers.</p> <p>However, training your own re-ranker on your specific data will likely beat benchmark models. With just 6,000 examples from your own data, you can train a better embedding model and cross-encoder than what's publicly available, costing around $1.50 and 40 minutes on a laptop.</p>"},{"location":"office-hours/week1-summary/#outside-of-personal-experiments-what-resources-or-mediums-do-you-rely-on-to-stay-up-to-date-on-rag","title":"Outside of personal experiments, what resources or mediums do you rely on to stay up to date on RAG?","text":"<p>Much of the content coming out is very hypey, and many research papers focus on public evaluations that don't mean as much as more fundamental work on data analysis, experimentation, and evaluation.</p> <p>When reading papers, focus more on how they present results and think about experimentation rather than specific methodologies or implementations. The things that work well are often too maintenance-heavy or expensive for production use cases with millions of PDFs.</p> <p>I like Anthropic's blog posts because they're fundamental\u2014discussing how to think about error bars, clustering, and other approaches that everyone can use, not just researchers with 40,000 rows in a database.</p> <p>Outside of that, a lot of information is in private Discords and Twitter. I'll have someone make a summary of the Discords with interesting \"alpha\" or insights.</p>"},{"location":"office-hours/week1-summary/#when-working-with-documents-with-metadata-should-search-and-retrieval-methods-change-based-on-the-level-of-metadata-provided-within-the-queries","title":"When working with documents with metadata, should search and retrieval methods change based on the level of metadata provided within the queries?","text":"<p>Yes, they should. For example, in a construction project, we found people really cared about who made the last edits on legal contracts or who sent particular information. The metadata was very important\u2014queries like \"which contracts did this person send us\" become like SQL queries.</p> <p>We learned that when answering questions about who's doing what, we should include their contact information. These are small details in improving a RAG system that create economic value.</p> <p>Similarly, if you're building information that will be queried across time periods, you probably care about when documents were published and last crawled to determine relevance. A query like \"what is the latest research in physics\" might look at the past 6 months, while \"what is new in AI\" might only look at the past two weeks because it moves so quickly.</p> <p>It comes down to analyzing the queries people are asking and figuring out what creates economic value.</p>"},{"location":"office-hours/week1-summary/#do-you-know-if-anthropic-is-working-on-an-answer-to-o1-or-r1-reasoning-models","title":"Do you know if Anthropic is working on an answer to O1 or R1 (reasoning models)?","text":"<p>Yes and no. If you use Claude's web app, it secretly has thinking tokens. Every time it says \"pondering\" or \"thinking,\" it's actually outputting thinking tokens that you can't see.</p> <p>If you ask Claude to replace the  token with {anyThinking}, you'll start seeing those thinking tokens. You can request this token in the API as well. <p>The real question is whether Anthropic has thinking models that use RLHF, and I'm not fully sure about that. Their CTO has stated they don't do distillation, but there are mixed interpretations of what that means.</p> <p>Claude 3.5 Sonnet is still impressive even without visible reasoning, including its vision capabilities. The bigger issue is that Anthropic is very concerned about safety and has questions about whether thinking tokens could lie to users or follow different policies.</p>"},{"location":"office-hours/week1-summary/#when-working-with-unstructured-data-mostly-pdfs-and-drawings-how-do-you-approach-data-labeling-and-what-models-do-you-use","title":"When working with unstructured data, mostly PDFs and drawings, how do you approach data labeling and what models do you use?","text":"<p>For unprocessed data, I look at companies like Llama Parse, Extend, and Reducto, which parse headers, bodies, tables, and figures so you can work with them separately.</p> <p>For the most part, Claude Sonnet does a very good job\u2014it's just a matter of how much data you need to process. For specific tasks like understanding figures, visual language models like Qwen via Ollama work well for single PDFs, though batch local processing is more challenging as tools like VLLM don't yet support these models.</p>"},{"location":"office-hours/week1-summary/#why-does-this-course-favor-lancedb-versus-other-vector-databases","title":"Why does this course favor LanceDB versus other vector databases?","text":"<p>The main reason is that I want everyone to experience running evaluations on not just embedding search but also full-text search. I want you to try hybrid search with or without a re-ranker.</p> <p>With LanceDB, incorporating these approaches is just one extra line of code. You can do a search with different modes (lexical, vector, hybrid) and easily add a re-ranker. It's the simplest way to try all these combinations and discover what works best.</p> <p>Additionally, LanceDB is backed by DuckDB, which means the same database that supports full-text search, semantic search, and re-rankers also supports SQL. If you want to analyze your queries with SQL, you can do that easily.</p> <p>Another advantage is that LanceDB can be hosted on S3 and is easy to set up for large amounts of data.</p>"},{"location":"office-hours/week1-summary/#which-industry-or-application-domain-do-you-think-is-most-difficult-for-llms","title":"Which industry or application domain do you think is most difficult for LLMs?","text":"<p>It's hard to say definitively, but generally:</p> <ol> <li>Tasks with complex images are difficult</li> <li>Highly regulated industries like legal and healthcare contexts present challenges</li> <li>Financial services, especially ratings agencies, face enormous regulatory hurdles</li> </ol> <p>The fundamental challenge is that anything difficult for humans to collect data on will be hard for an LLM. It's about how much volume of data we have per industry and what kind of feedback loops exist.</p> <p>If an LLM makes a decision that takes weeks to verify, it's going to be hard to improve. The timeline for regulatory approval in some industries (like ratings agencies) can be years, creating a massive barrier to implementing LLM-based solutions.</p>"},{"location":"office-hours/week1-summary/#did-you-find-a-use-case-where-re-rankers-improve-metrics","title":"Did you find a use case where re-rankers improve metrics?","text":"<p>Almost every case I've seen shows improvements with re-rankers, whether it's legal documents, question answering over books, or financial documents. A Cohere re-ranker typically improves performance by 6-12% while adding about 400-500ms of latency.</p> <p>Companies like Cohere are building industry-specific rankers that support financial text, medical text, and code. They're working hard to beat OpenAI embeddings, and they generally succeed.</p> <p>Re-rankers solve problems that embeddings miss, like distinguishing between \"I love coffee\" and \"I hate coffee,\" which look similar in embedding space but are clearly different with cross-attention in a re-ranker.</p>"},{"location":"office-hours/week1-summary/#can-you-share-resources-on-how-to-create-hybrid-embeddings-for-postgresql-vector-databases","title":"Can you share resources on how to create hybrid embeddings for PostgreSQL vector databases?","text":"<p>If you use a library called ParagraphDB, you can set up both sparse BM25 indices and dense embedding-based indices. This allows you to implement rank fusion.</p> <p>Pinecone has good resources about this topic that I can share.</p>"},{"location":"office-hours/week1-summary/#for-medicalhealthcare-administration-how-can-we-get-llms-to-be-something-that-are-trustworthy-with-serious-decisions","title":"For medical/healthcare administration, how can we get LLMs to be something that are trustworthy with serious decisions?","text":"<p>One approach is to use chain of thought models where we can read the reasoning to understand how the model arrived at a decision. Anthropic's concern may be that the chain of thought could be misleading.</p> <p>There's likely a future where we can build UIs that let humans verify not only the decision but also the chain of thought behind it. Then we can train models so that even the reasoning aligns with user preferences. If a model gets the right answer but with faulty reasoning, that's where we'd provide feedback.</p> <p>Another approach is to use ensembles\u2014sample a suite of LLMs and use majority voting on decisions to establish confidence. I often train multiple smaller language models to grade things on a 0-1 scale, then use a classical ML model (like logistic regression) to make the final prediction. This helps with explainability because you can see which features influenced the prediction.</p>"},{"location":"office-hours/week1-summary/#for-multimodal-retrieval-text-images-what-approaches-work-best","title":"For multimodal retrieval (text + images), what approaches work best?","text":"<p>For visual content like photographs, CLIP embeddings work well since they're inherently multimodal\u2014they can represent both images and text in the same embedding space.</p> <p>For instructional manuals with images, I'd pass the images to a language model and ask for a detailed summary of what the image shows, including all text in the image. Then embed that summary instead. This creates a text representation that points to the original image.</p> <p>The approach has two steps:</p> <ol> <li>Given an image, create a synthetic question that would retrieve it</li> <li>Create a summary that would be retrieved for that question</li> </ol> <p>For product marketing scenarios, CLIP embeddings can work well, but you need to define what \"similar\" means in your context. Does a red shirt match other red shirts, or just shirts of the same color? Should expensive silk shirts match inexpensive polyester versions?</p> <p>This is why fine-tuning embedding models to understand your specific definition of similarity is important.</p>"},{"location":"office-hours/week1-summary/#how-do-you-approach-chunking-very-long-documents-1500-2000-pages","title":"How do you approach chunking very long documents (1,500-2,000 pages)?","text":"<p>If you have extremely long documents, I'd first try a page-level approach to determine if answers typically exist on a single page or span multiple pages.</p> <p>One compelling approach is from a paper called RAPTOR. After chunking documents, they recluster the chunks. You embed every page, run a clustering model, and identify concepts that span multiple pages. Then summarize those clusters and use the summaries for retrieval\u2014if the summary is retrieved, you can include all related pages in the context.</p> <p>For metadata, look at your queries to determine what matters. If users frequently ask about publication dates or document authors, those should be included. The needs will become obvious as you analyze user queries\u2014you'll realize what's important and what creates economic value.</p> <p>Generally, if you can reorganize text chunks by clustering and bringing related information together, that's very valuable. For example, with tax law documents where laws are on pages 1-30 and exemptions on page 50, you could process the document once to place exemptions directly below the relevant laws. This preprocessing step might cost $10 of LLM calls per document, but for legal documents that might not change for years, it's worth the investment.</p>"},{"location":"office-hours/week1-summary/#do-you-have-a-go-to-approach-for-visual-document-image-embeddings-like-quarterly-reports-with-tables-images-graphs","title":"Do you have a go-to approach for visual document image embeddings (like quarterly reports with tables, images, graphs)?","text":"<p>For visual documents like quarterly reports full of tables and images:</p> <ol> <li>Dockling is a free library that works quite well, though it might take about 11 seconds per PDF</li> <li>Claude Sonnet also works well for extraction</li> <li>Reducto, Llama Parse, and other commercial tools can be worth the cost to save time</li> <li>For multilingual content, VDR2B-Multi v1 handles multiple languages well</li> </ol> <p>There's an ongoing discussion about using Gemini 2 (with its million-token context window) to convert documents to markdown and extract all the information. This approach is becoming more viable as models improve, potentially reducing the engineering needed for preprocessing.</p> <p>Recent testing shows Reducto still has higher accuracy (0.9 \u00b1 0.1) compared to Gemini (0.84 \u00b1 0.16), but the gap is narrowing. The reason Reducto performs so well is that they have people manually labeling thousands of PDFs to train their models.</p>"},{"location":"office-hours/week1-summary/#why-at-meta-did-you-prefer-sql-databases-over-graph-databases","title":"Why at Meta did you prefer SQL databases over graph databases?","text":"<p>Graph databases are useful when you need complex traversals, like finding all of Jason's followers who follow a specific account, then finding what they like, and sorting by aggregated likes per product.</p> <p>However, what we found is that most use cases are actually simpler\u2014often just requiring 2-3 left joins in SQL rather than complex graph traversals. From a skills perspective, it's easier to hire people who know SQL well than to find graph database experts.</p> <p>At scale, graphs are also hard to manage. Around 2017-2018, only LinkedIn had a true graph database because they needed to compute 3rd-degree friendships very quickly. For most companies, SQL databases offer better performance, easier maintenance, and more familiar tooling.</p> <p>Over a 12-year career, we kept trying different technologies (Hadoop, Spark, etc.) but always ended up returning to SQL. The pattern is consistent across many organizations.</p>"},{"location":"office-hours/week1-summary/#what-have-you-learned-about-prompt-caching","title":"What have you learned about prompt caching?","text":"<p>Prompt caching is a technique where language models can avoid reprocessing the beginning of prompts that are often identical.</p> <p>Different providers handle this differently:</p> <ul> <li>Anthropic caches prompts for 5 minutes; if you make the same request within that time, the entire message is cached</li> <li>OpenAI figures out the optimal prefix to cache automatically</li> </ul> <p>This is valuable because it can save significant processing time and costs, especially when you have many few-shot examples or large system prompts. If you have 50+ examples in your prompt, caching can dramatically improve performance.</p> <p>For models like Claude on Bedrock, prompt caching wasn't available a few months ago but is likely coming soon. It's the kind of feature that rolls out gradually across providers.</p>"},{"location":"office-hours/week1-summary/#for-visual-document-image-processing-whats-the-state-of-the-art","title":"For visual document image processing, what's the state of the art?","text":"<p>There's a recent discussion on Hacker News about using Gemini 2 (with its million-token context window) to process documents and convert them to markdown, extracting tables, layout information, and text.</p> <p>The engineering needed for document pre-processing is getting simpler as these models improve. Recent tests show Reducto still has higher accuracy (0.9 \u00b1 0.1) compared to Gemini (0.84 \u00b1 0.16), but the gap is narrowing.</p> <p>Reducto's performance comes from having people manually label thousands of PDFs, then training models on that high-quality data. This reinforces the point that with 6,000-10,000 high-quality labels from your own data, you can train models that outperform even the biggest general models on your specific tasks.</p>"},{"location":"office-hours/week1-summary/#how-does-brain-trust-work-with-the-notebooks-in-this-course","title":"How does Brain Trust work with the notebooks in this course?","text":"<p>Brain Trust just saves the results that your laptop is running locally. It's not executing anything or using a better database\u2014it's more like an observability tool (similar to Datadog).</p> <p>When we run the notebooks, everything is running on your laptop in LanceDB. The only thing Brain Trust sees is row IDs and scores. Think of it as a powerful UI over a database that's saving your logs, not as a computation service.</p>"},{"location":"office-hours/week1-summary/#whats-the-difference-between-bi-encoders-and-cross-encoders","title":"What's the difference between bi-encoders and cross-encoders?","text":"<p>A bi-encoder converts all documents into numbers (embeddings) first, and then the assumption is that when we compare those numbers, documents that look similar are similar. Because we pre-compute everything, we can search very quickly.</p> <p>A cross-encoder doesn't compare numbers\u2014it compares the actual sentences. This approach can't compare a million documents with a million other documents (too expensive), so instead it takes one question and 50 documents and compares each one individually. That's the \"cross\" part of cross-encoder.</p> <p>The advantage of cross-encoders is that a language model can compare words like \"love\" and \"hate\" in \"I love coffee\" and \"I hate coffee\" and understand they're different, whereas bi-encoders just have lists of numbers that don't capture this nuance.</p> <p>We'll cover this topic more deeply in Week 2, but the key takeaway is that bi-encoders are faster but less accurate, while cross-encoders are slower but better at understanding semantic distinctions.</p>"},{"location":"office-hours/week1-summary/#whats-the-process-for-fine-tuning-embedding-models","title":"What's the process for fine-tuning embedding models?","text":"<p>In Week 2, we'll cover this topic extensively. The overall message is that:</p> <ol> <li>It's probably a bad idea to train your own language model</li> <li>It's a very good idea to train your own embedding model</li> </ol> <p>Fine-tuning embedding models is much less resource-intensive\u2014it typically costs around $1.50 and takes about 40 minutes on a laptop. With just 6,000 examples from your domain, you can train embedding models and cross-encoders that outperform general-purpose models on your specific tasks.</p> <p>This is especially useful when you need embeddings to understand domain-specific concepts or when you're trying to define what \"similar\" means in your particular context (e.g., product recommendations where price range matters).</p>"},{"location":"office-hours/week1-summary/#how-do-you-understand-metrics-like-precision-and-recall-in-one-to-one-answer-scenarios","title":"How do you understand metrics like precision and recall in one-to-one answer scenarios?","text":"<p>For questions with exactly one correct answer, these metrics behave somewhat differently. Recall will be either 0% or 100% depending on whether K is large enough to include the correct answer.</p> <p>For example, if we want to retrieve exactly one document and there's only one correct answer, precision could be either 0% or 100%, and the same for recall.</p> <p>The metrics become more meaningful when:</p> <ol> <li>There are multiple relevant documents</li> <li>We're analyzing trends across many queries</li> <li>We're comparing different retrieval methods</li> </ol> <p>Even with one-to-one mappings, MRR (Mean Reciprocal Rank) is still useful to see where the correct answer appears in your results.</p> <p>What really matters isn't the absolute number but whether we can move these metrics in a positive direction with our interventions. It's like weighing yourself\u2014the absolute number may vary by scale, but if you've gained two pounds, you've definitely gained two pounds.</p>"},{"location":"office-hours/week2-summary/","title":"RAG Office Hours Q&amp;A Summary - Week 2","text":""},{"location":"office-hours/week2-summary/#how-would-you-evaluate-the-effect-of-different-parsing-strategies-in-rag-notably-on-documents-with-weird-layouts-tables-and-charts","title":"How would you evaluate the effect of different parsing strategies in RAG, notably on documents with weird layouts, tables, and charts?","text":"<p>For documents with complex layouts like tables and charts, there are multiple levels of evaluation:</p> <p>First, you need to evaluate OCR accuracy - checking whether text is being parsed correctly (e.g., is a 0 being parsed as an 8?). Then there's the bounding box detection problem - checking if tables are fully recognized as single bounding boxes using metrics like intersection over union.</p> <p>It's generally safer to evaluate OCR/parsing and retrieval separately because parsing errors can be hard to trace back when they're part of the full RAG pipeline. If you parse an 8 as a 0 and generate synthetic data from that, you won't be able to capture that error in your evaluations.</p> <p>I've leaned on parsing vendors because they're the most incentivized to have good and accurate labels. This lets me focus on retrieval, which is what will create the most value for my specific use case. While there are other businesses focused on PDF processing, no one will focus specifically on your ability to do retrieval well with your data.</p>"},{"location":"office-hours/week2-summary/#when-does-it-make-sense-to-create-a-targeted-summary-for-an-applications-objective-versus-fine-tuning-embedding-models","title":"When does it make sense to create a targeted summary for an application's objective versus fine-tuning embedding models?","text":"<p>This depends on whether you have data to fine-tune and what your embedding should capture. If you're writing the summary yourself, you're essentially making an assumption about what the embedding should look like.</p> <p>For example, with image embeddings, maybe the most common questions aren't just about what's in the photo but about the cinematic mood. In that case, it might make sense to have a language model create a summary describing the mood because I want to be able to search for \"atmospheric\" or \"dark and gloomy\" rather than just \"trees in a forest.\"</p> <p>However, if you have actual user interaction data, it's better to use that data to tell us what's similar rather than creating assumptions. For example, with blueprint images, an image model might just say \"this is a blueprint,\" but what I specifically did was extract information like number of rooms, bathrooms, sizes, and addresses - information that would be harder for a CLIP embedding to capture.</p> <p>In general, I'd much rather use the data my app generates than hard-code these summaries, but summaries can be useful when you need to extract specific structured information that embedding models might miss.</p>"},{"location":"office-hours/week2-summary/#what-are-the-recommended-approaches-for-evaluating-rag-for-single-documents-like-report-generation-for-a-proposal","title":"What are the recommended approaches for evaluating RAG for single documents, like report generation for a proposal?","text":"<p>When working with a single PDF document that might be varying in length (from 10 to 400 pages), semantic chunking can be valuable to separate paragraphs based on their semantic meaning rather than just token-based chunking. This is especially important when requirements for different disciplines might be found in different sections (e.g., structural requirements mentioned within architectural requirements).</p> <p>One approach is to generate synthetic questions per paragraph, asking \"What are the requirements being mentioned in this paragraph?\" rather than \"What question can you ask from this paragraph?\" This helps identify the key information.</p> <p>For retrieval, you can also inject a summary of the page that a paragraph was extracted from and embed them together. This way, when retrieving, you have both the specific chunk and context about where it comes from, which can improve recall.</p> <p>Whether adding summaries improves recall is an empirical question - if it increases recall by 1%, it might not be worth the extra LLM calls, but if it improves recall by 6-8%, it could be worth investigating further.</p>"},{"location":"office-hours/week2-summary/#could-you-distill-key-reasons-when-someone-should-consider-fine-tuning-open-source-embedding-models-over-proprietary-models","title":"Could you distill key reasons when someone should consider fine-tuning open source embedding models over proprietary models?","text":"<p>If you have 6,000-10,000 examples of question-document relevancy pairs, you can likely outperform closed-source models with a fine-tuned model. This is because your tasks can be much more specific than what general models are optimized for.</p> <p>It can also be more valuable if you need to embed massive datasets at scale. By spinning up your own GPUs, you can process much more text per second at a lower cost. For example, embedding 20GB of text data might take only 15 minutes and cost around $20, whereas using OpenAI APIs would be more expensive and much slower.</p> <p>The main downside is the need to maintain your inference server, which adds complexity. It's less about whether the model will perform well and more about whether you have the time and resources to maintain the infrastructure.</p>"},{"location":"office-hours/week2-summary/#is-there-a-reason-to-ever-fine-tune-the-llm-rather-than-or-in-combination-with-fine-tuning-the-retriever-model","title":"Is there a reason to ever fine-tune the LLM rather than or in combination with fine-tuning the retriever model?","text":"<p>I'm pretty open to businesses fine-tuning their retrieval models because companies like OpenAI or Anthropic aren't primarily focused on making retrieval better - they're not launching new embedding models daily. Companies like Cohere, on the other hand, are actually thinking about retrieval.</p> <p>If you spend effort fine-tuning an LLM, you need to consider inference, CUDA drivers, and whether your fine-tuned model will be competitive when the original model provider releases a new version in a few months.</p> <p>It's generally very costly to fine-tune language models, and you often don't get much benefit. However, if there are specific reasons - like tonality, personalization, or access to proprietary data - it might make sense. But for a team of 4-5 people, it's probably not a good idea to spend effort maintaining that kind of infrastructure.</p> <p>In contrast, fine-tuning embedding models can be done on a laptop, run on cloud instances, and be cost-effective. For most teams, the maintenance cost of running your own LLM is just too high to justify.</p>"},{"location":"office-hours/week2-summary/#one-weakness-of-rag-is-difficulty-in-detecting-relationships-between-concepts-because-the-retriever-model-isnt-aware-of-how-concepts-relate-to-each-other-should-we-fine-tune-the-llm-for-this","title":"One weakness of RAG is difficulty in detecting relationships between concepts because the retriever model isn't aware of how concepts relate to each other. Should we fine-tune the LLM for this?","text":"<p>Before considering fine-tuning the language model, I would ask: How much can we put into few-shot examples in the prompt? Can we come up with good chain-of-thought examples that describe these relationships? Can we provide a glossary?</p> <p>The maintenance cost of running an LLM is so high that it's worth really trying to squeeze out as much as possible through prompt engineering, longer system prompts, more few-shot examples, and prompt caching before considering fine-tuning.</p> <p>For example, Bloomberg spent millions on their own model, and within 5-6 months, GPT-4 was better. Instead of fine-tuning, consider using RAG to retrieve relationship information first, put that in the context, and then add the actual question. This is more maintainable and adaptable as new models are released.</p>"},{"location":"office-hours/week2-summary/#what-is-the-main-failure-modes-like-distribution-mismatch-or-biases-that-youve-seen-when-relying-on-synthetic-data-for-retrieval-fine-tuning","title":"What is the main failure modes (like distribution mismatch or biases) that you've seen when relying on synthetic data for retrieval fine-tuning?","text":"<p>The biggest issue is mismatch between user questions in reality versus in the synthetic data. Once you have synthetic data for fine-tuning retrieval models, it's hard to imagine a case where creating more data for your use case would make the model worse.</p> <p>What's more important is figuring out how to intelligently incorporate real-world examples from users into the few-shot examples for synthetic data generation, making it a more diverse process. You can check this by:</p> <ol> <li>Looking at the variance of embeddings against each other to see if they're too similar</li> <li>Checking general statistics like character count variance in questions</li> <li>Ensuring the synthetic data matches user data characteristics</li> </ol> <p>For example, if your customer questions typically have around 30 characters but your synthetic data averages 90 characters because the language model is too verbose, that's a simple distribution mismatch to fix.</p>"},{"location":"office-hours/week2-summary/#can-you-share-the-intuition-for-the-difference-between-a-fine-tuned-embedding-model-and-a-fine-tuned-re-ranker","title":"Can you share the intuition for the difference between a fine-tuned embedding model and a fine-tuned re-ranker?","text":"<p>The embedding model allows you to do search over a large number of documents - given an embedding model, you might retrieve the top 100 text chunks. The re-ranker model then takes these 100 chunks and finds the best 25.</p> <p>We generally want to use both, and the dataset to train these models is actually the same dataset. If you can only afford to fine-tune one, you might choose based on where your bottleneck is:</p> <ol> <li>Is recall at 100 already good (95%) but recall at 10 is poor (50%)? Then focus on the re-ranker.</li> <li>Are you missing relevant documents even in your top 100 results? Then focus on the embedding model.</li> </ol> <p>The key insight is that by having metrics on both stages, you can identify where to focus your improvement efforts.</p>"},{"location":"office-hours/week2-summary/#do-we-need-more-data-to-fine-tune-re-rankers-than-bi-encoders","title":"Do we need more data to fine-tune re-rankers than bi-encoders?","text":"<p>It depends on the model, but generally, Cohere has done a good job of being data-efficient for producing embedding models. The amount of data needed may vary by model and task.</p>"},{"location":"office-hours/week2-summary/#for-collaborative-filtering-models-how-do-you-address-the-cold-start-problem-new-usersitems-the-model-hasnt-seen-without-retraining-the-model","title":"For collaborative filtering models, how do you address the cold start problem (new users/items the model hasn't seen) without retraining the model?","text":"<p>There are multiple approaches to this. Instead of using classical collaborative filtering models, many systems now build models with user embeddings and item embeddings. The question becomes: can we use some other model to predict the embeddings we would have trained using interaction data?</p> <p>For example, in an e-commerce setting, if we trained our item embeddings using purchase data and a new item comes in, we could train a vision model to predict the embedding of the item based on its image and metadata. We can use that as an initial set of recommendations.</p> <p>The core idea is using other available data to predict the embeddings that would have come from interaction data (like checkout data). This approach helps bridge the gap for new items or users.</p>"},{"location":"office-hours/week2-summary/#how-do-you-handle-rag-for-multimodal-content-like-powerpoint-presentations-that-have-complex-layouts","title":"How do you handle RAG for multimodal content like PowerPoint presentations that have complex layouts?","text":"<p>For documents with complex layouts like PowerPoint presentations, the parsing and chunking processes are linked. You might want to evaluate them separately since parsing errors will be hard to detect in the full RAG pipeline.</p> <p>One approach is to use general-purpose parsing tools like Dockling, Claude Sonnet, or commercial tools like Reducto, Llama Parse, and Extend. For multilingual content, models like VDR2B-Multi v1 handle multiple languages well.</p> <p>Recent developments include using models like Gemini 2 (with its million-token context window) to convert documents to markdown and extract information, though specialized tools like Reducto still have higher accuracy (0.9 \u00b1 0.1 vs. 0.84 \u00b1 0.16 for Gemini). These gaps are narrowing as general models improve.</p>"},{"location":"office-hours/week2-summary/#why-did-you-prefer-sql-databases-over-graph-databases-at-metafacebook","title":"Why did you prefer SQL databases over graph databases at Meta/Facebook?","text":"<p>Graph databases are useful when you need complex traversals, but most use cases only require 2-3 left joins in SQL rather than complex graph operations. From a skills perspective, it's easier to hire people who know SQL well than to find graph database experts.</p> <p>At scale, graphs are also hard to manage. Around 2017-2018, only LinkedIn had a true graph database because they needed to compute 3rd-degree friendships very quickly. For most companies, SQL databases offer better performance, easier maintenance, and more familiar tooling.</p> <p>Over a 12-year career, we kept trying different technologies (Hadoop, Spark, etc.) but always ended up returning to SQL. Most cases don't require more than one or two traversals of your graph, making SQL a more practical choice.</p>"},{"location":"office-hours/week2-summary/#what-have-you-learned-about-prompt-caching","title":"What have you learned about prompt caching?","text":"<p>Prompt caching is a technique where language models can avoid reprocessing the beginning of prompts that are often identical:</p> <ul> <li>Anthropic caches prompts for 5 minutes; if you make the same request within that time, the entire message is cached</li> <li>OpenAI figures out the optimal prefix to cache automatically</li> </ul> <p>This is valuable because it can save significant processing time and costs, especially when you have many few-shot examples or large system prompts. If you have 50+ examples, caching can dramatically improve performance.</p> <p>For models like Claude on Bedrock, prompt caching wasn't available a few months ago but is likely coming soon. It's the kind of feature that rolls out gradually across providers.</p>"},{"location":"office-hours/week2-summary/#whats-the-difference-between-bi-encoders-and-cross-encoders","title":"What's the difference between bi-encoders and cross-encoders?","text":"<p>A bi-encoder converts all documents into numbers (embeddings) first, and then compares those numbers. Because we pre-compute everything, we can search very quickly.</p> <p>A cross-encoder doesn't compare numbers\u2014it compares the actual sentences. This approach can't compare a million documents with a million other documents (too expensive), so instead it takes one question and 50 documents and compares each one individually.</p> <p>The advantage of cross-encoders is that they can understand semantic distinctions like the difference between \"I love coffee\" and \"I hate coffee,\" whereas bi-encoders just have numeric representations that might miss this nuance.</p> <p>Bi-encoders are faster but less accurate, while cross-encoders are slower but better at understanding semantic distinctions.</p>"},{"location":"office-hours/week2-summary/#whats-the-process-for-fine-tuning-embedding-models","title":"What's the process for fine-tuning embedding models?","text":"<p>It's probably a bad idea to train your own language model, but it's a very good idea to train your own embedding model.</p> <p>Fine-tuning embedding models is much less resource-intensive\u2014it typically costs around $1.50 and takes about 40 minutes on a laptop. With just 6,000 examples from your domain, you can train embedding models and cross-encoders that outperform general-purpose models on your specific tasks.</p> <p>This is especially useful when you need embeddings to understand domain-specific concepts or when you're trying to define what \"similar\" means in your particular context (e.g., product recommendations where price range matters).</p>"},{"location":"office-hours/week2-summary/#what-non-intuitive-things-have-you-learned-about-recommendation-systems","title":"What non-intuitive things have you learned about recommendation systems?","text":"<p>The big insight about recommendation systems is that inventory matters a lot more than the actual algorithm. While Tiktok's algorithm is good, what really allows it to produce great recommendations is the vast amount of content available. Without those videos, you can't do much - and the same applies to RAG.</p> <p>The metadata you have and the inventory you have are much more important than the algorithm itself. For example, if recommendations for \"Greek restaurants near me\" are bad, the solution might be to add more Greek restaurants to your database, not to tweak the algorithm.</p> <p>Similarly, if queries after 7 PM perform poorly, maybe you're missing information about whether restaurants are open. The solution is to collect that data rather than change your algorithms.</p> <p>The hard work in recommendation systems is often: Do we have enough rows in the database? How much content do we have? And for that content, do we have the right metadata?</p>"},{"location":"office-hours/week2-summary/#when-working-with-documents-with-metadata-should-search-and-retrieval-methods-change-based-on-the-level-of-metadata-provided-within-the-queries","title":"When working with documents with metadata, should search and retrieval methods change based on the level of metadata provided within the queries?","text":"<p>Yes, they should. For example, in a construction project, we found people really cared about who made the last edits on legal contracts or who sent particular information. The metadata was very important for queries like \"which contracts did this person send us,\" which function more like SQL queries.</p> <p>Similarly, if you're building information that will be queried across time periods, you probably care about when documents were published and last crawled to determine relevance. A query like \"what is the latest research in physics\" might look at the past 6 months, while \"what is new in AI\" might only look at the past two weeks because it moves so quickly.</p> <p>It comes down to analyzing the queries people are asking and figuring out what creates economic value.</p>"},{"location":"office-hours/week2-summary/#do-you-have-a-go-to-approach-for-visual-document-image-embeddings-like-quarterly-reports-with-tables-images-graphs","title":"Do you have a go-to approach for visual document image embeddings (like quarterly reports with tables, images, graphs)?","text":"<p>For visual documents like quarterly reports full of tables and images:</p> <ol> <li>Dockling is a free library that works quite well, though it might take about 11 seconds per PDF</li> <li>Claude Sonnet also works well for extraction</li> <li>Commercial tools like Reducto, Llama Parse, and others can be worth the cost to save time</li> <li>For multilingual content, VDR2B-Multi v1 handles multiple languages well</li> </ol> <p>Recent testing shows Reducto still has higher accuracy (0.9 \u00b1 0.1) compared to Gemini (0.84 \u00b1 0.16), but the gap is narrowing. Reducto performs well because they have people manually labeling thousands of PDFs to train their models.</p>"},{"location":"office-hours/week2-summary/#how-do-you-handle-multilingual-rag","title":"How do you handle multilingual RAG?","text":"<p>Cohere has put the most effort into multilingual models, with both multilingual local LLMs and embedding models.</p> <p>I recommend figuring out which languages appear in your queries and ensuring your evaluation reflects that distribution. Check whether the models you're considering (Cohere, OpenAI) perform well on these languages.</p> <p>While translation might seem like an option, if it worked well, companies like OpenAI and Cohere would already be using synthetic translation data to improve their language models. To evaluate performance across languages, create synthetic questions in multiple languages and verify whether recall rates differ between languages.</p>"},{"location":"office-hours/week2-summary/#how-do-you-approach-chunking-very-long-documents-1500-2000-pages","title":"How do you approach chunking very long documents (1,500-2,000 pages)?","text":"<p>If you have extremely long documents, start with a page-level approach to determine if answers typically exist on a single page or span multiple pages.</p> <p>One compelling approach is from the RAPTOR paper. After chunking documents, they recluster the chunks by embedding every page, running a clustering model, and identifying concepts that span multiple pages. Then they summarize those clusters and use the summaries for retrieval\u2014if a summary is retrieved, all related pages are included in the context.</p> <p>For metadata, look at your queries to determine what matters. If users frequently ask about publication dates or document authors, those should be included. The needs will become obvious as you analyze user queries.</p> <p>If you can reorganize text chunks by clustering and bringing related information together, that's very valuable. For example, with tax law documents where laws are on pages 1-30 and exemptions on page 50, you could process the document once to place exemptions directly below the relevant laws. This preprocessing step might cost $10 of LLM calls per document, but for legal documents that might not change for years, it's worth the investment.</p>"},{"location":"office-hours/week2-summary/#how-do-you-understand-metrics-like-precision-and-recall-in-one-to-one-answer-scenarios","title":"How do you understand metrics like precision and recall in one-to-one answer scenarios?","text":"<p>For questions with exactly one correct answer, these metrics behave somewhat differently. Recall will be either 0% or 100% depending on whether K is large enough to include the correct answer.</p> <p>For example, if we want to retrieve exactly one document and there's only one correct answer, precision could be either 0% or 100%, and the same for recall.</p> <p>The metrics become more meaningful when:</p> <ol> <li>There are multiple relevant documents</li> <li>We're analyzing trends across many queries</li> <li>We're comparing different retrieval methods</li> </ol> <p>Even with one-to-one mappings, MRR (Mean Reciprocal Rank) is still useful to see where the correct answer appears in your results.</p> <p>What really matters isn't the absolute number but whether we can move these metrics in a positive direction with our interventions.</p>"},{"location":"office-hours/week2-summary/#how-does-a-long-context-window-affect-rag-systems","title":"How does a long context window affect RAG systems?","text":"<p>While having longer context windows allows for more content to be included, there are always tradeoffs with latency and cost. Just because we can fit more in context doesn't mean we should always do so.</p> <p>Like how Amazon could theoretically score every product in their inventory for each user but chooses not to because each 100ms of latency costs them 1% in revenue, we still need to make choices about what to include in context.</p> <p>The battery analogy is apt: iPhone batteries get more powerful every year, but battery life stays the same because we build more power-hungry apps. Similarly, as context windows grow, we'll find ways to use that additional capacity rather than making everything faster or cheaper.</p> <p>There will always be cost, performance, and latency tradeoffs to consider. Having a longer context window doesn't eliminate the need for efficient retrieval - it just changes what problems we can solve.</p>"},{"location":"office-hours/week2-summary/#what-tips-do-you-have-for-making-decisions-about-rag-system-architecture-without-prototyping-everything","title":"What tips do you have for making decisions about RAG system architecture without prototyping everything?","text":"<p>Start by asking for examples of 40-50 questions that customers might ask. Reading these helps build an intuition about what query mechanics need to exist.</p> <p>For example, if questions include \"what's the most recent news?\", you'll need date filters. If queries ask \"who do I talk to about fixing XYZ?\", you need features for finding contacts.</p> <p>This helps identify what metadata you need and whether you can access it. From there, building a demo with tools like LangChain or Llama Index should be quick. You may need to rewrite things later, but if the demo can answer generic questions, that's when you start thinking about synthetic data.</p> <p>The key is getting the system in front of beta testers, collecting feedback, and analyzing what's working and what's not. This helps prioritize the next features to build. If 80% of questions are actually about image search, then that's clearly the next thing to build, regardless of what methodology is trending on Twitter.</p>"},{"location":"office-hours/week2-summary/#how-do-you-optimally-blend-small-pools-of-real-data-with-large-synthetic-data-sets","title":"How do you optimally blend small pools of real data with large synthetic data sets?","text":"<p>Focus on whether blending improves your evaluation suite. If you have 500 real examples, put 250 in your training set and leave 250 for evaluation. Then experiment with different blends of synthetic and real data to see how they perform on your evaluation suite.</p> <p>You might find that as you use more synthetic data, you perform worse on your real user data but better on synthetic data. You can weight these differently - perhaps real data success is worth 1.2 points while synthetic data success is worth 0.9 points - to create a single score for system health.</p> <p>A lot of machine learning is empirical - you can't predict these things ahead of time. You need to run experiments and see what works.</p>"},{"location":"office-hours/week2-summary/#how-do-you-approach-function-calling-for-complex-workflows-that-require-multiple-function-calls","title":"How do you approach function calling for complex workflows that require multiple function calls?","text":"<p>Instead of having the language model immediately execute functions one at a time, prompt it to show the entire plan to the user and potentially ask for confirmation. Have a separate function called \"plan\" where the model says \"Based on this request, I think I'm going to use function 1, then 2, then 3. What do you think?\"</p> <p>When the user clicks yes or no, you've allowed human confirmation of the correct order. Since the plan already exists in the context, it's easier for the model to execute correctly.</p> <p>The second benefit is that user requests, plans, and accept/reject decisions can be used as few-shot examples. You can embed these examples so that next time someone asks a similar question, you can say \"Last time someone asked this, they approved calling functions 1, 2, 3 in this order.\"</p> <p>This helps build a dataset of few-shot examples over plans, making the system more reliable.</p>"},{"location":"office-hours/week2-summary/#how-do-you-constrain-a-rag-system-that-pulls-from-multiple-data-sources","title":"How do you constrain a RAG system that pulls from multiple data sources?","text":"<p>After conducting topic analysis of user questions, you can identify which types of questions you can answer well and which ones you struggle with. For low-percentage, low-performance questions, you might decide to simply decline those queries.</p> <p>For example, if your system doesn't handle contact information well, you could add to your prompt: \"If someone is asking about contact information, say no and tell them to message support.\" This saves face and avoids attempting questions you can't answer well.</p> <p>Conversely, if there are questions you can answer very well (even if they're a small percentage), highlight these as sample questions in your UI to guide users toward queries you're confident in handling.</p> <p>Much of the progress in making systems better comes from improving UI, better educating users about capabilities, and enhancing the quality of your inventory rather than just tweaking algorithms.</p>"},{"location":"office-hours/week2-summary/#do-you-sometimes-use-differently-tuned-embeddings-within-the-same-query","title":"Do you sometimes use differently tuned embeddings within the same query?","text":"<p>Unless you're at massive scale, having multiple embedding models for different content types (like product descriptions vs. comments) probably won't yield enough performance improvement to justify the maintenance cost.</p> <p>There's evidence that having a single unified model trained on all your data performs better than specialized models. In machine translation, we used to train separate models for each language pair, but researchers found that a single model trained to translate all languages performed better than any individual model.</p> <p>The unified model learns something about the underlying system that allows it to handle even rare cases better than specialized models would. The same principle likely applies to embedding models.</p>"},{"location":"office-hours/week2-summary/#how-do-you-reason-about-papers-and-new-research-in-rag-and-llms","title":"How do you reason about papers and new research in RAG and LLMs?","text":"<p>Most papers published weekly aren't that important, and many are just reinventing ideas from decades ago. Instead of flooding yourself with information, focus on running experiments with your own data and solving specific problems.</p> <p>The popular stuff on Twitter is generally reasonable to follow, but even then, much research is a distraction if you're building something. For example, a recent popular paper on \"reasoning powered RAG\" was essentially just using an LLM to judge relevancy pairs in a for loop - something basic that's been around for a while.</p> <p>Rather than chasing the latest research, focus on building strong evaluation suites, analyzing your data, and solving specific problems in your implementation. These are the durable skills that will last throughout your career.</p>"},{"location":"office-hours/week2-summary/#does-a-long-context-window-make-rag-obsolete","title":"Does a long context window make RAG obsolete?","text":"<p>No. Just like Amazon could theoretically score every product in their inventory for each user but chooses not to because each 100ms of latency costs them 1% in revenue, we still need to make choices about what to include in context.</p> <p>The battery analogy is apt: iPhone batteries get more powerful every year, but battery life stays the same because we build more power-hungry apps. Similarly, as context windows grow, we'll find ways to use that additional capacity rather than making everything faster or cheaper.</p> <p>There will always be cost, performance, and latency tradeoffs to consider. Having a longer context window doesn't eliminate the need for efficient retrieval - it just changes what problems we can solve.</p>"},{"location":"office-hours/week2-summary/#how-do-you-handle-the-upkeep-of-documents-that-go-in-and-out-of-scope-or-have-frequent-version-changes","title":"How do you handle the upkeep of documents that go in and out of scope or have frequent version changes?","text":"<p>For \"evergreen\" vs. \"Rhodian\" (frequently changing) documents, include published dates and make sure they're in the context so the language model is aware of them. For example, with HR holiday calendars for different years, include the dates so the model can reason about which is current.</p> <p>Consider implementing both published dates and last modified dates, and be explicit in your function calling to filter on these attributes (e.g., only return documents published or updated in the past year).</p> <p>The key question is how sensitive your model is to low precision, and whether that low precision is mainly happening because of your inability to expire outdated documents.</p>"},{"location":"office-hours/week2-summary/#how-do-you-approach-building-voice-ai-for-outbound-calls-or-structured-conversations","title":"How do you approach building voice AI for outbound calls or structured conversations?","text":"<p>Graph-based models or finite state machines have been very successful in the agent world. In this approach, you're in different states (introduction, data collection, etc.) with different system messages for each state.</p> <p>For example, when collecting payment data to book a meeting, you have logical checks to ensure the date is correct and that you have necessary information like a phone number. The set of function calls available also changes based on the state.</p> <p>Once you have fully successful conversations, you can summarize them and put them back in the system prompt to ensure transitions are more accurate. You can prompt the model with these few-shot examples to improve transition states: \"When I have 5 few shots, my transitions are more accurate. When I have 20 few shots it gets too confused. So now I've picked 15 for now.\"</p> <p>This finite state machine approach has been around for decades and is still very effective, with LLMs improving the transitions between states.</p>"},{"location":"office-hours/week2-summary/#when-working-with-metadata-should-you-include-it-in-the-chunk-or-add-it-separately","title":"When working with metadata, should you include it in the chunk or add it separately?","text":"<p>There are a few approaches:</p> <ol> <li>Embed the string without metadata but add metadata when sending to the LLM</li> <li>Embed the string with metadata included</li> </ol> <p>This is something to test empirically - does including metadata in the embedding hurt retrieval? Cohere's embedding models (like Compass) can embed JSON quite well.</p> <p>Including metadata in chunks is common practice as it allows answering questions like \"who wrote this document\" or \"what's their contact information.\" This metadata can then be used for function calls, such as \"Jason wrote the document 2 weeks ago, it has not been updated since. Here's Jason's email, click to write an email to Jason.\"</p>"},{"location":"office-hours/week2-summary/#how-can-you-best-apply-synthetic-data-generation-to-agent-workflows-with-multiple-tools","title":"How can you best apply synthetic data generation to agent workflows with multiple tools?","text":"<p>Instead of generating synthetic questions, you can generate synthetic queries that would trigger certain function calls. If you have 3-4 different functions, you can create synthetic queries that should call specific functions or combinations of functions.</p> <p>If each individual function call is accurate, then the combined sequence should also be accurate. You can also use planning to improve data generation - create questions that would result in specific functions being called in sequence, then verify that with certain requests, these functions are indeed called by the model.</p> <p>This approach helps ensure reliability across different types of function calling patterns.</p>"},{"location":"office-hours/week2-summary/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Fine-tuning priorities: Fine-tune embedding models, not LLMs. With just 6,000 examples, you can create embedding models that outperform general models on your specific tasks at minimal cost.</p> </li> <li> <p>Inventory matters more than algorithms: Having the right documents and metadata is more important than the algorithm itself. Missing information can't be retrieved no matter how good your algorithm is.</p> </li> <li> <p>Evaluation is empirical: Many decisions about chunking, including metadata, and blending synthetic data should be driven by empirical testing rather than theoretical assumptions.</p> </li> <li> <p>Parsing strategy: For complex documents, consider evaluating parsing/OCR separately from retrieval performance since parsing errors will be difficult to trace in the full pipeline.</p> </li> <li> <p>Function calling with planning: For complex agent workflows, have the model create a plan first and get user confirmation rather than executing functions immediately. This creates training data for future interactions.</p> </li> <li> <p>State machines still work: Graph-based/finite state machine approaches remain effective for structured conversations, with LLMs improving the transitions between states.</p> </li> <li> <p>Metadata inclusion: Include relevant metadata in chunks to answer questions about document properties like authorship, modification dates, and contact information.</p> </li> <li> <p>Long context doesn't eliminate RAG: Despite larger context windows, there will always be latency, cost, and performance tradeoffs that make efficient retrieval necessary.</p> </li> <li> <p>Research pragmatism: Focus on solving specific problems with your data rather than chasing the latest research papers, which often reinvent existing techniques.</p> </li> <li> <p>Cross-encoders vs. bi-encoders: Cross-encoders (re-rankers) understand semantic distinctions better but are slower; bi-encoders (embedding models) are faster but less nuanced. Use both for optimal performance.</p> </li> </ol>"},{"location":"office-hours/week3-summary/","title":"RAG Office Hours Q&amp;A Summary","text":""},{"location":"office-hours/week3-summary/#when-gathering-negative-feedback-from-documents-not-being-found-how-do-we-use-and-validate-the-reliability-of-an-llm-labeler","title":"When gathering negative feedback from documents not being found, how do we use and validate the reliability of an LLM labeler?","text":"<p>When it comes to getting negative feedback that documents were not found, I'd assume we're running into issues around low recall. This might manifest as low re-ranker scores or low cosine similarities with embedding models.</p> <p>What I would do is identify whether the language model itself can identify if documents are irrelevant. We'll need some manual labeling step. With our clients, we generally find questions that emit flags - maybe you tell a language model to always say \"we couldn't find relevant documents\" when it can't find anything.</p> <p>You can then label that as traffic is being processed. We might sample 1% of traffic, and some percentage of that might have that message. That's one level of detection.</p> <p>The second level would be building a streamlit UI where we can manually label whether we agree with the irrelevancy assessment. The hard task is determining if any of 10 text chunks are relevant to a question. The easier task is determining if a single text chunk is relevant to a question. That's easy for a human to do and also pretty easy to prompt for.</p> <p>This approach helps ensure the judgment we're using is aligned with human preferences. There's obviously a big difference between 60% alignment and 95% alignment, but this is a good start for figuring out whether low relevancy is causing the lack of documents.</p>"},{"location":"office-hours/week3-summary/#in-the-segmentation-topic-we-talked-about-inventories-and-capabilities-is-it-realistic-to-do-this-automatically-or-is-it-something-we-have-to-do-manually","title":"In the segmentation topic, we talked about inventories and capabilities. Is it realistic to do this automatically or is it something we have to do manually?","text":"<p>I would generally recommend doing this manually, because it's so important for what your business is trying to do that you need to actually think about these problems.</p> <p>We've delegated so much thinking to language models. If we just think a bit harder about our problem, we often find very specific issues.</p> <p>For example, with a client doing tax law resolution, the first 20 pages were massive articles, and then pages 30-40 were the exemptions to those articles. We spent maybe $20 of LLM calls to rewrite the documents so that the exemptions were close to the relevant articles. Now we have a single page/chunk covering an article and all its exemptions, with references to related articles.</p> <p>We run that job once a week when new tax laws come in. Since we only have about 45 documents we really care about, I'd rather spend the money upfront to get the process right rather than waste customer time requerying data.</p> <p>The real goal isn't to get a number right - it's to figure out what to do next. The AI can't tell us that. Your job isn't to automate this process; you're being paid to figure out what the next intervention should be.</p>"},{"location":"office-hours/week3-summary/#can-you-elaborate-on-your-view-on-rag-versus-recommendations-how-would-you-approach-the-use-case-of-friend-suggestions","title":"Can you elaborate on your view on RAG versus recommendations? How would you approach the use case of friend suggestions?","text":"<p>When you build a recommendation system, there are several steps:</p> <ol> <li>Sourcing - What inventory can I show my customer? In the friends case, this would be all users on the platform.</li> <li>Query - Either your user ID or a question embedding.</li> <li>Scoring - For simple RAG, this is cosine distance of embeddings and maybe re-ranker distance. For friends, it might include mutual connections, location, etc.</li> <li>Filtering - In RAG this might be top 10 results or embeddings greater than a threshold. For friends, filters might include having at least 3 mutual friends, same zip code, etc.</li> <li>Rendering the results</li> </ol> <p>When users take actions (adding/removing friends, opening files, deleting citations), you collect feedback to improve your system. When a language model sees 10 documents but only cites 3, those 3 are likely more relevant. You can use that signal to improve your re-ranker or embedding model.</p> <p>If the user deletes one of those citations, you have a triplet: documents the model thinks are important, plus a negative example. When training, these need to be adjusted accordingly.</p> <p>It's like different levels of signals in e-commerce: liking a product is a weaker signal than adding it to cart, which is weaker than buying it, which is different from buying and returning it. That's your portfolio of data collected over time.</p>"},{"location":"office-hours/week3-summary/#in-the-4th-lecture-you-mentioned-the-formula-expected-value-as-impact-times-the-number-of-queries-times-the-probability-of-success-can-you-explain-more-what-you-mean-by-impact","title":"In the 4th lecture you mentioned the formula expected value as impact times the number of queries, times the probability of success. Can you explain more what you mean by impact?","text":"<p>Impact here is a general term that the Facebook folks like to use. I generally think of impact as economic value.</p> <p>In the construction example I often mention, about 70% of questions were simple things like \"Where do I show up today?\" or \"How thick is the drywall?\" These weren't individually valuable.</p> <p>But we also found a set of questions that were super valuable - around scheduling and figuring out if contracts were signed. These were only about 10% of queries but extremely important. When we asked our clients, they said that preventing one missed contract could save $60,000 in delays.</p> <p>This told us these queries had high economic value, even though they were less frequent. So we invested resources in making sure we could query contracts and schedules to answer that segment.</p> <p>Impact is about how valuable a problem is and how much it's worth, rather than just how frequently it occurs. Every metric you track should enable you to take follow-up action afterward - it's not just about knowing the number.</p>"},{"location":"office-hours/week3-summary/#what-is-the-lifecycle-of-feedback-if-we-improve-the-ui-old-labels-might-be-out-of-date-and-new-data-will-be-labeled-differently-what-is-good-to-keep-versus-letting-go","title":"What is the lifecycle of feedback? If we improve the UI, old labels might be out of date and new data will be labeled differently. What is good to keep versus letting go?","text":"<p>This depends on how much data we have and the blend of that data. If we had a million labels before changing the UI, I'd push hard to keep the new UI somewhat similar to ensure the data we collect remains comparable.</p> <p>If we're really changing things dramatically, there are modeling techniques to control for this. You might pre-train on the old data for your embedding model, then use that as a foundation for training a newer model. You can also control for the source of data in your modeling.</p> <p>You can have different evaluations to verify performance on the old data versus the new data, then choose how to weight those scores. Generally, I'd try to keep things as generic as possible - you don't want a dataset that's too specific and won't generalize.</p> <p>For embedding models specifically, I'd typically include everything, as more data is generally better.</p>"},{"location":"office-hours/week3-summary/#is-it-interesting-to-collect-feedback-not-only-as-thumbs-up-or-thumbs-down-but-let-users-explain-in-text-what-is-wrong-with-the-answer","title":"Is it interesting to collect feedback not only as thumbs up or thumbs down, but let users explain in text what is wrong with the answer?","text":"<p>Yes and no. Thumbs up/down is super useful, and it would be hard to convince me not to use these binary labels. Going to a 5-star scale creates issues where you don't know if users consider 3 or 4 stars to be \"average.\"</p> <p>With free text feedback, you'll face two issues:</p> <ol> <li>Probably less than 10% of users will give a text response. If only 1% of users leave feedback at all, and only 10% of those leave text, you get very little text data, and you don't know how biased that sample is.</li> <li>You likely won't be able to read all the free text, so you'll build clustering models to analyze the feedback - in which case, you might as well just have 5 buttons for the most common issues (too slow, answer too long, format incorrect, etc.).</li> </ol> <p>It's about maximizing data per label. Having buttons for common issues will get you more useful data than open text fields.</p> <p>That said, free text can help you figure out what those buttons should be. For enterprise situations, we include the default buttons plus free text, and when users enter text, we post it to Slack where the team and customer can see it. This shows users their feedback is seen, making them more likely to provide it.</p> <p>But think about how often you've thumbs-downed a ChatGPT response, let alone written why. Most users simply won't take the time.</p>"},{"location":"office-hours/week3-summary/#how-do-you-handle-recall-when-dealing-with-large-knowledge-bases-with-a-messy-topology-near-identical-documents-overlapping-content-hub-pages-etc","title":"How do you handle recall when dealing with large knowledge bases with a messy topology (near-identical documents, overlapping content, hub pages, etc.)?","text":"<p>This is challenging, especially with something like a large software product knowledge base (44,000+ documents) where many people have been adding content, creating overlap and interstitial hub pages.</p> <p>One approach is to build a system where if you retrieve a subset of pages, you can reference the connections. Similar to how e-commerce sites show \"people who viewed this also viewed\" suggestions.</p> <p>As context windows get larger, you could implement a system where if you pull in a page that references other documents, you traverse one level and bring in those referenced documents too.</p> <p>You could also do clustering and summarization. If your repository is very valuable, maybe it costs 10 cents to process a page, but with a budget of 50 cents per query, you could chunk everything, cluster similar content, and then summarize the clusters. This essentially rewrites the knowledge base in a less duplicated way.</p> <p>The more fundamental question is about how you define relevance. Do you have a policy document on what makes a document relevant? Google has a detailed document on what makes a good search result. You need to establish and document your criteria so everyone has the same understanding.</p>"},{"location":"office-hours/week3-summary/#have-you-compared-the-effectiveness-of-classical-and-agent-based-rag-systems-with-capabilities-offered-by-models-like-gemini-flashlight-for-real-projects","title":"Have you compared the effectiveness of classical and agent-based RAG systems with capabilities offered by models like Gemini Flashlight for real projects?","text":"<p>I prefer not to think about systems as \"classical\" versus \"agent-based\" RAG systems. Most RAG systems are essentially function calling in a for-loop or while-loop.</p> <p>The goal is to provide the language model with two things:</p> <ol> <li>Good functions</li> <li>Good indices for each function to query that are well-defined</li> </ol> <p>You want to ensure each index has good recall, each function is useful for the system, and you have good prompts to help the model choose the right function.</p> <p>For real projects, it's not just about question answering but also about tool rendering. Some tool calls define UX elements - like a fitness company chatbot that renders modals for booking calendar events and following up with payment links. This becomes the economically valuable work - not just answering questions but helping the company make money.</p>"},{"location":"office-hours/week3-summary/#whats-the-moat-for-companies-building-rag-systems-when-so-much-is-being-open-sourced","title":"What's the moat for companies building RAG systems when so much is being open-sourced?","text":"<p>I generally think the moat is your labeled data. There probably isn't much difference between various newsfeed algorithms, but the moat is the inventory - the content that's already out there.</p> <p>If you have relationships in a specific sector like construction and can be the first to build connectors and bring in that data, that's an easy moat (though not as defensible).</p> <p>After that, it's about analyzing that data to understand what questions people are asking and building specialized tools for those needs. This is software that LLMs won't replace anytime soon.</p> <p>Then it's understanding what relevance actually means - fine-tuning re-ranking models, training custom embedding models. These are aspects that LLM companies won't compete against.</p> <p>The moat becomes your data - both relevancy data and access to the content itself - plus your understanding of customer needs and workflows. The more you understand what customers are truly trying to do (beyond just answering questions about PDFs), the better your product will be.</p>"},{"location":"office-hours/week3-summary/#in-the-ux-lectures-you-mentioned-that-explicit-copy-instead-of-just-thumbs-updown-can-impact-whether-people-give-feedback-have-you-observed-an-impact-based-on-what-the-copy-actually-says","title":"In the UX lectures, you mentioned that explicit copy instead of just thumbs up/down can impact whether people give feedback. Have you observed an impact based on what the copy actually says?","text":"<p>Absolutely. At Zapier, they asked \"How did we do?\" which was a very vague question that didn't get much feedback.</p> <p>When we A/B tested copy, the version that got 5x more feedback was \"Did we answer your question?\" This was much more specific and focused on the core value proposition, not about latency or formatting. If users said no, we'd follow up with \"Do you have any other feedback? Was it too slow? Was the formatting wrong?\" since we knew those were common failure modes.</p> <p>This not only got more feedback but also correlated better with customer satisfaction. The previous vague question made it hard to identify what was a good or bad answer - some might say we did poorly because we answered correctly but too slowly.</p> <p>At Raycast, our copy now is \"Did we take the correct actions?\" since we're showing function calls like \"Set a 1-hour lunch on my calendar and update my Slack status.\" We show users the sequence of function calls and ask if we're taking the correct actions.</p> <p>The key is that every metric you track should lead to a follow-up action. It's not just about knowing the number.</p>"},{"location":"office-hours/week3-summary/#how-can-we-extract-value-from-templatepre-filled-questions-in-chatbots","title":"How can we extract value from template/pre-filled questions in chatbots?","text":"<p>For a situation like a lawn care subscription company's chatbot where 70% of conversations start with template questions, I'd be curious to understand what the follow-up questions look like. This helps determine if we could create complete guides for common paths.</p> <p>If people start with a certain template question, do their follow-ups cluster in a specific domain? This can help you understand if your example questions are actually helpful or if you should be writing better content to answer these questions more comprehensively.</p> <p>One approach is to use a language model to summarize conversations, identifying what topics come after the template questions. This gives you insight into actual user intents that might be hidden behind that initial templated interaction.</p> <p>You should analyze which topics are economically important by looking at metrics like thumbs up/down data. For instance, we found that many negative ratings come from users who want to talk to a real person but can't easily figure out how to do that.</p> <p>It's also valuable to analyze what products you should recommend based on question patterns. If you're seeing thumbs-down ratings, analyze whether it's because you don't have the right content in your knowledge base, or if there are capabilities you're missing. Often, the solution might be as simple as hiring someone to write targeted content for frequently asked questions.</p>"},{"location":"office-hours/week3-summary/#how-do-you-handle-business-knowledge-translation-like-acronyms-in-rag","title":"How do you handle business knowledge translation (like acronyms) in RAG?","text":"<p>When you have documents that spell everything out formally but users want to query using acronyms (like \"What's the deal with ABC?\"), I'd generally just put this translation knowledge in the prompt unless you have an enormous number of acronyms.</p> <p>If you have fewer than 80 acronyms or terms that need translation, putting them directly in the prompt is the simplest and most effective approach. You only need to explore more complex approaches when you have evidence that this simple solution isn't working.</p> <p>You can also create synthetic data to test how well your system handles these acronym queries, which is usually straightforward to generate.</p>"},{"location":"office-hours/week3-summary/#what-are-the-best-practices-for-chunking-in-rag-systems","title":"What are the best practices for chunking in RAG systems?","text":"<p>The general advice from companies like OpenAI and Anthropic is to start with around 800 tokens with 50% overlap using a sliding window approach. That should be enough to get you started.</p> <p>After that initial setup, the real improvements come from understanding what kinds of questions are being asked and what the answers look like. If most questions can be answered by a single document, focus on improving document search and relevancy rather than chunking. If answers typically come from small paragraphs across many documents, then experiment more with chunking.</p> <p>We've spent weeks doing chunking experiments and often haven't seen significant improvements. It's rarely the case that changing from 500 to 800 tokens suddenly makes everything work better - that would suggest most answers require just a few more sentences in the same document, which is usually not the issue.</p> <p>What's been more helpful is looking at the questions and working backward: What are people trying to do, and what design assumptions can I make to better serve that? For instance, if users are searching for blueprints, maybe summarizing blueprints first would help, or perhaps including text above and below the blueprint, or even applying OCR and building a bounding box model to count rooms.</p> <p>Solve specific problems where you can justify that \"this is 20% of our questions\" - if you make those 20% twice as good, you've improved overall performance by 8%, which is meaningful.</p>"},{"location":"office-hours/week3-summary/#are-xml-tags-still-best-practice-for-prompting-models","title":"Are XML tags still best practice for prompting models?","text":"<p>Yes, we've learned that even the GPT-4 models now perform better with XML formatting. We have internal evaluations from Zenbase showing that XML is good not just for Anthropic models but also for ChatGPT models.</p> <p>The second thing we've found is that you generally want to have all the long context information at the beginning of the prompt - first the goal, then all the documents, with the actual questions at the bottom.</p> <p>Claude's prompt rewriter has been very helpful for showing how to write better prompts. I almost always run my prompts through it first before setting up evaluation suites, as it's a free way to get useful feedback.</p>"},{"location":"office-hours/week3-summary/#how-do-you-handle-tokenization-concerns-with-things-like-wallet-addresses","title":"How do you handle tokenization concerns with things like wallet addresses?","text":"<p>When dealing with data that contains wallet addresses (which are 52 characters of what looks like nonsense), I'd worry less about the tokenization itself and focus more on validation.</p> <p>For example, in situations where we use UUIDs, we reference content with a UUID, and we tell the model to cite everything. We then have an allowlist of valid UUIDs from our data, and we check that any UUID the model outputs exists in that allowlist.</p> <p>So if you have a use case where users ask about wallet IDs, focus on making sure the model can only reference valid wallet IDs from your dataset rather than worrying about how they're tokenized.</p> <p>These days, models aren't typically off by a few characters - they'll either get it right or completely make up new identifiers. Having logical checks in your code is more important than the tokenization strategy.</p> <p>You can also generate synthetic test data where you know which wallet addresses should appear in the answers and ensure there are no hallucinations.</p>"},{"location":"office-hours/week3-summary/#should-we-transform-content-from-narrative-format-to-qa-format-for-better-retrieval","title":"Should we transform content from narrative format to Q&amp;A format for better retrieval?","text":"<p>Yes, massively. This can be very beneficial, especially for a question-answering chatbot.</p> <p>It's already an assumption to think that everything is going to be in the form of a question. For some assistants, it might be more about conversations or past memories. If you know your use case is primarily Q&amp;A, then extracting question-answer pairs from your documents is valuable.</p> <p>You can build a system where when you embed a question, you retrieve the embedding of similar questions, but pull in both the question and its answer. This makes sense if your use cases are mostly Q&amp;A-based rather than narrative requests like \"tell me a story.\"</p> <p>One of the big assumptions in RAG is that the embedding of a question is similar to the embedding of a relevant document, which is actually a massive assumption that doesn't always hold true.</p> <p>To prevent retrieving too many similar question-answer pairs (which could be redundant when getting top-K results), consider doing clustering. You could extract 10 questions per document, then cluster similar questions together and rewrite them to create a more concise, focused knowledge base.</p>"},{"location":"office-hours/week3-summary/#can-you-recommend-any-open-source-libraries-or-tools-for-streaming-uis-and-interstitials","title":"Can you recommend any open source libraries or tools for streaming UIs and interstitials?","text":"<p>I can't necessarily recommend a specific library too strongly because most companies I've worked with have built these themselves. However, if you're in the Python world, using something like FastAPI and server-side events (SSE) API is probably the simplest approach. In the slides, we give an example of what this looks like - you're basically using the yield keyword from Python generators to emit events.</p> <p>If you're using JavaScript and part of the Vercel/React ecosystem, I think Vercel's AI library does a great job of handling structured streaming. Other libraries like LangChain, LlamaIndex, and Instructor also support partial streaming where you can send incomplete JSON to a frontend, which can then rerender it.</p> <p>For interstitials, I've been impressed with what Ankur from BrainTrust has done in their playground. I've reached out to him to ask about recommendations for this.</p> <p>With these tools, the implementation is fairly straightforward. The bigger challenge is often designing a UX that communicates progress effectively. Notion's approach is a good example - when you enter a search query, it shows \"making a search request,\" rewrites the request, then renders documents one by one, and finally shows steps like \"carefully reading documents,\" \"thinking,\" and \"formulating an answer.\" This is really just buying time while showing progress, but it dramatically improves the perceived responsiveness.</p>"},{"location":"office-hours/week3-summary/#why-arent-data-labeling-companies-a-bigger-focus-in-current-ai-discussions","title":"Why aren't data labeling companies a bigger focus in current AI discussions?","text":"<p>This is an interesting historical shift. Around 2018, data labeling was a huge focus because the biggest models were vision models that required massive amounts of labeled data. Vision models aren't very data-efficient - training ImageNet required labeling a million JPEGs. Companies like Scale AI won by excelling at tasks like self-driving car LiDAR labeling.</p> <p>As we've moved to LLMs, two things have changed:</p> <ol> <li>The big winners (like Scale AI) have already established themselves and now focus on large contracts. Smaller players either grew or struggled to find viable business models on smaller contracts.</li> <li>LLMs are much more data-efficient.</li> </ol> <p>The data efficiency of modern LLMs is remarkable. You're better off having 1,000 very high-quality labels to fine-tune a model than 10,000 mediocre labels. This means that instead of outsourcing labeling work, it often makes more sense to have subject matter experts do a one-month project to create the data you need.</p> <p>We're so sample-efficient now that offshore labeling doesn't make economic sense for many use cases, especially when LLMs have been shown to match or exceed the quality of offshore labeling for many tasks. If you have specific legal workflows, you're better off asking the lawyer on your team to do the labeling.</p> <p>The real challenge now is: how do you find people who are smarter than GPT-4 to label data to train the next generation of models? That hiring problem is different from the traditional labeling company approach.</p>"},{"location":"office-hours/week3-summary/#how-do-you-see-re-rankers-evolving-beyond-just-measuring-relevancy","title":"How do you see re-rankers evolving beyond just measuring relevancy?","text":"<p>Right now, most RAG systems only rank based on relevancy between a query and a document. But I think re-rankers will soon incorporate much more side information, similar to what we see in e-commerce recommendation systems.</p> <p>In e-commerce, we have additional rankers for things like price sensitivity, seasonality, and product age to determine if customers prefer trendy or timeless items. This hasn't really happened in the RAG world yet.</p> <p>As AI systems accumulate multiple years of memories about users, figuring out what information to put in context will become much more interesting. Re-rankers won't just measure string similarity between a question and document - they'll likely incorporate user features, environmental features, and contextual information to determine relevance.</p> <p>For example:</p> <ul> <li>Security constraints (only searching documents you have access to)</li> <li>Time/recency components for memories</li> <li>Domain authority when sources disagree</li> <li>User preferences based on past interactions</li> </ul> <p>Even systems like Deep Research might evolve to pull from sources you tend to agree with, or deliberately include sources that challenge your viewpoint. These personalized relevancy signals could dramatically improve RAG systems beyond simple semantic matching.</p>"},{"location":"office-hours/week3-summary/#key-takeaways-and-additional-resources","title":"Key Takeaways and Additional Resources","text":""},{"location":"office-hours/week3-summary/#key-takeaways","title":"Key Takeaways:","text":"<ul> <li>Data quality is becoming more important than ever - good models make data quality the differentiator</li> <li>When collecting feedback, be specific with your questions to increase response rates</li> <li>Focus on economically valuable workflows, not just answering questions</li> <li>For messy knowledge bases, consider clustering and summarization approaches</li> <li>The moat for RAG companies is proprietary data and domain expertise, not algorithms</li> <li>Binary feedback (thumbs up/down) generally gets more responses than free text</li> <li>Always have a clear next action from any metric you collect</li> <li>Focus on impact (economic value) rather than just query volume</li> </ul>"},{"location":"office-hours/week3-summary/#additional-resources","title":"Additional Resources:","text":"<ul> <li>Google Search Relevancy document/policy is a good reference for defining relevance</li> <li>RAPTOR paper for document summarization approaches</li> <li>Week 3-4 content in the course covers more on these topics</li> <li>For prompt rewriting, Claude's prompt rewriter is highly recommended</li> <li>When dealing with streaming UIs and latencies, Notion's approach of showing steps visually is a good reference</li> <li>For friends example in recommendation systems, consider platforms like Facebook's friend recommendation system as reference implementations</li> </ul> <p>Note: I'll continue to add resources and notes from future office hours sessions</p>"},{"location":"office-hours/week4-summary/","title":"RAG Office Hours Q&amp;A Summary - Week 4","text":""},{"location":"office-hours/week4-summary/#what-can-segments-mean-beyond-query-volume-and-satisfaction-values-in-a-rag-system","title":"What can \"segments\" mean beyond query volume and satisfaction values in a RAG system?","text":"<p>Segmentation really depends on who your customers are and what they're trying to do. With a generic chatbot, it's hard to figure out what segmentation means. But if you think about intents of a specific application, you can uncover different patterns.</p> <p>For example, with a nutrition company chatbot, you might discover segments within product search \u2013 different capabilities around understanding deliveries, rescheduling, recurring orders, etc. Data analysis helps figure out what's important to build for the customer.</p> <p>In a construction context, we found segments around:</p> <ul> <li>Users inputting specific project IDs (e.g., \"Tell me about RFC 1257\")</li> <li>Questions about time windows (\"What do I have due today?\" or \"What's happening this week?\")</li> <li>Counting items in documents</li> </ul> <p>The goal of segmentation is to help you figure out what new function tools to build and what workflows might be viable. Another example: for a product that takes screenshots of users' computers, we found 10% of customers asking \"How much time did I spend in this application?\" That's impossible to answer with just screenshots, but we realized we had a Postgres database of all screenshots with timestamps, so we built a specific tool to query, group, and sum that data to answer the question.</p> <p>The key is to find external understanding of your data \u2013 what are you worried about, and if you discover certain properties, what can you do about it?</p>"},{"location":"office-hours/week4-summary/#how-should-we-approach-segmentation-for-chatbots-where-the-output-is-a-whole-conversation-rather-than-just-a-query-response","title":"How should we approach segmentation for chatbots where the output is a whole conversation rather than just a query response?","text":"<p>If you have the compute resources, do similar classification and segmentation over your conversations. You'll uncover different insights beyond just tools.</p> <p>When analyzing queries alone, we're basically asking how well we can execute tools to answer in one generation. By analyzing conversations, we might find segments that tell us:</p> <ul> <li>Users think the chatbot talks too much or not enough</li> <li>Users are frustrated with responses</li> <li>Common patterns in how conversations progress</li> </ul> <p>The general idea is to gain an external understanding of your data \u2013 what properties are you concerned about, and if you discover X% of conversations have a certain property, what action can you take?</p> <p>For example, if you find many users asking the language model to rewrite answers in their own words, should that be part of your system prompt? Analysis might show only 10% want tone matching, while most users actually prefer the AI voice.</p>"},{"location":"office-hours/week4-summary/#what-approaches-do-you-recommend-for-topic-clustering-and-have-you-tried-using-thinking-models-to-generate-clusters","title":"What approaches do you recommend for topic clustering, and have you tried using thinking models to generate clusters?","text":"<p>I generally use what I call \"old school\" approaches \u2013 K-means and DBSCAN. I typically start with the default settings in BERTTopic, which has been very good. The topic modeling goal isn't to uncover topics for production use but to do data analysis that helps you understand your data better.</p> <p>For example, I might take Ada 2 embeddings, use K-means to pick 10-30 clusters, and look at 100 questions per cluster. That might take 2-3 days but teaches you a lot about your data. It's rarely the case that you run topic models and can just use them directly in your business.</p> <p>When working with thinking models for clustering, I still do the initial clustering first because I might have 20 million questions to analyze. I'll cluster that data, find good and bad examples across clusters, and put that into Claude 3.7 or similar models, asking them to:</p> <ul> <li>Name each cluster</li> <li>Provide a short description</li> <li>Give good examples of what belongs in the cluster</li> <li>Provide nuanced examples of what's not in the cluster</li> </ul> <p>This produces a YAML file that I can then use for classification. The language model helps expand our understanding, especially when we can't easily enumerate all possibilities ourselves.</p>"},{"location":"office-hours/week4-summary/#what-are-your-thoughts-on-chunk-size-and-chunk-overlap-is-it-worth-trying-out-different-chunking-strategies","title":"What are your thoughts on chunk size and chunk overlap? Is it worth trying out different chunking strategies?","text":"<p>I generally use 800 tokens with 50% overlap, which is what OpenAI recommends in their blog posts. In my experience, chunking strategies rarely make a significant difference compared to other improvements.</p> <p>There's only a small subset of questions where chunk size makes a difference \u2013 you would need a question that can only be answered by a paragraph where two concepts are exactly 500 tokens apart. Performance gains usually come from better re-ranking, contextual retrieval (where you rewrite text chunks given the entire document), or better filtering and metadata capabilities.</p> <p>I've rarely seen chunk size be the 10% improvement win \u2013 it might be a 1-2% improvement, which could just be noise. I would focus more on contextual retrieval if you have the compute budget for it.</p> <p>For semantic chunking (using an LLM to determine good chunking points), I'm actually pretty convinced that contextual retrieval is better than dynamically chunking. The real question is whether you need to cite things word-for-word (in which case you shouldn't rewrite chunks) or if you just need general question answering.</p> <p>I'd always spend more compute upfront to improve data quality. For example, I worked with a company doing Brazilian tax law with 50 documents, each 600 pages long. I asked, \"Why are you only spending 70 cents to process this PDF? Why not spend $30?\" If you're processing billions of dollars through the system, you should invest in good ingestion.</p>"},{"location":"office-hours/week4-summary/#what-strategies-can-improve-experimentation-speed-when-working-with-rag-systems","title":"What strategies can improve experimentation speed when working with RAG systems?","text":"<p>If you feel like you're not running enough experiments, focus on improving your infrastructure:</p> <ol> <li> <p>Write parallelized code: Many teams are still doing all their tests using for loops. Spending 1-2 hours learning to write parallelized code can dramatically reduce your experimentation time, going from days to hours. Using tools like multiprocessing to hit multiple endpoints simultaneously is much better than having code break on iteration 2,000.</p> </li> <li> <p>Improve data access and understanding: Document how to query your data effectively. It's a waste of time if you write a query to prepare data, and someone comes back a day later saying, \"That's wrong, we actually need to include only last week's data.\"</p> </li> <li> <p>Build modular pipelines: If your entire RAG application is a giant Python file, it will be hard to test. But if each search index is a separate POST request, you can test them individually. This allows you to focus on one component (like an image retriever system) and improve it from 30% to 80% accuracy in one afternoon before integrating it back into your router.</p> </li> <li> <p>Test locally when possible: Create smaller synthetic datasets for quick iteration before running larger tests.</p> </li> </ol> <p>Being able to test components in isolation is crucial for rapid experimentation. A lot of this comes down to good software engineering practices and thoughtful system design.</p>"},{"location":"office-hours/week4-summary/#how-do-you-handle-multiple-languages-in-a-rag-system-especially-when-job-titles-may-be-similar-but-written-differently-across-languages","title":"How do you handle multiple languages in a RAG system, especially when job titles may be similar but written differently across languages?","text":"<p>For multilingual challenges like job titles across different languages, I recommend two approaches:</p> <ol> <li> <p>Metadata extraction and filtering: Build classifiers to add more metadata to your ontology. For example, \"software engineering recruiter\" and \"software engineer\" go into two different classes, allowing you to filter for one and not the other. This improves search precision.</p> </li> <li> <p>Fine-tune embedding models with triplets: Create a dataset with examples like \"software engineer\" (query), \"python developer\" (positive example), and \"software engineering recruiter\" (hard negative). This teaches your model to separate similar-looking job titles that have different meanings.</p> </li> </ol> <p>For handling multiple languages, run tests to see whether translation improves performance. For instance, does your classifier perform better if you translate everything to English first, or if you use the original languages? If translating provides only a 1-2% improvement but requires complex infrastructure to maintain, it might make sense to accept slightly lower performance.</p> <p>If you lack training data for certain languages, consider using synthetic data creation. Use $2,000 of API credits to generate examples that cover edge cases in your domain, like distinguishing between \"real estate developer\" and \"python developer\" across languages.</p>"},{"location":"office-hours/week4-summary/#what-are-your-thoughts-on-vision-rag-and-what-databases-would-you-recommend-for-multimodal-embeddings","title":"What are your thoughts on vision RAG, and what databases would you recommend for multimodal embeddings?","text":"<p>Vision RAG isn't talked about as much because it's more expensive and most of the important data is typically in text. That said, there are valuable use cases \u2013 like a company that does RAG over video clips to help movie producers find content, using Gemini Flash to describe what's happening in scenes.</p> <p>For databases, I'd recommend looking at:</p> <ul> <li>ChromaDB</li> <li>LanceDB</li> <li>TurboBuffer (used by Notion and Cursor)</li> <li>PgVector with Scale (for relational data with many reads/writes)</li> </ul> <p>However, I'm finding that pure multimodal embeddings aren't always the best approach anymore. Often it's better to generate a text summary of the image data. For example, when trying to embed images and text in the same space, CLIP embeddings often work worse than just doing image captioning and then embedding that text.</p> <p>In week 5, I'll talk more about this \u2013 there are many things you can't do with multimodal embeddings. They're trained mostly with caption data, which limits their capabilities for certain tasks.</p>"},{"location":"office-hours/week4-summary/#what-are-your-experiences-with-the-model-context-protocol-mcp-and-how-might-it-change-rag-systems","title":"What are your experiences with the Model Context Protocol (MCP) and how might it change RAG systems?","text":"<p>MCP is becoming increasingly important because it allows different systems to connect with each other. When you own all the code, you don't really need MCP since you can just use function calling. But the ability to connect different systems is very compelling.</p> <p>Some interesting examples of MCP usage:</p> <ul> <li>Having an MCP server in Cursor to do image generation while building a video game</li> <li>Creating an MCP server to access network logs for debugging web applications</li> <li>Building MCP servers that connect to production databases so Cursor can understand your schema and write SQL</li> <li>Setting up an MCP server that writes conversation notes to Notion automatically</li> </ul> <p>What makes MCP powerful is that it standardizes these integrations and reduces boilerplate code. The protocol founders explain that it's easy to integrate with other servers when building your own client or server. Instead of rebuilding connectors with databases or services, you can reuse patterns and implementations.</p> <p>Claude 3.7 with Claude Code, for instance, has impressive agent functionality using MCP. It features better context management through commands like \"/compact\" which summarizes conversation history effectively without bloating the context window.</p>"},{"location":"office-hours/week4-summary/#how-can-we-use-synthetic-data-generation-for-summarization-tasks","title":"How can we use synthetic data generation for summarization tasks?","text":"<p>There are many creative ways to generate synthetic data. For summarization, you can:</p> <ol> <li> <p>Create reverse tasks: For example, start with the outcomes you care about (like action items) and ask an LLM to generate a transcript that would produce those items. Then you can verify if your summarization system correctly extracts the original action items from this synthetic transcript.</p> </li> <li> <p>Use data augmentation techniques: Look at techniques from other domains like speech detection, where researchers combine clean audio samples to create more complex scenarios (like overlapping speakers). You can apply similar principles to text.</p> </li> <li> <p>Apply transformations similar to image processing: In computer vision, we've long used techniques like converting color photos to black and white, then training models to predict the original colors. Similarly, we convert high-resolution images to low-resolution and train models to predict the original resolution. We can apply similar transformations to text data.</p> </li> </ol> <p>The key is to think about ways to go from your desired output backward to input data, or to systematically transform existing data in ways that preserve the information you care about while changing other aspects.</p>"},{"location":"office-hours/week4-summary/#when-using-structured-outputs-with-few-shot-prompts-should-the-examples-use-the-exact-same-json-schema-or-can-they-be-plain-text","title":"When using structured outputs with few-shot prompts, should the examples use the exact same JSON schema or can they be plain text?","text":"<p>I would almost always try to keep the JSON format consistent in your few-shot examples. This is somewhat superstitious, but I feel like the attention mechanism will always attend better to similar tokens.</p> <p>The schema itself is probably not what's going to break things these days. More likely, problems will arise from unintended properties of your examples. For instance, if all your action items in the few-shot examples are very short (under 4 words), your outputs will tend to be very short too. The examples communicate that these properties are correlated.</p> <p>I'd rather keep everything in JSON because there will be other random issues that come up. The only caution is to make sure you have checks in place so that when the language model has nothing in the context, it won't just automatically recite the few-shot examples.</p> <p>For complex contexts (like insurance claims that require understanding policies and forms), if including the context for each few-shot example would make your context window explode, consider few-shotting the thinking more importantly. Show examples of the reasoning process: \"I noticed the customer said they had 28 people, and our pricing page has different pricing for teams with less than 30 employees, so I'll use that pricing tier and mention they could get a better price with more employees...\"</p>"},{"location":"office-hours/week4-summary/#how-do-you-approach-rag-when-you-have-transcripts-or-unstructured-text-without-clear-paragraph-markers","title":"How do you approach RAG when you have transcripts or unstructured text without clear paragraph markers?","text":"<p>For transcripts without clear paragraph markers, a few approaches work well:</p> <ol> <li> <p>Use diarization models to get speaker tags, which can serve as natural boundaries (each dialog line becomes a chunk)</p> </li> <li> <p>Detect silences in the audio and chunk on those silences</p> </li> <li> <p>Consider the structure of your content - for instance, if it's an interview format, you might know it's always question-answer pairs, so you can embed those pairs together</p> </li> </ol> <p>It ultimately depends on your specific use case. For a general conversation, chunking on silences or using diarization with a sliding window over dialog will work. For job interviews or expert interviews, understanding the structure (question followed by answer) lets you optimize your chunking strategy.</p> <p>If you have mixed domains and raw transcripts without access to the original source, you might need to default to generic approaches like 800 tokens with 40% overlap, then rely more on contextual retrieval techniques.</p>"},{"location":"office-hours/week4-summary/#what-are-your-recommendations-for-building-slide-presentations-with-ai-tools","title":"What are your recommendations for building slide presentations with AI tools?","text":"<p>I've been using AI tools to build academic-style slides with LaTeX and Beamer. My process is:</p> <ol> <li>Load all relevant content into Cursor (in my case, all 6 hours of course transcripts)</li> <li>Create an outline for the presentation</li> <li>Use Claude to extract key case studies and insights from the transcripts</li> <li>Have the LLM generate slides using LaTeX Beamer format</li> <li>Use a simple auto-compiler (built with Watchdog) that recompiles the PDF whenever the file changes</li> </ol> <p>The advantages of this approach:</p> <ul> <li>You can create both slides and a detailed document from the same source</li> <li>The LLM can generate diagrams using TikZ (a graphics creation library)</li> <li>Everything is vector-based so it looks clean at any resolution</li> <li>You can have the LLM add callouts, highlights, and formatting</li> </ul> <p>This approach lets me essentially talk to my slides and have them update in real-time. I can say \"make this section shorter\" or \"add an example about X\" and see the changes immediately in the PDF preview.</p> <p>For those who prefer different formats, you could also try reveal.js for web-based presentations. The key is finding a workflow that lets you focus on content while the AI handles formatting and details.</p>"},{"location":"office-hours/week4-summary/#how-do-ai-coding-tools-compare-claude-code-aider-cursor-windsurf","title":"How do AI coding tools compare (Claude Code, Aider, Cursor, Windsurf)?","text":"<p>There's been significant evolution in AI coding tools, with different strengths and approaches:</p> <ul> <li> <p>Claude Code has impressive agent functionality with excellent context management. It features a \"/compact\" command that summarizes conversation history effectively without bloating the context window. Some users report it's more capable than Cursor for certain tasks, particularly with how it handles context and managing complexity.</p> </li> <li> <p>Aider is a CLI-based tool that gives very low-level control over the files you can edit. It's open source and allows granular control over which models you use at specific points. Some users have migrated from Cursor to Aider due to its flexibility, though it has a steeper learning curve.</p> </li> <li> <p>Cursor is widely used for its UI and integrations. It works well for incremental changes to code and has good MCP integrations, but some find its context management becomes less effective over time on complex projects.</p> </li> <li> <p>Windsurf is particularly good at handling projects with good requirements and system design. It excels at context management over time and keeping track of multiple files in a repository. It's especially valuable for staff engineers and system architects who start with clear system designs.</p> </li> </ul> <p>The key differentiation often comes down to context management - how well the tool maintains an understanding of your entire codebase and project requirements as you work. For complex projects, tools that help document the goals and requirements (like adding branch goals in comments) tend to perform better.</p>"},{"location":"office-hours/week4-summary/#how-do-you-use-deep-research-and-other-search-tools-effectively","title":"How do you use Deep Research and other search tools effectively?","text":"<p>Different search tools serve different purposes depending on context:</p> <ul> <li> <p>Claude's Deep Research works well for technical documentation, business-level competitive analysis, and generating comprehensive memos. Its tone is particularly well-suited for business communications that need minimal editing. Many users leverage it to materialize blog posts or analyses they want to read (e.g., \"Write me a blog post on why someone should look at MCP versus just using the Open API spec\").</p> </li> <li> <p>Grok's Deep Search has different strengths, with some users preferring it for timely news or quick research questions. Interestingly, usage patterns often split between mobile (Grok) and desktop (Claude/OpenAI) platforms based on when and where research is being done.</p> </li> <li> <p>Perplexity offers another approach to deep research, useful for generating product specs and learning resource reports, especially for colleagues without AI engineering backgrounds.</p> </li> </ul> <p>The quality of these tools has advanced to the point where they can effectively replace traditional research methods for many use cases, saving significant time for competitive analyses and technical investigations.</p>"},{"location":"office-hours/week4-summary/#what-makes-lovable-stand-out-for-no-code-app-generation","title":"What makes Lovable stand out for no-code app generation?","text":"<p>Lovable has emerged as a powerful tool for no-code app generation:</p> <ul> <li>It excels at creating fully functional applications with modern UIs from scratch, going beyond simple prototypes to production-ready systems</li> <li>Its deep integration with Supabase provides authentication, real-time features, and database capabilities out of the box</li> <li>Every code change gets pushed to GitHub, allowing developers to fix issues locally in tools like Cursor or Windsurf when needed</li> <li>Each commit creates a preview deployment on Cloudflare, streamlining the development and testing process</li> <li>The tool can implement complex features like row-level security, push notifications, and real-time commenting systems using websockets</li> </ul> <p>Users report that Lovable outperforms alternatives like V0 and Bolt for creating complete applications, though it can be expensive ($200+ for complex projects). The tight integration with Supabase is particularly valuable, with many users becoming paid Supabase customers after using Lovable to build their applications.</p>"},{"location":"office-hours/week4-summary/#what-emerging-techniques-are-promising-for-handling-long-documents-in-rag","title":"What emerging techniques are promising for handling long documents in RAG?","text":"<p>Handling long documents effectively is still evolving, with several promising approaches:</p> <ol> <li> <p>Hierarchical retrieval: Create summary or header-level embeddings for entire documents/chapters, then more granular embeddings for sections/paragraphs. This allows multi-stage retrieval that narrows down from document to specific passages.</p> </li> <li> <p>Graph-based approaches: Build knowledge graphs connecting concepts across documents, enabling retrieval that follows conceptual relationships rather than just lexical similarity.</p> </li> <li> <p>Hybrid sparse-dense retrieval: Combine embedding-based retrieval with keyword/BM25 approaches to capture both semantic and lexical matches, which is particularly valuable for documents with specialized terminology.</p> </li> <li> <p>Learning to rewrite: Train models to rewrite retrieved chunks into more coherent contexts that preserve the key information while eliminating redundancy.</p> </li> <li> <p>Recursive summarization: For extremely long documents, apply recursive summarization techniques that gradually compress information while maintaining key details.</p> </li> </ol> <p>Projects like LangChain's Document Transformer framework and repositories focusing on document processing show significant advances in these areas. The most effective systems often combine multiple approaches based on the specific characteristics of their document collections.</p>"},{"location":"office-hours/week4-summary/#how-can-i-approach-rag-for-messy-knowledge-bases-with-duplicate-documents","title":"How can I approach RAG for messy knowledge bases with duplicate documents?","text":"<p>When dealing with messy knowledge bases that contain duplicate or near-duplicate documents:</p> <ol> <li> <p>Pre-processing pipeline: Implement de-duplication strategies during ingestion. This could involve computing similarity scores between documents and merging or filtering based on a threshold.</p> </li> <li> <p>Metadata extraction and filtering: Add more metadata to your ontology by building classifiers for different document types or topics. This allows you to filter for specific categories during retrieval.</p> </li> <li> <p>Query classification: For ambiguous queries, implement both pre-retrieval and post-retrieval classification to identify query intent and determine when clarification is needed.</p> </li> <li> <p>Progressive disclosure: Consider displaying intermediate results with summarized information about potential topics before generating a complete answer. This helps users navigate ambiguity, especially for queries that could refer to multiple topics.</p> </li> <li> <p>Dynamic presentation: For high-latency requirements (e.g., responses needed in under 6 seconds), consider showing retrieved documents first while the full answer is being generated, allowing users to see some results immediately.</p> </li> </ol> <p>Remember that the goal isn't perfect retrieval but helping users find the information they need. Sometimes showing multiple possible interpretations of a query is more helpful than trying to guess the single \"right\" answer.</p>"},{"location":"office-hours/week4-summary/#when-is-it-better-to-use-dags-versus-agentic-approaches","title":"When is it better to use DAGs versus agentic approaches?","text":"<p>For specific workflows with well-defined steps, DAGs (Directed Acyclic Graphs) often provide more reliable and predictable results than fully agentic approaches:</p> <ol> <li> <p>Use DAGs when:</p> </li> <li> <p>The workflow has clear, sequential steps</p> </li> <li>You know the process is correct and just need to choose the right workflow</li> <li>You're implementing established protocols (like therapy approaches or compliance processes)</li> <li> <p>Predictability and consistency are critical</p> </li> <li> <p>Use agentic approaches when:</p> </li> <li>The problem space is exploratory</li> <li>Tasks require adaptation to unpredictable user input</li> <li>The workflow needs to evolve based on intermediate results</li> <li>You need to handle a wide variety of open-ended requests</li> </ol> <p>The distinction often comes down to control versus flexibility. DAGs provide more control over the exact process, while agentic approaches offer more flexibility but less predictability.</p> <p>For example, in a therapeutic chatbot following an established CBT protocol, a DAG approach ensures the conversation follows the correct therapeutic sequence. However, for an open-ended research assistant, an agentic approach allows for more dynamic problem-solving.</p>"},{"location":"office-hours/week4-summary/#how-do-i-create-effective-negative-examples-for-training-retrieval-models","title":"How do I create effective negative examples for training retrieval models?","text":"<p>Creating effective negative examples for training retrieval models involves several strategies:</p> <ol> <li> <p>Hard negative mining: Find examples that are semantically similar but actually irrelevant. For job listings, \"software engineer recruiter\" is a hard negative for \"software engineer\" queries - they look similar textually but represent different job categories.</p> </li> <li> <p>Top-K analysis: Run retrieval with your current model, then have an LLM evaluate which results in the top K are actually irrelevant. These make excellent negative examples because they expose weaknesses in your current model.</p> </li> <li> <p>Controlled random sampling: While pure random sampling provides some signal, it's often too easy for the model to distinguish. Instead, use controlled randomization that preserves some properties of the positive examples.</p> </li> </ol> <p>When working with triplet learning (query, positive example, negative example), the quality of your negative examples often has more impact on model performance than adding more positive examples. Focus on finding negative examples that are difficult to distinguish from positive ones.</p> <p>For multimodal or multilingual applications, you may need to create synthetic data, especially for languages with limited training data. This can be done by using LLMs to generate examples that explore edge cases in your domain.</p>"},{"location":"office-hours/week4-summary/#what-strategies-can-improve-response-time-in-rag-systems-with-tight-latency-requirements","title":"What strategies can improve response time in RAG systems with tight latency requirements?","text":"<p>For applications requiring responses in just a few seconds:</p> <ol> <li> <p>Progressive rendering: Show retrieved documents first (which can be returned in 150-400ms) while the LLM generates the complete answer in the background. This gives users immediate results while they wait for the full response.</p> </li> <li> <p>Caching: Implement aggressive caching for common queries. When a question-answer pair receives positive feedback (like being forwarded, shared, or rated highly), save it as a new document that can be quickly retrieved for similar questions.</p> </li> <li> <p>Response type classification: Use a lightweight classifier to determine if a query needs full retrieval and generation or if it can be answered with a simpler approach.</p> </li> <li> <p>Contextual snippet generation: During retrieval, generate quick summaries of each chunk that can be displayed alongside search results before the complete answer is ready.</p> </li> <li> <p>Parallel processing: Run multiple retrieval strategies in parallel and combine the results, rather than using sequential processing that adds to the total latency.</p> </li> </ol> <p>The key insight is to avoid an all-or-nothing approach to response generation. By decomposing the process into steps that can be displayed incrementally, you can significantly improve perceived latency even when the complete answer takes longer to generate.</p>"},{"location":"office-hours/week4-summary/#what-are-your-experiences-with-the-model-context-protocol-mcp","title":"What are your experiences with the Model Context Protocol (MCP)?","text":"<p>MCP (Model Context Protocol) is becoming increasingly important as it allows different AI systems to connect with each other:</p> <ol> <li> <p>Key benefits:</p> </li> <li> <p>Standardizes integrations between AI systems</p> </li> <li>Reduces boilerplate code when connecting to different services</li> <li> <p>Allows models to access data and functionality they wouldn't normally have permission to use</p> </li> <li> <p>Practical examples:</p> </li> <li> <p>Image generation servers in Cursor for creating assets while building applications</p> </li> <li>Servers that connect to network logs for debugging web applications</li> <li>Connectors to production databases that help models understand schemas and write SQL</li> <li> <p>Automation tools that write conversation notes directly to Notion or other note-taking systems</p> </li> <li> <p>Comparison to function calling:</p> </li> <li>When you own all the code, function calling may be simpler</li> <li>MCP becomes valuable when connecting separate systems with different permission models</li> <li>Provides a standardized way to expose capabilities across different AI platforms</li> </ol> <p>The protocol is still evolving but shows promise for creating more powerful AI systems by composing specialized components. Some implementations like Claude 3.7 with Claude Code demonstrate how MCP can enable better context management and more sophisticated agent capabilities.</p>"},{"location":"office-hours/week4-summary/#key-takeaways-and-additional-resources","title":"Key Takeaways and Additional Resources","text":""},{"location":"office-hours/week4-summary/#key-takeaways","title":"Key Takeaways:","text":"<ul> <li>The goal of segmentation is to understand customer needs and determine what tools to build next</li> <li>Chunking strategy (800 tokens, 50% overlap) is rarely the bottleneck - focus on contextual retrieval instead</li> <li>For topic modeling, start with BERTTopic defaults and then use thinking models to better understand clusters</li> <li>Spend more compute upfront to improve data quality - particularly for high-value documents</li> <li>Write parallelized code to dramatically speed up experimentation</li> <li>For multilingual RAG, test whether translation improves performance enough to justify the added complexity</li> <li>Consider transforming image content to text summaries rather than using pure multimodal embeddings</li> <li>MCP is becoming increasingly important for connecting different AI systems together</li> <li>Use structured JSON consistently in few-shot examples rather than plain text</li> <li>For slide creation, AI tools can generate both content and formatting in vector-based formats</li> <li>For long documents, consider hierarchical retrieval, graph-based approaches, hybrid sparse-dense retrieval, learning to rewrite, and recursive summarization</li> <li>For messy knowledge bases, implement pre-processing pipeline, metadata extraction and filtering, query classification, progressive disclosure, and dynamic presentation</li> <li>For DAGs versus agentic approaches, use DAGs when the workflow has clear, sequential steps, and use agentic approaches when the problem space is exploratory</li> <li>For negative examples, use hard negative mining, top-K analysis, and controlled random sampling</li> <li>For response time, implement progressive rendering, caching, response type classification, contextual snippet generation, and parallel processing</li> </ul>"},{"location":"office-hours/week4-summary/#additional-resources","title":"Additional Resources:","text":"<ul> <li>BERTTopic: https://maartengr.github.io/BERTopic/index.html</li> <li>MCP Agent: https://github.com/lastmile-ai/mcp-agent</li> <li>Claude Code: https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview</li> <li>RepoPrompt: https://repoprompt.com/</li> <li>Aider CLI coding tool: https://aider.chat/</li> <li>Lovable for no-code app generation with Supabase integration</li> <li>Cursor and Windsurf for AI-assisted coding environments</li> </ul>"},{"location":"office-hours/week5-summary/","title":"RAG Office Hours Q&amp;A Summary - Week 5","text":""},{"location":"office-hours/week5-summary/#how-should-we-handle-excel-files-with-multiple-sheets-and-tables","title":"How should we handle Excel files with multiple sheets and tables?","text":"<p>Handling Excel files with multiple sheets and tables is challenging, and few companies have solved this problem well. Experience from companies like Zapier shows that connecting Excel spreadsheets to automation tools requires many controls to work properly.</p> <p>The recommended approach is implementing checks on uploads to ensure files meet certain criteria. These checks might verify if an Excel file contains a single table, stays within size limits, or contains a specific number of tables. For simpler Excel files with single tables, implementing validation and processing works well, but for more complex files with multiple sheets and scattered tables, exporting to PDF might allow for easier parsing.</p> <p>It's important to segment data and build specific extractors based on the data's structure. Single-table files can go through a dedicated single-table pipeline, while multi-table files might work better through a PDF parsing pipeline.</p> <p>The most practical advice is to focus on simpler problems first and reject more complex ones until better solutions are developed. Solving for the simpler 40% of use cases while avoiding the most complex scenarios can be an effective strategy. Exporting Excel files as CSVs might also provide better compatibility with processing tools in many situations.</p>"},{"location":"office-hours/week5-summary/#what-tools-are-recommended-for-sql-generation","title":"What tools are recommended for SQL generation?","text":"<p>Claude Sonnet has proven effective for generating SQL queries. Success depends heavily on whether the system can retrieve the correct tables and CREATE statements.</p> <p>The key to successful SQL generation is having good descriptions and CREATE statements for the tables, as well as ensuring that embedding and search capabilities properly identify the right tables when needed.</p> <p>A recommended approach from Timescale involves first retrieving relevant tables, then retrieving pre-existing, approved SQL snippets. When both the correct tables and appropriate SQL patterns are in context, the generation process becomes significantly more reliable.</p> <p>The complexity increases with many tables and columns, but focusing on retrieving the correct tables first, then incorporating approved SQL snippets to guide the generation process creates a two-step approach that significantly reduces errors in SQL generation.</p>"},{"location":"office-hours/week5-summary/#what-is-the-linear-adapter-for-embeddings-and-how-does-it-work","title":"What is the Linear Adapter for embeddings and how does it work?","text":"<p>Linear adapters provide a cost-effective way to fine-tune embeddings. An embedding model takes data and produces a vector, with the dot product of two vectors indicating how similar they are. A linear adapter learns how to \"rotate\" these vectors slightly to better align with specific queries.</p> <p>The approach is very economical - if a vector has 500 dimensions, the linear adapter is just a 500 by 500 matrix that multiplies the original vector. This allows for significant improvements in embedding quality with minimal computational cost.</p> <p>Linear adapters can be compared to LoRA (Low-Rank Adaptation), but with key differences. LoRA works between many layers of a neural network, while a linear adapter works only at the end. Additionally, linear adapters can be applied to pre-trained embeddings like those from OpenAI without needing access to the original model weights.</p> <p>This approach enables domain-specific adaptations - for example, creating different adapters for marketing versus sales questions, or specialized adapters for legal, marketing, or tax information. The cost benefit is significant - training a linear adapter typically costs around $12 and can be done quickly, making it much more accessible than full model fine-tuning.</p> <p>Implementation uses the standard fine-tuning process with triplets (question, positive example, negative example), but specifically changes the embedding function for the query. This rotation of vectors into more effective alignments can significantly improve retrieval performance for domain-specific applications.</p>"},{"location":"office-hours/week5-summary/#how-does-partitioning-work-in-retrieval-systems","title":"How does partitioning work in retrieval systems?","text":"<p>Partitioning in retrieval systems refers to how data is organized and segmented, rather than being about individual users. In applications like Cursor, a \"user\" might represent a documentation page. When working with code libraries like Requests, there might be a dedicated Requests index that multiple users access, but the partition is organized around the library package, documentation URL, or codebase.</p> <p>Similarly, in applications like Notion, a \"user\" isn't an individual email account but represents an entire workspace. This means a company's complete Notion workspace would exist in a single index.</p> <p>An interesting approach is partition-specific fine-tuning, which involves using different models to embed questions versus text chunks. Standard fine-tuning uses one model for both the question and the text chunk, but it's possible to fine-tune only one side of that equation. This might involve using the same model to embed all text chunks but having a different model to embed the question.</p> <p>This technique proves particularly valuable in e-commerce settings. One embedding model might identify products that are similar to each other (creating a \"similar products\" carousel), while a different embedding model could identify complementary products (for \"frequently bought together\" recommendations). Both embeddings operate on the same product data but serve different retrieval purposes.</p>"},{"location":"office-hours/week5-summary/#what-is-the-model-context-protocol-mcp-and-how-does-it-differ-from-regular-apis","title":"What is the Model Context Protocol (MCP) and how does it differ from regular APIs?","text":"<p>The Model Context Protocol (MCP) functions like a USB-C connector for AI systems - it's about standardization rather than specific functionality. While the devices that connect may vary, the connection method itself is standardized.</p> <p>The key advantage of MCP over traditional APIs is the separation of client from backend. With a REST API, developers must write specific code to interact with each API, and each application needs to implement these integrations. MCP creates a standardized way for different systems to connect without custom code for each integration.</p> <p>Consider an enterprise example where different teams might focus on different functionality: one team might work on email search while another builds CRM search tools. Both teams can develop their MCP clients, and the chatbot developers can easily integrate both without writing extensive custom code for each system.</p> <p>This standardization means tools built with MCP work across multiple platforms without requiring custom integration code. A tool that works in one MCP-compatible environment (like Cursor) will work in others (like Claude desktop app) with minimal additional effort.</p> <p>Beyond just function execution, MCP also supports resources and prompts. The MCP developer can provide not just functionality but also the prompts needed to use that functionality effectively. This means client developers don't need to write their own prompts for common operations like summarization or action item extraction.</p> <p>This approach significantly reduces the need for glue code and allows developers to build applications without having to own all the client code, making integrations between different AI systems much more seamless.</p>"},{"location":"office-hours/week5-summary/#what-ai-tools-are-recommended-for-daily-work","title":"What AI tools are recommended for daily work?","text":"<p>Claude Code stands out among AI coding tools, particularly for its high-quality prompts. What makes it exceptional is how it handles context and continuity - when asked to write blog posts, it will first analyze existing posts to create a style guide, then reference that guide for every new post it writes.</p> <p>This attention to existing style and consistency makes it particularly effective for content creation tasks. Despite higher costs compared to some alternatives (potentially $100+ per weekend of heavy use), many users find the value justifies the expense, with some noting they would willingly pay $200 monthly for the service.</p> <p>For report generation specifically, there's significant value in tools that provide templated outputs. Ideally, a report generation tool would allow users to standardize formats across different reports - ensuring all market analysis reports follow the same structure, or that candidate evaluation reports maintain consistent formatting rather than varying significantly in format and depth.</p> <p>This points to a broader trend in AI tool development - the need for tools that not only generate content but do so in consistent, predictable formats that align with existing workflows and style guidelines.</p>"},{"location":"office-hours/week5-summary/#what-approaches-are-being-used-for-multimodal-applications","title":"What approaches are being used for multimodal applications?","text":"<p>Multimodal applications combining vision and language models are expanding into specialized domains. One example shared during office hours involved food image analysis, where a system extracts structured data from food photographs.</p> <p>These systems can identify cuisine type, restaurant information, nutritional content, and dietary characteristics like whether a food item is vegan. While acknowledging practical limitations (\"there's a limit to what you can effectively do\"), early experiments show promising results in extracting valuable information from visual food content.</p> <p>This example demonstrates how multimodal AI applications are moving beyond basic image recognition to extract detailed, structured information from visual content. The integration of vision models with language models allows systems to interpret and categorize visual information in ways that support practical applications like dietary tracking or restaurant recommendations.</p>"},{"location":"office-hours/week5-summary/#what-emerging-trends-should-we-be-aware-of","title":"What emerging trends should we be aware of?","text":"<p>Report generation emerges as a particularly important trend in AI applications. The ability to automatically generate structured reports from unstructured data represents significant economic value for organizations.</p> <p>Structured outputs and templates are increasingly valuable, especially for business use cases where standardized formats are essential. The ideal scenario allows for consistency in outputs - ensuring all reports of a specific type follow the same structure rather than varying significantly in format and organization.</p> <p>Several organizations are developing report generation capabilities for internal use, with teams requiring standardized reports on a regular basis. This trend spans multiple industries, with financial due diligence being one area where automated report generation from multiple PDF sources shows particular promise.</p> <p>The growing importance of fine-tuning approaches for handling data from multiple teams or domains also represents a significant trend. As organizations deploy AI systems across different business units, finding ways to effectively fine-tune models while maintaining performance becomes crucial.</p> <p>Report generation capabilities demonstrate how AI can move beyond simple question answering to create significant economic value through structured information synthesis - transforming unstructured data into formatted reports that follow organizational templates and standards.</p>"},{"location":"office-hours/week5-summary/#how-should-we-handle-retrieval-across-multiple-queries-in-a-conversation","title":"How should we handle retrieval across multiple queries in a conversation?","text":"<p>When handling retrieval across multiple queries within the same conversation, the simplest approach is often the most effective. Using function calls for retrieval is recommended, where each call retrieves text chunks and includes information in XML format specifying the context and the question.</p> <p>A key consideration is whether to keep retrieved context from previous queries in the message history for subsequent queries. This requires careful balancing, as including all previous context can consume tokens quickly.</p> <p>The recommended practice is to prompt the retrieval system to always generate fully specified queries. For example, if the first question is \"Where does Jason live?\" and the follow-up is \"What's the population of that city?\", the retrieval system should be prompted to expand the second query to \"What is the population of New York City?\" This approach can be implemented through few-shot examples that demonstrate how to handle conversational context.</p> <p>This strategy works because the model has access to both the previous question and answer in its context, allowing it to formulate complete, self-contained queries even when the user's input is ambiguous or relies on previous context.</p> <p>An additional benefit of this approach is the ability to generate follow-up questions based on retrieved content. For instance, if a text chunk mentions that \"Jason lived in different places when he was younger versus when he was older,\" the system can suggest follow-up questions like \"Where did Jason live when he was younger?\" This not only improves information discovery but also demonstrates to users that there are more interesting questions they could ask about the topic.</p>"},{"location":"office-hours/week5-summary/#what-innovations-are-happening-in-memory-and-context-management-for-agents","title":"What innovations are happening in memory and context management for agents?","text":"<p>Recent innovations in memory and context management for agents focus on creating more dynamic and self-improving systems. Rather than simply saving memories at the end of a conversation, newer approaches incorporate real-time memory creation and utilization during interactions.</p> <p>Frameworks like Letta are incorporating self-editing capabilities alongside memory management. This integration allows agents to refactor their understanding and approach during a conversation rather than only learning from interactions after they conclude.</p> <p>Implementation of these advances requires significant infrastructure changes, as memory layers affect prompt construction and the overall flow of agent interactions. The approach involves creating memories as the conversation progresses, which in turn influences the prompts used in subsequent exchanges.</p> <p>With newer models like Claude 3.7 Sonnet, there's a shift in how tools are used by agents. Similar to the adaptation period seen with Claude Opus or GPT-4, these models require different prompting patterns to effectively utilize tools. Studying how systems like Cloud Code implement their tool use can provide valuable insights for optimizing agent performance with newer models.</p>"},{"location":"office-hours/week5-summary/#what-can-we-learn-from-examining-cloud-codes-approach-to-prompts-and-tools","title":"What can we learn from examining Cloud Code's approach to prompts and tools?","text":"<p>Cloud Code's approach to prompts and tools offers valuable insights for designing effective AI systems. Analysis of the Cloud Code source (extracted from their minified code) reveals highly detailed and carefully structured prompts for various tools.</p> <p>Cloud Code implements a robust workflow where tools are designed to work together cohesively. Each tool's prompt contains specific instructions about how to use other tools first, creating a well-defined sequence of operations. For example, tools may include instructions to check for uniqueness before proceeding or to use specific validation approaches.</p> <p>The prompts are remarkably detailed, with extensive instructions for common operations. For instance, batch tools contain comprehensive guidelines just for creating pull requests. This level of specificity helps ensure consistent and reliable performance.</p> <p>Another notable aspect is Cloud Code's implementation of specialized tools like a notebook tool, showing how diverse functionality can be incorporated into a unified system. The prompts demonstrate that Claude is capable of working with numerous tools simultaneously when the system is properly designed.</p> <p>This examination highlights the importance of thoughtful prompt engineering and tool design in building effective AI systems. By providing clear, detailed instructions and establishing well-defined workflows between tools, systems can achieve more reliable and sophisticated functionality.</p>"},{"location":"office-hours/week5-summary/#how-can-we-create-automated-evaluation-reports-and-insights-for-rag-systems","title":"How can we create automated evaluation reports and insights for RAG systems?","text":"<p>Automated evaluation reports can significantly enhance RAG system development by providing structured insights and clear next steps. A comprehensive approach involves building a pipeline that:</p> <ol> <li>Takes validation datasets and runs them through the RAG system</li> <li>Computes various metrics (correctness, citation accuracy, URL validity)</li> <li>Generates visualizations segmented by topic, question type, and other dimensions</li> <li>Uses LLMs to analyze metrics and provide insights</li> <li>Creates recommendations for system improvements</li> </ol> <p>The reports can include both detailed analyses (15+ pages) and condensed slides for easier consumption in meetings. Key components include:</p> <ul> <li>Methodology explanations for stakeholders who may not be familiar with technical details</li> <li>System architecture diagrams</li> <li>Performance visualizations broken down by different segments</li> <li>Statistical analysis of which topics or question types perform well or poorly</li> <li>LLM-generated insights that explain patterns in the data</li> <li>Specific recommendations tied to the codebase</li> </ul> <p>This process can effectively \"close the loop\" in the flywheel of development by identifying specific areas for improvement. For example, the system might recommend improving schema handling, enhancing retrieval tools, or adding more data for underrepresented topics.</p> <p>The insights generated by LLMs analyzing the metrics can often align well with developer intuitions, but having them formally documented provides better clarity for prioritization and communication with stakeholders. These insights can be directly translated into development tickets, creating a streamlined workflow from evaluation to implementation.</p> <p>The ultimate goal is to use these insights to guide the next iteration of development, run the evaluation again, and continue improving through this structured feedback loop.</p>"},{"location":"office-hours/week5-summary/#what-strategies-help-maintain-context-when-working-with-ai-coding-assistants","title":"What strategies help maintain context when working with AI coding assistants?","text":"<p>When working with AI coding assistants like Cursor, maintaining context throughout a development session can be challenging. Several effective strategies have emerged from practical experience:</p> <p>Creating and maintaining to-do lists within project documentation provides an effective way to preserve context. By instructing the AI to update the to-do list after completing each task, you ensure the progress remains in context for subsequent interactions. This approach creates a record of what has been accomplished and what remains to be done.</p> <p>Templates within documentation files help maintain consistent structure across generated content. For example, having templates for different documentation sections ensures the AI follows established patterns when creating new content. This approach allows you to simply prompt the AI to \"make more concept templates that generally look like this,\" maintaining consistency through visual examples rather than complex rules.</p> <p>For more structured workflows, some developers create development plans using models like Claude Opus, which provide a roadmap the AI can follow. This helps prevent the AI from getting lost during implementation or going down unproductive paths.</p> <p>Many developers find that keeping AI agents away from certain code areas (particularly tests) helps maintain structure. This can be accomplished either through explicit instructions or by adding files to the cursor.ignore configuration, which prevents them from being indexed while still allowing the AI to run commands like pytest.</p> <p>Follow-up prompts at the end of interactions help maintain momentum. By asking what else needs to be done or what the next steps are, you encourage the AI to reference the to-do list and continue working on remaining tasks, creating a more cohesive development experience.</p>"},{"location":"workshops/","title":"Workshops","text":"<p>This series of workshops guides you through the complete process of building, evaluating, and continuously improving RAG systems.</p>"},{"location":"workshops/#workshop-series","title":"Workshop Series","text":"<ul> <li>Introduction: Beyond Implementation to Improvement</li> <li>Chapter 1: Starting the Flywheel with Data</li> <li>Chapter 2: From Evaluation to Product Enhancement</li> <li>Chapter 3 Part 1: Design Principles</li> <li>Chapter 3 Part 2: Feedback Collection</li> <li>Chapter 3 Part 3: Iterative Improvement</li> <li>Chapter 4: Understanding Your Users</li> <li>Chapter 5: Building Specialized Capabilities</li> <li>Chapter 6: Unified Product Architecture</li> </ul>"},{"location":"workshops/#workshop-structure","title":"Workshop Structure","text":"<p>Each workshop combines theoretical concepts with practical exercises that you can apply directly to your own RAG implementations. Workshops are designed to be completed sequentially, as each one builds on concepts from previous sessions.</p> <p>Prerequisites</p> <p>These workshops assume basic familiarity with RAG implementations and foundational AI concepts. If you're new to RAG, we recommend reviewing the Introduction before diving into the other chapters.</p>"},{"location":"workshops/chapter0/","title":"Beyond Implementation to Improvement: A Product Mindset for RAG","text":"<p>Chapter Overview</p> <p>In this opening chapter, I introduce you to a fundamental shift in how we approach RAG systems\u2014from static implementations to continuously improving products. You'll discover:</p> <ul> <li>Why the most successful RAG systems are built as products, not just technical implementations</li> <li>How to think about RAG as a recommendation engine wrapped around language models</li> <li>The \"improvement flywheel\" that transforms user interactions into system enhancements</li> <li>How to shift from ad-hoc tweaking to systematic, data-driven improvement</li> <li>The mindset that separates successful AI products from those that stagnate after launch</li> </ul>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#the-product-mindset-why-most-rag-implementations-fail","title":"The Product Mindset: Why Most RAG Implementations Fail","text":"<p>When organizations implement RAG systems, they often approach it as a purely technical challenge. They focus on selecting the right embedding model, vector database, and LLM, then consider the project \"complete\" once these components are integrated and deployed.</p> <p>This approach inevitably leads to disappointment. The system works well for demo queries and simple use cases, but struggles with the complexity and diversity of real-world questions. As users encounter these limitations, they lose trust in the system and engagement drops. Without clear metrics or improvement processes, teams resort to ad-hoc tweaking based on anecdotal feedback.</p> <p>The fundamental issue? They've built a technical implementation, not a product.</p> <p>Throughout my career building AI systems at Facebook, Stitch Fix, and for various clients, I've observed a consistent pattern: the teams that succeed are those that treat their RAG implementations as products that continuously evolve rather than projects that eventually conclude.</p> <p>Consider these contrasting approaches:</p> Implementation Mindset Product Mindset \"We need to implement RAG\" \"We need to solve specific user problems\" Technical metrics (embedding dimensions, context window) User-centered metrics (answer relevance, task completion) Project with a defined endpoint Ongoing system that improves over time Success = working demo Success = sustained user value One-time architecture decisions Evolutionary architecture that adapts Focus on model selection Focus on feedback loops and data collection <p>The product mindset recognizes that launching your RAG system is just the beginning. The real work\u2014and the real value\u2014comes from how you systematically improve it based on user interactions.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#rag-as-a-recommendation-engine","title":"RAG as a Recommendation Engine","text":"<p>Mental Model</p> <p>The most effective way to think about RAG isn't as a pipeline of retrieval, augmentation, and generation steps\u2014it's as a recommendation engine wrapped around language models.</p> <p>This shift in perspective is transformative. When you view RAG as a recommendation system, you naturally focus on the aspects that truly determine performance: selecting the most relevant information to present to the language model.</p> <pre><code>flowchart TD\n    A[User Query] --&gt; B[Query Understanding]\n    B --&gt; C[Multiple Retrieval Paths]\n\n    C --&gt; D[Document Index]\n    C --&gt; E[Image Index]\n    C --&gt; F[Table Index]\n    C --&gt; G[Code Index]\n\n    D --&gt; H[Filtering]\n    E --&gt; H\n    F --&gt; H\n    G --&gt; H\n\n    H --&gt; I[Scoring/Ranking]\n    I --&gt; J[Context Assembly]\n    J --&gt; K[Prompt Construction]\n    K --&gt; L[Generation]\n    L --&gt; M[Response to User]\n\n    M --&gt;|Feedback| A</code></pre> <p>This recommendation engine perspective reveals important insights:</p> <ol> <li> <p>Generation quality is capped by retrieval quality. No amount of prompt engineering can overcome providing the wrong information to the LLM.</p> </li> <li> <p>Different queries need different retrieval strategies. Just as Amazon uses different recommendation algorithms for books versus electronics, your RAG system needs specialized approaches for different query types.</p> </li> <li> <p>User feedback is essential. Recommendation systems learn from interactions\u2014what users click on, purchase, or engage with. Your RAG system should similarly learn from how users interact with responses.</p> </li> <li> <p>Cold start is a significant challenge. Just as Netflix needs to learn your preferences, RAG systems need data to understand what makes a good response for your specific use case.</p> </li> <li> <p>The best recommendations are personalized. As your system evolves, it should adapt to specific user preferences and patterns.</p> </li> </ol> <p>This perspective also explains why many RAG implementations underperform\u2014they're built like simple search engines rather than sophisticated recommendation systems with feedback loops and personalization.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#the-improvement-flywheel-from-static-to-dynamic-systems","title":"The Improvement Flywheel: From Static to Dynamic Systems","text":"<p>At the core of the product mindset is what I call the \"improvement flywheel\"\u2014a systematic process that transforms user interactions into continuous enhancements.</p> <pre><code>graph TD\n    A[Build Basic RAG] --&gt; B[Create Synthetic Evaluation Data]\n    B --&gt; C[Define Metrics]\n    C --&gt; D[Test Hypotheses]\n    D --&gt; E[Deploy &amp; Collect Real User Feedback]\n    E --&gt; F[Categorize &amp; Analyze User Questions]\n    F --&gt; G[Make Targeted Improvements]\n    G --&gt; H[Implement Monitoring]\n    H --&gt; B\n\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style E fill:#bbf,stroke:#333,stroke-width:2px\n    style G fill:#dfd,stroke:#333,stroke-width:2px</code></pre> <p>This flywheel addresses the most common challenges in RAG development:</p> Phase Business Challenge Technical Challenge Flywheel Solution Cold Start No data to guide design decisions No examples to train or evaluate against Generate synthetic questions from contentEstablish baseline metricsCompare architectural approaches Initial Deployment Understanding what users actually need Learning what causes poor performance Instrument application for data collectionImplement feedback mechanismsCapture query patterns and failure modes Growth Prioritizing improvements with limited resources Addressing diverse query types effectively Use topic modeling to segment questionsIdentify highest-impact opportunitiesBuild specialized capabilities for key segments Optimization Maintaining quality as usage scales Combining multiple specialized components Create unified routing architectureImplement monitoring and alertsEstablish continuous improvement processes <p>The beauty of this approach is that each phase feeds into the next, creating momentum that accelerates improvement over time. As you collect more data, you gain clearer insights into what's working and what isn't, allowing you to make increasingly targeted enhancements.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#optimizing-feedback-collection","title":"Optimizing Feedback Collection","text":"<p>The effectiveness of your improvement flywheel depends heavily on how you collect feedback. During our office hours, we discovered that simply changing feedback prompt copy from \"How did we do?\" to \"Did we answer your question?\" increased feedback rates by 5x. This more specific framing focused users on the core value proposition rather than secondary concerns like latency or formatting.</p> <p>When implementing feedback mechanisms, remember that: - Binary feedback (thumbs up/down) generally yields higher response rates than free text fields - For enterprise applications, combining button options with free text and posting responses to Slack creates transparency that encourages more feedback - Every metric you track should lead to a specific follow-up action\u2014it's not just about knowing the number</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#the-system-vs-ad-hoc-approaches","title":"The System vs. Ad Hoc Approaches","text":"<p>A system is a structured approach to solving problems that guides how we think about and tackle challenges. For RAG applications, this includes:</p> <ul> <li>A framework for evaluating technologies</li> <li>A decision-making process for prioritizing development efforts</li> <li>A methodology for diagnosing and improving performance</li> <li>Standard metrics and benchmarks for measuring success</li> </ul> <p>The contrast between systematic and ad-hoc approaches is stark:</p> <pre><code>flowchart LR\n    A[Ad-hoc Approach] --&gt;|Leads to| B[Guesswork &amp; Anxiety]\n    C[Systematic Approach] --&gt;|Leads to| D[Confidence &amp; Progress]\n\n    subgraph \"Ad-hoc Results\"\n    B --&gt;|Results in| E[Inconsistent Outcomes]\n    B --&gt;|Results in| F[Resource Waste]\n    B --&gt;|Results in| G[Unclear Priorities]\n    end\n\n    subgraph \"Systematic Results\"\n    D --&gt;|Results in| H[Measurable Improvements]\n    D --&gt;|Results in| I[Efficient Resource Use]\n    D --&gt;|Results in| J[Clear Priorities]\n    end\n\n    style A fill:#f99,stroke:#333,stroke-width:2px\n    style C fill:#9f9,stroke:#333,stroke-width:2px</code></pre>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#the-cost-of-lacking-a-system","title":"The Cost of Lacking a System","text":"<p>Without a systematic approach, teams face significant challenges:</p> Common Challenge Without a System With a System \"Make the AI better\" Anxiety and guesswork Clear metrics and priority areas Allocating engineering resources Political decisions Data-driven prioritization Evaluating improvement ideas Subjective opinions Objective measurement Communicating progress Vague assertions Concrete metrics and examples Addressing user complaints Reactive firefighting Proactive improvement <p>Having a system frees up mental energy for innovation and problem-solving by creating clarity around what's working, what isn't, and what to focus on next.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#from-engineer-to-product-thinker-the-mindset-shift","title":"From Engineer to Product Thinker: The Mindset Shift","text":"<p>To fully embrace the product mindset for RAG, you need to expand your thinking beyond technical implementation details. Here's what this mindset shift looks like in practice:</p> Technical Implementation Focus Product Development Focus \"Which embedding model has the best performance?\" \"Which embedding approach best solves our users' problems?\" \"How do we implement vector search?\" \"How do we discover which search features matter most to users?\" \"What's the optimal chunk size?\" \"How do we measure whether our chunking approach is working for users?\" \"How do we reduce hallucinations?\" \"How do we build user trust through transparent, accurate responses?\" \"Which model has the best capabilities?\" \"Which capabilities deliver the most value for our use case?\" <p>This shift doesn't mean abandoning technical rigor\u2014quite the opposite. It means applying that rigor to problems that actually matter to your users, guided by data rather than assumptions.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#case-study-the-improvement-flywheel-in-action","title":"Case Study: The Improvement Flywheel in Action","text":"<p>To illustrate the power of this approach, let's look at a real example from my consulting work with a legal technology company:</p> <ol> <li> <p>Initial Implementation: The company built a RAG system to help lawyers search through case law and legal documents. The initial implementation used standard embeddings and chunking, with middling performance.</p> </li> <li> <p>Synthetic Evaluation: We created a dataset of 200 synthetic legal queries with gold-standard answers derived from their knowledge base, establishing baseline metrics showing only 63% retrieval accuracy.</p> </li> <li> <p>Hypothesis Testing: Testing different chunking strategies and embedding models revealed that legal terminology required specialized approaches, boosting performance to 72%.</p> </li> <li> <p>Deployment and Feedback: We implemented explicit feedback buttons and implicit tracking (time spent, copy actions), collecting data on 5,000+ real queries over two months.</p> </li> <li> <p>Pattern Analysis: Topic modeling revealed distinct query categories (case citation, legal definition, procedural question) with varying performance.</p> </li> <li> <p>Targeted Improvements: We built specialized retrievers for each category (citation parser, definition extractor, procedure classifier), pushing overall performance to 87%.</p> </li> <li> <p>Monitoring and Refinement: Continuous tracking showed which query types were growing in popularity and which still needed improvement, guiding ongoing development.</p> </li> </ol> <p>The result wasn't just better technical performance\u2014it was significantly higher user adoption, reduced time spent on research, and ultimately, better legal outcomes for clients.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#what-youll-learn-in-this-book","title":"What You'll Learn in This Book","text":"<p>Throughout this book, I'll guide you through implementing every aspect of the improvement flywheel, with practical examples and code you can adapt to your own projects.</p> <p>Here's what we'll cover in the upcoming chapters:</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#chapter-1-starting-the-flywheel-with-data","title":"Chapter 1: Starting the Flywheel with Data","text":"<p>Learn how to overcome the cold-start problem through synthetic data generation, establish meaningful metrics that align with business goals, and create a foundation for data-driven improvement.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#chapter-2-from-evaluation-to-product-enhancement","title":"Chapter 2: From Evaluation to Product Enhancement","text":"<p>Discover how to transform evaluation insights into concrete product improvements through fine-tuning, re-ranking, and targeted capability development.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#chapter-3-the-user-experience-of-ai","title":"Chapter 3: The User Experience of AI","text":"<p>Explore how to design interfaces that both delight users and gather valuable feedback, creating the virtuous cycle at the heart of the improvement flywheel.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#chapter-4-understanding-your-users","title":"Chapter 4: Understanding Your Users","text":"<p>Learn techniques for segmenting users and queries to identify high-value opportunities and create prioritized improvement roadmaps.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#chapter-5-building-specialized-capabilities","title":"Chapter 5: Building Specialized Capabilities","text":"<p>Develop purpose-built solutions for different user needs, spanning documents, images, tables, and structured data.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#chapter-6-unified-product-architecture","title":"Chapter 6: Unified Product Architecture","text":"<p>Create a cohesive product experience that intelligently routes to specialized components while maintaining a seamless user experience.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#reflection-questions","title":"Reflection Questions","text":"<p>As you prepare for the next chapter, consider these questions about your current approach to RAG:</p> <ol> <li>Are you treating your RAG implementation as a completed project or an evolving product?</li> <li>What mechanisms do you have in place to learn from user interactions?</li> <li>How do you currently measure the success of your RAG application?</li> <li>What processes do you have for prioritizing improvements?</li> <li>How would your approach change if you viewed RAG as a recommendation engine rather than a pipeline?</li> </ol> <p>By shifting from an implementation mindset to a product mindset, you'll move from building RAG systems that work once to creating AI products that continuously improve and deliver increasing value over time.</p> <p>In the next chapter, we'll take the first concrete step in the improvement flywheel: creating synthetic evaluation data to establish a foundation for systematic enhancement.</p> <p>Author Note: This approach has been refined through work with organizations ranging from startups to Fortune 500 companies across diverse domains including legal, financial, healthcare, and e-commerce. While the technical details vary by domain, the fundamental principles of the improvement flywheel remain consistent\u2014focus on users, measure what matters, and systematically enhance based on data rather than assumptions.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter1/","title":"Kickstarting the Data Flywheel with Synthetic Data","text":"<p>Chapter Overview</p> <p>This chapter focuses on starting the improvement flywheel by establishing proper evaluation frameworks:</p> <ul> <li>Understanding common pitfalls in AI development</li> <li>Distinguishing between different types of metrics</li> <li>Creating synthetic data for evaluation</li> <li>Building frameworks to drive systematic improvements</li> <li>Measuring retrieval quality with precision and recall</li> <li>Implementing practical evaluation pipelines</li> </ul>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#introduction","title":"Introduction","text":"<p>Welcome to the first practical chapter on systematically improving RAG applications. This week is all about giving you the tools to kickstart the data flywheel\u2014focusing on evaluations, understanding common mistakes, and using synthetic data to address concerns before you even have users.</p> <p>The main challenge many teams face is knowing where to start when improving RAG applications. Without proper evaluation frameworks, teams often fall into a cycle of making random changes based on intuition rather than data. This chapter aims to break that cycle by establishing data-driven methods for improvement.</p> <p>Key Philosophy</p> <p>\"You can't improve what you don't measure. The goal is not to chase the latest AI techniques blindly, but to establish a flywheel of continuous improvement driven by clear metrics aligned with user outcomes.\"</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#common-pitfalls-in-ai-development","title":"Common Pitfalls in AI Development","text":"<p>When I work with companies building RAG applications, I consistently see the same vicious cycle play out. Let's explore these common pitfalls to ensure you don't fall into the same traps.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#the-reasoning-fallacy","title":"The Reasoning Fallacy","text":"<p>How often do we hear statements like \"we need more complex reasoning\" or \"the model isn't smart enough\"? In my experience, this is rarely the actual problem. Instead, it usually indicates a fundamental lack of user empathy and specificity in the tools we build.</p> <p>Common Mistake</p> <p>When you hear statements like \"we need more reasoning power,\" challenge yourself to answer:</p> <pre><code>- When was the last time we looked at data from customers?\n- When did we last read user feedback?\n- Are we actively asking for feedback?\n</code></pre> <p>The root issue is rarely that we need more complex reasoning\u2014it's that we don't understand what our users actually want. This leads to building generic tools that don't solve specific problems well.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#the-vague-metrics-problem","title":"The Vague Metrics Problem","text":"<p>Many developers unknowingly sabotage their applications by defining vague metrics. Thinking in terms like \"does it look better?\" or \"does it feel right?\" makes it impossible to measure progress objectively.</p> <p>You would be surprised how pervasive this problem is. I've worked with companies valued at $100 million that maintain fewer than 30 evaluation examples. When something changes in their system, they have no way of understanding what actually moved the needle.</p> <p>Without clear metrics, teams end up making random changes, seeing unclear results, becoming frustrated, and continuing the cycle of vagueness. Breaking this cycle requires establishing concrete, measurable objectives.</p> <pre><code>flowchart TD\n    A[Random Changes Based on Intuition] --&gt;|leads to| B[Unclear Results]\n    B --&gt;|creates| C[Frustration &amp; Uncertainty]\n    C --&gt;|triggers more| A\n    D[Concrete Metrics &amp; Evaluation Framework] --&gt;|breaks| A\n    style D fill:#90EE90,stroke:#006400,stroke-width:2px</code></pre>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#building-generic-solutions","title":"Building Generic Solutions","text":"<p>Another common mistake is building very generic solutions for broad problems instead of specific work that delivers economic value. This means focusing on features rather than outcomes.</p> <p>This typically happens because the mandate is too broad or the team has overpromised. The result? A generic tool with 30-40% churn rates that teams are too scared to fully launch because users might lose interest.</p> <p>The solution is clear: try to be world-class in just a few narrow domains and earn complexity as you discover what your customers truly care about. Start specific, then expand gradually based on validated user needs.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#leading-versus-lagging-metrics","title":"Leading versus Lagging Metrics","text":"<p>Understanding the distinction between leading and lagging metrics can transform your approach to improvement. This concept was profound to me when I worked at Facebook, and it applies perfectly to RAG applications.</p> <pre><code>flowchart TD\n    subgraph \"Leading Metrics&lt;br&gt;(Controllable Inputs)\"\n        A1[Number of Experiments Run]\n        A2[Retrieval Precision &amp; Recall]\n        A3[User Feedback Collection Rate]\n        A4[Evaluation Coverage]\n    end\n\n    subgraph \"Lagging Metrics&lt;br&gt;(Business Outcomes)\"\n        B1[User Satisfaction]\n        B2[Application Quality]\n        B3[Churn Rate]\n        B4[Revenue]\n    end\n\n    A1 --&gt;|\"predicts &amp; influences\"| B1\n    A1 --&gt;|\"predicts &amp; influences\"| B2\n    A2 --&gt;|\"predicts &amp; influences\"| B1\n    A2 --&gt;|\"predicts &amp; influences\"| B2\n    A3 --&gt;|\"predicts &amp; influences\"| B1\n    A3 --&gt;|\"predicts &amp; influences\"| B3\n    A4 --&gt;|\"predicts &amp; influences\"| B2\n    A4 --&gt;|\"predicts &amp; influences\"| B3\n\n    B1 --&gt;|\"eventually impacts\"| B3\n    B1 --&gt;|\"eventually impacts\"| B4\n    B2 --&gt;|\"eventually impacts\"| B1\n    B3 --&gt;|\"eventually impacts\"| B4\n\n    classDef leading fill:#D4F1F9,stroke:#05445E\n    classDef lagging fill:#FFD700,stroke:#B8860B\n\n    class A1,A2,A3,A4 leading\n    class B1,B2,B3,B4 lagging</code></pre>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#lagging-metrics","title":"Lagging Metrics","text":"<p>Lagging metrics are outcomes that are difficult to improve directly but easy to measure. They're measurements of past results, often unresponsive to immediate changes, and represent the outputs of your system.</p> <p>Examples of lagging metrics include:</p> <ul> <li>Application quality</li> <li>User satisfaction</li> <li>Churn rates</li> <li>Revenue</li> </ul> <p>Think of lagging metrics like your body weight or strength. You can easily measure how much you weigh or how much you can lift, but you can't directly change these factors in the short term.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#leading-metrics","title":"Leading Metrics","text":"<p>Leading metrics, by contrast, are factors you can easily change but might be harder to measure perfectly. They predict future performance and provide feedback on when and where to intervene.</p> <p>Examples of leading metrics include:</p> <ul> <li>Number of experiments run per week</li> <li>Evaluation coverage of different question types</li> <li>Retrieval precision and recall</li> <li>User feedback collection rate</li> </ul> <p>In physical fitness, leading metrics would be calories consumed, workout frequency, and sleep quality. These factors are within your direct control and predictive of future outcomes.</p> <p>The most important leading metric for early-stage RAG applications is simply the number of experiments run. Just like counting calories is more effective for weight management than weighing yourself constantly, tracking experiment velocity is more useful than obsessing over end-user metrics that change slowly.</p> <p>During team meetings, shift focus from asking about the outcomes of experiments to increasing the number of experiments you can run. What infrastructure investments would improve that velocity? How can you brainstorm new ideas to design better experiments? This mindset shift makes all the difference.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#absence-blindness-and-intervention-bias","title":"Absence Blindness and Intervention Bias","text":"<p>Two cognitive biases frequently derail RAG improvement efforts: absence blindness and intervention bias.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#absence-blindness","title":"Absence Blindness","text":"<p>Absence blindness is simple: you don't fix what you can't see. In RAG applications, I see this daily with almost every client. Teams focus obsessively on generation quality and latency\u2014the visible parts of the system\u2014while neglecting to check whether retrieval is even working correctly.</p> <pre><code>graph LR\n    classDef visible fill:#FF9999,stroke:#CC0000\n    classDef invisible fill:#CCCCCC,stroke:#666666,stroke-dasharray: 5 5\n\n    subgraph \"RAG System Components\"\n        A[Data Extraction]:::invisible\n        B[Text Chunking]:::invisible\n        C[Vector Embedding]:::invisible\n        D[Retrieval]:::invisible\n        E[Generation]:::visible\n        F[UI/UX]:::visible\n        G[Response Time]:::visible\n\n        A --&gt; B --&gt; C --&gt; D --&gt; E --&gt; F\n        E -.-&gt; G\n    end\n\n    subgraph \"Team Focus\"\n        H[\"\u2705 High Attention&lt;br&gt;(Visible Components)\"]:::visible\n        I[\"\u274c Low Attention&lt;br&gt;(Invisible Components)\"]:::invisible\n    end</code></pre> <p>Have you verified whether your retrieval brings back the right documents? Are your text chunks properly segmented? Is your data extraction pipeline working as expected? These invisible components often cause the most significant issues but receive the least attention.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#intervention-bias","title":"Intervention Bias","text":"<p>Intervention bias is our tendency to take action just to feel in control, regardless of whether that action is effective. In RAG applications, this manifests as constantly switching between models, adding prompt tweaks, or implementing new features without measuring their impact.</p> <p>Many teams make changes to feel like they're making progress rather than taking specific interventions against specific metrics and testing clear hypotheses. While it feels good to be taking action, randomized changes create technical debt and confuse attribution of improvements.</p> <p>The solution is to establish evaluations that enable methodical improvement\u2014evaluations that reflect and correlate with business outcomes you care about.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#the-rag-flywheel-and-retrieval-evaluations","title":"The RAG Flywheel and Retrieval Evaluations","text":"<p>The basic principle of improving RAG applications is that everything we've learned in search is incredibly relevant to retrieval. If you already have a basic RAG setup, the next step is to bring in synthetic questions that test your system's retrieval capabilities.</p> <pre><code>flowchart TD\n    A[\"1. Evaluate Retrieval&lt;br&gt;with Synthetic Data\"] --&gt;|\"identify gaps\"| B[\"2. Implement&lt;br&gt;Targeted Improvements\"]\n    B --&gt;|\"measure impact\"| C[\"3. Observe&lt;br&gt;Performance Changes\"]\n    C --&gt;|\"collect real data\"| D[\"4. Gather User&lt;br&gt;Behavior &amp; Feedback\"]\n    D --&gt;|\"refine evaluation set\"| A\n\n    style A fill:#FFD700,stroke:#B8860B,stroke-width:2px\n    style B fill:#98FB98,stroke:#006400,stroke-width:2px\n    style C fill:#ADD8E6,stroke:#0000A0,stroke-width:2px\n    style D fill:#FFA07A,stroke:#A52A2A,stroke-width:2px\n\n    %% Arrow styling to create flywheel effect\n    linkStyle 0,1,2,3 stroke-width:2px</code></pre>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#why-prioritize-retrieval-evaluations","title":"Why Prioritize Retrieval Evaluations","text":"<p>Many teams without machine learning backgrounds focus on subjective generation evaluations too early. While language models will continue improving at synthesis, it's our responsibility to improve search and retrieval.</p> <p>Retrieval evaluations offer several advantages over generation evaluations:</p> <ol> <li>Speed: Retrieval tests take milliseconds versus seconds or minutes for generation tests</li> <li>Cost: They're significantly cheaper to run</li> <li>Objectivity: Results are clear and unambiguous</li> <li>Scalability: You can run thousands of tests quickly</li> </ol> <p>When you focus on generation evaluations prematurely, factuality becomes subjective and confusing. As you adopt evaluation tools, you get flooded with information without clear actions. By contrast, improving precision and recall directly allows you to test whether lexical search, semantic search, or re-rankers improve your retrieval in a measurable way.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#understanding-precision-and-recall","title":"Understanding Precision and Recall","text":"<p>Before diving into implementation, let's clarify what precision and recall actually mean in the context of RAG:</p> <pre><code>graph TD\n    subgraph \"Document Universe\"\n        subgraph \"All Relevant Documents\"\n            A[\"Relevant &amp;&lt;br&gt;Retrieved&lt;br&gt;(True Positives)\"]\n            B[\"Relevant but&lt;br&gt;Not Retrieved&lt;br&gt;(False Negatives)\"]\n        end\n\n        subgraph \"All Retrieved Documents\"\n            A\n            C[\"Retrieved but&lt;br&gt;Not Relevant&lt;br&gt;(False Positives)\"]\n        end\n\n        D[\"Not Relevant &amp;&lt;br&gt;Not Retrieved&lt;br&gt;(True Negatives)\"]\n    end\n\n    P[\"Precision = &lt;br&gt;Relevant &amp; Retrieved&lt;br&gt;All Retrieved\"]\n    R[\"Recall = &lt;br&gt;Relevant &amp; Retrieved&lt;br&gt;All Relevant\"]\n\n    A ~~~ P\n    A ~~~ R\n\n    classDef relevant fill:#90EE90,stroke:#006400\n    classDef retrieved fill:#ADD8E6,stroke:#00008B\n    classDef both fill:#9370DB,stroke:#4B0082\n    classDef neither fill:#DCDCDC,stroke:#696969\n    classDef formula fill:#FFFACD,stroke:#8B8B00,stroke-width:2px\n\n    class A both\n    class B relevant\n    class C retrieved\n    class D neither\n    class P,R formula</code></pre> <p>Recall is the percentage of relevant documents that are successfully retrieved. If there are 10 correct documents and your system finds 4 of them, recall is 40%. High recall means your system finds most of the relevant documents, which is crucial when facts are distributed across multiple documents.</p> <p>Precision is the percentage of retrieved documents that are relevant. If your system returns 10 results but only 2 are relevant, precision is 20%. High precision means most retrieved documents are relevant.</p> <p>With advanced models, recall tends to be more important than precision, as these models can better ignore irrelevant information. With simpler models, precision becomes more critical because irrelevant information can cause confusion.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#case-studies-real-world-improvements","title":"Case Studies: Real-World Improvements","text":"<p>Let's examine two case studies where focusing on retrieval metrics led to rapid improvements.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#case-study-1-report-generation-from-expert-interviews","title":"Case Study 1: Report Generation from Expert Interviews","text":"<p>I worked with a company that generates reports from user research interviews. Consultants conduct 15-30 interviews with experts and request AI-generated reports summarizing key findings.</p> <p>Problem: Customers noticed that only a subset of relevant quotes were included in reports. In one case, a consultant knew that 6 experts had expressed similar opinions, but the report only cited 3 of them. This 50% recall rate damaged trust in the system.</p> <p>Approach: We set a goal to dramatically improve recall for these specific use cases. We manually built question-chunk relevance datasets by examining problematic examples and discovered that most issues could be addressed through better pre-processing.</p> <p>Result: By experimenting with text chunking methods before ingestion, we improved recall from 50% to 90% in just a few iterations. This improvement restored customer confidence and established a workflow for continuous enhancement.</p> <p>Key Takeaway: Text pre-processing that aligns with anticipated user queries can dramatically improve retrieval performance.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#case-study-2-blueprint-search-for-construction","title":"Case Study 2: Blueprint Search for Construction","text":"<p>Another client needed to implement AI search for construction blueprints, allowing workers to ask questions about building plans.</p> <p>Problem: Initial tests showed only 27% recall when trying to find the correct blueprint image for specific questions.</p> <p>Approach: We hypothesized that better image captions would improve retrieval. We implemented a visual language model to create detailed captions for blueprints, using chain-of-thought prompting to reason about the images and generate hypothetical questions users might ask.</p> <p>Result: In just four days of experimentation, recall improved from 27% to 85%. This allowed us to launch the feature and collect real user data, which revealed that 20% of queries involved counting objects in blueprints. This insight justified investing in bounding box models to count rooms and features automatically.</p> <p>Key Takeaway: Testing specific subsystems independently enables rapid baseline improvements. Synthetic data generation for specific use cases can dramatically improve retrieval.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#practical-implementation-building-your-evaluation-framework","title":"Practical Implementation: Building Your Evaluation Framework","text":"<p>With synthetic data in hand, you can build a proper evaluation framework. This framework should:</p> <ol> <li>Run questions through your retrieval system</li> <li>Compare retrieved chunks to ground truth</li> <li>Calculate precision and recall metrics</li> <li>Track performance over time as you make changes</li> </ol> <p>This framework becomes the foundation of your improvement flywheel. Every change to your system\u2014whether a new embedding model, chunking strategy, or ranking approach\u2014should be evaluated against these metrics.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#building-a-practical-evaluation-pipeline","title":"Building a Practical Evaluation Pipeline","text":"<p>Let's walk through building a simple but effective evaluation pipeline:</p> <pre><code>def evaluate_retrieval(evaluation_data, retriever_fn, k=10):\n    \"\"\"\n    Evaluate retrieval performance on a dataset.\n\n    Args:\n        evaluation_data: List of dicts with 'question' and 'relevant_docs' keys\n        retriever_fn: Function that takes question text and returns docs\n        k: Number of top results to consider\n\n    Returns:\n        Dict containing evaluation metrics\n    \"\"\"\n    results = []\n\n    for item in evaluation_data:\n        question = item['question']\n        ground_truth = set(item['relevant_docs'])\n\n        # Call retrieval system\n        retrieved_docs = retriever_fn(question, top_k=k)\n        retrieved_ids = [doc['id'] for doc in retrieved_docs]\n\n        # Calculate metrics\n        retrieved_relevant = set(retrieved_ids) &amp; ground_truth\n        precision = len(retrieved_relevant) / len(retrieved_ids) if retrieved_ids else 0\n        recall = len(retrieved_relevant) / len(ground_truth) if ground_truth else 1.0\n\n        # Store individual result\n        results.append({\n            'question_id': item.get('id', ''),\n            'question': question,\n            'precision': precision,\n            'recall': recall,\n            'retrieved_docs': retrieved_ids,\n            'relevant_docs': list(ground_truth),\n            'metadata': item.get('metadata', {})\n        })\n\n    # Aggregate metrics\n    avg_precision = sum(r['precision'] for r in results) / len(results)\n    avg_recall = sum(r['recall'] for r in results) / len(results)\n\n    return {\n        'avg_precision': avg_precision,\n        'avg_recall': avg_recall,\n        'detailed_results': results\n    }\n</code></pre>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#running-regular-evaluations","title":"Running Regular Evaluations","text":"<p>Make evaluation a regular part of your development cycle:</p> <ol> <li>Continuous testing: Run evaluations with every significant change to your system</li> <li>Weekly benchmarks: Schedule comprehensive evaluations weekly</li> <li>Version comparison: Always compare new changes against previous versions</li> <li>Failure analysis: Regularly review cases with 0% recall to identify patterns</li> <li>Difficulty progression: As scores improve, add more challenging test cases</li> </ol>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#integrating-with-development-workflow","title":"Integrating with Development Workflow","text":"<p>To maximize the value of your evaluation framework:</p> <ol> <li>Build a dashboard: Create a simple interface showing metrics over time</li> <li>Implement automatic testing: Run evaluations as part of your CI/CD pipeline</li> <li>Set alert thresholds: Get notified if metrics drop below critical levels</li> <li>Document all experiments: Keep track of changes and their impact on metrics</li> <li>Tie to business metrics: Connect retrieval metrics to business outcomes</li> </ol> <p>Remember that your evaluation framework should evolve with your application. Start simple and add complexity as you gain more insights and collect more data.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#creating-synthetic-data-for-evaluation","title":"Creating Synthetic Data for Evaluation","text":"<p>If you're starting a RAG application without user data, synthetic data generation is your best approach to kickstart improvement. While everyone talks about synthetic data, it's not as simple as asking an LLM for more data. You need to make LLMs create diverse, realistic datasets that reflect potential production traffic.</p> <pre><code>flowchart TD\n    A[\"Document Corpus\"] --&gt; B[\"Sample Representative&lt;br&gt;Text Chunks\"]\n    B --&gt; C[\"LLM-Powered&lt;br&gt;Question Generation\"]\n\n    C --&gt; D1[\"Factual&lt;br&gt;Questions\"]\n    C --&gt; D2[\"Inferential&lt;br&gt;Questions\"]\n    C --&gt; D3[\"Comparative&lt;br&gt;Questions\"]\n    C --&gt; D4[\"Hypothetical&lt;br&gt;Questions\"]\n\n    D1 &amp; D2 &amp; D3 &amp; D4 --&gt; E[\"Verify Retrievability&lt;br&gt;with Current System\"]\n\n    E --&gt; F1[\"Easy Questions&lt;br&gt;(80-90% expected recall)\"]\n    E --&gt; F2[\"Medium Questions&lt;br&gt;(50-70% expected recall)\"]\n    E --&gt; F3[\"Hard Questions&lt;br&gt;(30-50% expected recall)\"]\n\n    F1 &amp; F2 &amp; F3 --&gt; G[\"Comprehensive&lt;br&gt;Evaluation Dataset\"]\n\n    H[\"Real User&lt;br&gt;Questions\"] -.-&gt;|\"As they become&lt;br&gt;available\"| G\n\n    style A fill:#CCCCFF,stroke:#0000AA\n    style C fill:#FFD700,stroke:#B8860B\n    style E fill:#90EE90,stroke:#006400\n    style G fill:#FF7F50,stroke:#A52A2A\n    style H fill:#98FB98,stroke:#006400,stroke-dasharray: 5 5</code></pre>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#basic-approach-question-generation","title":"Basic Approach: Question Generation","text":"<p>The simplest approach is:</p> <ol> <li>Take a random text chunk from your corpus</li> <li>Ask a language model to generate a question that this text chunk would answer</li> <li>Verify that when you search with this question, the original text chunk is retrieved</li> </ol> <p>This creates a basic synthetic dataset to test how well your system can find the right information. Recall becomes a binary metric: either you find the source chunk or you don't.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#generating-diverse-synthetic-data","title":"Generating Diverse Synthetic Data","text":"<p>To make your synthetic data truly valuable, you need diversity. Here are several approaches to ensure comprehensive coverage:</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#1-variation-in-question-types","title":"1. Variation in Question Types","text":"<p>Create questions that test different retrieval capabilities:</p> <ul> <li>Factual questions: Direct questions with answers explicitly stated in the text</li> <li>Inferential questions: Questions requiring connecting multiple pieces of information</li> <li>Comparative questions: Queries that involve comparing different entities or concepts</li> <li>Hypothetical questions: \"What if\" scenarios related to the content</li> <li>Clarification questions: Queries asking for elaboration on specific topics</li> </ul>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#2-linguistic-diversity-techniques","title":"2. Linguistic Diversity Techniques","text":"<p>Vary how questions are phrased:</p> <ul> <li>Paraphrasing: Generate multiple phrasings of the same question</li> <li>Terminology variation: Use synonyms and domain-specific language</li> <li>Complexity levels: Mix simple direct questions with complex compound queries</li> <li>Query length: Include both short keyword-style and verbose natural language questions</li> <li>Formatting diversity: Questions, commands, statements that imply questions</li> </ul>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#3-chain-of-thought-generation","title":"3. Chain-of-Thought Generation","text":"<p>Use chain-of-thought prompting to create more nuanced questions:</p> <pre><code>Given this text chunk:\n[CHUNK]\n\nFirst, identify 3-5 key facts or concepts in this text.\nFor each key concept:\n1. Think about different ways someone might ask about it\n2. Consider various levels of prior knowledge the asker might have\n3. Imagine different contexts in which this information might be relevant\n\nNow, generate 5 diverse questions about this text that:\n- Vary in complexity and format\n- Would be asked by users with different backgrounds\n- Target different aspects of the information\n- Require different types of retrieval capabilities to answer correctly\n\nFor each question, explain your reasoning about why this is a realistic user question.\n</code></pre>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#4-few-shot-prompting-for-domain-specificity","title":"4. Few-Shot Prompting for Domain Specificity","text":"<p>If you have even a handful of real user questions, use them as examples:</p> <pre><code>I'm creating questions that users might ask about [DOMAIN].\nHere are some examples of real questions:\n\n1. [REAL QUESTION 1]\n2. [REAL QUESTION 2]\n3. [REAL QUESTION 3]\n\nHere's a text passage:\n[CHUNK]\n\nPlease generate 5 new questions similar in style and intent to the examples above,\nthat would be answered by this text passage.\n</code></pre>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#5-adversarial-question-generation","title":"5. Adversarial Question Generation","text":"<p>Deliberately create challenging questions:</p> <pre><code>Given this text passage:\n[CHUNK]\n\nGenerate 3 challenging questions that:\n1. Use different terminology than what appears in the passage\n2. Require understanding the implications of the content\n3. Might confuse a basic keyword search system\n4. Would still be reasonable questions a user might ask\n\nFor each question, explain why it's challenging and what makes it a good test case.\n</code></pre>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#building-comprehensive-evaluation-sets","title":"Building Comprehensive Evaluation Sets","text":"<p>Once you're generating diverse questions, structure your evaluation sets strategically:</p> <ol> <li>Coverage mapping: Ensure questions cover all document types and topics</li> <li>Difficulty distribution: Include easy (80-90% expected recall), medium (50-70%), and hard (30-50%) questions</li> <li>Ground truth validation: Have subject matter experts verify questions and relevant documents</li> <li>Domain segmentation: Create separate evaluation sets for different use cases or domains</li> <li>Synthetic/real blending: As you collect real data, gradually blend it with synthetic data</li> </ol>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#using-synthetic-data-beyond-evaluation","title":"Using Synthetic Data Beyond Evaluation","text":"<p>Your synthetic data can serve multiple purposes:</p> <ol> <li>Retrieval benchmarking: Measuring search quality</li> <li>Few-shot examples: Enhancing your LLM's understanding in prompts</li> <li>Fine-tuning data: Training specialized embeddings or rerankers</li> <li>User experience testing: Simulating realistic user sessions</li> </ol> <p>By investing time in creating high-quality synthetic data upfront, you establish a foundation that accelerates every aspect of your RAG development process.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#additional-resources","title":"Additional Resources","text":"<p>!!! info \"Tools and Libraries for RAG Evaluation\" - RAGAS: Open-source framework for evaluating RAG applications - LangChain Evaluation: Tools for evaluating retrieval and generation - Prompttools: Toolkit for testing and evaluating LLM applications - MLflow for Experiment Tracking: Open-source platform for managing ML lifecycle</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#reflection-questions","title":"Reflection Questions","text":"<p>!!! question \"Self-Assessment\" 1. What are your leading and lagging metrics for your RAG application? How do they relate to each other?</p> <pre><code>2. How might you generate more diverse and challenging synthetic questions for your specific domain?\n\n3. Where does your current evaluation framework fall short, and what additional metrics might be valuable?\n\n4. What experiment could you run this week to test a hypothesis about improving retrieval quality?\n\n5. How will you incorporate real user feedback into your evaluation framework as it becomes available?\n</code></pre>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#conclusion-and-next-steps","title":"Conclusion and Next Steps","text":"<p>In this chapter, we've established the foundation for systematically improving your RAG application by focusing on proper evaluation frameworks. Rather than making subjective judgments or random changes, you now have the tools to measure progress objectively and make data-driven decisions.</p> <p>By prioritizing retrieval metrics like precision and recall, you can run more experiments in less time, at lower cost, and with greater confidence in the results. This approach sets the stage for incorporating user feedback and advanced fine-tuning techniques.</p> <p>What's Coming Next</p> <p>In Chapter 2, we'll explore how to convert evaluations into training data for fine-tuning, allowing you to create specialized models that better reflect your unique business needs.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#summary","title":"Summary","text":"<p>Remember, the goal is not to chase the latest AI techniques blindly, but to establish a flywheel of continuous improvement driven by clear metrics aligned with user outcomes. Start with synthetic data, focus on retrieval before generation, and measure everything. These practices will serve as the foundation for all our subsequent improvements to your RAG application.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter2/","title":"Converting Evaluations into Training Data for Fine-Tuning","text":"<p>Chapter Overview</p> <p>This chapter explores how to transform evaluation data into valuable training assets:</p> <ul> <li>Converting evaluation examples into few-shot prompts</li> <li>Understanding the limitations of generic embeddings</li> <li>Creating datasets for fine-tuning retrieval models</li> <li>Learning how contrastive learning improves embeddings</li> <li>Testing approaches systematically</li> <li>Building a roadmap for continuous improvement</li> </ul>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#introduction","title":"Introduction","text":"<p>In the previous chapter, we established our evaluation framework and generated synthetic data to benchmark our RAG system. Now we take the crucial next step in our improvement journey: transforming these evaluations into practical training assets that can significantly boost performance.</p> <p>This chapter bridges the gap between evaluation and production improvement, showing how the same datasets serve both purposes. The fundamental philosophy here is simple but powerful: the data you collect for evaluation should never go to waste. Every question, every relevance judgment, and every performance insight can\u2014and should\u2014be repurposed to train your system.</p> <p>Key Philosophy</p> <p>\"Every evaluation example is a potential training example. The data flywheel transforms what begins as a handful of evaluation examples into few-shot prompts, then into training datasets for fine-tuning embedding models and re-rankers.\"</p> <p>As we'll explore, this transformation process follows a natural progression. What begins as a handful of evaluation examples can evolve into few-shot prompts, then into training datasets for fine-tuning embedding models and re-rankers. This is the essence of the RAG improvement flywheel\u2014data collected for one purpose fuels improvements across your entire system.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#why-generic-embeddings-fall-short","title":"Why Generic Embeddings Fall Short","text":"<p>Before we dive into the transformation process, we need to understand a fundamental challenge in RAG applications: generic embeddings from third-party providers often fall short for specialized applications. This isn't because they're poorly designed\u2014in fact, models like OpenAI's embeddings are remarkably capable. The issue is that they're designed to serve every possible use case, which means they serve no specific use case perfectly.</p> <p>Limitation of Generic Models</p> <p>Generic embedding models inherit assumptions about what \"similarity\" means\u2014assumptions that may not align with your specific needs. They don't know:</p> <pre><code>1. The specific datasets they were trained on\n2. The objective function that defined success during training\n3. The weighting given to different types of similarity\n4. The trade-offs made to accommodate diverse use cases\n</code></pre>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#the-elusive-nature-of-similarity","title":"The Elusive Nature of \"Similarity\"","text":"<p>At the heart of embedding models is a deceptively simple concept: they convert text (or other data) into numerical vectors that capture semantic meaning. The assumption is that items with similar meanings will have vectors that are close to each other when measured by cosine similarity or other distance metrics.</p> <p>Domain-Specific Similarity</p> <p>In e-commerce, what does it mean for two products to be similar? Are they similar because they're substitutes (different brands of red shirts) or complements (a shirt and matching pants)?</p> <pre><code>For music recommendations, are songs similar because they share the same genre, appear in the same playlists, or appeal to the same listeners?\n\nPerhaps the clearest example comes from dating apps. Should \"I love coffee\" and \"I hate coffee\" be considered similar or different? From a linguistic perspective, they're opposites. From a topic perspective, both profiles care enough about beverages to mention them prominently.\n</code></pre> <p>But here's the problem: \"similarity\" is poorly defined when we move beyond general language understanding. The correct answer isn't universal\u2014it depends entirely on your application's objectives.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#the-hidden-assumptions-in-provider-models","title":"The Hidden Assumptions in Provider Models","text":"<p>When you use embedding models from providers like OpenAI, Cohere, or others, you're inheriting a set of assumptions about what \"similarity\" means\u2014assumptions that may not align with your specific needs.</p> <p>Legal Document Search Failure</p> <p>One memorable case involved a legal document search application. The generic embeddings performed reasonably well for finding factual information but struggled with procedural questions. The embeddings didn't adequately capture the relationships between legal procedures and their applications\u2014a specific type of similarity vital to legal professionals but not emphasized in general-purpose training data.</p> <p>The key insight here isn't that provider embeddings are bad\u2014they're actually remarkable technical achievements. It's that their definition of similarity is generic, while your application needs are specific. And as we'll see, fine-tuning with your own data bridges this gap.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#from-evaluation-to-few-shot-examples","title":"From Evaluation to Few-Shot Examples","text":"<p>While fine-tuning embedding models can dramatically improve retrieval, we can often make significant improvements using a simpler approach: few-shot examples. Let's explore how to transform evaluation examples into effective few-shot prompts.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#the-power-of-examples-in-context","title":"The Power of Examples in Context","text":"<p>Few-shot learning has emerged as one of the most powerful techniques for guiding language model behavior. Rather than fine-tuning the model itself (which requires specialized infrastructure and significant data), few-shot learning simply includes examples within the prompt.</p> <p>How Few-Shot Learning Works</p> <p>When you provide a language model with examples of how to respond to similar queries, you activate its ability to recognize patterns and apply them to new inputs. It's like showing a human a few examples of a task before asking them to perform it themselves\u2014no specialized training required, just clear demonstrations.</p> <p>This approach is particularly powerful for RAG applications because different query types often need different retrieval and response strategies. A few well-chosen examples can help the model recognize the current query type and apply the appropriate strategy.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#selecting-the-right-examples","title":"Selecting the Right Examples","text":"<p>Not all evaluation examples make good few-shot prompts. I've seen teams simply grab random examples from their evaluation set, only to find the model's performance actually decreased. The problem wasn't the model\u2014it was the examples.</p> <p>Characteristics of Good Examples</p> <p>The best examples for few-shot learning share several characteristics:</p> <pre><code>1. They represent common query patterns your users actually use\n2. They demonstrate clear, step-by-step reasoning paths\n3. They cover a diverse range of topics or question types\n4. They avoid being too specific or unusual\n</code></pre> <p>Remember the synthetic data generation techniques we explored in Chapter 1? You can use those same methods to generate examples specifically for few-shot learning. The key difference is that for few-shot examples, you need not just questions and answers, but also the reasoning process that connects them.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#building-your-few-shot-library","title":"Building Your Few-Shot Library","text":"<p>Creating a comprehensive few-shot example library is a strategic investment that pays dividends across your entire RAG application. I recommend organizing your examples into a structured library:</p> <ol> <li>Start by filtering your evaluation data for the highest-quality examples</li> <li>Group them by query type or intent (factual questions, how-to guides, comparisons, etc.)</li> <li>Select representative examples from each group</li> <li>Format them consistently for inclusion in prompts</li> <li>Test their effectiveness with your RAG pipeline</li> <li>Iterate and refine based on performance</li> </ol> <p>Structured Few-Shot Prompt</p> <pre><code>You are an assistant specialized in answering questions about [domain].\n\n    Here are some examples of how to answer questions:\n\n    Question: [Example Question 1]\n    Thinking: [First, I'll identify the key entities in the question. Then I'll look for information about their relationship...]\n    Answer: [Example Answer 1]\n\n    Question: [Example Question 2]\n    Thinking: [This appears to be a comparison question. I should look for information about both entities and highlight similarities and differences...]\n    Answer: [Example Answer 2]\n\n    Now please answer the following question:\n    Question: [Actual User Query]\n    ```\n\nThis organized approach helps you maintain and evolve your example library as your application grows. You can track which examples work best for different situations, rotate examples to prevent overfitting, and continuously refine based on user interactions.\n\n## Practical Implementation: Building the Data Flywheel for Fine-Tuning\n\nWhile few-shot learning is powerful, fine-tuning your embedding models can create even more dramatic improvements in retrieval quality. The challenge is getting enough high-quality data to make fine-tuning worthwhile. This is where the concept of the data flywheel becomes crucial.\n\n### Starting the Flywheel\n\nI often tell teams that building a RAG application is like practicing martial arts\u2014there's a \"wax on, wax off\" aspect to it. You start with a small set of examples that serve as your evaluation benchmarks. As you collect more data, those examples become few-shot prompts. And as your dataset grows further, it becomes training data for fine-tuning.\n\n!!! info \"Data Collection Milestones\" - With 20 examples, you can build basic evaluation benchmarks - With 30 examples, you can create effective few-shot prompts - With 1000+ examples, you can fine-tune your retrieval models\n\nThe beauty of this approach is that you're continuously repurposing the same data for increasingly sophisticated improvements. Each stage builds on the previous one, creating a virtuous cycle of enhancement.\n\n!!! warning \"Start Collecting Now\"\nYou need to start collecting the right data now, even if you're not ready to fine-tune yet. The sooner you start logging relevant user interactions, the sooner you'll reach the critical mass needed for fine-tuning.\n\n### What Data Should You Log?\n\nFor RAG applications, the most valuable data points to collect include:\n\n1. The actual queries users make\n2. Which retrieved chunks were cited in the final responses\n3. Which responses received positive feedback (and which didn't)\n4. Which queries required refinement or follow-up\n\nThese signals help you understand which documents are truly relevant to which queries\u2014the fundamental relationship you're trying to capture in fine-tuned embeddings.\n\n!!! example \"Domain-Specific Relevance Signals\"\nFor other applications, the relevance signals will differ:\n\n    - In e-commerce: track which items are purchased together, viewed in sequence, or added to the same lists\n    - For music recommendations: log which songs appear in the same playlists or are hearted by the same users\n    - For dating apps: record which profiles match and go on to have meaningful conversations\n\nThe key is defining what \"relevance\" means in your specific context and systematically collecting data that captures this relationship.\n\n!!! warning \"Missed Opportunity Story\"\nI worked with one team that built a beautiful RAG application for internal documents but failed to implement any feedback collection mechanisms. Six months later, when they wanted to fine-tune their embeddings, they had to start from scratch with synthetic data because they had no record of which retrieved documents had actually been helpful to users. Don't make this mistake\u2014plan your data collection from day one.\n\n## Understanding Contrastive Learning for Embeddings\n\nTo understand how fine-tuning works for embedding models, we need to explore contrastive learning\u2014the technique that powers most modern embedding fine-tuning approaches.\n\n### Learning Through Contrasts\n\nContrastive learning is based on a simple but powerful idea: learning what things are similar by understanding what things are different. Rather than trying to predict absolute values, contrastive learning focuses on relative relationships.\n\n!!! info \"Triplet Structure\"\nThe most common implementation uses a structure called a triplet, which consists of:\n\n    1. An **anchor** (usually the query)\n    2. A **positive example** (a document that's relevant to the query)\n    3. A **negative example** (a document that's not relevant to the query)\n\nThe goal of training is straightforward: adjust the embedding model so that the distance between the anchor and positive example decreases, while the distance between the anchor and negative example increases. In other words, pull similar things closer together and push dissimilar things further apart.\n\n```mermaid\ngraph LR\n    A[Anchor: Query] --- P[Positive: Relevant Document]\n    A --- N[Negative: Irrelevant Document]\n    P -.- |\"Pull Closer\"| A\n    N -.- |\"Push Away\"| A\n</code></pre> <p>This approach is particularly effective for embedding models because it directly optimizes for the distance relationships we care about in retrieval tasks.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#creating-effective-triplets-for-rag","title":"Creating Effective Triplets for RAG","text":"<p>For RAG applications, there are several natural ways to create triplet datasets:</p> <ul> <li>Anchor: The user's query</li> <li>Positive: Document chunks that were cited in the final response or received positive feedback</li> <li>Negative: Document chunks that were retrieved but not cited, or received negative feedback</li> </ul> <p>Healthcare RAG Triplet</p> <p>Imagine a healthcare RAG application where a user asks:</p> <pre><code>```\nWhat are the side effects of medication X?\n```\n\nOur retrieval system might return several documents, including:\n\n```\nDocument A: \"Medication X may cause drowsiness, nausea, and in rare cases, allergic reactions.\"\n\nDocument B: \"Medication X is used to treat high blood pressure and should be taken with food.\"\n```\n\nIf Document A is cited in the response while Document B isn't, we can create a triplet:\n\n```json\n{\n  \"anchor\": \"What are the side effects of medication X?\",\n  \"positive\": \"Medication X may cause drowsiness, nausea, and in rare cases, allergic reactions.\",\n  \"negative\": \"Medication X is used to treat high blood pressure and should be taken with food.\"\n}\n```\n</code></pre> <p>Through many such examples, the model learns that queries about side effects should be closer to texts describing adverse reactions than to texts describing indications or administration instructions.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#the-challenge-of-hard-negatives-and-how-ux-can-help","title":"The Challenge of Hard Negatives and How UX Can Help","text":"<p>The triplet example above introduces an important subtlety in contrastive learning. Notice that our negative example, \"Medication X is used to treat high blood pressure,\" is still about the same medication\u2014it's just not about side effects. This makes it what we call a \"hard negative\"\u2014it's similar to what we're looking for in some ways (same medication) but different in crucial aspects (not about side effects).</p> <p>Value of Hard Negatives</p> <p>Hard negatives are much more valuable for training than \"easy negatives.\" If instead our negative example had been about car maintenance\u2014completely unrelated to medications\u2014the model wouldn't learn much from this contrast because it's already obvious that car maintenance isn't relevant to medication side effects.</p> <pre><code>The truly challenging distinction\u2014and the one that will improve our retrieval quality the most\u2014is teaching the model to distinguish between different aspects of related topics.\n</code></pre> <p>This is where hard negative mining becomes crucial. Hard negative mining is the process of finding negative examples that are challenging but instructive for the model.</p> <p>Designing UX for Better Training Data</p> <p>If you're serious about improving your embeddings, consider explicitly designing your UX to capture these signals:</p> <pre><code>1. **Document-level feedback mechanisms**: Add simple thumbs up/down options next to each retrieved document, not just for the final answer\n\n2. **Click tracking**: Record which documents users click on and which they ignore\u2014those ignored despite ranking highly are excellent hard negative candidates\n\n3. **Dwell time analysis**: If a user quickly returns from a document without spending time reading it, that's a strong signal it wasn't relevant\n\n4. **Explicit comparison interfaces**: For critical applications, consider interfaces that ask users to compare documents and select the most relevant one\n\n5. **Query reformulation tracking**: When a user modifies their query slightly and gets better results, you can pair the original query with documents from the improved results to create training pairs\n</code></pre> <p>One particularly effective approach I've seen involved a \"more like this\" button next to helpful documents. This not only improved the immediate user experience but also created clear signals about which documents were semantically related in the users' mental models\u2014relationships that might not be obvious from text content alone.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#the-power-of-re-rankers-in-rag-systems","title":"The Power of Re-Rankers in RAG Systems","text":"<p>While embedding models are the workhorses of retrieval, re-rankers provide an additional layer of refinement that can significantly improve results. Where embedding models (also called bi-encoders) encode queries and documents separately, re-rankers (cross-encoders) process them together to make more nuanced relevance judgments.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#bi-encoders-vs-cross-encoders-understanding-the-trade-offs","title":"Bi-Encoders vs. Cross-Encoders: Understanding the Trade-offs","text":"<p>The fundamental trade-off between embedding models and re-rankers is between speed and accuracy:</p> <p>Model Comparison</p> <p>Bi-encoders (embedding models): - Encode query and document independently - Allow pre-computation of document embeddings - Enable fast vector similarity operations - Work well for first-pass retrieval of candidates - Examples include OpenAI's text-embedding models, SBERT, MPNet</p> <pre><code>**Cross-encoders (re-rankers)**:\n- Process query and document together as a pair\n- Cannot pre-compute relevance scores\n- Provide more accurate relevance judgments\n- Work best for re-ranking a smaller set of candidates\n- Examples include Cohere Rerank, monoT5\n</code></pre> <p>This complementary relationship makes them perfect partners in a two-stage retrieval process: use embeddings to quickly find candidate documents, then use a re-ranker to sort them more accurately.</p> <p>Re-Ranker Success Story</p> <p>One team I worked with was debating whether to invest in fine-tuning their embeddings or implementing a re-ranker. When they tested both approaches, they found that fine-tuning embeddings improved recall from 65% to 78%, while adding a re-ranker (even without fine-tuning) improved it to 82%. Combining both approaches pushed performance to 91%\u2014a transformative improvement from where they started.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#creating-training-data-for-re-rankers","title":"Creating Training Data for Re-Rankers","text":"<p>Re-rankers benefit from more nuanced training data than binary relevant/not-relevant labels. While you can start with the same data you use for embedding fine-tuning, consider enriching it with graded relevance scores:</p> <ol> <li>Create pairs of (query, document) with relevance scores on a scale (often 0-5)</li> <li>Include a range of scores to help the model learn gradations of relevance</li> <li>Train the model to predict these more nuanced relevance scores</li> </ol> <p>Graded Relevance Example</p> <p><code>json     {       \"query\": \"How do I reset my password?\",       \"documents\": [         {\"text\": \"Step-by-step password reset guide\", \"score\": 5},         {\"text\": \"General account management information\", \"score\": 3},         {\"text\": \"Creating a strong password\", \"score\": 2},         {\"text\": \"About our company\", \"score\": 0}       ]     }</code></p> <p>This richer data helps the re-ranker understand not just what's relevant versus irrelevant, but also what's highly relevant versus somewhat relevant\u2014a distinction that can significantly improve user experience.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#testing-different-approaches-systematically","title":"Testing Different Approaches Systematically","text":"<p>With your evaluation framework from Chapter 1 and your growing dataset of examples, you can now test various improvement approaches systematically. This experimental mindset is critical to making steady progress.</p> <p>Good Experimentation Practices</p> <p>Good experimentation requires discipline and structure. For each test:</p> <pre><code>1. Form a clear hypothesis: \"Implementing a re-ranker will improve recall@10 by at least 15%\"\n2. Define success criteria before running the experiment\n3. Isolate a single variable whenever possible\n4. Measure impact on your established metrics\n5. Document both successful and unsuccessful experiments\n</code></pre> <p>Common experiments worth running include:</p> <ol> <li>Embedding model comparisons: Test different models (OpenAI, Cohere, open-source alternatives)</li> <li>Chunking strategy variations: Try different chunk sizes and overlap percentages</li> <li>Retrieval method comparisons: Compare lexical, semantic, and hybrid approaches</li> <li>Re-ranking impact assessment: Measure the effect of adding a re-ranker</li> <li>Few-shot prompt variations: Test different examples and formats</li> </ol> <p>Debate Resolved Through Data</p> <p>One team I worked with spent weeks debating which embedding model to use, with different team members advocating for their preferred option. Instead of continuing the debate, they implemented a simple experiment: they indexed their documents with three different embedding models and measured recall on their evaluation set. The results settled the debate in hours, not weeks, and the team moved forward with data-backed confidence.</p> <p>This cyclical process creates a data-driven improvement flywheel that continuously enhances your system's performance.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#building-a-roadmap-for-continuous-improvement","title":"Building a Roadmap for Continuous Improvement","text":"<p>Based on your experimental results, you can now build a roadmap for ongoing improvements. This isn't just about technical enhancements\u2014it's about creating a systematic process for evolution.</p> <p>Prioritization Framework</p> <p>When deciding what to improve next, consider multiple factors:</p> <pre><code>1. **Impact**: Which changes will most dramatically improve key metrics?\n2. **Effort**: How much work is required to implement each change?\n3. **Dependencies**: Which improvements depend on others being completed first?\n4. **Risk**: What is the chance of negative side effects or regressions?\n</code></pre> <p>I've found that impact/effort prioritization works particularly well for RAG improvements. Plot potential enhancements on a simple 2x2 grid with impact on one axis and effort on the other, then focus on high-impact, low-effort improvements first. These \"quick wins\" build momentum and demonstrate value while you prepare for more complex enhancements.</p> <p>Prioritization in Action</p> <p>In one project, we identified that implementing BM25 hybrid retrieval would be high-impact and medium-effort, while fine-tuning custom embeddings would be high-impact but high-effort. We prioritized the hybrid retrieval first, which gave us immediate gains while we collected data for the eventual embedding fine-tuning.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#additional-resources","title":"Additional Resources","text":"<p>Tools and Libraries</p> <p>Key Concepts</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#understanding-embedding-models","title":"Understanding Embedding Models","text":"<ol> <li> <p>Sentence Transformers Library (https://www.sbert.net/): This library provides easy-to-use implementations for state-of-the-art embedding models, supporting both pairwise datasets and triplets for fine-tuning.</p> </li> <li> <p>Modern BERT (https://huggingface.co/sentence-transformers): These newer models offer 8,000 token sequence lengths and generally outperform classic BERT-based models.</p> </li> <li> <p>Cohere Re-ranking Models (https://cohere.com/rerank): Cohere offers state-of-the-art re-ranking capabilities with a fine-tuning API that makes it relatively easy to customize for your specific needs.</p> </li> </ol>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#contrastive-learning-in-depth","title":"Contrastive Learning In-Depth","text":"<p>Contrastive learning trains models to recognize similarities and differences between items by pushing and pulling examples in the embedding space:</p> <ul> <li>Triplet Loss: Optimizes the distance between anchor-positive pairs relative to anchor-negative pairs</li> <li>InfoNCE Loss: Contrasts a positive pair against multiple negative examples</li> <li>Multiple Negatives Ranking Loss: Handles batches of queries with multiple negatives per query</li> </ul>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#scaling-and-efficiency-considerations","title":"Scaling and Efficiency Considerations","text":"<p>For large datasets or production workloads:</p> <ul> <li>Consider parallel processing frameworks (like Modal) to accelerate embedding and training</li> <li>Experiment with multi-GPU training for faster iterations</li> <li>Evaluate the trade-offs between API costs and self-hosting</li> <li>Test multiple model variations simultaneously to find optimal configurations</li> </ul>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#reflection-questions","title":"Reflection Questions","text":"<p>!!! question \"Self-Assessment\" 1. What specific definition of \"similarity\" is most important for your application's domain?</p> <pre><code>2. How would you create effective few-shot examples from your existing evaluation data?\n\n3. What user interactions in your application could provide valuable training signals for fine-tuning?\n\n4. If you had to prioritize one retrieval improvement for your system, would it be embeddings, re-ranking, or something else? Why?\n\n5. What experiments could you run to test your hypotheses about improving retrieval quality?\n</code></pre>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#conclusion-and-next-steps","title":"Conclusion and Next Steps","text":"<p>In this chapter, we've explored how to transform evaluation data into valuable training assets:</p> <ol> <li>Converting evaluation examples into few-shot prompts</li> <li>Understanding the limitations of generic embeddings</li> <li>Creating datasets for fine-tuning retrieval models</li> <li>Learning how contrastive learning improves embeddings</li> <li>Testing approaches systematically</li> <li>Building a roadmap for continuous improvement</li> </ol> <p>The key insight to take away is that data collection and repurposing form the foundation of systematic RAG improvement. Every question, every piece of feedback, and every evaluation can fuel your improvement flywheel if properly captured and utilized.</p> <p>What's Coming Next</p> <p>In Chapter 3, we'll dive into deployment strategies, user feedback collection methods, and how to use this feedback to further refine your RAG application. We'll explore practical techniques for gathering implicit and explicit feedback, designing effective user interfaces, and closing the loop between user interactions and system improvements.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#summary","title":"Summary","text":"<p>The data flywheel approach transforms what begins as evaluation into training assets that continuously improve your RAG system. By understanding the limitations of generic models, implementing few-shot examples, and preparing for fine-tuning, you create a foundation for ongoing enhancement. This systematic approach ensures that every piece of data you collect contributes to a cycle of improvement that makes your application increasingly effective for your specific use case.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter3-1/","title":"Feedback Collection: Building Your Improvement Flywheel","text":"<p>Chapter Overview</p> <p>This chapter explores the essential role of feedback collection in RAG systems, introducing the concept of a feedback flywheel for systematic improvement. You'll learn practical strategies for making feedback mechanisms visible and engaging to users, techniques for collecting segmented feedback that provides actionable insights, and methods for mining user behavior to generate training data. The chapter emphasizes how effective feedback collection transforms your RAG application from a static tool into a continuously improving system that grows more valuable with every interaction.</p>"},{"location":"workshops/chapter3-1/#introduction","title":"Introduction","text":"<p>The true power of RAG isn't in its initial deployment\u2014it's in how the system improves over time through feedback collection. Many RAG implementations focus exclusively on the technical details of retrieval and generation while neglecting the critical infrastructure needed to collect and utilize user feedback.</p> <p>In this chapter, we'll explore how to build effective feedback mechanisms that transform your RAG application from a static implementation into a continuously improving system that grows more valuable with every user interaction. This approach creates a \"feedback flywheel\"\u2014a virtuous cycle where user interactions provide the data needed to make the system better, which in turn attracts more users and generates more feedback.</p> <p>The Invisible Feedback Problem</p> <p>Many RAG implementations hide feedback mechanisms in obscure UI locations or use generic \"thumbs up/down\" buttons that provide minimal insight. Research suggests that users interact with these minimal feedback options less than 0.1% of the time, providing insufficient data for meaningful improvements.</p> <p>Feedback collection is the lifeblood of systematic RAG improvement. Without it, you're flying blind\u2014unable to identify which aspects of your system are performing well and which need enhancement. Robust feedback mechanisms tell you:</p> <ul> <li>Which queries your retrieval system handles poorly</li> <li>Which document segments are most valuable for answering specific questions</li> <li>Where your generation step produces inaccurate or unhelpful responses</li> </ul> <p>This chapter focuses on the practical implementation of feedback mechanisms in RAG applications. We'll cover strategies for making feedback visible and engaging, approaches for segmenting feedback to make it more actionable, and techniques for mining user behavior to generate training datasets.</p>"},{"location":"workshops/chapter3-1/#feedback-visibility-make-it-impossible-to-miss","title":"Feedback Visibility: Make It Impossible to Miss","text":"<p>The first principle of effective feedback collection is visibility. Your feedback mechanisms should be prominent and engaging, not hidden in dropdown menus or settings pages. Users should encounter feedback options naturally as part of their interaction flow.</p> <p>High-Visibility Feedback UI</p> <p>Consider the difference between these two approaches:</p> <pre><code>**Low Visibility:** A small thumbs up/down icon in the corner of the response\n\n**High Visibility:**\n\nAfter receiving an answer, users see:\n\n\"Was this answer helpful? [Yes] [Somewhat] [No]\"\n\nIf they click \"Somewhat\" or \"No\":\n\n\"What could be improved?\"\n- [ ] More detailed explanation\n- [ ] More relevant information\n- [ ] Incorrect information\n- [ ] Better formatting\n- [ ] Other: ____________\n</code></pre> <p>The second approach not only makes feedback impossible to miss but also structures it in a way that provides more actionable insights. Research shows that visible, engaging feedback mechanisms can increase feedback rates from less than 1% to over 30%.</p>"},{"location":"workshops/chapter3-1/#implementation-strategies","title":"Implementation Strategies","text":"<p>Here are several patterns for implementing high-visibility feedback mechanisms:</p> <ol> <li>Inline Feedback: Place feedback options directly beneath each response</li> <li>Modal Prompts: Show a feedback modal after a certain number of interactions</li> <li>Follow-up Questions: Include feedback collection as part of conversational flow</li> <li>Email Follow-ups: Send follow-up emails asking for feedback on recent sessions</li> </ol> <p>Each approach has advantages for different use cases. The key is to make feedback collection a natural part of the user experience rather than an afterthought.</p> <pre><code>def render_response_with_feedback(response: str, query_id: str):\n    \"\"\"\n    Render a response with prominent feedback mechanisms.\n\n    Parameters:\n    - response: The generated response text\n    - query_id: Unique identifier for the query\n\n    Returns:\n    - HTML content with embedded feedback UI\n    \"\"\"\n    html = f\"\"\"\n    &lt;div class=\"response-container\"&gt;\n        &lt;div class=\"response-content\"&gt;\n            {response}\n        &lt;/div&gt;\n\n        &lt;div class=\"feedback-container\"&gt;\n            &lt;p class=\"feedback-prompt\"&gt;Was this response helpful?&lt;/p&gt;\n\n            &lt;div class=\"feedback-options\"&gt;\n                &lt;button onclick=\"recordFeedback('{query_id}', 'helpful')\"&gt;\n                    Very helpful\n                &lt;/button&gt;\n\n                &lt;button onclick=\"recordFeedback('{query_id}', 'somewhat')\"&gt;\n                    Somewhat helpful\n                &lt;/button&gt;\n\n                &lt;button onclick=\"recordFeedback('{query_id}', 'not_helpful')\"&gt;\n                    Not helpful\n                &lt;/button&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;script&gt;\n    function recordFeedback(id, rating) {\n        // Record the initial feedback\n        fetch('/api/feedback', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json'\n            },\n            body: JSON.stringify({\n                query_id: id,\n                rating: rating\n            })\n        })\n        .then(response =&gt; response.json())\n        .then(data =&gt; {\n            // If the rating wasn't \"very helpful\", show the detailed feedback form\n            if (rating !== 'helpful') {\n                showDetailedFeedbackForm(id, rating);\n            } else {\n                showThankYouMessage();\n            }\n        });\n    }\n\n    function showDetailedFeedbackForm(id, initialRating) {\n        // Display a more detailed feedback form based on the initial rating\n        // ...\n    }\n    &lt;/script&gt;\n    \"\"\"\n\n    return html\n</code></pre>"},{"location":"workshops/chapter3-1/#segmented-feedback-make-it-actionable","title":"Segmented Feedback: Make It Actionable","text":"<p>Generic feedback like thumbs up/down provides minimal insight for improvement. To make feedback truly actionable, segment it into specific aspects of your RAG pipeline.</p> <p>The Problem with Generic Feedback</p> <p>A simple \"thumbs down\" could mean many things: - The retrieval system found irrelevant documents - The generation step produced inaccurate information - The answer was technically correct but poorly formatted - The answer was too brief or too verbose</p> <pre><code>Without knowing which aspect failed, you can't target improvements effectively.\n</code></pre> <p>Segmented feedback isolates specific parts of your RAG pipeline, helping you identify exactly where issues occur. Instead of asking \"Was this helpful?\" consider questions like:</p> <ul> <li>\"Did this answer directly address your question?\"</li> <li>\"Was the information factually accurate?\"</li> <li>\"Were sources relevant to your query?\"</li> <li>\"Was the response clear and well-organized?\"</li> </ul> <p>Each question targets a different aspect of your system, allowing you to pinpoint areas for improvement.</p>"},{"location":"workshops/chapter3-1/#collecting-segmented-negative-feedback","title":"Collecting Segmented Negative Feedback","text":"<p>Negative feedback is particularly valuable for improvement, but users often abandon interactions after having a bad experience. To maximize the collection of negative feedback:</p> <ol> <li>Make feedback collection immediate\u2014don't wait until the end of a session</li> <li>Use progressive disclosure to collect more detailed feedback after an initial negative response</li> <li>Keep detailed feedback optional but make it easy to provide</li> <li>Explain how feedback will be used to improve the system</li> </ol> <p>Here's how you might implement segmented negative feedback collection:</p> <pre><code>def collect_negative_feedback(query_id: str, response_id: str):\n    \"\"\"\n    Generate a detailed negative feedback form.\n\n    Parameters:\n    - query_id: ID of the original query\n    - response_id: ID of the response being rated\n\n    Returns:\n    - HTML for a segmented feedback form\n    \"\"\"\n    return f\"\"\"\n    &lt;div class=\"detailed-feedback-form\"&gt;\n        &lt;p&gt;Thanks for your feedback. Could you tell us more about what wasn't helpful?&lt;/p&gt;\n\n        &lt;form id=\"detailed-feedback-{response_id}\"&gt;\n            &lt;input type=\"hidden\" name=\"query_id\" value=\"{query_id}\"&gt;\n            &lt;input type=\"hidden\" name=\"response_id\" value=\"{response_id}\"&gt;\n\n            &lt;h4&gt;What issues did you notice? (Select all that apply)&lt;/h4&gt;\n\n            &lt;div class=\"feedback-section\"&gt;\n                &lt;h5&gt;Retrieval Issues:&lt;/h5&gt;\n                &lt;label&gt;\n                    &lt;input type=\"checkbox\" name=\"retrieval_issue\" value=\"irrelevant_sources\"&gt;\n                    Sources weren't relevant to my question\n                &lt;/label&gt;\n                &lt;label&gt;\n                    &lt;input type=\"checkbox\" name=\"retrieval_issue\" value=\"missing_information\"&gt;\n                    Important information seemed to be missing\n                &lt;/label&gt;\n            &lt;/div&gt;\n\n            &lt;div class=\"feedback-section\"&gt;\n                &lt;h5&gt;Generation Issues:&lt;/h5&gt;\n                &lt;label&gt;\n                    &lt;input type=\"checkbox\" name=\"generation_issue\" value=\"inaccurate\"&gt;\n                    Information was factually incorrect\n                &lt;/label&gt;\n                &lt;label&gt;\n                    &lt;input type=\"checkbox\" name=\"generation_issue\" value=\"formatting\"&gt;\n                    Response was poorly organized or formatted\n                &lt;/label&gt;\n                &lt;label&gt;\n                    &lt;input type=\"checkbox\" name=\"generation_issue\" value=\"incomplete\"&gt;\n                    Answer was incomplete or too brief\n                &lt;/label&gt;\n            &lt;/div&gt;\n\n            &lt;div class=\"feedback-section\"&gt;\n                &lt;h5&gt;Additional Comments:&lt;/h5&gt;\n                &lt;textarea name=\"comments\" placeholder=\"Any other feedback to help us improve?\"&gt;&lt;/textarea&gt;\n            &lt;/div&gt;\n\n            &lt;button type=\"submit\"&gt;Submit Feedback&lt;/button&gt;\n        &lt;/form&gt;\n    &lt;/div&gt;\n\n    &lt;script&gt;\n    document.getElementById(\"detailed-feedback-{response_id}\").addEventListener(\"submit\", function(e) {\n        e.preventDefault();\n\n        // Collect form data\n        const formData = new FormData(e.target);\n        const feedbackData = {\n            query_id: formData.get(\"query_id\"),\n            response_id: formData.get(\"response_id\"),\n            retrieval_issues: formData.getAll(\"retrieval_issue\"),\n            generation_issues: formData.getAll(\"generation_issue\"),\n            comments: formData.get(\"comments\")\n        };\n\n        // Submit detailed feedback\n        fetch('/api/detailed-feedback', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json'\n            },\n            body: JSON.stringify(feedbackData)\n        })\n        .then(response =&gt; response.json())\n        .then(data =&gt; {\n            showThankYouMessage();\n        });\n    });\n    &lt;/script&gt;\n    \"\"\"\n</code></pre>"},{"location":"workshops/chapter3-1/#learning-from-user-behavior-the-implicit-feedback-gold-mine","title":"Learning from User Behavior: The Implicit Feedback Gold Mine","text":"<p>While explicit feedback (ratings, comments) is valuable, users express opinions through their actions even when they don't provide direct feedback. These behavioral signals\u2014often called implicit feedback\u2014can be a gold mine for system improvement.</p> <p>Key implicit feedback signals include:</p> <ul> <li>Query refinements: When users rephrase a query immediately after receiving a response</li> <li>Abandonment: When users abandon a session after receiving a response</li> <li>Engagement time: How long users engage with a response</li> <li>Link clicks: Which citations or references users click on</li> <li>Copypaste actions: What parts of responses users copy to their clipboard</li> <li>Scrolling behavior: Whether users read the entire response or just skim</li> </ul> <p>By tracking these behaviors, you can identify patterns that indicate success or failure even when users don't provide explicit feedback.</p> <p>Implicit Feedback Collection</p> <pre><code>// Track query refinements\nfunction trackQueryRefinement(originalQueryId, newQuery) {\n  fetch('/api/track-refinement', {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      original_query_id: originalQueryId,\n      new_query: newQuery,\n      time_difference_ms: Date.now() - lastQueryTimestamp\n    })\n  });\n}\n\n// Track citation clicks\nfunction trackCitationClick(queryId, responseId, citationIndex, documentId) {\n  fetch('/api/track-citation-click', {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      query_id: queryId,\n      response_id: responseId,\n      citation_index: citationIndex,\n      document_id: documentId,\n      timestamp: Date.now()\n    })\n  });\n}\n\n// Track user engagement time\nlet viewStartTime = Date.now();\nlet isVisible = true;\n\ndocument.addEventListener('visibilitychange', function() {\n  if (document.visibilityState === 'hidden') {\n    isVisible = false;\n    trackEngagementTime();\n  } else {\n    isVisible = true;\n    viewStartTime = Date.now();\n  }\n});\n\nwindow.addEventListener('beforeunload', function() {\n  trackEngagementTime();\n});\n\nfunction trackEngagementTime() {\n  if (!isVisible) return;\n\n  const engagementTimeMs = Date.now() - viewStartTime;\n\n  fetch('/api/track-engagement', {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      query_id: currentQueryId,\n      response_id: currentResponseId,\n      engagement_time_ms: engagementTimeMs\n    })\n  });\n}\n</code></pre>"},{"location":"workshops/chapter3-1/#mining-hard-negatives-from-user-behavior","title":"Mining Hard Negatives from User Behavior","text":"<p>One particularly valuable form of implicit feedback is the identification of \"hard negatives\"\u2014documents that appear relevant based on keyword or semantic matching but are actually irrelevant or misleading for a particular query.</p> <p>When a user submits a query, views the response and citations, then immediately refines their query or provides negative feedback, there's a good chance that the retrieved documents were not helpful. These interactions provide strong signals about weaknesses in your retrieval system.</p> <p>By tracking these patterns, you can build datasets of queries paired with documents that should NOT be retrieved\u2014invaluable training data for improving embedding models or reranking systems.</p> <p>Here's a simple algorithm for mining hard negatives from user interactions:</p> <pre><code>def identify_potential_hard_negatives(\n    query_id: str,\n    retrieved_docs: List[str],\n    user_actions: List[Dict]\n) -&gt; List[Dict]:\n    \"\"\"\n    Identify potential hard negative documents based on user interactions.\n\n    Parameters:\n    - query_id: The ID of the original query\n    - retrieved_docs: List of document IDs retrieved for this query\n    - user_actions: Sequence of user actions following the response\n\n    Returns:\n    - List of potential hard negative documents with confidence scores\n    \"\"\"\n    hard_negative_candidates = []\n\n    # Check for immediate query refinement (within 30 seconds)\n    refinement = next(\n        (\n            action for action in user_actions\n            if action[\"type\"] == \"query\" and action[\"timestamp\"] - query_timestamp &lt; 30\n        ),\n        None\n    )\n\n    if refinement:\n        # All retrieved docs from the original query are potential hard negatives\n        # But we assign different confidence scores based on user actions\n        for doc_id in retrieved_docs:\n            confidence = 0.7  # Base confidence for refinement case\n\n            # Check if user viewed this document (reduces confidence it's a hard negative)\n            if any(a[\"type\"] == \"view_document\" and a[\"document_id\"] == doc_id for a in user_actions):\n                confidence -= 0.3\n\n            # Check if user explicitly marked this document as irrelevant\n            if any(a[\"type\"] == \"rate_document\" and a[\"document_id\"] == doc_id and a[\"rating\"] == \"irrelevant\" for a in user_actions):\n                confidence = 0.9  # Very high confidence it's a hard negative\n\n            hard_negative_candidates.append({\n                \"query_id\": query_id,\n                \"document_id\": doc_id,\n                \"confidence\": confidence,\n                \"reason\": \"query_refinement\"\n            })\n\n    # Check for negative feedback on response\n    negative_feedback = next(\n        (\n            action for action in user_actions\n            if action[\"type\"] == \"rate_response\" and action[\"rating\"] in [\"not_helpful\", \"inaccurate\"]\n        ),\n        None\n    )\n\n    if negative_feedback:\n        # Documents that weren't explicitly marked as helpful may be hard negatives\n        for doc_id in retrieved_docs:\n            # Skip documents the user explicitly marked as helpful\n            if any(a[\"type\"] == \"rate_document\" and a[\"document_id\"] == doc_id and a[\"rating\"] == \"helpful\" for a in user_actions):\n                continue\n\n            confidence = 0.5  # Base confidence for negative feedback case\n\n            # Adjust confidence based on document interactions\n            if any(a[\"type\"] == \"rate_document\" and a[\"document_id\"] == doc_id and a[\"rating\"] == \"irrelevant\" for a in user_actions):\n                confidence = 0.9  # Very high confidence it's a hard negative\n\n            hard_negative_candidates.append({\n                \"query_id\": query_id,\n                \"document_id\": doc_id,\n                \"confidence\": confidence,\n                \"reason\": \"negative_feedback\"\n            })\n\n    # Filter out low-confidence candidates\n    return [candidate for candidate in hard_negative_candidates if candidate[\"confidence\"] &gt; 0.4]\n</code></pre> <p>By collecting these potential hard negatives over time, you can build a dataset for fine-tuning embedding models or training re-rankers to avoid these problematic documents in future queries.</p>"},{"location":"workshops/chapter3-1/#building-a-feedback-driven-roadmap","title":"Building a Feedback-Driven Roadmap","text":"<p>The ultimate goal of feedback collection is to guide your improvement roadmap. Rather than making enhancement decisions based on intuition or technical interest, you can prioritize based on user needs revealed through feedback.</p> <p>A feedback-driven roadmap:</p> <ol> <li>Identifies the most common issues reported by users</li> <li>Quantifies the impact of each issue on user satisfaction</li> <li>Ranks potential improvements by expected impact</li> <li>Establishes clear metrics to evaluate whether changes actually improve the user experience</li> </ol> <p>This approach ensures that engineering efforts focus on changes that will have the greatest impact on user satisfaction rather than on the most technically interesting problems.</p> <pre><code>def analyze_feedback_for_roadmap(feedback_data: List[Dict], time_period_days: int = 30):\n    \"\"\"\n    Analyze feedback data to identify high-priority improvement areas.\n\n    Parameters:\n    - feedback_data: Collection of user feedback and interaction data\n    - time_period_days: Time window to analyze (default: 30 days)\n\n    Returns:\n    - Dictionary of issue categories with frequency and impact scores\n    \"\"\"\n    recent_cutoff = datetime.now() - timedelta(days=time_period_days)\n    recent_feedback = [f for f in feedback_data if f[\"timestamp\"] &gt; recent_cutoff]\n\n    # Group feedback by issue category\n    issues_by_category = defaultdict(list)\n\n    for feedback in recent_feedback:\n        if \"issues\" in feedback:\n            for issue in feedback[\"issues\"]:\n                category = issue[\"category\"]\n                issues_by_category[category].append({\n                    \"feedback_id\": feedback[\"id\"],\n                    \"severity\": issue.get(\"severity\", \"medium\"),\n                    \"user_id\": feedback.get(\"user_id\", \"anonymous\"),\n                    \"timestamp\": feedback[\"timestamp\"]\n                })\n\n    # Calculate impact scores\n    impact_scores = {}\n\n    for category, issues in issues_by_category.items():\n        # Count unique users affected\n        unique_users = len(set(issue[\"user_id\"] for issue in issues if issue[\"user_id\"] != \"anonymous\"))\n\n        # Calculate frequency\n        frequency = len(issues) / len(recent_feedback)\n\n        # Calculate severity score (0-1)\n        severity_map = {\"low\": 0.3, \"medium\": 0.6, \"high\": 1.0}\n        avg_severity = sum(severity_map[issue[\"severity\"]] for issue in issues) / len(issues)\n\n        # Calculate recency factor (more recent issues weighted higher)\n        most_recent = max(issue[\"timestamp\"] for issue in issues)\n        recency_factor = 1.0 - ((datetime.now() - most_recent).days / time_period_days)\n\n        # Calculate overall impact score\n        impact = (frequency * 0.4) + (avg_severity * 0.3) + (unique_users / 100 * 0.2) + (recency_factor * 0.1)\n\n        impact_scores[category] = {\n            \"frequency\": frequency,\n            \"unique_users\": unique_users,\n            \"avg_severity\": avg_severity,\n            \"recency_factor\": recency_factor,\n            \"overall_impact\": impact,\n            \"example_feedbacks\": [issue[\"feedback_id\"] for issue in issues[:5]]\n        }\n\n    # Sort by impact score\n    sorted_impact = sorted(\n        impact_scores.items(),\n        key=lambda x: x[1][\"overall_impact\"],\n        reverse=True\n    )\n\n    return {\n        \"analyzed_period_days\": time_period_days,\n        \"total_feedback_count\": len(recent_feedback),\n        \"prioritized_issues\": sorted_impact\n    }\n</code></pre>"},{"location":"workshops/chapter3-1/#conclusion-feedback-as-foundation","title":"Conclusion: Feedback as Foundation","text":"<p>Effective feedback collection is the foundation of systematic RAG improvement. Without robust feedback mechanisms, you're left guessing about which aspects of your system need enhancement and whether your changes actually improve the user experience.</p> <p>By implementing the strategies outlined in this chapter\u2014making feedback visible, segmenting it for actionability, mining user behaviors for implicit signals, and using feedback to drive your roadmap\u2014you establish a data-driven approach to continuous improvement.</p> <p>In the next chapter, we'll explore how to reduce perceived latency through streaming and progressive responses, building on the feedback foundation to create a more engaging user experience.</p>"},{"location":"workshops/chapter3-1/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>How visible are the feedback mechanisms in your current RAG implementation? What changes could make them more prominent and engaging?</p> </li> <li> <p>What implicit signals could you collect from user interactions with your system? How might these complement explicit feedback?</p> </li> <li> <p>How could you segment feedback to better pinpoint issues in specific parts of your RAG pipeline?</p> </li> <li> <p>What processes would you need to implement to translate feedback into a prioritized improvement roadmap?</p> </li> <li> <p>How might you incentivize users to provide more detailed feedback, especially after negative experiences?</p> </li> </ol>"},{"location":"workshops/chapter3-1/#summary","title":"Summary","text":"<p>Effective feedback collection is essential for systematic improvement of RAG systems. By making feedback mechanisms visible and engaging, segmenting feedback to target specific pipeline components, mining implicit signals from user behavior, and using feedback to drive your improvement roadmap, you create a foundation for continuous enhancement. The feedback flywheel turns raw user interactions into actionable insights that guide your development priorities and measure the impact of your improvements.</p>"},{"location":"workshops/chapter3-1/#additional-resources","title":"Additional Resources","text":"<ol> <li> <p>Nielsen Norman Group, \"User Feedback Mechanisms for Mobile and Web\"</p> </li> <li> <p>Google Research, \"Beyond A/B Testing: Implicit Feedback for UI Improvement\"</p> </li> <li> <p>Qualtrics, \"Designing Feedback Forms That Users Actually Complete\"</p> </li> <li> <p>GitHub Repository: RAG-Feedback-Collection - Templates and examples for implementing feedback mechanisms in RAG applications</p> </li> </ol>"},{"location":"workshops/chapter3-2/","title":"Overcoming Latency: Streaming and Interstitials","text":"<p>Chapter Overview</p> <p>This chapter explores how to overcome the critical challenge of latency in RAG applications. You'll learn strategies for streaming responses, designing meaningful interstitials, and employing various technical optimizations to enhance both actual and perceived performance. The chapter demonstrates how techniques like streaming structured data and dynamic content updates can transform waiting time from a frustrating experience into an engaging one, ultimately improving user satisfaction and feedback collection rates. By implementing these approaches, you'll create RAG applications that feel responsive even during complex processing operations.</p>"},{"location":"workshops/chapter3-2/#introduction-the-psychology-of-waiting","title":"Introduction: The Psychology of Waiting","text":"<p>In our quest to build exceptional RAG applications, we often focus on the quality of responses while overlooking a critical aspect of user experience: latency. Even the most accurate and helpful answer loses value if users grow frustrated waiting for it to appear.</p> <p>The reality is that RAG processes\u2014retrieval, generation, validation, citation lookup\u2014take time. This inherent latency creates a fundamental challenge: how do we keep users engaged and confident while these processes run?</p> <p>Perceived performance often matters more than actual performance. Research shows that users perceive responsive systems as faster even when the total completion time is identical. This psychological principle is at the heart of the strategies we'll explore in this chapter.</p> <p>The Perception Gap</p> <p>Studies show that perceived wait times can be up to 25% longer than actual wait times when users have no visibility into system progress. Conversely, showing meaningful progress can make perceived wait times up to 40% shorter than actual wait times.</p> <p>We'll explore two complementary approaches to addressing latency:</p> <ol> <li>Streaming responses to show progress and deliver content incrementally</li> <li>Designing meaningful interstitials that engage users while processing occurs</li> </ol> <p>These techniques not only improve user experience but also lead to higher engagement and more feedback collection, strengthening the improvement flywheel we established in the previous chapter.</p>"},{"location":"workshops/chapter3-2/#the-power-of-animation-creating-the-illusion-of-speed","title":"The Power of Animation: Creating the Illusion of Speed","text":"<p>Before diving into streaming implementations, let's understand why animated indicators are so effective at improving perceived performance. Research in cognitive psychology reveals that humans perceive time differently when observing movement.</p> <p>The Power of Progress Indicators</p> <p>In a study by the Nielsen Norman Group, users reported a 15-20% faster perceived load time when shown an animated progress indicator compared to a static wait screen, even though the actual load times were identical.</p> <p>Animated indicators work by:</p> <ol> <li>Giving users confidence that the system is actively working</li> <li>Drawing attention away from the passage of time</li> <li>Setting expectations about progress and completion</li> </ol> <p>The most effective indicators for RAG systems are those that convey meaningful information about what's happening behind the scenes, not just generic loading animations.</p> <p>Consider how differently users perceive these three waiting experiences:</p> <ol> <li>A static screen with no feedback</li> <li>A generic spinning wheel</li> <li>A step-by-step indicator showing \"Searching relevant documents (2/5 complete)...\"</li> </ol> <p>The third approach not only feels faster but also builds trust by providing transparency into the process.</p>"},{"location":"workshops/chapter3-2/#streaming-responses-the-ultimate-progress-indicator","title":"Streaming Responses: The Ultimate Progress Indicator","text":"<p>Streaming takes the concept of progress indicators to its logical conclusion by delivering content to users as it's generated, rather than waiting for the entire response to complete. This creates a dramatically better user experience by:</p> <ol> <li>Showing immediate activity, reducing uncertainty</li> <li>Providing useful content while generation continues</li> <li>Allowing users to begin reading before the full response is ready</li> </ol> <p>In a traditional RAG implementation, users submit a query and wait in silence until the full response appears. With streaming, they see the response unfold in real-time\u2014a far more engaging experience.</p> <pre><code>sequenceDiagram\n    participant User\n    participant Frontend\n    participant Backend\n    participant Retriever\n    participant Generator\n\n    User-&gt;&gt;Frontend: Submits query\n    Frontend-&gt;&gt;Backend: Sends query\n    Note over Frontend: Shows \"Thinking...\" animation\n\n    Backend-&gt;&gt;Retriever: Requests relevant documents\n    Retriever-&gt;&gt;Backend: Returns documents\n    Note over Backend: Documents retrieved\n\n    Backend-&gt;&gt;Generator: Generates response with documents\n    Note over Frontend: Shows \"Generating response...\"\n\n    loop Streaming\n        Generator-&gt;&gt;Backend: Streams token chunks\n        Backend-&gt;&gt;Frontend: Forwards token chunks\n        Frontend-&gt;&gt;User: Displays incremental response\n    end\n\n    Note over Frontend: Full response displayed</code></pre> <p>This sequence diagram illustrates how streaming transforms the user experience from a binary \"waiting/complete\" pattern to a continuous flow of information. Users can begin consuming and processing the response while the system continues generating later portions.</p>"},{"location":"workshops/chapter3-2/#technical-implementation-of-streaming","title":"Technical Implementation of Streaming","text":"<p>Implementing streaming requires coordination across your entire stack:</p> <ol> <li>A generation endpoint that supports streaming</li> <li>Backend routes that maintain open connections</li> <li>Frontend components that render incremental updates</li> </ol> <p>Most modern language models and APIs support streaming, though the specific implementation varies:</p> <pre><code># Example using OpenAI's API for streaming\nimport openai\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import StreamingResponse\nimport asyncio\n\napp = FastAPI()\n\n@app.post(\"/query/stream\")\nasync def stream_query_response(request: Request):\n    \"\"\"\n    Stream a response to a user query.\n\n    This endpoint:\n    1. Processes the incoming query\n    2. Retrieves relevant documents\n    3. Streams the generated response\n    \"\"\"\n    # Parse the incoming request\n    data = await request.json()\n    query = data.get(\"query\")\n\n    # Retrieve relevant documents (non-streaming part)\n    documents = retrieve_documents(query)\n    context = prepare_context(documents)\n\n    # Set up streaming response\n    async def event_generator():\n        # Create a streaming completion\n        response = await openai.ChatCompletion.acreate(\n            model=\"gpt-4\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                {\"role\": \"user\", \"content\": f\"Query: {query}\\n\\nContext: {context}\"}\n            ],\n            stream=True  # Enable streaming\n        )\n\n        # Yield chunks as they arrive\n        async for chunk in response:\n            if chunk.choices[0].delta.get(\"content\"):\n                yield f\"data: {chunk.choices[0].delta.content}\\n\\n\"\n            await asyncio.sleep(0.01)  # Small delay to control flow rate\n\n        yield \"data: [DONE]\\n\\n\"\n\n    # Return a streaming response\n    return StreamingResponse(\n        event_generator(),\n        media_type=\"text/event-stream\"\n    )\n</code></pre> <p>On the frontend, you'll need to handle Server-Sent Events (SSE) or WebSockets to receive and display the streamed content:</p> <pre><code>function streamResponse(query) {\n  // Clear previous response\n  const responseElement = document.getElementById(\"response\");\n  responseElement.innerHTML = \"\";\n\n  // Show that we're working on it\n  responseElement.innerHTML = '&lt;div class=\"typing-indicator\"&gt;Thinking...&lt;/div&gt;';\n\n  // Make a request to the streaming endpoint\n  fetch(\"/query/stream\", {\n    method: \"POST\",\n    headers: { \"Content-Type\": \"application/json\" },\n    body: JSON.stringify({ query: query }),\n  })\n    .then((response) =&gt; {\n      // Create a reader for the response stream\n      const reader = response.body.getReader();\n      const decoder = new TextDecoder();\n\n      // Remove the typing indicator\n      responseElement.innerHTML = \"\";\n\n      // Function to process stream chunks\n      function readChunk() {\n        reader.read().then(({ done, value }) =&gt; {\n          if (done) return;\n\n          // Decode and display the chunk\n          const chunk = decoder.decode(value);\n          responseElement.innerHTML += chunk;\n\n          // Scroll to the bottom to show new content\n          responseElement.scrollTop = responseElement.scrollHeight;\n\n          // Read the next chunk\n          readChunk();\n        });\n      }\n\n      // Start reading chunks\n      readChunk();\n    })\n    .catch((error) =&gt; {\n      responseElement.innerHTML = `Error: ${error.message}`;\n    });\n}\n</code></pre>"},{"location":"workshops/chapter3-2/#showing-function-call-arguments","title":"Showing Function Call Arguments","text":"<p>One unique advantage of streaming is the ability to show users not just the final response but also the thinking and processing that led to it. This creates engagement and builds trust by making the system's operation more transparent.</p> <p>For example, you can stream the function calls and arguments that your RAG system is using:</p> <pre><code>// Show thought process and function calls during streaming\nlet currentThought = \"\";\nlet lastUpdateTime = Date.now();\n\nfunction processStreamChunk(chunk) {\n  // Check if this is a thought or function call marker\n  if (chunk.startsWith(\"THINKING: \")) {\n    // Update the current thought\n    currentThought = chunk.replace(\"THINKING: \", \"\");\n\n    // Only update the display occasionally to avoid flickering\n    if (Date.now() - lastUpdateTime &gt; 200) {\n      document.getElementById(\"thinking\").innerText = currentThought;\n      lastUpdateTime = Date.now();\n    }\n  } else if (chunk.startsWith(\"FUNCTION: \")) {\n    // Extract and display function call information\n    const functionData = JSON.parse(chunk.replace(\"FUNCTION: \", \"\"));\n\n    document.getElementById(\"functions\").innerHTML += `\n      &lt;div class=\"function-call\"&gt;\n        &lt;div class=\"function-name\"&gt;${functionData.name}&lt;/div&gt;\n        &lt;div class=\"function-args\"&gt;${JSON.stringify(functionData.arguments, null, 2)}&lt;/div&gt;\n      &lt;/div&gt;\n    `;\n  } else {\n    // Regular content chunk for the response\n    document.getElementById(\"response\").innerHTML += chunk;\n  }\n}\n</code></pre> <p>This approach gives users insight into how their query is being processed, creating engagement during what would otherwise be idle waiting time.</p>"},{"location":"workshops/chapter3-2/#streaming-structured-data","title":"Streaming Structured Data","text":"<p>Streaming isn't limited to plain text\u2014you can stream structured data like citations, follow-up questions, or data visualizations. This technique is especially valuable for complex RAG applications where responses have multiple components.</p> <p>Here's how you might implement structured streaming for a response that includes an answer, citations, and follow-up questions:</p> <pre><code>async def stream_structured_response(query: str):\n    \"\"\"\n    Stream a structured response with multiple components.\n\n    Parameters:\n    - query: The user's question\n\n    Returns:\n    - A streaming response with structured components\n    \"\"\"\n    # Retrieve documents (non-streaming)\n    documents = retrieve_documents(query)\n\n    # Start streaming response components\n    async def generate_stream():\n        # Send response type indicator\n        yield json.dumps({\"type\": \"start\", \"components\": [\"answer\", \"citations\", \"followup\"]}) + \"\\n\"\n\n        # Stream the answer generation\n        answer_chunks = generate_answer_stream(query, documents)\n        async for chunk in answer_chunks:\n            yield json.dumps({\"type\": \"answer\", \"content\": chunk}) + \"\\n\"\n            await asyncio.sleep(0.02)\n\n        # Stream citations after the answer\n        citations = extract_citations(documents)\n        for citation in citations:\n            yield json.dumps({\n                \"type\": \"citation\",\n                \"id\": citation[\"id\"],\n                \"title\": citation[\"title\"],\n                \"text\": citation[\"text\"][:100] + \"...\",\n                \"relevance\": citation[\"relevance\"]\n            }) + \"\\n\"\n            await asyncio.sleep(0.05)\n\n        # Generate and stream follow-up questions\n        followups = generate_followup_questions(query, documents)\n        yield json.dumps({\"type\": \"followup\", \"questions\": followups}) + \"\\n\"\n\n        # Signal completion\n        yield json.dumps({\"type\": \"end\"}) + \"\\n\"\n\n    return StreamingResponse(generate_stream(), media_type=\"application/json\")\n</code></pre> <p>On the frontend, you'd handle this structured stream by updating different UI components based on the message type:</p> <pre><code>function handleStructuredStream(stream) {\n  const reader = stream.getReader();\n  const decoder = new TextDecoder();\n\n  function processChunk() {\n    reader.read().then(({ done, value }) =&gt; {\n      if (done) return;\n\n      // Decode and parse the chunk\n      const chunk = decoder.decode(value);\n      const lines = chunk.split(\"\\n\").filter((line) =&gt; line.trim() !== \"\");\n\n      // Process each line as a separate JSON message\n      for (const line of lines) {\n        try {\n          const message = JSON.parse(line);\n\n          switch (message.type) {\n            case \"start\":\n              // Initialize UI components\n              document.getElementById(\"answer\").innerHTML = \"\";\n              document.getElementById(\"citations\").innerHTML = \"\";\n              document.getElementById(\"followup\").innerHTML = \"\";\n              break;\n\n            case \"answer\":\n              // Append to the answer section\n              document.getElementById(\"answer\").innerHTML += message.content;\n              break;\n\n            case \"citation\":\n              // Add a new citation\n              const citationEl = document.createElement(\"div\");\n              citationEl.classList.add(\"citation\");\n              citationEl.innerHTML = `\n                &lt;h4&gt;${message.title}&lt;/h4&gt;\n                &lt;p&gt;${message.text}&lt;/p&gt;\n                &lt;div class=\"relevance-meter\" style=\"width: ${message.relevance * 100}%\"&gt;&lt;/div&gt;\n              `;\n              document.getElementById(\"citations\").appendChild(citationEl);\n              break;\n\n            case \"followup\":\n              // Add follow-up questions\n              const followupEl = document.getElementById(\"followup\");\n              followupEl.innerHTML = \"&lt;h3&gt;Follow-up Questions:&lt;/h3&gt;\";\n\n              for (const question of message.questions) {\n                const questionEl = document.createElement(\"button\");\n                questionEl.classList.add(\"followup-question\");\n                questionEl.innerText = question;\n                questionEl.onclick = () =&gt; submitQuery(question);\n                followupEl.appendChild(questionEl);\n              }\n              break;\n\n            case \"end\":\n              // Complete the response, perhaps with a final animation\n              document.querySelector(\".loading-indicator\").style.display =\n                \"none\";\n              break;\n          }\n        } catch (error) {\n          console.error(\"Error parsing stream message:\", error);\n        }\n      }\n\n      // Continue reading\n      processChunk();\n    });\n  }\n\n  // Start processing chunks\n  processChunk();\n}\n</code></pre> <p>This approach creates a dynamic, engaging experience where different parts of the response appear progressively, keeping users engaged throughout the generation process.</p>"},{"location":"workshops/chapter3-2/#meaningful-interstitials-making-waiting-engaging","title":"Meaningful Interstitials: Making Waiting Engaging","text":"<p>For situations where some processing must happen before any content can be displayed, well-designed interstitials can transform waiting time from a frustrating experience into an engaging one.</p> <p>The key principle is to make interstitials meaningful rather than generic. Instead of a simple spinning wheel, show information that helps users understand what's happening and build confidence that their query is being handled effectively.</p> <p>Meaningful vs. Generic Interstitials</p> <p>Generic Interstitial: \"Loading...\"</p> <pre><code>**Meaningful Interstitial:**\n\"Searching 382,549 documents in our knowledge base...\"\n\"Finding relevant precedent cases from 2021-2022...\"\n\"Analyzing 3 legal frameworks that might apply to your question...\"\n</code></pre> <p>Meaningful interstitials should:</p> <ol> <li>Be specific about what the system is doing</li> <li>Include actual metrics when possible (number of documents, etc.)</li> <li>Update dynamically to show progress</li> <li>Maintain a confident, authoritative tone</li> </ol> <p>Here's how you might implement meaningful interstitials:</p> <pre><code>async def generate_interstitials(query: str):\n    \"\"\"\n    Generate meaningful interstitial messages for a query.\n\n    Parameters:\n    - query: The user's question\n\n    Returns:\n    - A sequence of interstitial messages\n    \"\"\"\n    # Analyze the query to determine appropriate interstitials\n    category = classify_query(query)\n\n    # Define category-specific interstitials\n    interstitials = {\n        \"technical\": [\n            \"Scanning documentation and code repositories...\",\n            \"Identifying relevant code examples and patterns...\",\n            \"Analyzing technical specifications and requirements...\",\n        ],\n        \"legal\": [\n            \"Searching legal databases and precedents...\",\n            \"Reviewing relevant case law and statutes...\",\n            \"Analyzing jurisdictional applicability...\",\n        ],\n        \"medical\": [\n            \"Consulting medical literature and guidelines...\",\n            \"Reviewing clinical studies and research papers...\",\n            \"Analyzing treatment protocols and best practices...\",\n        ],\n        # Add other categories as needed\n    }\n\n    # Add domain-specific metrics if available\n    try:\n        # For technical queries, add repository info\n        if category == \"technical\":\n            repo_count = get_repository_count()\n            interstitials[\"technical\"].append(f\"Searching across {repo_count} code repositories...\")\n\n        # For legal queries, add document counts\n        elif category == \"legal\":\n            case_count = get_case_count()\n            interstitials[\"legal\"].append(f\"Analyzing {case_count} potentially relevant cases...\")\n    except:\n        # Fall back to generic but still domain-specific messages\n        pass\n\n    # Get the relevant list based on category, or use default\n    message_list = interstitials.get(category, [\n        \"Processing your query...\",\n        \"Searching for relevant information...\",\n        \"Analyzing related documents...\"\n    ])\n\n    # Return the message list\n    return message_list\n</code></pre> <p>On the frontend, you'd display these interstitials in sequence during the waiting period:</p> <pre><code>async function showInterstitials(query) {\n  // Request interstitial messages\n  const response = await fetch(\"/interstitials\", {\n    method: \"POST\",\n    headers: { \"Content-Type\": \"application/json\" },\n    body: JSON.stringify({ query }),\n  });\n\n  const interstitials = await response.json();\n  const interstitialElement = document.getElementById(\"interstitial\");\n\n  // Display each interstitial in sequence\n  let index = 0;\n\n  function showNextInterstitial() {\n    if (index &gt;= interstitials.length) index = 0;\n\n    // Fade out current message\n    interstitialElement.classList.add(\"fade-out\");\n\n    // After fade out, update text and fade in\n    setTimeout(() =&gt; {\n      interstitialElement.textContent = interstitials[index];\n      interstitialElement.classList.remove(\"fade-out\");\n      interstitialElement.classList.add(\"fade-in\");\n\n      // Remove the fade-in class after animation completes\n      setTimeout(() =&gt; {\n        interstitialElement.classList.remove(\"fade-in\");\n      }, 500);\n\n      index++;\n    }, 500);\n  }\n\n  // Show first interstitial immediately\n  interstitialElement.textContent = interstitials[0];\n  index++;\n\n  // Change interstitial every few seconds\n  const intervalId = setInterval(showNextInterstitial, 3000);\n\n  // Return a function to clear the interval when response arrives\n  return () =&gt; clearInterval(intervalId);\n}\n</code></pre>"},{"location":"workshops/chapter3-2/#optimizing-actual-performance","title":"Optimizing Actual Performance","text":"<p>While perceived performance is critical, we shouldn't neglect actual performance optimizations. Here are several strategies for reducing real latency in RAG applications:</p>"},{"location":"workshops/chapter3-2/#1-optimize-your-retrieval-pipeline","title":"1. Optimize Your Retrieval Pipeline","text":"<p>The retrieval phase is often the most time-consuming part of a RAG system. Consider these optimizations:</p> <ul> <li>Use approximate nearest neighbor search instead of exact search for large collections</li> <li>Implement a tiered retrieval approach that filters candidates quickly before precise ranking</li> <li>Pre-compute and cache embeddings for your document collection</li> <li>Shard your vector database to distribute search across multiple instances</li> </ul>"},{"location":"workshops/chapter3-2/#2-implement-caching","title":"2. Implement Caching","text":"<p>Caching dramatically improves performance for repeated or similar queries:</p> <ul> <li>Semantic caching: Cache results based on embedding similarity, not just exact matches</li> <li>Fragment caching: Cache individual retrieved documents even if the full query is new</li> <li>Result caching: Store complete responses for common queries</li> </ul> <p>Here's a simple implementation of semantic caching:</p> <pre><code>from scipy.spatial.distance import cosine\n\nclass SemanticCache:\n    def __init__(self, embedding_function, similarity_threshold=0.92):\n        \"\"\"\n        Initialize a semantic cache.\n\n        Parameters:\n        - embedding_function: Function to convert text to embeddings\n        - similarity_threshold: Threshold above which queries are considered similar\n        \"\"\"\n        self.embedding_function = embedding_function\n        self.similarity_threshold = similarity_threshold\n        self.cache = []  # List of (query, embedding, result) tuples\n\n    def get(self, query):\n        \"\"\"\n        Try to retrieve a result from cache based on semantic similarity.\n\n        Parameters:\n        - query: The current query\n\n        Returns:\n        - Cached result if a similar query exists, None otherwise\n        \"\"\"\n        if not self.cache:\n            return None\n\n        # Compute embedding for the current query\n        query_embedding = self.embedding_function(query)\n\n        # Find the most similar cached query\n        best_similarity = 0\n        best_result = None\n\n        for cached_query, cached_embedding, cached_result in self.cache:\n            similarity = 1 - cosine(query_embedding, cached_embedding)\n\n            if similarity &gt; best_similarity:\n                best_similarity = similarity\n                best_result = cached_result\n\n        # Return the result if similarity is above threshold\n        if best_similarity &gt;= self.similarity_threshold:\n            return best_result\n\n        return None\n\n    def add(self, query, result):\n        \"\"\"\n        Add a query-result pair to the cache.\n\n        Parameters:\n        - query: The query string\n        - result: The result to cache\n        \"\"\"\n        query_embedding = self.embedding_function(query)\n        self.cache.append((query, query_embedding, result))\n\n        # Optionally implement cache eviction strategies here\n</code></pre>"},{"location":"workshops/chapter3-2/#3-implement-progressive-loading","title":"3. Implement Progressive Loading","text":"<p>Load different components of your response progressively, with the most important parts first:</p> <ul> <li>Show the direct answer before loading citations</li> <li>Display key findings before detailed explanations</li> <li>Show high-confidence sections before speculative ones</li> </ul>"},{"location":"workshops/chapter3-2/#4-optimize-model-usage","title":"4. Optimize Model Usage","text":"<p>Language model inference can be optimized through:</p> <ul> <li>Quantization: Use 8-bit or 4-bit quantized models where appropriate</li> <li>Distillation: Train smaller, faster models for specific query types</li> <li>Parallel inference: Process multiple documents or query components simultaneously</li> <li>Model selection: Use smaller models for simpler tasks, reserving larger models for complex reasoning</li> </ul>"},{"location":"workshops/chapter3-2/#the-connection-between-streaming-performance-and-feedback","title":"The Connection Between Streaming, Performance, and Feedback","text":"<p>The techniques discussed in this chapter aren't just about improving user experience\u2014they directly strengthen the feedback collection mechanisms we established in Chapter 3.1.</p> <p>Research consistently shows that users provide more feedback when systems feel responsive and engaging. When users abandon sessions due to perceived slowness, you lose valuable feedback opportunities. By implementing streaming and meaningful interstitials, you create an experience that keeps users engaged, increasing the likelihood they'll provide feedback.</p> <p>In our experience, implementations with effective streaming collect 30-40% more feedback compared to traditional \"wait and display\" approaches. This creates a virtuous cycle where better performance leads to more feedback, which enables more targeted improvements, which in turn enhances performance.</p> <p>The most successful RAG applications aren't just accurate\u2014they're responsive, engaging, and transparent. By applying the techniques in this chapter, you create an experience that keeps users engaged throughout the interaction, building trust and encouraging the feedback that fuels continuous improvement.</p>"},{"location":"workshops/chapter3-2/#conclusion-performance-as-experience-design","title":"Conclusion: Performance as Experience Design","text":"<p>Throughout this chapter, we've explored how to overcome latency through a combination of streaming responses, meaningful interstitials, and technical optimizations. The key insight is that performance isn't just a technical concern\u2014it's a fundamental aspect of experience design.</p> <p>By implementing streaming, you transform the user experience from a binary \"waiting/complete\" pattern to a continuous flow of information. By designing meaningful interstitials, you make waiting time both informative and engaging. And by optimizing actual performance, you reduce the waiting time itself.</p> <p>These approaches work in concert to create a responsive, engaging RAG experience that keeps users invested and encourages feedback. The result is not just a faster system but a more effective one that continuously improves through the feedback flywheel.</p> <p>In the next chapter, we'll build on these foundations by exploring quality-of-life improvements like interactive citations, chain-of-thought reasoning, and validation patterns. These elements further enhance the user experience while creating additional opportunities for feedback collection.</p>"},{"location":"workshops/chapter3-2/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>What aspects of your RAG application's user experience are most affected by latency?</p> </li> <li> <p>How could you modify your current interface to show meaningful progress during retrieval and generation?</p> </li> <li> <p>What information could you stream incrementally to improve perceived performance?</p> </li> <li> <p>Which components of your RAG pipeline are the biggest contributors to actual latency? How might you optimize them?</p> </li> <li> <p>How would implementing streaming affect your feedback collection mechanisms?</p> </li> </ol>"},{"location":"workshops/chapter3-2/#summary","title":"Summary","text":"<p>Latency is a critical challenge in RAG applications that affects both user experience and feedback collection. By implementing streaming responses, you transform waiting into an engaging experience where users see the answer unfold in real time. With meaningful interstitials, you make necessary waiting periods more informative and less frustrating. These techniques, combined with actual performance optimizations, create RAG applications that feel responsive and trustworthy even when complex processing is occurring. The result is not just better user experience but also more feedback, fueling the continuous improvement cycle.</p>"},{"location":"workshops/chapter3-2/#additional-resources","title":"Additional Resources","text":"<ol> <li> <p>Nielsen Norman Group, \"Progress Indicators Make a Slow System Less Insufferable\"</p> </li> <li> <p>Google Developers, \"Measuring Perceived Performance\"</p> </li> <li> <p>OpenAI Documentation, \"Streaming API Best Practices\"</p> </li> <li> <p>GitHub Repository: Streaming-RAG-Implementation - Example implementation of a streaming RAG application</p> </li> </ol>"},{"location":"workshops/chapter3-3/","title":"3.3 Quality of Life Improvements: Citations, Chain of Thought, and Validation","text":"<p>Chapter Overview</p> <p>This chapter explores essential quality-of-life improvements that transform RAG systems from functional to exceptional. You'll learn how to implement interactive citations that build trust while collecting valuable feedback, techniques for making reasoning transparent through chain-of-thought approaches, and validation methods that catch errors before they reach users. These enhancements not only improve immediate user experience but also strengthen your feedback flywheel by creating additional opportunities for collecting insights. By implementing these techniques, you'll create a RAG system that users genuinely enjoy using\u2014one that explains its reasoning, justifies its answers, and demonstrates reliability through self-correction.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#introduction-from-functional-to-exceptional","title":"Introduction: From Functional to Exceptional","text":"<p>In the previous two chapters, we established critical foundations for successful RAG applications: robust feedback collection mechanisms in Chapter 3.1 and techniques to overcome latency in Chapter 3.2. Now we're ready to elevate our system from merely functional to truly exceptional by addressing the subtle but powerful elements that transform user experience.</p> <p>Think of a RAG system as similar to a home. The feedback mechanisms are like the foundation\u2014essential but invisible to most visitors. The streaming capabilities are like the utilities\u2014noticed primarily when they're not working well. What we'll cover in this chapter are the finishing touches that visitors actually see and interact with directly: the paint, the furniture, the artwork\u2014the elements that transform a house into a home.</p> <p>These \"quality of life\" improvements may seem like optional enhancements, but they often make the difference between systems that are occasionally useful and those that become indispensable tools that users rely on daily. They build trust through transparency, improve reasoning through explicit thinking processes, and prevent errors before they reach users.</p> <p>In this chapter, we'll explore three categories of improvements:</p> <ol> <li>Citations: How to transform static references into interactive elements that build trust while providing valuable feedback signals</li> <li>Chain of Thought: Techniques to make reasoning transparent, improving both accuracy and user confidence</li> <li>Validation: Methods to catch errors before they reach users, creating more reliable experiences</li> </ol> <p>Each of these approaches not only enhances immediate user experience but also strengthens the feedback flywheel we've been building throughout these chapters. By implementing these techniques, you'll create a RAG system that users not only tolerate but genuinely enjoy using\u2014a system that explains its reasoning, justifies its answers, and catches its own mistakes.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#beyond-the-basics-elevating-user-experience","title":"Beyond the Basics: Elevating User Experience","text":"<p>When I first started working with RAG systems, I viewed success primarily through the lens of retrieval accuracy and response relevance. If a system could find the right information and generate a coherent answer, I considered it a job well done. But as I gained experience deploying these systems to real users, I realized that truly exceptional RAG applications go beyond these basic capabilities to deliver experiences that build trust, showcase reasoning, and prevent errors before they happen.</p> <p>In this chapter, we'll explore what I've come to think of as \"quality of life improvements\"\u2014techniques that transform a technically sound RAG system into a delightful user experience. These approaches don't necessarily improve retrieval or generation in a fundamental way, but they dramatically enhance how users perceive and interact with your system. And as we'll see, many of these techniques create valuable opportunities for additional feedback collection, strengthening the flywheel we established in Chapter 3.1.</p> <p>After implementing the feedback collection mechanisms from Chapter 3.1 and the streaming techniques from Chapter 3.2, you've already built a solid foundation. Now we'll add the finishing touches that create a truly exceptional experience.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#citations-building-trust-through-transparency","title":"Citations: Building Trust Through Transparency","text":"","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#the-dual-purpose-of-citations","title":"The Dual Purpose of Citations","text":"<p>When I first started working with RAG systems, I viewed citations as primarily a trust-building mechanism\u2014a way to show users that the system's responses were grounded in actual documents rather than fabricated. While this is certainly valuable, I've come to realize that citations serve an equally important purpose: they're golden opportunities for feedback collection.</p> <p>Think about it this way. When a user sees a citation in a response, they're naturally curious about the source. By making citations interactive and engaging, you create touchpoints for feedback that feel natural and contextual. This isn't just about slapping a thumbs-up button on your interface; it's about integrating feedback collection into the core user experience.</p> <p>The most effective approach turns citations from static references into interactive elements that users can engage with:</p> <ol> <li>Quote different parts of responses and visually link them to specific citations</li> <li>Allow users to expand citations to review the full context</li> <li>Enable users to provide feedback on individual citations</li> <li>Let users remove irrelevant citations and request regeneration</li> </ol> <pre><code>graph TD\n    A[Generated Response] --&gt;|Contains| B[Interactive Citations]\n    B --&gt;|User Expands| C[Citation Content]\n    B --&gt;|User Marks Relevant| D[Positive Training Example]\n    B --&gt;|User Marks Irrelevant| E[Negative Training Example]\n    B --&gt;|User Removes| F[Regeneration Request]\n\n    style B fill:#f9d77e,stroke:#333,stroke-width:2px</code></pre> <p>I worked with a legal research team that implemented this approach for their in-house attorneys. Each generated response included interactive citations linked to specific case law or statutes. Attorneys could click on citations to see the full context, and importantly, they could mark citations as relevant or irrelevant to their query. When they marked a citation as irrelevant, the system would regenerate the response without using that source.</p> <p>This interaction served two purposes: it immediately improved the user experience by removing unhelpful information, and it generated invaluable training data for our retrieval system. Each marked citation became labeled data that helped us fine-tune our embedding models. Within three months, we had collected over 50,000 labeled examples\u2014a dataset that would have been prohibitively expensive to create manually.</p> <p>Citations as UI Elements</p> <p>Design your citations not just as references but as interactive UI elements. When users can explore, evaluate, and modify citations, they become active participants in improving your system rather than passive consumers of information.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#crafting-citation-rich-responses","title":"Crafting Citation-Rich Responses","text":"<p>Creating effective citations begins with how you prompt your language model. Instead of treating citations as an afterthought, build them into your response generation process from the ground up.</p> <p>Here's a prompt template that encourages detailed, well-structured citations:</p> <pre><code>def create_citation_prompt(query: str, documents: list):\n    \"\"\"\n    Create a prompt that encourages detailed citation usage.\n\n    Parameters:\n    - query: The user's question\n    - documents: Retrieved documents for context\n\n    Returns:\n    - A structured prompt that will generate well-cited responses\n    \"\"\"\n    # Format document context with identifiers\n    formatted_docs = []\n    for i, doc in enumerate(documents):\n        formatted_docs.append(f\"DOCUMENT [{i+1}]: {doc.title}\\n{doc.content}\")\n\n    context = \"\\n\\n\".join(formatted_docs)\n\n    prompt = f\"\"\"\n    Answer the following question based ONLY on the provided documents.\n    For each piece of information in your answer, include a citation to the specific document it came from using the format [X] where X is the document number.\n\n    If the documents don't contain enough information to fully answer the question, say so clearly and cite which documents you used for the partial answer.\n\n    At the end of your answer, include a \"Sources\" section that lists all the documents you cited.\n\n    QUESTION: {query}\n\n    DOCUMENTS:\n    {context}\n\n    ANSWER (with citations):\n    \"\"\"\n\n    return prompt\n</code></pre> <p>On the frontend, you can transform these citations into interactive elements:</p> <pre><code>function renderCitedResponse(response) {\n  // Parse the response to extract citations\n  const citationRegex = /\\[(\\d+)\\]/g;\n  const citedResponse = response.answer.replace(\n    citationRegex,\n    (match, docNum) =&gt; {\n      // Create an interactive citation element\n      return `&lt;span class=\"citation\" \n                   data-doc-id=\"${docNum}\" \n                   onclick=\"expandCitation(${docNum})\"&gt;\n                ${match}\n              &lt;/span&gt;`;\n    },\n  );\n\n  // Render the response\n  document.getElementById(\"response\").innerHTML = citedResponse;\n\n  // Render the sources section\n  const sourcesElement = document.getElementById(\"sources\");\n  response.sources.forEach((source, index) =&gt; {\n    const sourceElement = document.createElement(\"div\");\n    sourceElement.className = \"source\";\n    sourceElement.innerHTML = `\n      &lt;div class=\"source-header\"&gt;\n        &lt;span class=\"source-number\"&gt;[${index + 1}]&lt;/span&gt;\n        &lt;span class=\"source-title\"&gt;${source.title}&lt;/span&gt;\n        &lt;div class=\"feedback-buttons\"&gt;\n          &lt;button onclick=\"markRelevant(${index})\"&gt;\u2713 Relevant&lt;/button&gt;\n          &lt;button onclick=\"markIrrelevant(${index})\"&gt;\u2717 Irrelevant&lt;/button&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n      &lt;div class=\"source-content\" id=\"source-content-${index}\" style=\"display: none;\"&gt;\n        ${source.content}\n      &lt;/div&gt;\n    `;\n    sourcesElement.appendChild(sourceElement);\n  });\n}\n</code></pre> <p>This creates an interactive experience where citations are visually distinct, clickable elements. When users engage with these elements, you can collect valuable feedback while enhancing their understanding of the response.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#chain-of-thought-making-thinking-visible","title":"Chain of Thought: Making Thinking Visible","text":"","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#the-underutilized-superpower","title":"The Underutilized Superpower","text":"<p>One of the most underutilized yet powerful techniques for improving RAG responses is chain of thought prompting\u2014asking the model to reason step by step before providing its final answer. This approach typically provides a 10% performance improvement for classification and reasoning tasks, which might sound modest until you realize it's often the difference between a system that's occasionally helpful and one that's consistently reliable.</p> <p>I've found chain of thought particularly valuable for complex retrieval tasks where multiple documents need to be synthesized or where subtle judgments about relevance are required. By making the reasoning explicit, you can identify where things might be going wrong and provide more targeted guidance.</p> <p>Performance Impact</p> <p>In our testing across multiple domains, chain of thought prompting consistently improved answer accuracy by 8-15%, with the biggest gains coming in complex reasoning scenarios like multi-hop questions and comparative analyses.</p> <p>When implementing chain of thought, structure it clearly to separate the thinking process from the final response. XML tags work well for this purpose, creating distinct sections that can be processed differently by your application:</p> <pre><code>def chain_of_thought_prompt(query: str, documents: list):\n    \"\"\"\n    Create a prompt that encourages step-by-step reasoning.\n\n    Parameters:\n    - query: The user's question\n    - documents: Retrieved documents for context\n\n    Returns:\n    - A prompt that will generate reasoning steps and a final answer\n    \"\"\"\n    context = \"\\n\\n\".join([f\"DOCUMENT: {doc.content}\" for doc in documents])\n\n    prompt = f\"\"\"\n    You will answer the user's question based on the provided documents.\n    First, think step by step about how to answer the question using the documents.\n    Then provide your final answer.\n\n    Structure your response like this:\n    &lt;thinking&gt;\n    Your step-by-step reasoning process here...\n    &lt;/thinking&gt;\n\n    &lt;answer&gt;\n    Your final answer here, with citations to specific documents...\n    &lt;/answer&gt;\n\n    USER QUESTION: {query}\n\n    DOCUMENTS:\n    {context}\n    \"\"\"\n\n    return prompt\n</code></pre> <p>Taking this a step further, you can stream the thinking process as a separate UI component or interstitial. This serves two purposes: it makes the waiting time more engaging by showing users that complex reasoning is happening, and it allows users to intervene if they notice the reasoning going astray.</p> <pre><code>let thinkingComplete = false;\nlet currentSection = null;\nlet thinking = \"\";\nlet answer = \"\";\n\nfunction processStreamedToken(token) {\n  // Check for section tags\n  if (token.includes(\"&lt;thinking&gt;\")) {\n    currentSection = \"thinking\";\n    return;\n  } else if (token.includes(\"&lt;/thinking&gt;\")) {\n    thinkingComplete = true;\n    currentSection = null;\n    // Render the complete thinking section\n    document.getElementById(\"thinking-container\").style.display = \"block\";\n    document.getElementById(\"thinking\").innerHTML = thinking;\n    return;\n  } else if (token.includes(\"&lt;answer&gt;\")) {\n    currentSection = \"answer\";\n    return;\n  } else if (token.includes(\"&lt;/answer&gt;\")) {\n    currentSection = null;\n    return;\n  }\n\n  // Process tokens based on current section\n  if (currentSection === \"thinking\") {\n    thinking += token;\n    // If thinking isn't complete yet, show it streaming\n    if (!thinkingComplete) {\n      document.getElementById(\"thinking-container\").style.display = \"block\";\n      document.getElementById(\"thinking\").innerHTML = thinking;\n    }\n  } else if (currentSection === \"answer\") {\n    answer += token;\n    document.getElementById(\"answer\").innerHTML = answer;\n  }\n}\n</code></pre> <p>I worked with a financial advisory firm that implemented this approach for their investment recommendation system. As the model reasoned through market conditions, client preferences, and portfolio considerations, this thinking was streamed to the advisor in a separate panel. If the advisor noticed a misunderstanding or faulty assumption in the reasoning, they could pause generation and refine their query before a final recommendation was produced.</p> <p>This interactive approach not only improved recommendation quality but also created a valuable feedback loop where advisors could correct misunderstandings early in the process. Each correction became training data that helped the system learn and improve over time.</p> <p>On the frontend, you can implement this with an expandable \"See reasoning\" section that users can toggle to view the model's step-by-step analysis. This transparency builds trust by demystifying the AI process and gives users insight into how conclusions were reached.</p> <p>The beauty of chain of thought isn't just that it improves immediate response quality\u2014it also creates a more explainable, trustworthy system that users feel comfortable relying on. In domains where decisions matter and consequences are real, this transparency can be the difference between a system that's used occasionally and one that becomes an indispensable tool.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#monologues-solving-the-context-management-problem","title":"Monologues: Solving the Context Management Problem","text":"","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#reasoning-in-limited-windows","title":"Reasoning in Limited Windows","text":"<p>As context windows grow larger, one might think that managing complex information would become easier. Counterintuitively, though, larger context windows often create new challenges for language models, which can struggle to attend to the most relevant information among thousands of tokens.</p> <p>Monologuing\u2014having the model explicitly reiterate key information before generating a response\u2014has emerged as a powerful technique to enhance reasoning and quality, especially with large contexts and complex documents.</p> <p>The process is wonderfully simple: ask the model to \"think out loud\" about what information is relevant before generating the final answer. This serves several purposes:</p> <ol> <li>It helps the model re-read and reinforce important context</li> <li>It allows the model to organize scattered information into a coherent structure</li> <li>It creates natural separation between reasoning and response</li> <li>It produces valuable data for future fine-tuning</li> </ol> <p>Here's an example prompt for implementing monologues:</p> <pre><code>def monologue_prompt(query: str, documents: list, pricing_data: str):\n    \"\"\"\n    Create a prompt that encourages monologuing for improved comprehension.\n\n    Parameters:\n    - query: The user's question about pricing options\n    - documents: Relevant call transcripts or customer information\n    - pricing_data: Pricing documentation and guidelines\n\n    Returns:\n    - A prompt that will generate a structured monologue before answering\n    \"\"\"\n    context = \"\\n\\n\".join([f\"TRANSCRIPT: {doc.content}\" for doc in documents])\n\n    prompt = f\"\"\"\n    You'll help generate a pricing quote based on the call transcript and pricing documentation.\n\n    First, reiterate the key variables that determine pricing options according to the documentation.\n    Then, identify specific parts of the transcript that relate to these variables.\n    Next, determine which pricing options from the documentation are most relevant.\n    Finally, provide a recommended pricing quote with justification.\n\n    QUESTION: {query}\n\n    TRANSCRIPT:\n    {context}\n\n    PRICING DOCUMENTATION:\n    {pricing_data}\n\n    MONOLOGUE AND ANSWER:\n    \"\"\"\n\n    return prompt\n</code></pre> <p>I want to share a specific case study that demonstrates the power of monologues. We were working with a SaaS company that needed to generate pricing quotes based on sales call transcripts and a complex pricing document. The initial approach\u2014simply providing the transcript and pricing document as context\u2014resulted in quotes that were often inconsistent or missed key information from the transcript.</p> <p>We implemented a monologue-based approach that instructed the model to:</p> <ol> <li>First reiterate the variables that determine pricing options</li> <li>Then identify relevant parts of the transcript that mention these variables</li> <li>Next determine which pricing tiers and options apply</li> <li>Finally generate the actual quote with justification</li> </ol> <p>The results were remarkable\u2014quote accuracy improved from 62% to 94%, and sales representatives reported much greater confidence in the system's recommendations. The monologue approach helped the model connect scattered information from the transcript with the precise rules in the pricing document, creating much more coherent reasoning.</p> <p>Monologue in Action</p> <p>Query: What pricing should we offer based on this call transcript?</p> <pre><code>**Monologue:**\nLet me identify the key pricing variables from our documentation:\n1. Number of users (determines tier)\n2. Required features (basic, professional, enterprise)\n3. Length of contract commitment (monthly vs. annual)\n4. Add-on options (API access, premium support)\n\nFrom the transcript, I can see:\n- The customer mentioned needing \"around 50 user seats\" (line 127)\n- They specifically asked about the \"workflow automation feature\" (line 213)\n- They stated \"we're looking for an annual commitment\" (line 289)\n- They expressed interest in \"API access for integration\" (line 342)\n\nAccording to our pricing documentation:\n- 50 users falls into the Medium Business tier ($30/user/month)\n- Workflow automation is part of the Professional plan\n- Annual commitment provides a 20% discount\n- API access is a $500/month add-on\n\n**Answer:**\nBased on the call transcript, I recommend offering the Professional plan with API access add-on:\n- 50 users \u00d7 $30/user/month = $1,500/month\n- API access add-on: $500/month\n- Subtotal: $2,000/month\n- Annual commitment (20% discount): $19,200/year\n\nThis aligns with their needs for workflow automation and API access while providing the annual discount they're expecting.\n</code></pre> <p>This example shows how monologues can dramatically improve comprehension and reasoning, especially for complex tasks with multiple documents. The approach doesn't require any special architecture\u2014just thoughtful prompting that encourages the model to organize information before generating a response.</p> <p>Monologues can also improve tonality and quality by separating reasoning from response generation. Have the model first reason about what to say, then say it in the desired tone. This creates datasets for future fine-tuning without reasoning steps, allowing you to eventually distill the reasoning process into more efficient models.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#validation-patterns-the-safety-net-approach","title":"Validation Patterns: The Safety Net Approach","text":"","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#catching-errors-before-they-reach-users","title":"Catching Errors Before They Reach Users","text":"<p>In the early days of RAG systems, there was a tendency to treat the language model's response as the final word\u2014whatever it generated went straight to the user, for better or worse. As these systems have evolved and stakes have increased, we've learned the value of adding validation layers that catch issues before they reach users.</p> <p>Think of validation patterns as safety nets for your RAG system. Just as a circus performer feels more confident with a net below, your system can be more ambitious knowing that validation checks will catch potential errors.</p> <p>Before implementing complex agent systems or multi-step pipelines, consider adding simple validation patterns to your RAG application. For latency-insensitive applications\u2014where an extra second or two of processing won't harm the user experience\u2014validators can significantly increase trust and satisfaction by ensuring responses meet quality standards.</p> <pre><code>sequenceDiagram\n    participant User\n    participant RAG as RAG System\n    participant Validator\n\n    User-&gt;&gt;RAG: Submits Query\n    RAG-&gt;&gt;RAG: Retrieves Documents\n    RAG-&gt;&gt;RAG: Generates Response\n    RAG-&gt;&gt;Validator: Submits Response for Validation\n\n    alt Response Passes Validation\n        Validator-&gt;&gt;RAG: Approves Response\n        RAG-&gt;&gt;User: Delivers Validated Response\n    else Response Fails Validation\n        Validator-&gt;&gt;RAG: Returns Specific Issues\n        RAG-&gt;&gt;RAG: Regenerates Response\n        RAG-&gt;&gt;Validator: Submits Revised Response\n        Validator-&gt;&gt;RAG: Approves Response\n        RAG-&gt;&gt;User: Delivers Validated Response\n    end</code></pre> <p>Validators act as a quality control layer that checks responses before they reach the user. The process is straightforward:</p> <ol> <li>Generate your reasoning, citations, and response as usual</li> <li>Pass the results to a secondary system (LLM or simple programmatic tests)</li> <li>Evaluate whether the response meets quality criteria</li> <li>If issues are found, provide specific feedback and regenerate</li> </ol> <p>I worked with a healthcare information provider that implemented a simple factual consistency validator for their patient-facing RAG system. After generating a response about treatment options, the validator would check whether all mentioned treatments were actually present in the retrieved documents and whether any contraindications or warnings had been omitted. If discrepancies were found, the response would be regenerated with specific instructions to correct the issues.</p> <p>This approach reduced factual errors by over 80% with minimal impact on latency\u2014a trade-off well worth making given the sensitive nature of healthcare information. The validator wasn't complex or expensive to implement, but it dramatically improved reliability and trustworthiness.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#a-practical-example-url-validation","title":"A Practical Example: URL Validation","text":"<p>Let me share a concrete example that illustrates the power of simple validators. I worked with a marketing team building a system to generate personalized follow-up emails that included links to case studies and marketing materials. The language model was excellent at crafting personalized messages, but we encountered a persistent problem: about 4% of generated emails contained URLs that either didn't exist or linked to internal resources that weren't publicly accessible.</p> <p>Rather than scrapping the approach or implementing a complex agent system, we added a straightforward validator that ran after response generation:</p> <pre><code>def validate_urls_in_email(email_body: str, allowed_domains: list):\n    \"\"\"\n    Validate that all URLs in an email are valid and from allowed domains.\n\n    Parameters:\n    - email_body: The generated email content\n    - allowed_domains: List of allowed domains for links\n\n    Returns:\n    - (is_valid, issues): Tuple of validation result and list of issues\n    \"\"\"\n    # Extract all URLs using regex\n    url_regex = r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n    urls = re.findall(url_regex, email_body)\n\n    issues = []\n\n    # Check each URL\n    for url in urls:\n        # Check if the domain is allowed\n        domain = urlparse(url).netloc\n        if domain not in allowed_domains:\n            issues.append(f\"URL {url} contains disallowed domain {domain}\")\n            continue\n\n        # Check if the URL exists (returns 200)\n        try:\n            response = requests.head(url, timeout=3)\n            if response.status_code != 200:\n                issues.append(f\"URL {url} returned status code {response.status_code}\")\n        except Exception as e:\n            issues.append(f\"URL {url} failed to connect: {str(e)}\")\n\n    return len(issues) == 0, issues\n\ndef regenerate_email_if_needed(query: str, initial_email: str, allowed_domains: list):\n    \"\"\"\n    Validate and potentially regenerate an email if URLs are problematic.\n    \"\"\"\n    is_valid, issues = validate_urls_in_email(initial_email, allowed_domains)\n\n    if is_valid:\n        return initial_email\n\n    # If validation failed, regenerate with specific guidance\n    issues_text = \"\\n\".join(issues)\n    regeneration_prompt = f\"\"\"\n    The previously generated email contained the following URL issues:\n    {issues_text}\n\n    Please regenerate the email, either:\n    1. Removing any problematic URLs entirely, or\n    2. Replacing them with valid URLs from these domains: {', '.join(allowed_domains)}\n\n    Original request: {query}\n    \"\"\"\n\n    regenerated_email = generate_email(regeneration_prompt)\n    return regenerated_email\n</code></pre> <p>The results were remarkable. After implementing this validator, the error rate dropped from 4% to 0% after just one retry. Even more interestingly, when we later fine-tuned the model on this dataset of corrected responses, the base error rate before validation dropped to nearly zero. The model had learned from its corrections, internalizing the patterns of valid URLs and avoiding problematic ones altogether.</p> <p>This example illustrates a broader principle: validation doesn't just catch errors\u2014it creates valuable training data that can improve your system over time. Each correction becomes a learning opportunity, gradually reducing the need for the validator itself.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#strategic-rejection-of-work","title":"Strategic Rejection of Work","text":"","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#when-i-dont-know-is-the-right-answer","title":"When \"I Don't Know\" is the Right Answer","text":"<p>Rather than delaying deployment until all edge cases are solved, implement strategic rejection for scenarios where your system isn't yet strong enough. This allows you to deploy sooner while collecting data to improve problematic segments.</p> <p>The approach is simple but powerful:</p> <ol> <li>Identify segments where performance is consistently poor</li> <li>Create rejection messages that set appropriate expectations</li> <li>Provide feedback forms to gather information about rejected queries</li> <li>Give users the option to proceed with caution if they wish</li> </ol> <p>This pattern works particularly well for specialized domains where some questions might require expertise your system hasn't yet developed. By acknowledging limitations transparently, you build trust while focusing on the areas where you can deliver value reliably.</p> <pre><code>def should_reject_query(query: str, confidence_threshold: float = 0.85):\n    \"\"\"\n    Determine if a query should be politely rejected.\n\n    Parameters:\n    - query: The user's question\n    - confidence_threshold: Minimum confidence to accept the query\n\n    Returns:\n    - (should_reject, reason): Whether to reject and why\n    \"\"\"\n    # Analyze the query\n    query_category = classify_query(query)\n    query_complexity = assess_complexity(query)\n    expected_confidence = predict_confidence(query, query_category, query_complexity)\n\n    # Check against thresholds\n    if expected_confidence &lt; confidence_threshold:\n        reason = f\"This appears to be a {query_category} question with {query_complexity} complexity. \" \\\n                 f\"Based on similar questions, our confidence is {expected_confidence:.2f}, \" \\\n                 f\"which is below our threshold of {confidence_threshold:.2f}.\"\n        return True, reason\n\n    return False, None\n\ndef handle_query_with_rejection(query: str):\n    \"\"\"\n    Process a query with potential rejection if the system isn't confident.\n    \"\"\"\n    should_reject, reason = should_reject_query(query)\n\n    if should_reject:\n        return {\n            \"type\": \"rejection\",\n            \"message\": f\"I'm not confident I can answer this question accurately. {reason}\",\n            \"allow_override\": True,\n            \"feedback_requested\": True\n        }\n    else:\n        # Process normally\n        documents = retrieve_documents(query)\n        response = generate_response(query, documents)\n        return {\n            \"type\": \"answer\",\n            \"message\": response\n        }\n</code></pre> <p>Design your rejection system with precision-recall tradeoffs in mind - avoid rejecting questions you can actually answer well. The rejection should always be polite, explain the limitation, and where possible, suggest alternative approaches or questions the system can handle.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#showcasing-capabilities","title":"Showcasing Capabilities","text":"","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#guide-users-to-what-you-do-well","title":"Guide Users to What You Do Well","text":"<p>Explicitly highlight what your system does well to guide user behavior:</p> <ul> <li>Show suggested query types that leverage your strengths</li> <li>Create UI elements that highlight special capabilities</li> <li>Provide examples of successful interactions</li> </ul> <p>Perplexity provides an excellent example of this approach. Their interface showcases different capabilities (web search, academic papers, math equations) with specific UI elements, guiding users toward interactions that will be successful.</p> <p>By highlighting certain capabilities, you not only improve user satisfaction by focusing on strengths, but you also set appropriate expectations about what the system doesn't handle well. This creates a more predictable experience where users know what to expect.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#putting-it-all-together-the-complete-experience","title":"Putting It All Together: The Complete Experience","text":"<p>When implemented together, these quality of life improvements create a comprehensive, trustworthy experience that elevates your RAG application above typical implementations:</p> <ol> <li>Streaming creates an engaging, responsive experience that keeps users engaged</li> <li>Citations build trust and provide opportunities for feedback collection</li> <li>Chain of thought makes reasoning transparent and improves accuracy</li> <li>Monologues enhance comprehension of complex information</li> <li>Validation catches errors before they reach users</li> <li>Strategic rejection sets appropriate expectations</li> <li>Capability showcasing guides users to successful interactions</li> </ol> <p>Each element reinforces the others, creating a system that feels polished, trustworthy, and genuinely helpful. Users don't just get answers\u2014they understand where those answers come from, see the reasoning behind them, and trust that they've been validated for accuracy.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#preparing-for-the-next-chapter","title":"Preparing for the Next Chapter","text":"<p>With these quality of life improvements in place, your RAG system now provides an exceptional user experience that builds trust, encourages engagement, and generates valuable feedback. In the next chapter, we'll explore how to make sense of all the data you're collecting through topic modeling and clustering techniques. These approaches will help you identify patterns in user queries and system performance, revealing the highest-impact opportunities for improvement.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#conclusion-the-complete-rag-experience","title":"Conclusion: The Complete RAG Experience","text":"<p>Throughout this chapter, we've explored techniques that transform a technically sound RAG system into an exceptional user experience. Let's recap the key principles we've covered:</p> <ol> <li> <p>Interactive citations build trust and collect feedback - By making citations explorable and interactive, you simultaneously build confidence and gather valuable training signals.</p> </li> <li> <p>Chain of thought reasoning improves accuracy and transparency - Making thinking visible not only leads to better answers but also helps users understand how conclusions were reached.</p> </li> <li> <p>Monologues enhance comprehension of complex information - Encouraging the model to organize and reiterate key information improves reasoning in complex contexts.</p> </li> <li> <p>Validation patterns catch errors before they reach users - Simple validation checks dramatically improve reliability without requiring complex agent architectures.</p> </li> <li> <p>Strategic rejection sets appropriate expectations - Being transparent about limitations builds trust while collecting data for future improvements.</p> </li> </ol> <p>These improvements work in concert with the feedback mechanisms from Chapter 3.1 and the streaming techniques from Chapter 3.2 to create a comprehensive, user-centered RAG experience. Each element reinforces the others: citations provide opportunities for feedback, streaming makes the thinking process engaging, and validation ensures that what users see is reliable.</p> <p>This completes our exploration of deployment and feedback collection. We've now built a robust system that not only delivers accurate information but does so in a way that users find trustworthy, engaging, and helpful. The system collects feedback naturally, feels responsive despite complex processing, and provides transparency into its reasoning and sources.</p> <p>In Chapter 4, we'll shift our focus to analyzing the wealth of data you're now collecting. Through topic modeling and clustering techniques, you'll learn to identify patterns in user queries and system performance, revealing focused opportunities for improvement. This marks an exciting transition from building a great system to understanding how it's being used in the real world and systematically enhancing its capabilities based on that understanding.</p> <p>By implementing the techniques from all three parts of Chapter 3, you've built the foundation for a continuous improvement cycle driven by user feedback and data analysis\u2014a system that doesn't just answer questions but gets better with every interaction.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter4/","title":"Topic Modeling and Clustering: Finding Patterns in User Feedback","text":"<p>Chapter Overview</p> <p>This chapter explores how to identify patterns in user queries and feedback to prioritize improvements:</p> <pre><code>- Understanding the difference between topics and capabilities\n- Applying topic modeling techniques to user queries\n- Categorizing queries for targeted improvements\n- Identifying high-value, high-impact opportunities\n- Creating a strategic roadmap for RAG enhancement\n</code></pre>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter4/#introduction-from-collecting-feedback-to-understanding-it","title":"Introduction: From Collecting Feedback to Understanding It","text":"<p>In Chapter 3, we built a comprehensive system for deployment and feedback collection: we designed mechanisms to capture valuable user signals in Chapter 3.1, implemented streaming to create engaging experiences in Chapter 3.2, and added quality-of-life improvements to enhance trust and transparency in Chapter 3.3. Now we're faced with a new challenge\u2014one that nearly every team encounters after successful deployment: making sense of all that feedback data.</p> <p>The first time I deployed a successful RAG system with robust feedback collection, I felt a mix of triumph and panic. Within weeks, we had thousands of queries, ratings, and interaction signals. But when my manager asked, \"So what should we improve next?\" I realized I had no systematic way to answer that question. We were drowning in data but struggling to extract actionable insights.</p> <p>This is where topic modeling and clustering become transformative. While it's tempting to dive into individual feedback instances or fixate on particularly negative comments, the real power comes from identifying patterns that reveal systematic opportunities for improvement. By grouping similar queries and analyzing performance patterns, you move from reacting to individual feedback to making strategic decisions about where to invest your limited resources.</p> <p>Key Philosophy</p> <p>\"Not all improvements are created equal. The art of systematic RAG development is identifying which capabilities will deliver the most value to your users.\"</p> <p>Think of this process as similar to a product manager analyzing customer segments. Just as not all customers have the same needs or value, not all query types deserve the same attention. Some query categories might represent a small percentage of volume but be critical to your most valuable users. Others might be frequent but easily satisfied with simple improvements. The goal is to move beyond \"making the AI better\" to precisely targeting your efforts where they'll have the maximum impact.</p> <p>In this chapter, we'll explore practical techniques for segmenting and analyzing user queries, identifying patterns in performance, and creating a strategic roadmap for improvement. By the end, you'll have a data-driven framework for deciding exactly where to focus your efforts and which capabilities to develop next.</p>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter4/#understanding-topics-and-capabilities","title":"Understanding Topics and Capabilities","text":"<p>Before diving into techniques, let's clarify two key concepts: topics and capabilities.</p>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter4/#topics-vs-capabilities","title":"Topics vs. Capabilities","text":"<p>!!! info \"Key Definitions\" - Topics: Subject matter domains in your content (e.g., \"account management,\" \"pricing,\" \"technical specifications\") - Capabilities: Functional abilities your system should have (e.g., \"summarization,\" \"comparison,\" \"step-by-step instructions\")</p> <p>These concepts provide two complementary lenses for analyzing user needs. When I work with teams to improve their RAG systems, I often find they conflate these two dimensions, leading to unfocused efforts. A topic tells you what users are asking about, while a capability tells you what they want the system to do with that information.</p> <pre><code>graph LR\n    A[User Queries] --&gt; B[Topic Analysis]\n    A --&gt; C[Capability Analysis]\n\n    B --&gt; D[What users ask about]\n    C --&gt; E[What users want the system to do]\n\n    D --&gt; F[Content Coverage Improvements]\n    E --&gt; G[Functional Improvements]\n\n    F --&gt; H[System Enhancements]\n    G --&gt; H</code></pre> <p>I remember working with a healthcare company whose initial analysis simply categorized queries by medical conditions. While helpful, this one-dimensional view missed critical patterns. When we added capability analysis, we discovered that queries about common conditions like diabetes primarily needed comparison capabilities (\"How does medication A compare to B?\"), while queries about rare conditions needed comprehensive summarization (\"Give me an overview of treatment options for X\"). This insight completely changed their improvement priorities.</p> <p>Topics vs. Capabilities Example</p> <p>Consider these user queries in a product support RAG:</p> <pre><code>- \"How do I reset my password?\"\n  - **Topic**: Account security\n  - **Capability**: Step-by-step instructions\n\n- \"Compare the Pro and Basic plans\"\n  - **Topic**: Pricing/Plans\n  - **Capability**: Comparison\n\n- \"Summarize the latest product release notes\"\n  - **Topic**: Product updates\n  - **Capability**: Summarization\n</code></pre> <p>Understanding both dimensions lets you prioritize more effectively. For instance, you might discover that comparison capabilities underperform across multiple topics, revealing a systematic weakness rather than a content gap.</p>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter4/#topic-modeling-techniques","title":"Topic Modeling Techniques","text":"<p>Now let's explore approaches for identifying topics in your user queries.</p>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter4/#automated-topic-modeling","title":"Automated Topic Modeling","text":"<p>Several algorithms can help identify topics in your query dataset:</p> <ol> <li>LDA (Latent Dirichlet Allocation): Traditional statistical approach</li> <li>BERTopic: Leverages transformer models for better semantic understanding</li> <li>LLM-based clustering: Uses language models to group and label topics</li> </ol> <p>When I began applying topic modeling to RAG applications in 2022, I used traditional techniques like LDA. While these worked reasonably well for longer documents, they often struggled with the brevity and specificity of user queries. Modern approaches like BERTopic and LLM-based clustering have dramatically improved our ability to identify meaningful patterns in even short, ambiguous queries.</p>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter4/#useful-clustering-libraries","title":"Useful Clustering Libraries","text":"<p>!!! tip \"Recommended Clustering Tools\" - BERTopic: A powerful library that combines transformers with traditional clustering - Top2Vec: Generates topic vectors without predefined numbers of topics - LLMClustering: Uses large language models for clustering with zero-shot classification - HDBSCAN: Density-based clustering that works well for identifying natural groupings - OpenAI Text Embedding Clustering: Using OpenAI embeddings with standard clustering algorithms</p> <p>Each of these approaches has different strengths. BERTopic excels at finding coherent topics in varied document collections, while LLM-based approaches can provide more intuitive topic labels. The key is that these tools are for analysis purposes\u2014they help you understand patterns in your data that would be difficult to detect manually.</p> <p>Cross-Reference</p> <p>Remember the synthetic data approaches from Chapter 1? You can use similar techniques to generate additional examples for each discovered topic to improve your evaluation sets.</p>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter4/#example-bertopic-implementation","title":"Example: BERTopic Implementation","text":"<p>BERTopic Code Example</p> <pre><code>from bertopic import BERTopic\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n    # Assuming 'queries' is a list of user query strings\n    # Configure BERTopic with more options for better control\n    vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\", min_df=5)\n\n    topic_model = BERTopic(\n        language=\"english\",\n        min_topic_size=10,\n        vectorizer_model=vectorizer,\n        calculate_probabilities=True,\n        verbose=True\n    )\n\n    # Fit the model and transform documents to topics\n    topics, probs = topic_model.fit_transform(queries)\n\n    # View topic information\n    topic_info = topic_model.get_topic_info()\n\n    # Extract the top 10 topics\n    top_topics = topic_info.head(10)\n\n    # For each top topic, get the representative documents\n    for topic_id in top_topics['Topic']:\n        if topic_id == -1:  # Skip the outlier topic\n            continue\n        print(f\"Topic {topic_id}: {topic_model.get_topic(topic_id)}\")\n        topic_docs = [queries[i] for i, t in enumerate(topics) if t == topic_id]\n        for doc in topic_docs[:5]:\n            print(f\"  - {doc}\")\n        print()\n\n    # Visualize topics\n    topic_model.visualize_topics()\n\n    # Save model for later use\n    topic_model.save(\"topic_model\")\n    ```\n\nIn one e-commerce project, this approach revealed that over 20% of user queries fell into a \"product comparison\" topic that our system handled poorly. We hadn't prioritized comparison capabilities because individual comparison queries seemed infrequent, but topic modeling revealed they formed a substantial segment when viewed collectively.\n\n#### Visualizing BERTopic Results\n\nBERTopic provides excellent visualization capabilities that can help communicate findings to stakeholders:\n\n```python\n# Create interactive intertopic distance map\ntopic_model.visualize_topics()\n\n# Show hierarchical clustering of topics\ntopic_model.visualize_hierarchy()\n\n# Visualize topic similarity\ntopic_model.visualize_heatmap()\n\n# Show topic distribution over time (if you have timestamp data)\ntopics_over_time = topic_model.topics_over_time(queries, timestamps)\ntopic_model.visualize_topics_over_time(topics_over_time)\n</code></pre> <p>These visualizations are invaluable for communicating your findings to stakeholders and building consensus around improvement priorities.</p>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter4/#llm-based-clustering-and-classification","title":"LLM-Based Clustering and Classification","text":"<p>While traditional clustering methods work well, I've found that LLM-based approaches can provide more intuitive and actionable results, especially for domain-specific applications.</p> <p>OpenAI Embeddings + HDBSCAN Clustering</p> <pre><code>import numpy as np\nimport openai\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport hdbscan\n\n    # Get embeddings for queries\n    def get_embeddings(queries, model=\"text-embedding-3-large\"):\n        embeddings = []\n\n        for query in queries:\n            response = openai.Embedding.create(\n                model=model,\n                input=query\n            )\n            embedding = response['data'][0]['embedding']\n            embeddings.append(embedding)\n\n        return np.array(embeddings)\n\n    # Get embeddings\n    query_embeddings = get_embeddings(queries)\n\n    # Cluster the embeddings\n    clusterer = hdbscan.HDBSCAN(\n        min_cluster_size=5,\n        min_samples=3,\n        metric='euclidean',\n        cluster_selection_method='eom'\n    )\n\n    cluster_labels = clusterer.fit_predict(query_embeddings)\n\n    # Add cluster labels to original data\n    df = pd.DataFrame({\n        'query': queries,\n        'cluster': cluster_labels\n    })\n\n    # For each cluster, get representative examples\n    for cluster_id in sorted(set(cluster_labels)):\n        if cluster_id == -1:\n            continue  # Skip noise points\n\n        cluster_examples = df[df['cluster'] == cluster_id]['query'].values\n        print(f\"Cluster {cluster_id}:\")\n        for example in cluster_examples[:5]:\n            print(f\"  - {example}\")\n        print()\n\n    # Use LLM to label each cluster\n    def get_cluster_label(examples):\n        prompt = f\"\"\"Given these example queries that belong to the same topic:\n        {examples}\n\n        Provide a short, descriptive label (3-5 words) for this topic that captures what these queries have in common.\n        \"\"\"\n\n        response = openai.ChatCompletion.create(\n            model=\"gpt-4\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant that labels query clusters.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        )\n\n        return response.choices[0].message.content.strip()\n\n    # Label each cluster\n    for cluster_id in sorted(set(cluster_labels)):\n        if cluster_id == -1:\n            continue\n\n        examples = df[df['cluster'] == cluster_id]['query'].values[:10]\n        examples_text = \"\\n\".join([f\"- {ex}\" for ex in examples])\n        label = get_cluster_label(examples_text)\n\n        print(f\"Cluster {cluster_id} \u2192 {label}\")\n    ```\n\nThis hybrid approach uses state-of-the-art embeddings with traditional clustering algorithms, then leverages LLMs to provide human-readable labels\u2014combining the best of both worlds.\n\n### From Clustering to Classification\n\n!!! important \"Analysis vs. Production\"\nIt's critical to understand that clustering is primarily an **analytical tool**, not a production monitoring solution. The clusters you discover during analysis should inform the development of robust classifiers that can categorize incoming queries in real-time.\n\nIn my experience, the journey from analysis to production follows a clear pattern:\n\n1. **Exploratory Phase**: Use clustering to discover patterns and identify key query categories\n2. **Taxonomy Development**: Create a standardized set of topics and capabilities\n3. **Classifier Development**: Build and train classifiers that can apply this taxonomy to new queries\n4. **Production Implementation**: Deploy the classifiers as part of your monitoring pipeline\n\nI once worked with a team that made the mistake of trying to use their clustering solution directly in production. The clusters would shift over time as new data arrived, making it impossible to track consistent metrics or identify clear trends. By switching to a classification approach based on their initial clustering insights, they created a stable monitoring system that could track performance over time.\n\n#### Building Production-Ready Classifiers\n\nOnce you've identified your key query categories through clustering, you can build classifiers using several approaches:\n\n1. **LLM-based Classification with Few-Shot Examples**\n\n   Large language models excel at classification tasks when provided with a few examples. This approach leverages the model's pre-trained knowledge and ability to understand context. By providing 2-3 examples per category in your prompt, you can achieve surprisingly accurate classification without any formal training.\n\n   The key advantage of this approach is its flexibility\u2014you can add new categories on the fly without retraining. It's particularly valuable in the early stages of your project when your taxonomy is still evolving. However, it does require API calls for each classification, which can increase latency and costs in high-volume applications.\n\n2. **Embedding-based Classification**\n\n   This approach uses vector embeddings to represent queries and categories in a shared semantic space. By calculating the average embedding for example queries in each category, you create \"centroid\" vectors that represent each category. New queries are classified by finding the closest category centroid.\n\n   Embedding-based classification is computationally efficient once the category embeddings are created, making it suitable for real-time applications. It works particularly well for capturing semantic similarity and can handle nuanced relationships between concepts. The downside is that it may struggle with categories that are distinguished by specific keywords rather than overall semantic meaning.\n\n3. **BERT-based Classification**\n\n   Fine-tuning a BERT model (or similar transformer architecture) creates a specialized classifier that can achieve high accuracy with sufficient training data. BERT models understand context bidirectionally and can capture subtle linguistic patterns that distinguish different query types.\n\n   The BERT approach shines when you have a stable taxonomy and sufficient examples for each category (typically 50+ per category for good results). Once trained, these models can run efficiently on GPU hardware and handle high query volumes. The main drawback is the initial training requirement and the need to retrain when adding new categories.\n\nIn practice, I've found that a hybrid approach works best for most RAG applications:\n\n- Start with LLM-based classification during the exploratory phase when your taxonomy is still evolving\n- Transition to embedding-based methods as your categories stabilize and query volume increases\n- Eventually implement a fine-tuned BERT classifier once you've collected sufficient training examples\n\nThis progression allows you to begin categorizing queries immediately while building toward a more efficient and accurate system over time. The insights gained from early classification efforts directly inform your training data collection for more sophisticated models.\n\n#### Setting Up Production Monitoring\n\nWith your classifiers in place, you can now implement a production monitoring system that tracks performance by query category:\n\n```python\ndef log_query_metrics(query, response, user_feedback=None):\n    \"\"\"\n    Log metrics for a user query, categorizing it and tracking performance.\n\n    Parameters:\n    - query: The user's query\n    - response: The system's response\n    - user_feedback: Optional user feedback (thumbs up/down, etc.)\n    \"\"\"\n    # Classify the query\n    query_category = classify_query(query, category_examples)\n    query_capability = identify_capability(query)\n\n    # Calculate metrics (customize based on your evaluation framework)\n    retrieval_quality = calculate_retrieval_quality(query, response)\n    generation_quality = calculate_generation_quality(response)\n\n    # Log to your monitoring system\n    log_entry = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"query\": query,\n        \"category\": query_category,\n        \"capability\": query_capability,\n        \"retrieval_quality\": retrieval_quality,\n        \"generation_quality\": generation_quality,\n        \"user_feedback\": user_feedback\n    }\n\n    # Add to your logging/monitoring solution\n    monitoring_db.insert_one(log_entry)\n</code></pre> <p>This structured approach allows you to:</p> <ul> <li>Track performance trends by category over time</li> <li>Identify emerging query types that might need new categories</li> <li>Spot performance degradation in specific categories</li> <li>Prioritize improvements based on volume and success rates</li> </ul> <p>In one healthcare RAG application, this monitoring approach revealed that while overall satisfaction metrics remained stable, there was significant degradation in queries about medication interactions\u2014an issue that wouldn't have been visible without category-specific monitoring.</p>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter4/#manual-query-categorization","title":"Manual Query Categorization","text":"<p>While automated methods are helpful, manual review offers valuable insights that algorithms might miss. In every RAG project I've worked on, a combination of automated and manual approaches yielded the best results.</p> <ol> <li>User intent labeling: Classify queries by what users are trying to accomplish</li> <li>Difficulty assessment: Rate queries by how challenging they are to answer</li> <li>Success/failure tagging: Label queries based on system performance</li> </ol> <p>Manual review is particularly valuable for understanding the \"why\" behind query patterns. For example, in a legal research application, we initially categorized certain queries as \"precedent searches\" based on automated clustering. However, manual review revealed that users weren't primarily interested in precedents; they were looking for contradictions between cases to support arguments. This nuance completely changed how we approached retrieval for those queries.</p> <p>Sample Size Considerations</p> <p>You don't need to manually categorize every query. A representative sample of 100-200 queries can provide significant insights while keeping the task manageable.</p>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter4/#combining-automated-and-manual-approaches","title":"Combining Automated and Manual Approaches","text":"<p>The most effective approach combines automation with human judgment:</p> <ol> <li>Run automated clustering to identify potential topics</li> <li>Manually review and refine clusters</li> <li>Use human expertise to identify themes automation might miss</li> <li>Create a curated taxonomy of topics and capabilities</li> </ol> <p>This hybrid approach provides the efficiency of automation with the nuanced understanding that comes from human review. I've found that after just a few hours of manual review, domain experts can identify patterns that might have taken weeks to discover purely through automated means.</p>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter4/#capability-analysis-framework","title":"Capability Analysis Framework","text":"<p>Beyond topics, understanding the capabilities users need is crucial. This analysis reveals functional gaps in your system that span across multiple topics.</p>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter4/#common-rag-capabilities","title":"Common RAG Capabilities","text":"<p>!!! info \"Core RAG Capabilities\" - Factual retrieval: Finding and presenting specific facts - Summarization: Condensing information into concise overviews - Comparison: Highlighting similarities and differences - Step-by-step procedures: Breaking down complex tasks - Reasoning with context: Drawing conclusions from multiple pieces of information - Numerical analysis: Working with numbers, statistics, and calculations - Visual information processing: Handling charts, images, and diagrams</p> <p>These capabilities often require specialized approaches beyond basic retrieval and generation. For example, effective comparison capabilities might need parallel queries or specialized prompting techniques, while numerical analysis might require formula extraction and calculation capabilities.</p> <p>Cross-Reference</p> <p>These capabilities become the foundation for specialized indices we'll build in Chapter 5.</p>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter4/#analyzing-capability-gaps","title":"Analyzing Capability Gaps","text":"<p>Once you've identified the capabilities your users need, the next step is determining where your system succeeds and struggles:</p> <ol> <li>Group user queries by required capability</li> <li>Analyze performance metrics across each group</li> <li>Identify capabilities with poor performance</li> <li>Categorize failure modes for each capability</li> </ol> <p>In a financial services RAG application I worked with, this analysis revealed that queries requiring numerical analysis had satisfaction rates 40% lower than other capability types. The system could find relevant documents containing financial data, but struggled to perform calculations or interpret trends from that data. This clear capability gap became a priority for improvement.</p> <pre><code>quadrantChart\n    title Capability Assessment Matrix\n    x-axis Low Demand --&gt; High Demand\n    y-axis Low Performance --&gt; High Performance\n    quadrant-1 \"Low Priority\"\n    quadrant-2 \"Maintain\"\n    quadrant-3 \"Quick Wins\"\n    quadrant-4 \"Strategic Focus\"\n    \"Factual Retrieval\": [0.8, 0.7]\n    \"Summarization\": [0.6, 0.3]\n    \"Comparison\": [0.7, 0.2]\n    \"Step-by-step\": [0.9, 0.5]\n    \"Reasoning\": [0.5, 0.3]\n    \"Numerical Analysis\": [0.4, 0.2]\n    \"Visual Processing\": [0.3, 0.1]</code></pre> <p>This visualization instantly reveals where to focus: comparison capabilities are in high demand but performing poorly, making them an ideal strategic focus area. Step-by-step procedures represent quick-win opportunities, while visual processing might be a lower priority despite its performance issues.</p>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter4/#prioritization-frameworks","title":"Prioritization Frameworks","text":"<p>With topics and capabilities identified, how do you prioritize improvements? Too often, teams make these decisions based on which user complained most loudly or which improvement seems easiest to implement. A systematic approach requires balancing multiple factors.</p>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter4/#the-impacteffort-matrix","title":"The Impact/Effort Matrix","text":"<p>The classic impact/effort matrix provides a simple but effective framework for prioritization. I've used this approach with numerous teams to transform vague improvement plans into clear roadmaps with stakeholder buy-in.</p> <p>Impact/Effort Prioritization</p> <p>Plot potential improvements on two axes:</p> <pre><code>- **Impact**: Frequency \u00d7 Value to users\n- **Effort**: Development complexity and resources required\n\nPrioritize high-impact, low-effort improvements first, followed by high-impact, high-effort strategic investments.\n</code></pre> <pre><code>quadrantChart\n    title Impact vs. Effort Prioritization\n    x-axis Low Effort --&gt; High Effort\n    y-axis Low Impact --&gt; High Impact\n    quadrant-1 \"Consider\"\n    quadrant-2 \"Defer\"\n    quadrant-3 \"Quick Wins\"\n    quadrant-4 \"Strategic Projects\"\n    \"Improve Password Reset\": [0.2, 0.6]\n    \"Add Product Comparison\": [0.7, 0.8]\n    \"Fix Citation Links\": [0.3, 0.5]\n    \"Add Table Support\": [0.8, 0.7]\n    \"Optimize Retrieval Speed\": [0.4, 0.9]\n    \"Support Image Analysis\": [0.9, 0.4]\n    \"Enhance Error Messages\": [0.2, 0.5]</code></pre> <p>When applying this framework to a customer service RAG application, we identified that adding product comparison capabilities would require significant effort but would address a high-impact need. Meanwhile, fixing citation links offered a quick win that could be implemented immediately while the larger strategic project was underway.</p>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter4/#the-value-driven-approach","title":"The Value-Driven Approach","text":"<p>Beyond simple impact/effort, a more sophisticated approach considers multiple dimensions of value:</p> <ol> <li>Business value: Revenue impact or cost savings</li> <li>Strategic alignment: Support for key business initiatives</li> <li>Technical foundation: Enabling future improvements</li> <li>User satisfaction: Impact on overall experience</li> </ol> <p>I once worked with a team that initially deprioritized table extraction capabilities based purely on query frequency. However, when we considered business value, we discovered that table-based queries, while less common, primarily came from procurement teams making high-value purchasing decisions. The business impact of improving this capability far outweighed its relatively low frequency.</p> <p>Value Calculation</p> <p>Value = (Frequency \u00d7 Importance \u00d7 Success Gap) / Effort</p> <pre><code>Where:\n- Frequency = How often users ask about this topic/capability\n- Importance = How critical it is to their goals (1-5 scale)\n- Success Gap = Current success rate subtracted from 100%\n- Effort = Implementation complexity (1-5 scale)\n</code></pre> <p>This formula balances multiple factors to identify the highest-value improvements. It recognizes that the most valuable improvements aren't always the most requested features\u2014they're the ones that address critical needs where your system currently underperforms.</p>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter4/#creating-a-strategic-roadmap","title":"Creating a Strategic Roadmap","text":"<p>Based on your analysis, the next step is developing a strategic roadmap for improvements. This roadmap should balance quick wins with strategic investments, creating momentum while building toward long-term capabilities.</p>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter4/#roadmap-components","title":"Roadmap Components","text":"<p>Strategic Roadmap Elements</p> <p>A complete RAG improvement roadmap includes:</p> <pre><code>- **Short-term wins**: Quick, high-impact improvements\n- **Medium-term projects**: Substantial capability enhancements\n- **Long-term investments**: Fundamental architectural improvements\n- **Continuous improvement**: Ongoing refinement processes\n- **Evaluation milestones**: Points to measure progress\n</code></pre> <p>I've found that successful roadmaps maintain a balance of 60% capability-focused improvements and 40% topic-focused improvements. The capability improvements tend to have broader impact across multiple query types, while topic improvements address specific content needs for high-value domains.</p> <p>Cross-Reference</p> <p>This roadmap should align with the specialized indices and multimodal RAG strategies we'll explore in Chapter 5.</p>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter4/#example-strategic-roadmap","title":"Example Strategic Roadmap","text":"<p>Consider this example roadmap based on analysis of a product support RAG:</p> <pre><code>gantt\n    title RAG Improvement Roadmap\n    dateFormat  YYYY-MM-DD\n\n    section Quick Wins\n    Improve Password Reset Instructions     :2025-04-01, 7d\n    Fix Citation Formatting                 :2025-04-01, 5d\n    Enhance Error Messages                  :2025-04-08, 7d\n\n    section Medium-Term Projects\n    Add Product Comparison Capability       :2025-04-15, 14d\n    Implement Summarization                 :2025-04-22, 21d\n\n    section Strategic Investments\n    Table Extraction &amp; Querying             :2025-05-01, 30d\n    Image Analysis Integration              :2025-05-15, 45d\n\n    section Continuous Improvement\n    Weekly Metric Reviews                   :2025-04-01, 180d\n    Monthly Content Updates                 :2025-04-01, 180d</code></pre> <p>This roadmap balances immediate improvements with longer-term strategic investments. It begins with quick wins to build momentum and demonstrate value, followed by more substantial capability enhancements, and finally foundational investments in specialized capabilities that will differentiate the system.</p>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter4/#advanced-analysis-techniques","title":"Advanced Analysis Techniques","text":"<p>Beyond basic clustering, several advanced techniques can provide deeper insights into user needs and system performance.</p>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter4/#embedding-visualization","title":"Embedding Visualization","text":"<p>Use dimensionality reduction to visualize query clusters, revealing relationships that might not be apparent in simple categorization. I've found this approach particularly valuable for identifying edge cases and understanding the boundaries between different query categories.</p> <p>UMAP Visualization Code</p> <pre><code>import numpy as np\nimport umap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n    # Assuming 'query_embeddings' is an array of embedding vectors\n    # and 'topic_labels' contains the topic for each query\n\n    # Reduce embeddings to 2D for visualization\n    reducer = umap.UMAP(random_state=42)\n    embedding_2d = reducer.fit_transform(np.array(query_embeddings))\n\n    # Create a scatter plot colored by topic\n    plt.figure(figsize=(12, 10))\n    sns.scatterplot(\n        x=embedding_2d[:, 0],\n        y=embedding_2d[:, 1],\n        hue=topic_labels,\n        palette=\"tab20\",\n        alpha=0.7\n    )\n    plt.title(\"Query Embeddings by Topic\")\n    plt.show()\n    ```\n\nIn a legal research application, this visualization revealed that queries about international law formed distinct clusters based on jurisdictions, which helped us refine our retrieval strategies for cross-jurisdictional questions.\n\n### Failure Mode Analysis\n\nBeyond identifying what fails, understanding why certain queries fail can reveal deeper patterns in your system's limitations:\n\n1. Identify common patterns in failed queries\n2. Analyze retrieval performance for each failure case\n3. Review generation quality for retrieved contexts\n4. Identify whether failures stem from content gaps or capability limitations\n\nIn one project, we discovered that a significant portion of failures weren't due to missing content or poor retrieval, but rather the system's inability to reconcile conflicting information from different sources. This insight led us to develop specific prompting techniques for handling contradictory information, dramatically improving satisfaction scores for those query types.\n\n!!! warning \"Common Failure Modes\" - **Retrieval failures**: System can't find relevant documents - **Context limitations**: Relevant documents exist but don't contain needed information - **Generation problems**: The retrieved context is good, but the generation is poor - **Hallucinations**: The system fabricates information not in the context - **Format issues**: The information is correct but poorly presented\n\nUnderstanding these failure modes is critical because each requires a different solution approach. Retrieval failures might need better embedding models or query processing, while hallucinations often require improved prompting or validation steps.\n\n## From Analysis to Action: Addressing Inventory vs. Capability Issues\n\nOnce you've identified patterns in your user queries and system performance, the next step is determining what type of solution is needed. In my experience, most improvement opportunities fall into two categories: inventory issues and capability issues.\n\n### Inventory Issues: Expanding Your Knowledge Base\n\nInventory issues occur when your system lacks the content needed to answer certain questions. The solution typically involves expanding your knowledge base or improving ingestion processes.\n\nSigns of inventory issues include:\n\n- Low cosine similarities across retrieved documents\n- No results from lexical search\n- LLM refusing to answer due to missing information\n- Retrieved chunks never being cited in responses\n\nI worked with a healthcare company whose RAG system frequently failed to answer questions about recent treatment guidelines. Analysis revealed the problem wasn't in the retrieval or generation components\u2014they simply hadn't updated their knowledge base with the latest medical literature. By implementing an automated ingestion pipeline for new guidelines, they resolved the issue without any changes to the core RAG system.\n\n### Capability Issues: Enhancing Functional Abilities\n\nCapability issues arise when your system has the relevant information but lacks the functionality to process or present it effectively. These often require more sophisticated solutions, such as:\n\n- Building specialized retrievers for specific query types\n- Implementing metadata extraction and filtering\n- Developing new prompting strategies\n- Creating custom rendering for different content types\n\nA financial services RAG system I consulted on had complete information about investment products but performed poorly on comparison queries. The problem wasn't missing content\u2014it was that the system couldn't effectively compare features across multiple products. By implementing a specialized comparison capability that ran parallel queries and formatted results in a tabular view, satisfaction for these queries improved by over 60%.\n\nBy distinguishing between inventory and capability issues, you can target your solutions more effectively. Inventory issues often have straightforward fixes through content expansion, while capability issues may require the specialized techniques we'll explore in Chapter 5.\n\n## Resources and Further Reading\n\nTo help you implement the techniques described in this chapter, here's a curated list of resources for topic modeling, clustering, and query classification in RAG applications:\n\n### Topic Modeling and Clustering Libraries\n\n- **[BERTopic Documentation](https://maartengr.github.io/BERTopic/index.html)**: Comprehensive documentation for BERTopic with tutorials and examples\n- **[UMAP](https://umap-learn.readthedocs.io/en/latest/)**: Dimensionality reduction technique used in many modern clustering approaches\n- **[Sentence-Transformers](https://www.sbert.net/)**: Library for creating embeddings that work well for semantic similarity tasks\n- **[LDAvis](https://github.com/cpsievert/LDAvis)**: Interactive visualization of topic models\n- **[Top2Vec](https://github.com/ddangelov/Top2Vec)**: Alternative topic modeling approach that works well with short texts\n\n### Classification and Production Monitoring\n\n- **[Hugging Face Text Classification](https://huggingface.co/tasks/text-classification)**: Resources for building and deploying text classifiers\n- **[Scikit-learn Classification Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)**: Tools for evaluating classifier performance\n- **[FastAPI Monitoring with Prometheus](https://fastapi.tiangolo.com/advanced/monitoring/)**: Implementing metrics monitoring for your API\n- **[OpenAI Classification Examples](https://platform.openai.com/docs/guides/classification)**: Guidelines for implementing classification with OpenAI models\n- **[MLflow for Model Tracking](https://mlflow.org/docs/latest/tracking.html)**: Tools for tracking classifier performance over time\n\n### Academic Papers and Articles\n\n- **[Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence](https://aclanthology.org/2021.acl-short.96.pdf)**: Research on modern topic modeling approaches\n- **[BERTopic: Neural topic modeling with a class-based TF-IDF procedure](https://arxiv.org/abs/2203.05794)**: Academic paper describing the BERTopic methodology\n- **[Topic Modeling in Embedding Spaces](https://arxiv.org/abs/1907.04907)**: Theoretical foundations for embedding-based topic modeling\n\n### Tutorials and Guides\n\n- **[Topic Modeling with BERTopic](https://towardsdatascience.com/topic-modeling-with-bertopic-e4896fc3b961)**: Step-by-step tutorial on implementing BERTopic\n- **[From Clustering to Classification](https://neptune.ai/blog/clustering-to-classification)**: Guide on transitioning from unsupervised to supervised approaches\n- **[Building a Real-time Classification System](https://engineering.fb.com/2021/01/26/ml-applications/real-time-classification/)**: Facebook Engineering's approach to real-time text classification\n- **[Query Categorization for Analytics](https://netflixtechblog.com/supporting-content-decision-makers-with-machine-learning-995b7b76006f)**: Netflix's approach to search query analysis\n\n### Visualization Tools\n\n- **[Plotly Dash](https://plotly.com/dash/)**: Framework for building interactive dashboards to visualize clustering results\n- **[Streamlit](https://streamlit.io/)**: Easy-to-use framework for building data apps to explore topic models\n- **[Bokeh](https://bokeh.org/)**: Interactive visualization library for modern web browsers\n\nThese resources should provide a solid foundation for implementing the concepts discussed in this chapter. As you move from analysis to production, remember that the goal is to create a stable classification system that allows you to track performance consistently over time, not just to discover interesting patterns in your data.\n\n## Conclusion and Next Steps\n\nIn this chapter, we've explored how to apply topic modeling and clustering to identify patterns in user queries and prioritize improvements:\n\n1. Understanding the difference between topics and capabilities\n2. Applying topic modeling techniques to user queries\n3. Categorizing queries for targeted improvements\n4. Identifying high-value, high-impact opportunities\n5. Creating a strategic roadmap for RAG enhancement\n\nThis analysis transforms vague directives like \"make the AI better\" into specific, data-driven improvement plans. By understanding what your users are asking about and what capabilities they need, you can focus your efforts where they'll have the greatest impact.\n\nA critical insight from this chapter is the distinction between clustering for analysis and classification for production. While exploratory clustering helps you discover patterns and identify key categories, deploying stable classifiers based on these insights is what enables effective monitoring over time. This transition\u2014from unsupervised discovery to supervised classification\u2014is essential for building a production RAG system that can be systematically improved based on consistent metrics.\n\nBy implementing the production monitoring approaches outlined in this chapter, you create a data pipeline that continuously validates your improvement efforts against real user needs. This establishes a feedback loop that goes beyond simple thumbs-up/down ratings to provide granular, category-specific insights that drive targeted enhancements.\n\nThe segmentation and prioritization framework we've developed here serves as the foundation for the specialized improvements we'll explore in the coming chapters. Rather than trying to enhance every aspect of your system simultaneously, you now have a methodology for identifying the specific enhancements that will deliver the most value.\n\nAs we move into [Chapter 5](chapter5.md), we'll explore how to implement specialized indices and retrieval techniques for the high-priority capability gaps you've identified. Each of these techniques addresses specific query types and user needs, creating a system that doesn't just answer general questions but excels at the precise capabilities your users value most.\n\nThe journey from collecting feedback to understanding patterns to implementing targeted improvements forms the core of systematic RAG development\u2014a methodical approach that replaces guesswork with data-driven decision making.\n\n!!! tip \"What's Coming Next\"\nIn [Chapter 5](chapter5.md), we'll explore specialized improvements through multimodal search and contextual retrieval. We'll build on the priorities identified in this chapter to create targeted solutions for specific query types.\n</code></pre>","tags":["topic-modeling","clustering","prioritization","query-analysis"]},{"location":"workshops/chapter5/","title":"Multimodal RAG: Building Specialized Search Indices","text":"<p>Chapter Overview</p> <p>This chapter explores how to move beyond basic retrieval to create specialized indices for different content types:</p> <ul> <li>Understanding why specialized retrievers outperform general-purpose solutions</li> <li>Implementing strategies for document, image, and table retrieval</li> <li>Measuring performance at both router and retriever levels</li> <li>Building a comprehensive multimodal RAG architecture</li> <li>Applying specialized techniques to enhance retrieval precision</li> </ul>","tags":["multimodal","specialized-indices","structured-data","query-routing"]},{"location":"workshops/chapter5/#introduction","title":"Introduction","text":"<p>In our journey through systematically improving RAG applications, we've reached a pivotal moment. The previous sessions have equipped us with the fundamental building blocks: the RAG playbook, synthetic data generation, fine-tuning approaches, user experience design for data collection, and segmentation techniques. Now, in Session 5, we turn our attention to a concept that often separates basic implementations from truly exceptional ones\u2014multimodal RAG and specialized search indices.</p> <p>Key Insight</p> <p>The fundamental insight that drives this session is deceptively simple yet profound: not all queries are created equal. Different types of information require different approaches to retrieval. Just as you wouldn't use a hammer for every home repair task, you shouldn't rely on a single retrieval mechanism for every type of query your users might have.</p> <p>Today, we'll explore how to identify distinct capabilities through segmentation and address each with tailored approaches. We'll see why specialized models solving specific problems consistently outperform general-purpose solutions, and how this paradigm shift can transform your RAG implementation from adequate to exceptional.</p>","tags":["multimodal","specialized-indices","structured-data","query-routing"]},{"location":"workshops/chapter5/#the-power-of-specialization","title":"The Power of Specialization","text":"","tags":["multimodal","specialized-indices","structured-data","query-routing"]},{"location":"workshops/chapter5/#beyond-the-monolithic-approach","title":"Beyond the Monolithic Approach","text":"<p>Traditional RAG implementations often begin with a single, monolithic index\u2014a one-size-fits-all approach that attempts to handle every query with the same retrieval mechanism. While this approach can work for simple use cases, it quickly reaches its limits when confronted with the rich diversity of real-world queries.</p> <p>Diverse Query Needs</p> <p>Consider a hardware store's knowledge base. A customer searching for a specific product by model number requires a fundamentally different search approach than someone asking about the durability of various power tools, or another customer trying to find items within a specific weight range. The first query is best served by lexical search matching exact strings, the second by semantic search understanding concepts and opinions, and the third by structured data queries.</p> <p>This diversity of information needs is why major search engines like Google have developed specialized tools\u2014Maps for location-based queries, Photos for visual search, YouTube for video content, and classic web search for text-based information. While these began as separate products, the true innovation came when Google learned to seamlessly route users to the appropriate tool based on the nature of their query.</p> <p>From Previous Cohort</p> <p>\"I've been building separate indices for years without realizing that's what I was doing. This framework just helps me do it more systematically.\"</p>","tags":["multimodal","specialized-indices","structured-data","query-routing"]},{"location":"workshops/chapter5/#the-mathematics-of-specialization","title":"The Mathematics of Specialization","text":"<p>The superiority of specialized approaches isn't just theoretical\u2014it's mathematically demonstrable. When distinct segments exist within a population of queries, a collection of local decision models will consistently outperform a global model trying to handle all cases.</p> <p>This principle manifests in modern machine learning in multiple ways. We see it in the evolution from monolithic models to mixtures of experts, where specialized sub-models handle different types of inputs. We see it in the trend toward decomposing complex tasks into simpler subtasks that can be solved independently before being recombined.</p> <pre><code>graph TD\n    A[Monolithic Approach] --&gt; B[One-size-fits-all]\n    C[Specialized Approach] --&gt; D[Domain-specific Models]\n\n    B --&gt;|Limited Performance| E[General Coverage]\n    D --&gt;|Optimized Performance| F[Targeted Coverage]\n\n    F --&gt; G[Better Overall Results]\n    E --&gt; G\n\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style C fill:#bbf,stroke:#333,stroke-width:2px</code></pre> <p>Organizational Benefits</p> <p>Beyond the performance benefits, specialized indices offer practical organizational advantages:</p> <pre><code>1. **Division of labor**: Teams can work on isolated, well-defined problems rather than tangling with the entire system\n2. **Incremental improvement**: Adding a new specialized index is less disruptive than rebuilding an entire system\n3. **Targeted innovation**: Teams can innovate within their specific domain without risking the stability of the whole\n</code></pre> <p>Industry Perspective</p> <p>\"Building specialized indices isn't just about performance\u2014it's about creating a sustainable path for continuous improvement.\"</p>","tags":["multimodal","specialized-indices","structured-data","query-routing"]},{"location":"workshops/chapter5/#two-paths-to-better-retrieval","title":"Two Paths to Better Retrieval","text":"<p>When improving retrieval capabilities for RAG applications, two complementary strategies emerge. Think of them as opposite sides of the same coin\u2014one extracting structure from the unstructured, the other creating retrieval-optimized representations of structured data.</p>","tags":["multimodal","specialized-indices","structured-data","query-routing"]},{"location":"workshops/chapter5/#strategy-1-extracting-metadata","title":"Strategy 1: Extracting Metadata","text":"<p>The first approach involves defining and extracting more metadata from your text chunks. Instead of viewing your content as an undifferentiated mass of text, you identify valuable structured information that can be exposed to search engines.</p> <p>!!! example \"Metadata Extraction Examples\" - In finance applications, distinguishing between fiscal years and calendar years - For legal document systems, classifying contracts as signed or unsigned - When processing call transcripts, categorizing them by type (job interviews, stand-ups, design reviews)</p> <p>This approach essentially asks: \"What structured information is hiding within our unstructured content that would make it easier to search?\"</p> <p>Financial Metadata Model</p> <pre><code>from pydantic import BaseModel\nfrom datetime import date\nfrom typing import Optional, List\n\n    class FinancialStatement(BaseModel):\n        \"\"\"Structured representation of a financial statement document.\"\"\"\n        company: str\n        period_ending: date\n        revenue: float\n        net_income: float\n        earnings_per_share: float\n        fiscal_year: bool = True  # Is this fiscal year (vs calendar year)?\n\n    def extract_financial_data(document_text: str) -&gt; FinancialStatement:\n        \"\"\"\n        Extract structured financial data from document text using LLM.\n\n        Args:\n            document_text: Raw text from financial document\n\n        Returns:\n            Structured FinancialStatement object with extracted data\n        \"\"\"\n        # Use LLM to extract the structured information\n        # Implementation depends on your LLM framework\n        # ...\n    ```\n\nBy extracting these structured elements from quarterly reports, organizations can enable precise filtering and comparison that would have been impossible with text-only search.\n\n### Strategy 2: Building Synthetic Text Chunks\n\nThe second approach reverses the flow: taking structured data (or even unstructured data) and producing synthetic text chunks optimized for retrieval. These chunks serve as semantic pointers to the original content, enabling more effective recall.\n\n!!! tip \"Synthetic Text Applications\" - For image collections: Generate detailed descriptions capturing searchable aspects - For research interviews: Extract common questions and answers to form an easily searchable FAQ - For numerical data: Create natural language descriptions of key trends and outliers\n\nThese synthetic chunks become intermediaries\u2014easier to search than the original content, but pointing back to that source material when needed for the final response.\n\n!!! example \"Image Description Generator\"\n```python\ndef generate_enhanced_image_description(image_path: str) -&gt; str:\n\"\"\"\nGenerate a detailed, searchable description of an image.\n\n        Args:\n            image_path: Path to the image file\n\n        Returns:\n            Detailed description optimized for semantic search\n        \"\"\"\n        # Prompt for rich image description\n        system_prompt = \"\"\"\n        Analyze this image in detail, including:\n        1. All people, objects, and text visible\n        2. The setting and environment\n        3. Actions being performed\n        4. Visual qualities (colors, lighting, composition)\n        5. Emotional tone or mood\n        6. Any unique or distinctive elements\n\n        Format your response as a detailed paragraph that would help\n        someone find this image when searching with natural language.\n        \"\"\"\n\n        # Implementation using your preferred vision LLM\n        # ...\n    ```\n\nBoth strategies essentially create materialized views of your existing data, processed by AI through either structuring or rewriting. The appropriate strategy depends on your data and the types of queries you need to support\u2014and many systems benefit from applying both approaches to different parts of their content.\n\n```mermaid\ngraph LR\n    A[Strategy 1: Extraction] --&gt;|\"From Unstructured \u2192 Structured\"| B[Metadata &amp; Filters]\n    C[Strategy 2: Generation] --&gt;|\"From Structured \u2192 Searchable Text\"| D[Synthetic Descriptions]\n\n    B --&gt; E[Enhanced Retrievability]\n    D --&gt; E\n</code></pre>","tags":["multimodal","specialized-indices","structured-data","query-routing"]},{"location":"workshops/chapter5/#measuring-what-matters","title":"Measuring What Matters","text":"<p>As we introduce specialized indices, our measurement framework expands to assess performance at two levels:</p> <p>!!! info \"Two-Level Measurement Framework\" 1. Are we selecting the right retrieval method for each query? 2. Is each retrieval method finding the right information?</p> <p>The overall probability of finding the correct information becomes a product of these two probabilities:</p> <p>Performance Formula</p> <p><code>P(finding correct data) = P(selecting correct retriever) \u00d7 P(finding correct data | correct retriever)</code></p> <p>This formula provides a powerful diagnostic tool. When your system underperforms, it helps identify whether the issue lies with retriever selection or with the individual retrievers themselves.</p> <p>Diagnostic Example</p> <p>If you find that your system correctly routes 95% of queries to the appropriate retriever, but those retrievers only find relevant information 60% of the time, your priority should be improving retrieval quality rather than router accuracy.</p> <p>This two-level evaluation framework ensures you invest your improvement efforts where they'll have the greatest impact.</p> <p>Performance Evaluation Code</p> <pre><code>def evaluate_system_performance(test_queries: List[dict]) -&gt; dict:\n\"\"\"\nEvaluate the overall system performance across components.\n\n        Args:\n            test_queries: List of test queries with ground truth\n\n        Returns:\n            Dictionary with performance metrics at each level\n        \"\"\"\n        results = {\n            \"overall_success_rate\": 0,\n            \"router_accuracy\": 0,\n            \"retriever_performance\": {}\n        }\n\n        correct_router_selections = 0\n        retriever_successes = {retriever_type: 0 for retriever_type in RETRIEVER_TYPES}\n        retriever_attempts = {retriever_type: 0 for retriever_type in RETRIEVER_TYPES}\n\n        for query in test_queries:\n            # Check if router selected correct retriever\n            selected_retriever = route_query(query[\"text\"])\n            correct_retriever = query[\"expected_retriever\"]\n\n            if selected_retriever == correct_retriever:\n                correct_router_selections += 1\n\n            # Check if selected retriever found relevant content\n            retrieval_result = retrieve_with_method(query[\"text\"], selected_retriever)\n            retriever_attempts[selected_retriever] += 1\n\n            if is_relevant(retrieval_result, query[\"expected_content\"]):\n                retriever_successes[selected_retriever] += 1\n\n                # Only count as overall success if both router and retriever worked\n                if selected_retriever == correct_retriever:\n                    results[\"overall_success_rate\"] += 1\n\n        # Calculate final metrics\n        total_queries = len(test_queries)\n        results[\"router_accuracy\"] = correct_router_selections / total_queries\n\n        for retriever_type in RETRIEVER_TYPES:\n            if retriever_attempts[retriever_type] &gt; 0:\n                results[\"retriever_performance\"][retriever_type] = {\n                    \"success_rate\": retriever_successes[retriever_type] / retriever_attempts[retriever_type],\n                    \"sample_size\": retriever_attempts[retriever_type]\n                }\n\n        results[\"overall_success_rate\"] /= total_queries\n\n        return results\n    ```\n\n## Specialized Approaches for Different Modalities\n\nDifferent types of content demand different retrieval strategies. Let's explore approaches for three common modalities: documents, images, and tables.\n\n### Document Search: Beyond Basic Chunking\n\nFor document retrieval, the foundation remains chunking documents with appropriate metadata and applying both lexical and semantic search techniques. However, several refinements can dramatically improve performance:\n\n!!! info \"Advanced Document Retrieval Techniques\" - **Contextual Retrieval**: Rather than using fixed chunks, dynamically rewrite or expand chunks based on the query context. This creates \"query-aware\" text representations that better match user intent.\n\n    - **Hybrid Retrieval Signals**: Combine semantic similarity with other signals like recency, authority, and citation frequency to create a more nuanced ranking function.\n\n    - **Multi-stage Retrieval**: Implement a cascade of increasingly sophisticated (and computationally expensive) retrieval and ranking steps, filtering out irrelevant content at each stage.\n\n```mermaid\nflowchart LR\n    A[Query] --&gt; B[Initial Retrieval]\n    B --&gt; C[Candidate Chunks]\n    C --&gt; D[Re-ranking]\n    D --&gt; E[Dynamic Expansion]\n    E --&gt; F[Final Context]\n</code></pre> <p>The result is a document retrieval system that might return different types of content depending on the query:</p> <ul> <li>For some queries, concise summaries of key information</li> <li>For others, entire documents leveraging long-context models</li> <li>For yet others, specific text chunks or structured data extracts</li> </ul> <p>This flexibility allows the system to balance precision, recall, and presentation based on what best serves each query.</p> <p>Document Processor with Contextual Retrieval</p> <pre><code>from typing import List, Dict, Any\nimport re\n\n    def process_document_for_retrieval(document: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Process a document for enhanced retrieval capabilities.\n\n        Args:\n            document: The raw document text\n\n        Returns:\n            Dictionary with processed document components\n        \"\"\"\n        # Extract structured metadata\n        metadata = extract_document_metadata(document)\n\n        # Create standard chunks with overlap\n        chunks = chunk_document(document, chunk_size=800, overlap=0.5)\n\n        # Generate summaries at different levels\n        document_summary = summarize_document(document)\n        section_summaries = [summarize_section(section) for section in extract_sections(document)]\n\n        # Extract any structured data tables\n        tables = extract_tables(document)\n\n        return {\n            \"metadata\": metadata,\n            \"chunks\": chunks,\n            \"document_summary\": document_summary,\n            \"section_summaries\": section_summaries,\n            \"tables\": tables,\n            \"full_document\": document  # Keep original for potential long-context processing\n        }\n\n    def contextual_retrieval(query: str, document_store: List[Dict[str, Any]]) -&gt; List[str]:\n        \"\"\"\n        Perform contextual retrieval that adapts based on query type.\n\n        Args:\n            query: User query\n            document_store: Processed document store\n\n        Returns:\n            List of most relevant text chunks for the query\n        \"\"\"\n        # Analyze query to determine retrieval strategy\n        query_analysis = analyze_query(query)\n\n        if query_analysis[\"requires_specific_detail\"]:\n            # Use chunk-level retrieval for specific information\n            return retrieve_relevant_chunks(query, document_store)\n\n        elif query_analysis[\"requires_overview\"]:\n            # Use summary-level retrieval for broader questions\n            return retrieve_relevant_summaries(query, document_store)\n\n        elif query_analysis[\"requires_structured_data\"]:\n            # Use table retrieval for data-oriented questions\n            return retrieve_relevant_tables(query, document_store)\n\n        else:\n            # Fall back to hybrid approach\n            chunks = retrieve_relevant_chunks(query, document_store)\n            summaries = retrieve_relevant_summaries(query, document_store)\n            return rerank_combined_results(query, chunks + summaries)\n    ```\n\n### Image Search: Bridging Visual and Textual Understanding\n\nImage search presents unique challenges. Visual language models were trained primarily on captioning data, creating a potential mismatch between how queries are phrased and how images are represented.\n\n!!! warning \"Embedding Spaces Mismatch\"\nThe naive approach\u2014applying the same embedding strategy used for text\u2014often fails because question embeddings and image caption embeddings exist in fundamentally different semantic spaces.\n\nTo bridge this gap, more sophisticated image summarization techniques are essential:\n\n!!! example \"Advanced Image Description Techniques\"\n**Rich Prompting**: Move beyond simple \"what's in this image?\" prompts to detailed instructions that anticipate likely queries. Compare:\n\n    *Basic*: \"Describe this image.\"\n\n    *Enhanced*: \"Describe this image in detail, noting the number of people, their apparent relationship, the setting, lighting conditions, objects present, and any text visible in the image.\"\n\n!!! info \"Additional Image Enhancement Approaches\" - **Contextual Enrichment**: Incorporate surrounding text, OCR results from the image, and metadata about the image's source and purpose.\n\n    - **Visual Reasoning**: Use chain-of-thought prompting to guide the model through a reasoning process about the image content, resulting in more comprehensive descriptions.\n\n    - **Bounding Boxes and Visual Grounding**: For applications where precise location or counting is important, supplement descriptions with information about the spatial arrangement of elements.\n\n!!! example \"Rich Image Description Prompt\"\n```python\ndef generate_rich_image_description(image, ocr_text=None, surrounding_text=None):\n\"\"\"\nGenerate a comprehensive description optimized for retrieval.\n\n        Args:\n            image: Image data or path\n            ocr_text: Optional text extracted from the image\n            surrounding_text: Optional text surrounding the image in its original context\n\n        Returns:\n            Detailed description of the image\n        \"\"\"\n        prompt = f\"\"\"\n        # Image Analysis Task\n\n        ## Context Information\n        {\"OCR Text from image: \" + ocr_text if ocr_text else \"No OCR text available.\"}\n        {\"Surrounding context: \" + surrounding_text if surrounding_text else \"No surrounding context available.\"}\n\n        ## Analysis Instructions\n        Analyze the following image in extreme detail:\n\n        1. First, describe the visual scene, setting, and overall composition\n        2. List all people visible, their approximate positions, actions, and expressions\n        3. Enumerate all objects visible in the image\n        4. Note any text visible in the image\n        5. Describe colors, lighting, and visual style\n        6. If applicable, identify the type of image (photograph, diagram, screenshot, etc.)\n        7. Use chain-of-thought reasoning: think about what is happening and why\n        8. Generate 5-7 potential questions someone might ask when searching for this image\n        9. Suggest 5-10 relevant tags for this image\n\n        ## Final Description\n        Based on your analysis, provide a comprehensive 3-5 sentence description that would\n        help people find this image when searching with natural language queries.\n        \"\"\"\n\n        # Use this prompt with your vision model implementation\n        # ...\n    ```\n\nThe enhanced description dramatically improves retrieval capability when troubleshooting specific defects or components.\n\n### Table Search: Structured Data in Context\n\nTables present a dual challenge: they contain structured data but exist within unstructured contexts. Two main approaches prove effective:\n\n!!! info \"Table Retrieval Approaches\"\n**Approach 1: Table as Document**\n\n    For finding specific rows or comparing data across tables, chunk the table (preserving headers) and apply semantic search techniques. Generate summaries that capture the table's purpose and key insights to improve retrieval.\n\n    This works well for questions like \"Which product had the highest Q3 sales?\" or \"Show me all tables with warranty information.\"\n\n    **Approach 2: Table as Database**\n\n    For detailed data analysis, treat tables as queryable databases. The key challenge becomes identifying which table(s) to query for a given question.\n\n    Standardize schemas using CREATE TABLE statements or table descriptions, then build semantic search against these table representations. Include sample data when possible to help clarify the table's contents.\n\n!!! example \"Table Processor Implementation\"\n```python\nfrom typing import List, Dict, Any, Optional\nimport pandas as pd\n\n    class TableProcessor:\n        \"\"\"Process tables for enhanced retrievability and querying.\"\"\"\n\n        def process_table(self, table_data: pd.DataFrame, table_name: str,\n                        source_doc: Optional[str] = None) -&gt; Dict[str, Any]:\n            \"\"\"\n            Process a table for both document-like and database-like retrieval.\n\n            Args:\n                table_data: The table as a pandas DataFrame\n                table_name: Name of the table\n                source_doc: Optional source document information\n\n            Returns:\n                Dictionary with processed table components\n            \"\"\"\n            # Generate schema representation\n            schema = self._generate_schema_representation(table_data)\n\n            # Generate natural language summary\n            summary = self._generate_table_summary(table_data, table_name)\n\n            # Generate sample queries this table could answer\n            sample_queries = self._generate_sample_queries(table_data, table_name)\n\n            # Convert to text chunks for semantic search\n            text_chunks = self._table_to_text_chunks(table_data)\n\n            return {\n                \"table_name\": table_name,\n                \"schema\": schema,\n                \"summary\": summary,\n                \"sample_queries\": sample_queries,\n                \"text_chunks\": text_chunks,\n                \"raw_data\": table_data,\n                \"source_document\": source_doc\n            }\n\n        def _generate_schema_representation(self, df: pd.DataFrame) -&gt; str:\n            \"\"\"Generate a SQL-like schema representation.\"\"\"\n            types = []\n            for col in df.columns:\n                dtype = df[col].dtype\n                if pd.api.types.is_numeric_dtype(dtype):\n                    sql_type = \"NUMERIC\"\n                elif pd.api.types.is_datetime64_dtype(dtype):\n                    sql_type = \"TIMESTAMP\"\n                else:\n                    sql_type = \"TEXT\"\n\n                # Add sample values for better understanding\n                sample_values = df[col].dropna().unique()[:3]\n                sample_str = f\"Sample values: {', '.join(str(x) for x in sample_values)}\"\n\n                types.append(f\"{col} {sql_type} -- {sample_str}\")\n\n            return f\"CREATE TABLE table (\\n  \" + \",\\n  \".join(types) + \"\\n);\"\n\n        def _generate_table_summary(self, df: pd.DataFrame, table_name: str) -&gt; str:\n            \"\"\"Generate a natural language summary of the table.\"\"\"\n            # Use an LLM to summarize the table contents\n            # Implementation depends on your LLM framework\n            # ...\n\n        def _generate_sample_queries(self, df: pd.DataFrame, table_name: str) -&gt; List[str]:\n            \"\"\"Generate sample natural language queries this table could answer.\"\"\"\n            # Use an LLM to generate sample queries\n            # ...\n\n        def _table_to_text_chunks(self, df: pd.DataFrame) -&gt; List[str]:\n            \"\"\"Convert table to text chunks for semantic search.\"\"\"\n            # Implementation for chunking table content\n            # ...\n    ```\n\nOnce the right table is identified, either:\n\n- Place the table directly into the context for simple analysis\n- Generate SQL queries or pandas code for more complex analysis\n\n## SQL Query Generation: A Case Study in Capability Building\n\nSQL query generation exemplifies many of the principles we've discussed. It involves both an inventory challenge (finding the right tables) and a capability challenge (writing effective queries).\n\n!!! warning \"Limitations of Direct Translation\"\nThe classical approach\u2014training a model to translate natural language directly to SQL\u2014often struggles with complex schemas and business-specific query patterns.\n\n!!! tip \"RAG Playbook for SQL Generation\"\nA more effective strategy applies our RAG playbook:\n\n    1. **Build an inventory of tables and their descriptions**\n    2. **Create synthetic questions targeting this inventory**\n    3. **Measure retrieval performance for table selection**\n    4. **Collect exemplar SQL queries demonstrating important capabilities**\n    5. **Include these exemplars when generating new queries**\n\nThis approach addresses a fundamental challenge in SQL generation: the same question can be interpreted in multiple valid ways. Consider \"Show me month-over-month revenue growth\":\n\n- Does \"month\" mean calendar month or a 28-day period?\n- Should weekends be excluded for B2B applications?\n- Is \"growth\" absolute or percentage?\n- Should the calculation include or exclude certain revenue types?\n\nWithout business context, even the most advanced models can only guess. By including relevant exemplars that demonstrate how your organization typically answers such questions, you guide the model toward your preferred interpretations.\n\n!!! example \"SQL Query Generator with Examples\"\n```python\nfrom typing import List, Dict, Any, Optional\n\n    class SQLQueryGenerator:\n        \"\"\"Generate SQL queries based on natural language using RAG with examples.\"\"\"\n\n        def __init__(self, db_schema_store, query_example_store):\n            \"\"\"\n            Initialize the SQL query generator.\n\n            Args:\n                db_schema_store: Repository of table schemas\n                query_example_store: Repository of example queries\n            \"\"\"\n            self.db_schema_store = db_schema_store\n            self.query_example_store = query_example_store\n\n        def generate_query(self, user_question: str) -&gt; str:\n            \"\"\"\n            Generate a SQL query based on a natural language question.\n\n            Args:\n                user_question: Natural language question\n\n            Returns:\n                SQL query string\n            \"\"\"\n            # Step 1: Identify relevant tables\n            relevant_tables = self._retrieve_relevant_tables(user_question)\n\n            # Step 2: Retrieve relevant query examples\n            relevant_examples = self._retrieve_relevant_examples(user_question)\n\n            # Step 3: Generate SQL using context\n            sql_query = self._generate_sql_with_context(\n                user_question,\n                relevant_tables,\n                relevant_examples\n            )\n\n            return sql_query\n\n        def _retrieve_relevant_tables(self, question: str) -&gt; List[Dict[str, Any]]:\n            \"\"\"Find tables relevant to the question using semantic search.\"\"\"\n            # Implementation using db_schema_store\n            # ...\n\n        def _retrieve_relevant_examples(self, question: str) -&gt; List[Dict[str, Any]]:\n            \"\"\"Find query examples relevant to the question.\"\"\"\n            # Implementation using query_example_store\n            # ...\n\n        def _generate_sql_with_context(self, question: str,\n                                      tables: List[Dict[str, Any]],\n                                      examples: List[Dict[str, Any]]) -&gt; str:\n            \"\"\"\n            Generate SQL using the question, tables, and examples as context.\n\n            This is where we'd use an LLM with a carefully crafted prompt that:\n            1. Shows the table schemas\n            2. Provides relevant query examples\n            3. Asks for a query answering the user's question\n            \"\"\"\n            # LLM implementation\n            # ...\n\n    # Example usage\n    def process_user_query(question: str) -&gt; Dict[str, Any]:\n        \"\"\"Process a natural language query to SQL and execute it.\"\"\"\n        # Step 1: Generate SQL\n        sql_query = sql_generator.generate_query(question)\n\n        # Step 2: Execute query (with safety checks)\n        if is_safe_query(sql_query):\n            results = execute_query(sql_query)\n            return {\n                \"question\": question,\n                \"sql_query\": sql_query,\n                \"results\": results\n            }\n        else:\n            return {\"error\": \"Generated query failed safety checks\"}\n    ```\n\n!!! quote \"Data Science Success Story\"\n\"We spent months trying to fine-tune models for SQL generation with limited success. Once we switched to retrieving exemplar queries from our analytics repository, accuracy jumped by 30% overnight.\"\n\n## Bringing It All Together\n\nAs we prepare for our final session on routing and unified systems, let's solidify the key insights from today's exploration of multimodal RAG:\n\n!!! abstract \"Key Takeaways\" 1. **The power of specialization**: Building dedicated retrieval mechanisms for different content types and query patterns consistently outperforms monolithic approaches.\n\n    2. **Two complementary strategies**: Extract structured data from unstructured content, or create synthetic text chunks that point to source data\u2014both serve as AI-powered materialized views.\n\n    3. **Measurement drives improvement**: Use precision and recall at both the router and retriever levels to identify your system's limiting factors.\n\n    4. **Modality-specific optimizations**: Each content type requires tailored approaches, from contextual retrieval for documents to rich descriptions for images to exemplar-based generation for SQL.\n\n```mermaid\nflowchart TD\n    A[User Query] --&gt; B[Query Analyzer]\n    B --&gt; C[Query Router]\n\n    C --&gt;|Document Query| D[Document Retriever]\n    C --&gt;|Image Query| E[Image Retriever]\n    C --&gt;|Table Query| F[Table Retriever]\n    C --&gt;|SQL Query| G[SQL Generator]\n\n    D --&gt; H[Result Combiner]\n    E --&gt; H\n    F --&gt; H\n    G --&gt; H\n\n    H --&gt; I[Response Generator]\n    I --&gt; J[User Response]\n</code></pre> <p>The beauty of this framework is its recursive nature. The same playbook\u2014synthetic data generation, segmentation, capability identification\u2014applies whether you're building your first retrieval system or your fifth specialized index.</p> <p>!!! tip \"Implementation Strategy\" 1. Start small: Begin with one or two specialized retrievers for your highest-impact query types 2. Measure relentlessly: Track performance metrics for each retriever and overall system 3. Expand incrementally: Add new retrievers as you identify segments that would benefit 4. Refine continuously: Use user feedback to improve both routing and retrieval quality</p> <p>Engineering Insight</p> <p>\"No matter how much better AI gets, you'll always be responsible for retrieval. Understanding what to retrieve and how to retrieve it remains the core challenge even as models become more capable.\"</p> <p>Cross-Reference</p> <p>In Chapter 6, we'll explore how to bring these specialized components together through effective routing strategies, creating a unified system that seamlessly directs users to the appropriate retrievers based on their queries.</p>","tags":["multimodal","specialized-indices","structured-data","query-routing"]},{"location":"workshops/chapter5/#reflection-questions","title":"Reflection Questions","text":"<p>As you prepare for our final session, consider these questions about your own RAG implementation:</p> <p>!!! question \"Self-Assessment\" 1. Which query segments in your application might benefit from specialized retrieval approaches?</p> <pre><code>2. What structured metadata could you extract from your content to enable more precise filtering?\n\n3. For which content types might synthetic summaries improve retrieval performance?\n\n4. How would you measure whether your performance bottleneck is in retriever selection or in the retrievers themselves?\n\n5. What exemplars from your domain could help guide generation tasks like SQL queries or code snippets?\n</code></pre> <p>By systematically addressing these questions, you'll be well-positioned to transform your RAG system from adequate to exceptional\u2014one specialized index at a time.</p>","tags":["multimodal","specialized-indices","structured-data","query-routing"]},{"location":"workshops/chapter5/#summary","title":"Summary","text":"<p>This chapter has shown that the path to advanced RAG implementations involves moving from monolithic retrieval to specialized indices tailored to different content types and query patterns. We've explored strategies for document, image, and tabular data, and seen how the same fundamental principles of measurement and improvement apply across modalities.</p> <p>The specialized approach offers not just better performance but also organizational advantages: teams can work independently on different retrievers, new capabilities can be added incrementally, and the system becomes more maintainable and extensible over time.</p> <p>As we move toward our final chapter on unified retrieval systems, remember that this specialization pattern reflects the broader evolution of machine learning systems: from monolithic models to mixtures of experts and back again as capabilities advance. By building specialized retrievers today, you're not just improving current performance\u2014you're creating a foundation that will continue to evolve with the field.</p>","tags":["multimodal","specialized-indices","structured-data","query-routing"]},{"location":"workshops/chapter6/","title":"Unified Product Architecture: Building a Cohesive RAG System","text":"<p>Chapter Overview</p> <p>This chapter explores how to build a coherent system that intelligently combines specialized retrieval components:</p> <ul> <li>Designing tool interfaces that bridge language models and specialized indices</li> <li>Implementing effective query routing to direct requests to appropriate tools</li> <li>Testing and measuring performance of both retrieval and routing components</li> <li>Creating user interfaces that leverage both AI and direct tool access</li> <li>Building systems that scale across teams and complexity levels</li> </ul>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#introduction-beyond-specialized-retrievers","title":"Introduction: Beyond Specialized Retrievers","text":"<p>In the previous chapter, we explored how to build specialized retrievers for different content types. We discussed strategies for handling documents, images, tables, and other specialized data formats. While these specialized components improve retrieval quality dramatically, they create a new challenge: how do we build a cohesive system that knows when to use each specialized component?</p> <p>This is the challenge of query routing\u2014the process of understanding what a user is asking for and directing their query to the most appropriate retrieval tool or combination of tools. Effective query routing is what transforms a collection of specialized capabilities into a unified, seamless product experience.</p> <p>Key Insight</p> <p>\"The quality of your RAG system isn't just determined by how well each individual retriever performs, but by how effectively your system routes queries to the right retrievers at the right time. Even perfect retrievers fail if they're used for the wrong queries.\"</p> <p>The unified architecture approach we'll explore in this chapter completes our improvement flywheel by:</p> <ol> <li>Using the specialized capabilities we built based on user segmentation</li> <li>Implementing intelligent routing between these components</li> <li>Creating interfaces that help users understand system capabilities</li> <li>Building feedback loops that continuously improve both routing and retrieval</li> </ol> <p>Let's begin by examining the architectural patterns that enable effective query routing in RAG systems.</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#the-api-mindset-tools-as-interfaces-between-models-and-data","title":"The API Mindset: Tools as Interfaces Between Models and Data","text":"<p>At the heart of unified RAG architecture is a simple but powerful pattern: treating each specialized retriever as an API that language models can call. This \"tools as APIs\" approach creates a clear separation of concerns between:</p> <ol> <li>Tool Interfaces: The definitions that describe what each tool does and what parameters it accepts</li> <li>Tool Implementations: The specialized code that performs retrieval against specific indices</li> <li>Routing Logic: The system that determines which tools to call for a given query</li> </ol> <p>History of Tool Interfaces</p> <p>The tool interface pattern has evolved rapidly in AI systems. What began as simple \"function calling\" in APIs like OpenAI's functions or Anthropic's tools has now developed into more sophisticated frameworks with multiple tool selection strategies. This pattern mimics the development of web API frameworks like REST and GraphQL, but with language models as the primary \"clients\" of these APIs.</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#why-the-api-approach-works","title":"Why the API Approach Works","text":"<p>Treating specialized retrievers as APIs offers several key advantages:</p> <ol> <li>Clear Boundaries: Teams can work independently on different tools</li> <li>Testability: Each component can be tested in isolation</li> <li>Reusability: Tools can be used by both language models and developers</li> <li>Scalability: New capabilities can be added without changing existing components</li> <li>Performance: Parallel execution becomes easier to implement</li> </ol> <pre><code>graph TD\n    A[User Query] --&gt; B[Query Router]\n    B --&gt; C[Tool Selection]\n    C --&gt; D[Document Tool]\n    C --&gt; E[Image Tool]\n    C --&gt; F[Table Tool]\n    D --&gt; G[Ranking]\n    E --&gt; G\n    F --&gt; G\n    G --&gt; H[Context Assembly]\n    H --&gt; I[Response Generation]\n    I --&gt; J[User Interface]</code></pre> <p>This architecture resembles modern microservice patterns where specialized services handle specific tasks. The difference is that the \"client\" making API calls is often a language model rather than another service.</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#implementing-tool-interfaces-for-retrieval","title":"Implementing Tool Interfaces for Retrieval","text":"<p>Let's look at how to implement this pattern with a concrete example. Imagine we're building a construction information system that includes blueprints, text documents, and project schedules.</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#building-a-blueprint-search-tool","title":"Building a Blueprint Search Tool","text":"<p>Based on our analysis in Chapter 5, we've determined that users often search for blueprints by description and date range. We'll define a tool interface that captures this functionality:</p> <pre><code>from pydantic import BaseModel\n\nclass SearchBlueprint(BaseModel):\n    description: str\n    start_date: str | None = None\n    end_date: str | None = None\n\n    def execute(\n        self,\n    ) -&gt; List[BlueprintResult]:\n        \"\"\"\n        Search for blueprints matching the description and date range.\n\n        Args:\n            description: Text to search for in blueprint descriptions\n            start_date: Optional start date in YYYY-MM-DD format\n            end_date: Optional end date in YYYY-MM-DD format\n\n        Returns:\n            List of matching blueprint documents\n        \"\"\"\n        # Implementation details would depend on your database\n        query = self._build_query(\n            query=self.description,\n            start_date=self.start_date,\n            end_date=self.end_date)\n        results = self._execute_query(query)\n        return self._format_results(results)\n\n        ...\n</code></pre>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#building-a-document-search-tool","title":"Building a Document Search Tool","text":"<p>Similarly, we can define a tool for searching text documents:</p> <pre><code>from pydantic import BaseModel\n\nclass SearchText(BaseModel):\n    query: str\n    document_type: Literal[\"contract\", \"proposal\", \"bid\"] | None = None\n\n    def execute(\n        self,\n    ) -&gt; List[DocumentResult]:\n        if self.document_type:\n            filter_params[\"type\"] = self.document_type\n\n        results = self._search_database(\n            query=self.query,\n            filters=filter_params)\n        return self._format_results(results)\n</code></pre>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#the-power-of-tool-documentation","title":"The Power of Tool Documentation","text":"<p>Notice the detailed docstrings and examples in these tool definitions. These aren't just for human developers\u2014they're critical for language models to understand how and when to use each tool. The examples in particular help models recognize the patterns of queries that should trigger each tool.</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#aside-on-mcp","title":"Aside on MCP","text":"<p>The Model Context Protocol (MCP) is an open standard developed by Anthropic that standardizes how applications provide context to large language models. Conceptually similar to the tool interface pattern we've discussed, MCP creates a universal protocol for connecting AI systems to various data sources and tools.</p> <p>Think of MCP like a \"USB-C port for AI applications\" \u2013 just as USB-C provides a standardized way to connect devices to various peripherals, MCP provides a standardized way for AI models to interact with different data sources and tools.</p> <p>Key benefits of MCP include:</p> <ol> <li>Standardization: Developers can build against a single protocol instead of maintaining separate connectors for each data source</li> <li>Interoperability: AI systems can maintain context as they move between different tools and datasets</li> <li>Ecosystem: Pre-built connectors for popular systems like GitHub, Slack, and databases can be shared and reused</li> <li>Security: The protocol is designed with security considerations for connecting AI to sensitive data sources</li> </ol> <p>MCP represents an important step toward the unified architecture vision we've discussed in this chapter, offering a standardized way to implement the \"tools as APIs\" pattern across different AI systems and data sources.</p> <p>MCP is Still Emerging</p> <p>While MCP represents a promising approach to standardizing AI tool interfaces, it's important to note that it's still very new. As of now, there aren't many production-ready MCP implementations available, and the ecosystem of useful MCPs is still in its early stages of development. Organizations adopting MCP should be prepared for an evolving standard and limited availability of pre-built connectors. As with any emerging technology, early adopters will need to invest in building custom implementations and should expect the standard to evolve over time.</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#building-the-routing-layer","title":"Building the Routing Layer","text":"<p>Once we have defined our specialized retrieval tools, we need a system that can route queries to the appropriate tools. This routing layer is responsible for:</p> <ol> <li>Understanding the user's query</li> <li>Determining which tool(s) to call</li> <li>Extracting the necessary parameters from the query</li> <li>Calling the appropriate tools with those parameters</li> <li>Combining results when multiple tools are used</li> </ol> <p>Modern language models excel at this kind of task, especially when provided with clear tool definitions and examples.</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#implementing-a-simple-router","title":"Implementing a Simple Router","text":"<p>Here's a basic implementation of a query router:</p> <pre><code>import instructor\nfrom instructor import Mode\nfrom openai import OpenAI\n\nclient = OpenAI()\nclient = instructor.from_openai(client)\n\nclass ClarifyQuestion(BaseModel):\n    question: str\n\nclass AnswerQuestion(BaseModel):\n    content: str\n    follow_ups: List[str] | None = None\n\nclass SearchBlueprint(BaseModel):\n    blueprint_description: str\n    start_date: str | None = None\n    end_date: str | None = None\n\nclass SearchText(BaseModel):\n    query: str\n    document_type: Literal[\"contract\", \"proposal\", \"bid\"] | None = None\n\ndef route_query(query: str) -&gt; SearchBlueprint:\n    return client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\n                You are a helpful assistant that creates search blueprints for building plans.\n                Use the SearchBlueprint model to structure your responses. Here are some examples:\n\n                &lt;examples&gt;\n                ...\n                &lt;/examples&gt;\n                \"\"\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"Can you find me the plans for a the 123 main st building?\"\n            }\n    ],\n    response_model=Iterable[SearchBlueprint | SearchText | AnswerQuestion | ClarifyQuestion]\n)\n</code></pre>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#using-few-shot-examples-to-improve-routing","title":"Using Few-Shot Examples to Improve Routing","text":"<p>The effectiveness of the router depends significantly on providing good examples of when to use each tool. These few-shot examples help the model understand the patterns that should trigger different tools.</p> <p>For instance, a system prompt for routing might include examples like:</p> <pre><code>- \"Find blueprints for the city hall built in 2010.\"\n{\n    \"blueprint_description\": \"city hall blueprints\",\n    \"start_date\": \"2010-01-01\",\n    \"end_date\": \"2010-12-31\"\n}\n- \"I need plans for residential buildings constructed after 2015.\"\n{\n    \"blueprint_description\": \"residential building plans\",\n    \"start_date\": \"2015-01-01\",\n    \"end_date\": null\n}\n- \"Can you find me the plans for a the 123 main st building?\"\n{\n    \"blueprint_description\": \"123 main st building\",\n    \"start_date\": null,\n    \"end_date\": null\n}\n- \"Show me blueprints for schools built between 2018 and 2020.\"\n{\n    \"blueprint_description\": \"school blueprints\",\n    \"start_date\": \"2018-01-01\",\n    \"end_date\": \"2020-12-31\"\n}\n- \"I need the contract for the Johnson project.\"\n{\n    \"query\": \"Johnson project contract\",\n    \"document_type\": \"contract\"\n}\n- \"What's the difference between a blueprint and a floor plan?\"\n{\n    \"content\": \"Blueprints are technical architectural drawings that include detailed specifications for construction, while floor plans focus primarily on the layout and dimensions of rooms and spaces within a building.\",\n    \"follow_ups\": [\"How do I read a blueprint?\", \"Can you show me examples of floor plans?\"]\n}\n- \"Can you explain what a load-bearing wall is?\"\n{\n    \"content\": \"A load-bearing wall is a structural element that supports the weight of the building above it, helping to transfer the load to the foundation. Removing or modifying load-bearing walls requires careful engineering considerations.\",\n    \"follow_ups\": [\"How can I identify a load-bearing wall?\", \"What happens if you remove a load-bearing wall?\"]\n}\n- \"I'm not sure what kind of building plans I need for my renovation.\"\n{\n    \"question\": \"Could you tell me more about your renovation project? What type of building is it, what changes are you planning to make, and do you need plans for permits or for construction guidance?\"\n}\n&lt;/examples&gt;\n</code></pre> <p>As your system collects more data about successful interactions, these examples can be dynamically updated with the most relevant ones for each tool.</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#testing-query-routing-effectiveness","title":"Testing Query Routing Effectiveness","text":"<p>Just as we need metrics for retrieval quality, we need metrics for routing quality. The fundamental question is: are we selecting the right tools for each query?</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#tool-selection-metrics","title":"Tool Selection Metrics","text":"<p>To evaluate tool selection, we need a test dataset with queries annotated with the correct tool(s) to use. From there, we can calculate:</p> <ol> <li>Tool Precision: When we select a tool, how often is it actually the right one?</li> <li>Tool Recall: How often do we select all the tools that should be selected?</li> <li>Tool F1 Score: The harmonic mean of precision and recall</li> </ol> Query ID Query Text Expected Tools Realised Tools Precision Recall 1 Retrieve blueprints for the museum expansion SearchBlueprint SearchBlueprint 100% 1/1 2 Find schedule and documents for the library renovation SearchSchedule, SearchText SearchSchedule 100% 1/2 3 Get both blueprints and schedule for campus construction SearchBlueprint, SearchSchedule SearchBlueprint, SearchSchedule 100% 2/2 4 Show me contract details and permit requirements for the new office SearchText, SearchBlueprint SearchText, SearchBlueprint, SearchSchedule 67% 2/2 5 Identify materials and design specs for the downtown skyscraper SearchText, SearchBlueprint SearchBlueprint, SearchText 100% 2/2 6 Get full details on industrial park planning SearchBlueprint, SearchText, SearchSchedule SearchText, SearchInvoice, SearchPermit 33% 1/3 7 Find emergency repair guidelines for the abandoned warehouse SearchRepair, SearchBlueprint 8 Obtain comprehensive analysis for the urban redevelopment project SearchBlueprint, SearchText, SearchSchedule, SearchPermit SearchBlueprint 100% 1/4 9 Explain zoning regulations for the new industrial area SearchZoning SearchBlueprint, SearchText 0% 0/1","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#analyzing-tool-selection-failures","title":"Analyzing Tool Selection Failures","text":"<p>When tool selection fails, we need to understand why. A confusion matrix is particularly useful here, showing which tools are being confused with one another.</p> <p>For example, if we find that the <code>SearchBlueprint</code> tool is never being selected even when it should be, we might need to improve its description or add more examples to the system prompt.</p> <p>Confusion Matrix Analysis</p> <p>Imagine our evaluation produces this confusion matrix:</p> Expected\\Selected SearchText SearchBlueprint SearchSchedule SearchText 85 5 10 SearchBlueprint 40 50 10 SearchSchedule 15 5 80 <p>This shows that SearchBlueprint is frequently mistaken for SearchText, indicating that we need to better differentiate these tools.</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#balancing-example-diversity-vs-relevance-in-tool-selection","title":"Balancing Example Diversity vs. Relevance in Tool Selection","text":"<p>When selecting few-shot examples for tool routing, there's an inherent tension between choosing examples that are highly relevant to the current query and maintaining diversity to cover a wide range of use cases. This balance is critical for effective tool selection.</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#the-diversity-relevance-tradeoff","title":"The Diversity-Relevance Tradeoff","text":"<p>Finding the right balance between relevance and diversity requires thoughtful strategies:</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#clustering-based-selection","title":"Clustering-Based Selection","text":"<p>Rather than simply selecting the k-most similar examples to the current query: - Cluster your historical examples based on semantic similarity - Select representatives from multiple clusters, prioritizing those closer to the current query - This ensures both relevance and coverage of different query patterns</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#tool-coverage-guarantees","title":"Tool Coverage Guarantees","text":"<p>Ensure representation of all important tools in your few-shot examples: - Start by allocating a minimum number of slots for each tool - Fill these slots with the most relevant examples for each tool - Use remaining slots for the most relevant examples overall - This prevents underrepresented tools from being \"forgotten\"</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#diversity-sampling-techniques","title":"Diversity Sampling Techniques","text":"<p>Implement algorithmic approaches to balance diversity and relevance: - Use Maximum Marginal Relevance (MMR) to select examples that balance similarity to the query with diversity from already-selected examples - Apply determinantal point processes (DPPs) to select a diverse yet relevant subset - Incorporate exploration-exploitation techniques from recommendation systems</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#dynamic-lambda-adjustment","title":"Dynamic Lambda Adjustment","text":"<p>Adapt the diversity-relevance balance based on query characteristics: - For unusual or ambiguous queries, increase diversity to cover more possibilities - For clear, common queries, emphasize relevance - Use metrics like perplexity or entropy of tool probability distribution to gauge query clarity</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#temporal-diversity","title":"Temporal Diversity","text":"<p>Consider the temporal dimension in example selection: - Include both recent and historical examples - Recent examples capture evolving user behaviors and data patterns - Historical examples maintain stability for core functionality - Weight by both relevance and recency with a tunable decay factor</p> <p>Example: Balanced Tool Selection</p> <p>Imagine you have these tools: <code>SearchBlueprint</code>, <code>SearchText</code>, <code>SearchSchedule</code>, and <code>SearchPermit</code>. When a user asks about \"construction timeline for the new office building\", an optimal set of examples might include:</p> <ul> <li>2 examples of <code>SearchSchedule</code> for construction timelines (highest relevance)</li> <li>1 example of <code>SearchBlueprint</code> for buildings (related domain)</li> <li>1 example of <code>SearchText</code> for office specifications (related entity)</li> <li>1 recent example showing how these tools can be used together (temporal freshness)</li> </ul> <p>This combination ensures the model understands the primary intent (schedules) while maintaining awareness of other potentially relevant tools.</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#the-system-architecture-perspective","title":"The System Architecture Perspective","text":"<p>Stepping back, our unified RAG architecture consists of three main layers:</p> <ol> <li>Interface Layer: Defines the tools/APIs that can be used</li> <li>Implementation Layer: Builds the actual retrieval capabilities</li> <li>Routing Layer: Directs queries to the appropriate tools</li> </ol> <p>This separation of concerns enables different teams to work on different aspects of the system:</p> <ul> <li>One team can focus on defining and refining tool interfaces</li> <li>Another can implement and optimize each specialized retriever</li> <li>A third can improve the routing logic and evaluation</li> </ul> <p>This architecture scales well as your system grows, allowing for:</p> <ol> <li>Adding new tools without changing existing ones</li> <li>Improving individual retrievers independently</li> <li>Enhancing routing logic without affecting tool implementations</li> </ol>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#the-combined-success-formula","title":"The Combined Success Formula","text":"<p>Throughout this book, we've focused on a data-driven approach to systematic improvement. In the context of unified architecture, we can express the overall success probability of our system with a simple formula:</p> \\[ P(\\text{success}) = P(\\text{find right document} \\mid \\text{right tool}) \\times P(\\text{right tool}) \\] <p>This formula highlights that our system's performance depends on both:</p> <ol> <li>How well each retriever works when used correctly</li> <li>How often we select the right retriever for the query</li> </ol>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#a-diagnostic-framework-for-improvement","title":"A Diagnostic Framework for Improvement","text":"<p>This seemingly simple formula provides a powerful diagnostic framework. When your RAG system isn't performing well, it helps pinpoint exactly where the problem lies and what type of solution to pursue:</p> <ul> <li>If tool selection recall is low, focus on improving the routing layer</li> <li>If retrieval recall is low (given the right tool), focus on improving that specific retriever</li> </ul> <p>Example: Imagine users report that when asking about blueprints, they only get satisfactory answers 40% of the time. There are two very different scenarios that could cause this:</p> <p>Scenario 1: The router correctly selects the blueprint search tool 95% of the time, but the blueprint search itself only finds the right blueprints 42% of the time.</p> <ul> <li>P(right tool) = 0.95</li> <li>P(find right document | right tool) = 0.42</li> <li>P(success) = 0.95 \u00d7 0.42 = 0.40 (40%)</li> </ul> <p>Scenario 2: The blueprint search is excellent at finding the right blueprints 80% of the time when used, but the router only selects it 50% of the time (often choosing document search instead).</p> <ul> <li>P(right tool) = 0.50</li> <li>P(find right document | right tool) = 0.80</li> <li>P(success) = 0.50 \u00d7 0.80 = 0.40 (40%)</li> </ul> <p>Same 40% success rate, but completely different problems requiring different solution strategies:</p> <p>For Scenario 1 (retrieval problem):</p> <ul> <li>Generate synthetic data to improve the blueprint search capability</li> <li>Fine-tune embedding models specifically for blueprint content</li> <li>Improve the extraction and structuring of blueprint metadata</li> <li>Experiment with different chunking strategies for blueprints</li> </ul> <p>For Scenario 2 (routing problem):</p> <ul> <li>Add more few-shot examples showing when to use the blueprint tool</li> <li>Improve the blueprint tool description to make it more distinctive</li> <li>Add user feedback from successful interactions into your examples</li> <li>Consider UI changes to help users explicitly request blueprints</li> </ul>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#measuring-components-independently","title":"Measuring Components Independently","text":"<p>To apply this framework effectively, you need to measure both components independently:</p> <ol> <li>Per-tool recall: How often each retriever finds the right information when used</li> <li>Tool selection accuracy: How often the router selects the right tool(s) for each query</li> </ol> <p>A simple dashboard showing these metrics gives you immediate insight into where to focus your improvement efforts.</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#expanding-the-formula","title":"Expanding the Formula","text":"<p>The formula can be expanded further to account for the user experience:</p> \\[ P(\\text{success}) = P(\\text{success} \\mid \\text{right tool}) \\times P(\\text{right tool} \\mid \\text{query}) \\times P(\\text{query}) \\] <p>Where:</p> <ul> <li>\\(P(\\text{success} \\mid \\text{right tool})\\) is the retriever quality</li> <li>\\(P(\\text{right tool} \\mid \\text{query})\\) is the router quality</li> <li>\\(P(\\text{query})\\) represents the distribution of queries users actually make</li> </ul> <p>This expanded formula reveals a third dimension for improvement: you can increase overall success by influencing which queries users make. If certain query types have higher success rates, you can design your UI to encourage those queries or educate users on the most effective ways to interact with your system.</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#from-metrics-to-roadmap","title":"From Metrics to Roadmap","text":"<p>This formula provides a clear framework for planning both product and research efforts:</p> P(success | right tool) P(right tool | query) Strategy High High These are strengths to highlight in your product Low High Research focus needed on specific retrievers High Low Focus on improving router or exposing tools directly Low Low Consider whether this query type is worth supporting <p>By systematically measuring and improving these components, you create a continuous improvement flywheel for your unified RAG architecture.</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#user-interfaces-direct-tool-access","title":"User Interfaces: Direct Tool Access","text":"<p>One powerful insight from the routing architecture is that tools designed for language models can often be exposed directly to users as well. Just as Google offers specialized interfaces like Google Maps, YouTube, and Google Images alongside its main search, your RAG application can offer both:</p> <ol> <li>A natural language interface using the router</li> <li>Direct access to specialized tools for specific needs</li> </ol> <p>Dual-Mode UI</p> <p>Imagine a construction information system that offers:</p> <ul> <li>A chat interface for general questions</li> <li>A blueprint search interface with date filters</li> <li>A document search interface with type filters</li> <li>A schedule search with timeline visualization</li> </ul> <p>This dual-mode interface has several advantages:</p> <ol> <li>Expert users can go directly to the tool they need</li> <li>New users can use natural language until they learn the system</li> <li>User interactions with direct tools provide training data for routing</li> <li>Clear capabilities help users understand what the system can do</li> </ol> <p>The key insight is that RAG isn't just about adding chat to your product\u2014it's about building a comprehensive information discovery system where chat is just one interface option.</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#user-feedback-as-training-data","title":"User Feedback as Training Data","text":"<p>A particularly valuable aspect of direct tool access is that user interactions can provide high-quality training data for improving both retrieval and routing:</p> <ol> <li>When users select a specific tool, that's a signal about their intent</li> <li>When users click on search results, that's a signal about relevance</li> <li>When users refine their search, that's a signal about what was missing</li> </ol> <p>These interactions can be logged and used to:</p> <ul> <li>Fine-tune embedding models with user-confirmed relevant documents</li> <li>Improve router accuracy by learning from user tool selections</li> <li>Create better few-shot examples based on successful interactions</li> </ul> <p>This creates another improvement flywheel: as users interact with the system, it collects data that makes both retrieval and routing better, which leads to higher user satisfaction and more interactions.</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#common-challenges-and-solutions","title":"Common Challenges and Solutions","text":"<p>As you implement a unified RAG architecture, you'll likely encounter several common challenges. Here are practical solutions to address them:</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#low-recall-for-specific-tools","title":"Low Recall for Specific Tools","text":"<p>Challenge: Some specialized tools are rarely selected by the router, even when they should be.</p> <p>Solution:</p> <ol> <li>Analyze examples where the tool should have been selected</li> <li>Enhance the tool description to be more distinctive</li> <li>Add more few-shot examples focused on this tool</li> <li>Consider whether the tool's purpose overlaps with others and needs clearer boundaries</li> </ol>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#slow-response-times","title":"Slow Response Times","text":"<p>Challenge: Using multiple tools in sequence creates high latency.</p> <p>Solution:</p> <ol> <li>Implement parallel execution of tool calls</li> <li>Use streaming responses to show partial results while others are loading</li> <li>Prioritize high-confidence tool calls and execute others asynchronously</li> <li>Cache common query results for faster responses</li> </ol>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#data-leakage-in-testing","title":"Data Leakage in Testing","text":"<p>Challenge: Testing results appear artificially high because test queries are similar to few-shot examples.</p> <p>Solution:</p> <ol> <li>Maintain separate test and development sets</li> <li>Use unique identifiers for each test case to prevent overlap</li> <li>Regularly refresh test data with new examples</li> <li>Test with variations of the same query to verify robustness</li> </ol>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#confusing-user-experience","title":"Confusing User Experience","text":"<p>Challenge: Users don't understand when to use chat versus direct tools.</p> <p>Solution:</p> <ol> <li>Provide clear affordances for each interface option</li> <li>Use onboarding to explain when each approach is most effective</li> <li>Show suggested direct tools alongside chat responses</li> <li>Collect user feedback on interface preferences</li> </ol>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#building-a-roadmap-for-continuous-improvement","title":"Building a Roadmap for Continuous Improvement","text":"<p>The unified architecture approach isn't implemented in a single sprint\u2014it's built incrementally based on user needs and system performance. Here's a roadmap for implementing this approach:</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#phase-1-foundation","title":"Phase 1: Foundation","text":"<ol> <li>Define clear interfaces for 2-3 core retrieval tools</li> <li>Implement basic routing between these tools</li> <li>Establish baseline metrics for both routing and retrieval</li> <li>Deploy a simple chat interface with direct links to specialized tools</li> </ol>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#phase-2-optimization","title":"Phase 2: Optimization","text":"<ol> <li>Analyze routing errors and improve tool descriptions and examples</li> <li>Implement parallel execution for better performance</li> <li>Add instrumentation to collect user feedback and interactions</li> <li>Enhance direct tool interfaces based on usage patterns</li> </ol>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#phase-3-expansion","title":"Phase 3: Expansion","text":"<ol> <li>Add more specialized tools based on user needs</li> <li>Implement more sophisticated routing with multi-tool execution</li> <li>Build a feedback loop that automatically improves routing based on usage</li> <li>Develop a unified search experience that intelligently combines results</li> </ol>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#phase-4-personalization","title":"Phase 4: Personalization","text":"<ol> <li>Track individual user preferences and interaction patterns</li> <li>Develop personalized routing based on user history</li> <li>Create user-specific fine-tuning datasets</li> <li>Build adaptive interfaces that highlight the most relevant tools for each user</li> </ol> <p>This phased approach allows you to deliver value quickly while continuously improving based on real-world usage.</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#conclusion-the-end-of-the-beginning","title":"Conclusion: The End of the Beginning","text":"<p>Throughout this book, we've explored how to systematically improve RAG applications by treating them as continuously evolving products rather than static implementations. We've covered:</p> <ol> <li>Starting the flywheel with synthetic data</li> <li>Converting evaluations into training data</li> <li>Building feedback collection mechanisms</li> <li>Understanding users through segmentation</li> <li>Creating specialized retrieval capabilities</li> <li>Unifying these capabilities into a cohesive architecture</li> </ol> <p>This unified architecture approach represents the culmination of our improvement flywheel\u2014a system that not only retrieves the right information but knows which specialized capability to use for each user need.</p> <p>But this isn't the end of the journey\u2014it's just the end of the beginning. The true value of the product mindset is that it creates systems that continuously improve based on user interactions. As your users engage with your unified RAG application, you'll collect more data, refine your understanding of their needs, enhance your specialized capabilities, and improve your routing logic.</p> <p>The result isn't just a more capable technical system\u2014it's a product that delivers increasingly valuable experiences to your users over time. And that, ultimately, is what separates successful AI products from those that launch with fanfare but quickly fade into irrelevance.</p>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>How clear are the boundaries between tool interfaces and implementations in your current system? Could different teams work on them independently?</p> </li> <li> <p>What metrics do you currently use to evaluate routing quality separate from retrieval quality?</p> </li> <li> <p>Have you considered exposing specialized search capabilities directly to users alongside chat?</p> </li> <li> <p>How are you collecting user feedback that could help improve both routing and retrieval?</p> </li> <li> <p>What's your biggest current limitation\u2014routing quality or retrieval quality? How do you know?</p> </li> </ol>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]},{"location":"workshops/chapter6/#additional-resources","title":"Additional Resources","text":"<ol> <li>The Model Context Protocol (MCP) - A standard for defining tool interfaces that work across different LLMs</li> </ol>","tags":["query-routing","tool-selection","unified-architecture","orchestration","microservices"]}]}