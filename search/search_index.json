{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The RAG Flywheel","text":""},{"location":"#data-driven-product-development-for-ai-applications","title":"Data-Driven Product Development for AI Applications","text":"<p>A systematic approach to building self-improving AI systems</p> <p>About This Book</p> <p>This book provides a structured approach to evolving Retrieval-Augmented Generation (RAG) from a technical implementation into a continuously improving product. You'll learn to combine product thinking with data science principles to create AI systems that deliver increasing value over time.</p>"},{"location":"#the-rag-improvement-flywheel","title":"The RAG Improvement Flywheel","text":"<p>At the core of this book is the RAG improvement flywheel - a continuous cycle that transforms user interactions into product enhancements.</p> <pre><code>graph TD\n    A[Synthetic Data &amp; Evaluation] --&gt; B[Learning from Evaluations]\n    B --&gt; C[UX Design &amp; Feedback Collection]\n    C --&gt; D[User Segmentation &amp; Analysis]\n    D --&gt; E[Building Specialized Capabilities]\n    E --&gt; F[Unified Product Architecture]\n    F --&gt; A\n\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style C fill:#bbf,stroke:#333,stroke-width:2px\n    style E fill:#dfd,stroke:#333,stroke-width:2px</code></pre> <p>Beyond Technical Implementation</p> <p>This book goes beyond teaching you how to implement RAG. It shows you how to think about RAG as a product that continuously evolves to meet user needs and deliver business value.</p>"},{"location":"#chapters","title":"Chapters","text":""},{"location":"#introduction-beyond-implementation-to-improvement","title":"Introduction: Beyond Implementation to Improvement","text":"<p>Understand why systematic improvement matters and how to approach RAG as a product rather than just a technical implementation.</p>"},{"location":"#chapter-1-starting-the-flywheel","title":"Chapter 1: Starting the Flywheel","text":"<p>Learn how to overcome the cold-start problem, establish meaningful metrics, and create a data foundation that drives product decisions.</p>"},{"location":"#chapter-2-from-evaluation-to-enhancement","title":"Chapter 2: From Evaluation to Enhancement","text":"<p>Transform evaluation insights into concrete product improvements through fine-tuning, re-ranking, and targeted enhancements.</p>"},{"location":"#chapter-3-the-user-experience-of-ai","title":"Chapter 3: The User Experience of AI","text":"<p>Design interfaces that both delight users and gather valuable feedback, creating a virtuous cycle of improvement.</p>"},{"location":"#chapter-4-understanding-your-users","title":"Chapter 4: Understanding Your Users","text":"<p>Segment users and queries to identify high-value opportunities and create targeted improvement strategies.</p>"},{"location":"#chapter-5-building-specialized-capabilities","title":"Chapter 5: Building Specialized Capabilities","text":"<p>Develop purpose-built solutions for different user needs spanning documents, images, tables, and structured data.</p>"},{"location":"#chapter-6-unified-product-architecture","title":"Chapter 6: Unified Product Architecture","text":"<p>Create a cohesive product experience that intelligently routes to specialized components while maintaining a seamless user experience.</p>"},{"location":"#key-takeaways-product-principles-for-ai-applications","title":"Key Takeaways: Product Principles for AI Applications","text":"<p>Core principles that will guide your approach to building AI products regardless of how the technology evolves.</p>"},{"location":"#talks-and-presentations","title":"Talks and Presentations","text":"<p>Explore insights from industry experts and practitioners through our collection of talks, lightning lessons, and presentations:</p>"},{"location":"#featured-talks","title":"Featured Talks","text":"<ul> <li>Fine-tuning Re-rankers and Embedding Models for Better RAG Performance - Practical approaches to enhancing retrieval quality (Ayush from LanceDB)</li> <li>RAG Anti-patterns in the Wild - Common mistakes and how to fix them (Skylar Payne)</li> <li>Semantic Search Over the Web with Exa - Building AI-first search engines (Will Bryk)</li> <li>Understanding Embedding Performance through Generative Evals - Custom evaluation methodologies (Kelly Hong)</li> <li>Online Evals and Production Monitoring - Monitoring AI systems at scale (Ben Hylak &amp; Sidhant Bendre)</li> </ul> <p>View all talks \u2192</p>"},{"location":"#for-product-leaders-engineers-and-data-scientists","title":"For Product Leaders, Engineers, and Data Scientists","text":"<p>What You'll Learn</p> <p>For Product Leaders - How to establish metrics that align with business outcomes - Frameworks for prioritizing AI product improvements - Approaches to building product roadmaps for RAG applications - Methods for communicating AI improvements to stakeholders For Engineers - Implementation patterns that facilitate rapid iteration - Architectural decisions that enable continuous improvement - Techniques for building modular, specialized capabilities - Approaches to technical debt management in AI systems</p> <p>For Data Scientists - Methods for creating synthetic evaluation datasets - Techniques for segmenting and analyzing user queries - Frameworks for measuring retrieval effectiveness - Approaches to continuous learning from user interactions</p>"},{"location":"#quick-wins-high-impact-rag-improvements","title":"Quick Wins: High-Impact RAG Improvements","text":"<p>Based on real-world implementations, here are proven improvements you can implement quickly:</p> <p>Top 5 Quick Wins</p> <ol> <li>Change Feedback Copy </li> <li>Replace \"How did we do?\" with \"Did we answer your question?\"</li> <li>Impact: 5x increase in feedback collection</li> <li> <p>Effort: 1 hour</p> </li> <li> <p>Use Markdown Tables</p> </li> <li>Format structured data as markdown tables instead of JSON/CSV</li> <li>If tables are complex, represent it in XML</li> <li>Impact: 12% better lookup accuracy</li> <li> <p>Effort: 2-4 hours</p> </li> <li> <p>Add Streaming Progress</p> </li> <li>Show \"Searching... Analyzing... Generating...\" with progress</li> <li>Stream the response as it's being generated when possible</li> <li>Impact: 45% reduction in perceived latency</li> <li> <p>Effort: 1 sprint</p> </li> <li> <p>Implement Page-Level Chunking</p> </li> <li>For documentation, respect page boundaries, and use page-level chunking. Humans tend to create semantically coherent chunks at the page level.</li> <li>Impact: 20-30% better retrieval for docs</li> <li>Effort: 1 day</li> </ol> <p>Medium-Term Improvements (2-4 weeks)</p> <ul> <li>Fine-tune embeddings: $1.50 and 40 minutes for 6-10% improvement</li> <li>Add re-ranker: 15-20% retrieval improvement</li> <li>Build specialized tools: 10x better for specific use cases</li> <li>Implement contextual retrieval: 30% better context understanding</li> <li>Create Slack feedback integration: 5x more enterprise feedback</li> </ul> <p>Learn from the Experts</p> <p>Before implementing, learn from these practical talks: - RAG Anti-patterns in the Wild - Common mistakes across industries and how to fix them - Document Ingestion Best Practices - Production-ready parsing for tables, PDFs, and complex documents</p>"},{"location":"#about-the-author","title":"About the Author","text":"<p>Jason Liu brings practical experience from his work at Facebook, Stitch Fix, and as a consultant for companies like HubSpot, Zapier, and many others. His background spans computer vision, recommendation systems, and RAG applications across diverse domains.</p> <p>Author's Philosophy</p> <p>\"The most successful AI products aren't the ones with the most sophisticated models, but those built on disciplined processes for understanding users, measuring performance, and systematically improving. This book will show you how to create not just a RAG application, but a product that becomes more valuable with every interaction.\"</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Begin your journey by reading the Introduction or jump directly to Chapter 1 to start building your evaluation framework and data foundation.</p> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"misc/introduction/","title":"Introduction to RAG","text":"<p>Retrieval-Augmented Generation (RAG) is a powerful approach that combines the strengths of retrieval-based systems with generative AI models. This hybrid approach helps to address limitations of large language models (LLMs) by providing them with relevant context from external knowledge sources.</p>"},{"location":"misc/introduction/#what-is-rag","title":"What is RAG?","text":"<p>RAG is a technique that:</p> <ol> <li>Retrieves relevant information from a knowledge base in response to a query</li> <li>Augments a prompt with this retrieved information</li> <li>Generates a response using an LLM with the augmented context</li> </ol> <p>This approach helps overcome several limitations of traditional LLMs:</p> <ul> <li>Outdated knowledge: LLMs can only access information they were trained on</li> <li>Hallucinations: LLMs sometimes generate incorrect information</li> <li>Limited context window: LLMs have constraints on how much text they can process at once</li> <li>Lack of specific domain knowledge: LLMs may not have deep expertise in specialized domains</li> </ul>"},{"location":"misc/introduction/#why-improve-rag-systems","title":"Why Improve RAG Systems?","text":"<p>While basic RAG implementations can be effective, there are numerous opportunities to enhance performance:</p> <ul> <li>Better retrieval: Improving how relevant information is found</li> <li>Smarter augmentation: Optimizing how retrieved information is incorporated</li> <li>Enhanced generation: Fine-tuning the response creation process</li> <li>Comprehensive evaluation: Measuring and tracking system performance</li> </ul> <p>This documentation serves as a guide to systematically improving each component of RAG systems, with practical implementation advice and real-world examples.</p>"},{"location":"misc/introduction/#core-components","title":"Core Components","text":"<p>A RAG system typically consists of:</p> <ol> <li>Knowledge Base: The corpus of documents or information sources</li> <li>Embedding Model: Transforms text into vector representations</li> <li>Vector Database: Stores and enables similarity search on embeddings</li> <li>Retriever: Finds relevant information from the knowledge base</li> <li>Prompt Engineering: Constructs effective prompts with retrieved information</li> <li>LLM: Generates the final response</li> </ol> <p>In the following sections, we'll explore each component in detail and provide strategies for optimization.</p>"},{"location":"misc/landingpage/","title":"Systematically Improve Your RAG Applications","text":""},{"location":"misc/landingpage/#stop-guessing-start-building-rag-that-actually-works","title":"Stop Guessing. Start Building RAG That Actually Works.","text":"<p>:star: Top rated AI course on Maven.com (4.7/5 stars, +200 students) :star:</p> <p>Confidently build and refine Retrieval-Augmented Generation (RAG) systems that deliver real-world impact. Our 6-week, hands-on course takes you from the fundamentals of evaluating quality all the way through building stable, production-grade capabilities.</p> <p>Enroll now on Maven (starts Feb 4)</p>"},{"location":"misc/landingpage/#what-people-are-saying","title":"What People Are Saying","text":"Review Name Role \"Practical lessons from every lecture... learning from a community on the vanguard of this emerging field.\" Max Software Engineer, Launch School \"Excellent job of stressing the fundamentals... useful metric tools to measure and improve RAG systems.\" Christopher Senior Data/AI Architect, Procurement Sciences AI \"Jason and Dan help set you on the right path... emphasis on looking at your data and building a metrics-based flywheel.\" Vitor Staff Software Engineer, Zapier \"A game-changer! ... They've got this knack for breaking down complex RAG concepts into a framework that just clicks.\" Moose Founder &amp; CEO, Sociail, Inc. <p>\"Jason helped us break down our vision into actionable steps, providing clear recommendations on the best models for each use case. His guidance gave us a tangible roadmap for our next steps and introduced practical techniques that drive continuous product improvements. Grateful for his expertise and support!\" \u2014 Camu Team (a16z backed)</p>"},{"location":"misc/landingpage/#the-problem-with-rag-today","title":"The Problem With RAG Today","text":"<p>Over the last few years, \"RAG\" has become a buzzword, but making these systems genuinely robust and effective often feels like guesswork. Most teams waste time on:</p> <ul> <li>\u274c Vague metrics like \"make the AI better\"</li> <li>\u274c Random experiments without data</li> <li>\u274c Focusing on generation while ignoring retrieval</li> <li>\u274c Building one-size-fits-all systems that underperform</li> </ul> <p>This course cuts through the confusion by giving you a clear, repeatable process: from collecting the right data and generating synthetic evaluations, to gradually incorporating new retrieval indices, routing strategies, fine-tuned embeddings, and practical UX improvements.</p>"},{"location":"misc/landingpage/#what-youll-get","title":"What You'll Get","text":"<p>In just 6 weeks, you'll learn a proven system to:</p> <ul> <li>\u2705 Build Proper Evaluations - Create synthetic data to measure real improvement</li> <li>\u2705 Find What Matters - Segment queries to identify high-impact opportunities</li> <li>\u2705 Improve Search Quality - Build specialized indices that actually retrieve what users need</li> <li>\u2705 Collect Valuable Feedback - Design UI that generates continuous improvement data</li> <li>\u2705 Optimize Embeddings - Fine-tune models that understand YOUR definition of relevance</li> </ul>"},{"location":"misc/landingpage/#trusted-by-professionals-from-leading-organizations","title":"Trusted by Professionals from Leading Organizations:","text":"Company Industry OpenAI AI Research &amp; Development Anthropic AI Research &amp; Development Google Search Engine, Technology Microsoft Software, Cloud Computing TikTok Social Media Databricks Data Platform Amazon E-commerce, Cloud Computing Airbnb Travel Zapier Automation HubSpot Marketing Software Shopify E-commerce Platform PwC Professional Services Booz Allen Hamilton Consulting Bain &amp; Company Consulting Northrop Grumman Aerospace &amp; Defense Visa Financial Services KPMG Professional Services Company Industry Decagon Technology Anysphere AI GitLab Software Development Intercom Customer Engagement Lincoln Financial Financial Services DataStax Database Technology Timescale Database Technology PostHog Product Analytics Gumroad E-commerce Platform Miro Collaboration Workday Enterprise Software Accenture Consulting, Technology Services Mozilla Non-profit Redhat Software Development Nvidia AI"},{"location":"misc/landingpage/#what-makes-this-course-different","title":"What Makes This Course Different","text":"<p>This isn't theory - it's a practical system used by leading companies to:</p> <ol> <li> <p>Stop treating RAG as an AI problem\\    \"RAG is really just a recommendation system squeezed between two LLMs\"</p> </li> <li> <p>Focus on what you can control\\    Improve search quality first - generation quality follows automatically</p> </li> <li> <p>Build improvement flywheels\\    Create systems that get better with every user interaction</p> </li> </ol> <p>No more fumbling in the dark. This program shows you step-by-step how to:</p> <ol> <li>Set up meaningful evaluations</li> <li>Identify high-impact opportunities</li> <li>Continuously refine retrieval</li> <li>Integrate feedback loops</li> <li>Enhance product experiences</li> </ol> <p>Enroll now on Maven (starts Feb 4)</p>"},{"location":"misc/landingpage/#not-ready-for-a-course-check-out-my-free-rag-playbook","title":"Not ready for a course? Check out my free RAG Playbook","text":"<p>Not ready to invest in a paid course yet? Start with my free RAG Playbook newsletter course. You'll get bite-sized lessons delivered straight to your inbox, covering the fundamentals of RAG systems and practical tips for improvement.</p> <p>Free 6 Day RAG Crash Course</p> <p>Once you're comfortable with the basics and ready to take your RAG skills to the next level, consider enrolling in our comprehensive course in February 2024.</p>"},{"location":"misc/landingpage/#what-youll-learn","title":"What You'll Learn","text":"<p>Our six-week program is designed to take you from RAG basics to advanced implementation strategies. Perfect for those who deployed RAG systems and want to improve them and cover the last mile of RAG. Here's a breakdown of what you can expect:</p>"},{"location":"misc/landingpage/#weeks-1-2-foundations-and-evaluation","title":"Weeks 1-2: Foundations and Evaluation","text":"<ul> <li>Synthetic Data Generation: Learn to create high-quality synthetic data for rapid testing and development. Understand the importance of diversity in your test sets and how to avoid common pitfalls.</li> <li>Fast Evaluation Techniques: Implement quick, iterative improvements using unit test-like evaluations. Focus on basic retrieval metrics like precision and recall to optimize your system efficiently.</li> <li>Query Segmentation: Discover how to categorize and analyze user queries to identify patterns and gaps in your system's performance. Learn to prioritize improvements based on impact, volume, and success likelihood.</li> <li>Metrics That Matter: Understand the difference between leading and lagging metrics. Learn how to set actionable goals that drive real improvements in your RAG system.</li> </ul>"},{"location":"misc/landingpage/#weeks-3-4-advanced-retrieval-and-routing","title":"Weeks 3-4: Advanced Retrieval and Routing","text":"<ul> <li>Specialized Indices: Build targeted indices for different content types (documents, images, tables) to improve retrieval accuracy. Learn advanced techniques for handling multimodal data.</li> <li>Query Routing: Implement sophisticated query routing systems using parallel function calling. Understand how to select the right tools and APIs for different query types.</li> <li>Combining Search Methods: Master the art of blending lexical, semantic, and metadata-based search for optimal results. Learn when and how to use re-rankers effectively.</li> <li>Structured Data Extraction: Explore techniques for extracting and leveraging structured data from various sources to enhance your RAG capabilities.</li> </ul>"},{"location":"misc/landingpage/#week-5-fine-tuning-and-embeddings","title":"Week 5: Fine-tuning and Embeddings","text":"<ul> <li>Embedding Model Optimization: Learn when and how to fine-tune embedding models for your specific use case. Understand the impact of domain-specific data on model performance.</li> <li>Data Collection Strategies: Implement effective feedback mechanisms and logging systems to gather valuable data for future improvements.</li> <li>Re-ranker Implementation: Discover how to fine-tune and implement re-rankers for better search results. Learn about the latest advancements in ranking technologies.</li> <li>Representation Learning: Dive deep into the nuances of creating effective representations for various entities in your system, from user queries to document summaries.</li> </ul>"},{"location":"misc/landingpage/#week-6-product-design-and-user-experience","title":"Week 6: Product Design and User Experience","text":"<ul> <li>Feedback Collection: Design intuitive and effective feedback mechanisms to continuously improve your system. Learn how to incentivize user feedback without disrupting the experience.</li> <li>Streaming Implementations: Implement streaming for improved user experience and perceived performance. Understand the psychological impacts of responsiveness on user satisfaction.</li> <li>Advanced Prompting Techniques: Master the art of crafting effective prompts, including chain-of-thought reasoning and dynamic few-shot learning.</li> <li>UI/UX Best Practices: Explore cutting-edge UI/UX designs for RAG applications, including innovative ways to display citations, confidence levels, and alternative answers.</li> </ul>"},{"location":"misc/landingpage/#why-this-course","title":"Why This Course?","text":"<p>In the rapidly evolving field of AI and machine learning, staying ahead means mastering the fundamentals while keeping pace with the latest advancements. Our course offers:</p> <ul> <li>Practical, Hands-on Learning: Every concept is accompanied by real-world examples and exercises. You'll be implementing and testing ideas from day one.</li> <li>Industry-Relevant Case Studies: Learn from actual scenarios encountered in production environments at leading tech companies.</li> <li>Expert Instruction: Benefit from 12 hours of dedicated time with instructors who have years of experience in building and optimizing RAG systems.</li> <li>Community of Professionals: Connect with a diverse group of peers from companies like Amazon, Adobe, and Zapier. Share insights, challenges, and solutions in a collaborative environment.</li> <li>Cutting-edge Content: Stay updated with the latest trends and technologies in RAG, including advanced embedding techniques, multi-modal retrieval, and emerging evaluation metrics.</li> <li>Personalized Feedback: Receive tailored advice on your specific RAG challenges through interactive Q&amp;A sessions and project reviews.</li> </ul>"},{"location":"misc/landingpage/#more-from-our-students","title":"More From Our Students","text":"Review Name Role \"Practical and grounded in actual industry experience... like getting the inside scoop from folks who've been in the trenches.\" Ashutosh Senior Principal Scientist, Adobe \"System-oriented approach... Highly relevant, directly applicable, and save time in building prototypes.\" Mani Senior Principal Software Engineer, Red Hat \"Pragmatic with lots of advice that you won't find in any course. What I look for in good courses are instructors with strong points of view and Jason has them in abundance. If you follow all the steps given, you are definitely on a fast track to building your AI...\" Naveen SVP of Engineering, BoostUp.ai \"Jason's AI Consultant course brought out lots of new avenues and concepts in the AI Consultant journey which I was previously not aware of - AIDA, what to have in a landing page, contract negotiation and more! It was definitely an eye-opener and helped...\" Laks Independent AI Researcher and Enthusiast \"If you are an expert in the field of AI and want to build a successful business as an independent consultant, this course is for you. Jason teaches you how to build proof and shows how to interact with clients to achieve dream outcomes for everyone involved...\" Philipp AI Consultant, peachstone.ai \"The course completely changed my mindset around communicating value and pricing accordingly. The tips on how to gradually build your audience were super valuable. Highly recommend for anyone starting out or just looking to level up...\" Erikas Senior AI engineer \"Jason's course is packed with actionable insights and advice. It's not a theoretical course on what to do, it's an actual practical guide on real life example and insights that you can start applying right away. Jason is very responsive and approachable...\" Guido Cohort 1 \"Jason's course was packed with actionable insights and advice. It's not a theoretical course on what to do, it's an actual practical guide on real life example and insights that you can start applying right away. Jason is responsive and approachable...\" Dylan AI Consultant, Iwana Labs"},{"location":"misc/landingpage/#risk-free-guarantee","title":"Risk-Free Guarantee","text":"<p>We're so confident in the value of this course that we offer a money-back guarantee. If you don't feel you're making significant progress in improving your RAG applications after 4 weeks, we'll refund your course fee, no questions asked.</p>"},{"location":"misc/landingpage/#bonus-1500-in-free-credits","title":"Bonus: $1,500+ in Free Credits","text":"<p>Get free credits for essential RAG tools including Cohere, LanceDB, Modal Labs, and more!</p>"},{"location":"misc/landingpage/#secure-your-teams-spot-today","title":"Secure Your Team's Spot Today","text":"<p>The field of RAG is evolving quickly. Don't fall behind.</p> <p>Enroll now on Maven (starts Feb 4)</p> <p>How to Get Reimbursed</p> <pre><code>Hey {manager},\n\nI've found a course called \"Systematically Improving RAG Applications\" that I believe would be incredibly valuable for our team. Here are the key points:\n\n- Expert Instruction: Learn from Jason Liu, who has 8 years of experience in recommendation systems and RAG applications.\n- Comprehensive Curriculum: 6-week course covering everything from synthetic data generation to advanced query routing and embedding optimization.\n- Practical Application: Hands-on sessions for implementing quick testing methods and live data streaming.\n- Strategic Insights: Learn to improve search quality, implement effective feedback loops, and make data-driven decisions.\n- Efficiency Gains: Techniques to increase work speed, user satisfaction, and retention rates.\n- Future-Readiness: Focus on rapid testing and adoption of emerging technologies in the RAG space.\n- Added Value: Over $1,500 in free credits for tools like Cohere, LanceDB, and Modal Labs.\n- Risk-Free: Money-back guarantee if we don't see improvements within 5 weeks.\n\nThe course costs $1,650. I plan to share the learnings with our entire team, multiplying the value of this investment. You can find more details here: https://maven.com/applied-llms/rag-playbook\n\nWhat are your thoughts on this opportunity?\n\nThanks,\n{Your Name}\n\nP.S. I've heard that other teams are sending multiple team members to build shared context efficiently. Should we consider a similar approach?\n</code></pre>"},{"location":"misc/learning-goals/","title":"Learning Goals: Developing AI Product Sense","text":"","tags":["learning-objectives","product-sense","rag","ai-products"]},{"location":"misc/learning-goals/#why-this-book-exists","title":"Why This Book Exists","text":"<p>When I started teaching RAG development, I noticed something important. Most courses teach you the technical parts \u2013 how to connect to an API, how to set up a vector database, or how to write a prompt. But they miss something critical: how to think about your AI product as a whole system that gets better over time.</p> <p>This book exists to fill that gap. My goal isn't just to teach you technical skills, but to help you develop what I call \"AI product sense\" \u2013 the ability to think strategically about AI products, make smart decisions about what to improve, and create systems that get better with use.</p>","tags":["learning-objectives","product-sense","rag","ai-products"]},{"location":"misc/learning-goals/#what-youll-take-away","title":"What You'll Take Away","text":"<p>By the end of this book, you'll be able to:</p> <ol> <li> <p>Build improvement systems, not just features</p> </li> <li> <p>Design RAG applications that collect valuable feedback</p> </li> <li>Create measurement frameworks to track performance</li> <li> <p>Set up processes that turn user interactions into training data</p> </li> <li> <p>Make data-driven decisions</p> </li> <li> <p>Set up proper evaluation metrics before making changes</p> </li> <li>Generate synthetic data to test improvements</li> <li> <p>Analyze patterns in user behavior to identify opportunities</p> </li> <li> <p>Prioritize efforts for maximum impact</p> </li> <li> <p>Identify which improvements will deliver the most value</p> </li> <li>Understand which user segments benefit most from specific features</li> <li> <p>Develop roadmaps based on data rather than hunches</p> </li> <li> <p>Build specialized solutions for specific problems</p> </li> <li> <p>Design purpose-built retrievers for different content types</p> </li> <li>Create query routers that direct questions to the right tools</li> <li> <p>Implement feedback mechanisms tailored to different use cases</p> </li> <li> <p>Think beyond technical implementation</p> </li> <li> <p>Consider user experience in every design decision</p> </li> <li>Understand the business impact of your technical choices</li> <li>Communicate effectively about AI capabilities and limitations</li> </ol>","tags":["learning-objectives","product-sense","rag","ai-products"]},{"location":"misc/learning-goals/#what-makes-this-different-the-product-sense-approach","title":"What Makes This Different: The Product Sense Approach","text":"<p>This isn't just a technical manual \u2013 it's a guide to thinking about AI products in a more holistic way. While other resources might teach you how to implement a specific technique, I'm going to teach you how to:</p> <ul> <li> <p>Ask the right questions: Instead of \"How do I use the latest embedding model?\", you'll learn to ask \"What problem am I really trying to solve for my users?\"</p> </li> <li> <p>See the whole system: You'll understand how each part of your RAG system affects the others, and how small changes can have big impacts.</p> </li> <li> <p>Make strategic choices: You'll learn when to use specialized solutions, when to collect more data, and when to focus on improving user experience.</p> </li> <li> <p>Build with evolution in mind: Everything you build will be designed to get better over time, not just work once and be forgotten.</p> </li> </ul>","tags":["learning-objectives","product-sense","rag","ai-products"]},{"location":"misc/learning-goals/#who-benefits-from-this-approach","title":"Who Benefits from This Approach","text":"<p>The AI product sense you'll develop applies across many roles:</p> <ul> <li> <p>Engineers will learn to build systems that improve with use, not just meet initial requirements.</p> </li> <li> <p>Product managers will understand how to prioritize AI features based on user impact, not just technical interest.</p> </li> <li> <p>Data scientists will see how their models fit into broader systems and user experiences.</p> </li> <li> <p>Founders and leaders will develop a framework for thinking about AI investments and strategic decisions.</p> </li> </ul> <p>No matter your role, you'll finish this book with a clearer understanding of how to build AI products that deliver real value and improve over time.</p>","tags":["learning-objectives","product-sense","rag","ai-products"]},{"location":"misc/learning-goals/#the-true-goal-creating-learning-systems","title":"The True Goal: Creating Learning Systems","text":"<p>The deepest goal of this book isn't just to help you build better RAG applications. It's to change how you think about AI products altogether.</p> <p>By the end, you'll see that the most valuable AI systems aren't the ones with the most advanced models or the fanciest features. They're the ones designed to learn and improve with every interaction \u2013 the ones that create a positive cycle where user engagement leads to better performance, which leads to more user engagement.</p> <p>This mindset \u2013 this product sense \u2013 is what separates truly transformative AI implementations from the many disappointing ones we see in the market today. And it's what I'm most excited to share with you through this book.</p>","tags":["learning-objectives","product-sense","rag","ai-products"]},{"location":"misc/what-i-want-you-to-takeaway/","title":"Product Principles for AI Applications","text":"<p>Hello there! Jason here. After spending these chapters together exploring the world of RAG systems, I want to make sure you walk away with more than just technical knowledge. While the code examples and architectures are valuable, the real lessons I hope you've learned go much deeper.</p>","tags":["product thinking","principles","mindset","improvement"]},{"location":"misc/what-i-want-you-to-takeaway/#the-flywheel-mindset","title":"The Flywheel Mindset","text":"<p>If there's one concept I want permanently etched in your mind, it's the improvement flywheel. Throughout my career\u2014from Facebook to Stitch Fix to my consulting work\u2014I've seen the same pattern: teams that build systems that get better with use succeed, while those that build static implementations eventually fail.</p> <p>Your RAG application should be smarter next month than it is today. If it isn't, something is wrong with your process, not your technology.</p>","tags":["product thinking","principles","mindset","improvement"]},{"location":"misc/what-i-want-you-to-takeaway/#stop-guessing-start-measuring","title":"Stop Guessing, Start Measuring","text":"<p>I've watched too many brilliant engineers waste countless hours debating which embedding model or chunking strategy is \"best\" without ever defining how they'd measure \"best\" in the first place.</p> <p>Don't fall into this trap. Before you change anything in your system, know exactly how you'll measure the impact of that change. Without this discipline, you're just accumulating technical debt while pretending to make improvements.</p>","tags":["product thinking","principles","mindset","improvement"]},{"location":"misc/what-i-want-you-to-takeaway/#users-over-models","title":"Users Over Models","text":"<p>The most sophisticated RAG system that doesn't actually solve user problems is worthless. Period.</p> <p>I've built systems that generated millions in revenue using outdated models because they solved real problems well. And I've seen state-of-the-art implementations fail because they missed the mark on user needs.</p> <p>When in doubt, talk to your users. Read their feedback. Watch them use your system. This will teach you more than any research paper or GitHub repository ever could.</p>","tags":["product thinking","principles","mindset","improvement"]},{"location":"misc/what-i-want-you-to-takeaway/#specialization-beats-generalization","title":"Specialization Beats Generalization","text":"<p>The path to exceptional RAG isn't finding the single best approach\u2014it's identifying the different types of queries your users have and building specialized solutions for each.</p> <p>This principle applies everywhere: specialized embeddings outperform general ones, targeted retrievers beat one-size-fits-all approaches, and segmented generation strategies outshine monolithic prompts.</p>","tags":["product thinking","principles","mindset","improvement"]},{"location":"misc/what-i-want-you-to-takeaway/#data-compounds-like-interest","title":"Data Compounds Like Interest","text":"<p>In the early days of any RAG application, progress feels slow. You're manually creating synthetic queries, writing evaluation examples, and fine-tuning with limited data.</p> <p>Don't get discouraged. Every piece of data you collect now becomes the foundation for automated improvements later. The first hundred examples are the hardest\u2014after that, your flywheel starts spinning faster with each cycle.</p>","tags":["product thinking","principles","mindset","improvement"]},{"location":"misc/what-i-want-you-to-takeaway/#methods-matter-more-than-models","title":"Methods Matter More Than Models","text":"<p>Models will change. What was state-of-the-art when I wrote this will likely be outdated by the time you're reading it.</p> <p>But the methods for systematic improvement are timeless. The processes for collecting feedback, evaluating performance, identifying patterns, and prioritizing improvements will serve you regardless of which models you're using.</p>","tags":["product thinking","principles","mindset","improvement"]},{"location":"misc/what-i-want-you-to-takeaway/#the-hardest-problems-arent-technical","title":"The Hardest Problems Aren't Technical","text":"<p>In my experience, the biggest challenges in building successful RAG applications rarely involve model selection or hyperparameter tuning. They're about:</p> <ul> <li>Convincing stakeholders to invest in measurement infrastructure</li> <li>Getting users to provide meaningful feedback</li> <li>Prioritizing improvements when resources are limited</li> <li>Balancing quick wins against long-term architectural needs</li> </ul> <p>The skills to navigate these challenges are as important as your technical abilities.</p>","tags":["product thinking","principles","mindset","improvement"]},{"location":"misc/what-i-want-you-to-takeaway/#start-small-but-start-now","title":"Start Small, But Start Now","text":"<p>You don't need a perfect RAG implementation to begin this journey. You don't need millions of examples or custom-trained models. You can start with a basic retriever, a few dozen synthetic queries, and simple thumbs-up/down feedback.</p> <p>What matters is establishing the process for improvement from day one. Even a basic system that improves systematically will eventually outperform a sophisticated system that remains static.</p>","tags":["product thinking","principles","mindset","improvement"]},{"location":"misc/what-i-want-you-to-takeaway/#building-a-culture-of-continuous-improvement","title":"Building a Culture of Continuous Improvement","text":"<p>Beyond the technical aspects, successful RAG products require the right organizational culture:</p> <ul> <li> <p>Celebrate learning over correctness: Teams that view failures as learning opportunities improve faster than those focused on being right the first time.</p> </li> <li> <p>Share ownership of metrics: When everyone from engineers to product managers to business stakeholders aligns on key metrics, improvement accelerates.</p> </li> <li> <p>Make feedback visible: Surface user feedback and performance metrics in dashboards, team meetings, and planning sessions to keep improvement central to your work.</p> </li> <li> <p>Budget for refinement: Explicitly allocate resources for post-launch improvement rather than moving the entire team to the next project.</p> </li> <li> <p>Document your journey: Keep records of what you've tried, what worked, and what didn't. This institutional knowledge becomes invaluable as your team grows.</p> </li> </ul> <p>Remember, this field is still young. The techniques we've covered are just the beginning. As you continue your journey, you'll discover new approaches and face unique challenges. But if you take these core principles to heart, you'll have the foundation to adapt and thrive regardless of how the technology evolves.</p> <p>Build systems that learn. Measure before you change. Put users first. Specialize where it matters. Trust the process.</p> <p>I can't wait to see what you build.</p> <p>\u2013 Jason</p>","tags":["product thinking","principles","mindset","improvement"]},{"location":"office-hours/","title":"Office Hours","text":"<p>This section contains summaries and key insights from our weekly office hours sessions, where we discuss challenges, solutions, and best practices for RAG systems.</p> <p>If you want to see all the questions and answers, you can find them in the FAQ page.</p> <p>See all FAQ Questions</p>"},{"location":"office-hours/#available-sessions","title":"Available Sessions","text":""},{"location":"office-hours/#cohort-2","title":"Cohort 2","text":"<ul> <li>Week 1 Summary</li> <li>Week 2 Summary</li> <li>Week 3 Summary</li> <li>Week 4 Summary</li> <li>Week 5 Summary</li> <li>Week 6 Summary</li> </ul>"},{"location":"office-hours/#cohort-3","title":"Cohort 3","text":"<ul> <li>Week 1 Session 1</li> <li>Week 1 Session 2</li> <li>Week 2 Session 1</li> <li>Week 2 Session 2</li> <li>Week 3 Session 1</li> <li>Week 4 Session 1</li> <li>Week 4 Session 2</li> </ul>"},{"location":"office-hours/#additional-sessions","title":"Additional Sessions","text":"<p>Each summary captures the questions, answers, and discussions from our sessions, providing additional context and real-world examples to complement the workshop materials.</p>"},{"location":"office-hours/#questions-and-topics","title":"Questions and Topics","text":"<p>Office hours cover a wide range of topics including:</p> <ul> <li>Practical implementation challenges</li> <li>Data collection strategies</li> <li>Evaluation methodologies</li> <li>User experience considerations</li> <li>Architecture decisions</li> <li>Product development approaches</li> </ul> <p>Get the Most from Office Hours</p> <p>Review the workshop materials before office hours sessions to come prepared with specific questions and challenges from your own RAG implementations.</p> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"office-hours/AGENTS/","title":"Office Hours Documentation Guide","text":""},{"location":"office-hours/AGENTS/#overview","title":"Overview","text":"<p>This directory contains comprehensive office hours documentation from the Systematically Improving RAG course across multiple cohorts. The files capture real-world Q&amp;A sessions with practical insights on RAG implementation, evaluation strategies, and business applications.</p>"},{"location":"office-hours/AGENTS/#file-structure","title":"File Structure","text":"<pre><code>docs/office-hours/\n\u251c\u2500\u2500 cohort2/               # First cohort (Jan-Feb 2024)\n\u2502   \u251c\u2500\u2500 week1-summary.md   # DSpy, graph databases, multimodal retrieval\n\u2502   \u251c\u2500\u2500 week2-summary.md   # Fine-tuning, synthetic data, multi-agent approaches\n\u2502   \u251c\u2500\u2500 week3-summary.md   # Feedback handling, recommendation systems\n\u2502   \u251c\u2500\u2500 week4-summary.md   # Customer segmentation, query analysis\n\u2502   \u251c\u2500\u2500 week5-summary.md   # Excel processing, SQL generation\n\u2502   \u2514\u2500\u2500 week6-summary.md   # Deep Research, long context evaluation\n\u251c\u2500\u2500 cohort3/               # Second cohort (May-June 2024)\n\u2502   \u251c\u2500\u2500 week-1-1.md        # Precision-recall tradeoffs, business value\n\u2502   \u251c\u2500\u2500 week-1-2.md        # Small language models, multi-turn conversations\n\u2502   \u251c\u2500\u2500 week-2-1.md        # Medical RAG, specialized domains\n\u2502   \u251c\u2500\u2500 week-2-2.md        # Time management, community engagement\n\u2502   \u251c\u2500\u2500 week-3-1.md        # Re-ranking models, embedding fine-tuning\n\u2502   \u251c\u2500\u2500 week-4-1.md        # Model selection, pricing strategies\n\u2502   \u251c\u2500\u2500 week-4-2.md        # Dynamic visualizations, customer feedback\n\u2502   \u251c\u2500\u2500 week-5-1.md        # Citation accuracy, temporal reasoning\n\u2502   \u2514\u2500\u2500 week-5-2.md        # Specialized indices, data engineering\n\u2514\u2500\u2500 index.md               # Navigation and overview\n</code></pre>"},{"location":"office-hours/AGENTS/#documentation-standards","title":"Documentation Standards","text":""},{"location":"office-hours/AGENTS/#yaml-frontmatter-format","title":"YAML Frontmatter Format","text":"<p>All office hours files use consistent metadata:</p> <pre><code>---\ntitle: Week X - Office Hour Y\ndate: 'YYYY-MM-DD'\ncohort: X\nweek: X\nsession: X\ntype: Office Hour / Office Hour Summary\ntranscript: ../path/to/transcript.txt\ndescription: Brief summary of main content areas\ntopics:\n  - Topic 1\n  - Topic 2\n  - Topic 3\n---\n</code></pre>"},{"location":"office-hours/AGENTS/#content-structure","title":"Content Structure","text":"<ol> <li>Title and Study Notes: Brief introduction to session focus</li> <li>Question Sections: <code>## Question text</code> format with detailed answers</li> <li>Key Takeaways: <code>***Key Takeaway:***</code> summaries for important insights</li> <li>FAQs Section: Comprehensive frequently asked questions</li> </ol> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"office-hours/faq/","title":"Frequently Asked Questions","text":"<p>This comprehensive FAQ is compiled from all office hours sessions across multiple cohorts.</p> <p>Quick Navigation</p> <p>Use your browser's search (Ctrl+F) to find specific terms or questions, or browse through the questions below.</p>"},{"location":"office-hours/faq/#what-is-your-take-on-dspy-should-we-use-it","title":"What is your take on DSpy? Should we use it?","text":"<p>Generally, I think DSpy allows you to do some kind of prompt optimization by synthetically creating a bunch of few-shot examples and then identifying which of these examples could improve the performance of your system.</p> <p>Personally, I feel like most of the time you should be spending a lot of time actually just tweaking those prompts yourself. The most valuable part of looking at data, few-shots, and examples is you building an intuition of what customers are looking for and what mistakes the system is making.</p> <p>Your product isn't just a prompt\u2014it includes how you collect feedback, how you set expectations in the UI, how you think about data extraction, and how you represent chunks in the context. If you spend the time to look at how the model is making mistakes and what users are asking for, you'll make much more progress in improving the product as a whole.</p> <p>DSpy is fine especially when you have very specific evaluations. For example, maybe you have a 35-class classification task where all you care about is accuracy. Then DSpy really works because you can figure out which of the 10 examples you need to maximize your accuracy.</p> <p>But most of the time, that's not the case. If I'm building a model to extract sales insights from a transcript, I don't have a dataset of \"here's all the sales insights.\" The real work might be extracting everything and hand-labeling some stuff. Because these tasks are very hard to hill-climb (when metrics aren't just classification accuracy), tools like DSpy don't work as well.</p> <p>Another good use of DSpy is around using LLMs as judges. If you have a tonality or factuality evaluation you really care about, it makes sense to label a hundred examples yourself and then use prompt optimization tools to create your own judge that aligns with your grades.</p>"},{"location":"office-hours/faq/#is-it-useful-to-prompt-language-models-with-an-understanding-of-structure-and-rationale-for-their-actions","title":"Is it useful to prompt language models with an understanding of structure and rationale for their actions?","text":"<p>Yes, absolutely. Understanding structure and rationale is critical because your product includes the ways you collect feedback, set expectations in the UI, perform data extraction, and represent chunks in the context.</p> <p>It's not just about the prompt\u2014it's a whole system. And if you can spend time looking at how the model makes mistakes and what users are asking for, you'll make much more progress in improving the product holistically.</p> <p>When you build an intuition for what's happening, you can make smarter design decisions across the entire product experience.</p>"},{"location":"office-hours/faq/#how-do-we-introduce-a-concept-of-time-and-vector-search-to-answer-questions-like-whats-the-latest-news-without-needing-to-move-to-a-graph-database","title":"How do we introduce a concept of time and vector search to answer questions like \"What's the latest news?\" without needing to move to a graph database?","text":"<p>The answer is to use a SQL database. If you use something like Timescale or PostgreSQL, there are many ways of doing time filtering.</p> <p>One specific thing to note is the difference between pgvector and pgvector-scale. Pgvector does not do exhaustive search, so there's a chance you don't recall all information because of how the database segments things. With pgvector-scale, it will exhaustively search every single row in your database if required. This small difference means a lot if you're trying to find very specific details.</p> <p>The general idea is to use structured extraction to identify start and end dates, prompt your language model with an understanding of what those dates are, and then use filtering. You would do an embedding search plus a BETWEEN statement in your time query. This works pretty well.</p>"},{"location":"office-hours/faq/#is-knowledge-graph-rag-production-ready-by-now-do-you-recommend-it","title":"Is knowledge graph RAG production ready by now? Do you recommend it?","text":"<p>In my 10 years of doing data science and machine learning, I generally stay away from any kind of graph modeling. The reason is that every time I've seen a company go into this graph-based world, within 4-5 years they decide to move back to a PostgreSQL database.</p> <p>There are several issues with graph databases:</p> <ol> <li>They're really hard to learn - it's much easier to hire talent that knows PostgreSQL than graph databases.</li> <li>Defining schemas in PostgreSQL and joins is well-defined, whereas in graph databases there's often too much debate and not enough best practices.</li> <li>Most cases don't require more than one or two traversals of your graph.</li> </ol> <p>When I was at Facebook, their graph was actually just a very large MySQL database. This makes me cautious about using graph databases unless you have expert users.</p> <p>The only company I really believe could effectively use a graph database is LinkedIn, because they need to compute things like nearest neighbors up to three or five degrees away.</p> <p>Even for cases like Microsoft's approach where you build a document graph with entities and relationships, I'd prefer to use fine-tuned embeddings. A graph can be defined as an adjacency matrix, and fine-tuning your embeddings can get you pretty close to the similarity definition that a graph could maintain.</p> <p>I'd rather start with data and say, \"There are certain kinds of queries that really need a graph structure\" and let that justify the graph structure. Most technology needs to be justified by what the product needs to deliver rather than thinking about technology first.</p>"},{"location":"office-hours/faq/#would-you-recommend-using-colbert-models-or-other-specialized-retrieval-approaches","title":"Would you recommend using Colbert models or other specialized retrieval approaches?","text":"<p>All of these models do similar things at their core. To decide what to use, we should start with a synthetic dataset to measure precision and recall. Then the real question becomes: do any of these interventions (graph RAG, Colbert models, embeddings, re-rankers) beat the baseline in terms of precision and recall?</p> <p>It might be that graph for a certain problem is only 2% better, in which case it might not be worth the complexity. But if you found that, for parsing hospital records, graph RAG is 40% better on recall and precision, then it doesn't matter what I think\u2014the data would speak for itself.</p> <p>For Colbert specifically, it probably does very well for certain tasks. For example, statements like \"I love coffee\" and \"I hate coffee\" would be very similar in embedding space because embeddings don't fully understand negation. With a Colbert model, the cross-attention mechanism can figure out that these statements are different.</p> <p>But you need to tell the model what's important in your context. Without enough tests to guide us, it's hard to know if these interventions work. Usually, it's hard to beat the baseline of embedding search with a good re-ranker. Colbert might do 4-5% better, but you need to justify that improvement against the added complexity.</p>"},{"location":"office-hours/faq/#when-working-with-legal-documents-that-have-multi-level-outlines-and-reference-sections-from-other-documents-what-approach-would-you-recommend","title":"When working with legal documents that have multi-level outlines and reference sections from other documents, what approach would you recommend?","text":"<p>This could be done with a graph, but it could also be done with a simpler pointer system. When you load data, you can pull in other references. For example, in a construction project, whenever we pull up an image, we also pull up the paragraph above and below the image, augmenting the context.</p> <p>We can do the same for legal documents\u2014if it references another page or citation, we pull in that citation. Technically, this is a graph, but it's often easier to build this as a few LEFT JOINs in a PostgreSQL table.</p> <p>When we pull in text chunks, if there are references, we just do another left join back to the original chunk. These systems tend to be much simpler to reason about than dealing with reference types in a graph. Usually, that level of complexity really needs to be earned when building bigger systems.</p>"},{"location":"office-hours/faq/#are-we-going-to-cover-any-fundamentals-of-how-to-systematically-do-generation","title":"Are we going to cover any fundamentals of how to systematically do generation?","text":"<p>In terms of generation, a lot comes down to prompting and using LLMs as judges, which we'll talk about in Week 3 when discussing product experience.</p> <p>If you have specific aspects of generation you want to explore, it's mostly about ensuring formatting is correct and chain of thought is reasonable. The challenge is that you can't systematically improve generation primarily because generation evaluations are much more subjective.</p> <p>If it's just formatting, that can be very explicit. But challenges with generation will mostly be addressed through LLM-as-judge approaches and different levels of regular expressions.</p> <p>For example, we have an evaluation for summarization that simply measures what percentage shorter the summary is relative to the original input. These are very basic evaluations for summarization.</p>"},{"location":"office-hours/faq/#whats-your-take-on-using-rag-for-report-generation-in-response-to-requests-for-proposals","title":"What's your take on using RAG for report generation in response to requests for proposals?","text":"<p>The expert on report generation will talk in Week 4. Look out for a talk from Vantager, who does this for financial due diligence. Companies can give them existing reports, which they parse into a spec, and then when you upload new PDFs, it automatically generates a report for you.</p> <p>There's a lot of economic value that can come from report generation, and it's probably more valuable than just doing generic question answering.</p>"},{"location":"office-hours/faq/#what-is-your-experience-using-reasoning-models-as-the-answer-generator-model","title":"What is your experience using reasoning models as the answer generator model?","text":"<p>Before there were specific reasoning models, I've been pushing everyone to at least have thinking tokens and a reasoning block in the output. This gives language models time to think and allows you to render in a way that minimizes perceived latency.</p> <p>Now that O1 and DeepSeek are available, unless latency is a concern, I would try to use these reasoning models. O3 Mini is fairly affordable, and O1 is very affordable. You can render the product in a way that makes users feel it's faster.</p> <p>DeepSeek's reasoning capability is one reason it stood out to people\u2014they can actually see it think. For many practitioners, we've been asking language models to think step by step for quite a while.</p>"},{"location":"office-hours/faq/#how-do-we-set-user-expectations-on-the-delay-while-using-reasoning-models","title":"How do we set user expectations on the delay while using reasoning models?","text":"<p>The first UI tip is to stream out the thinking part of the model to the customer. Things will feel about 45% faster just because something is moving on the page.</p> <p>The second approach, which DeepSeek does well, is to have a button called \"Think harder\" or \"Reasoning.\" If users don't use it, they get the faster V3 model, but if they press reasoning, it switches to the R1 model. This both tells users you want the model to think (which they know will be slower) and, by rendering the thought tokens, improves the perceived latency.</p>"},{"location":"office-hours/faq/#how-should-we-handle-multiple-rag-sources-with-different-levels-of-information","title":"How should we handle multiple RAG sources with different levels of information?","text":"<p>When you have multiple RAG sources (like a calendar and a news site with more detailed event information), it can slow down the system when you want to use an LLM to act as a judge and provide a holistic answer.</p> <p>One approach is to predict what types of questions are easy versus hard and route them effectively. Another approach is to improve the user experience by rendering sources before rendering the text. Show an animation like \"I am thinking\" and have document 1, 2, and 3 appear, then \"I'm reading,\" and finally the answer.</p> <p>Notion AI's UX does this well\u2014it says \"thinking about your question,\" \"searching documents,\" animates the documents coming in, and then starts talking. The key is to keep the screen moving to make users believe something is happening.</p> <p>Adding a loading screen that moves can make users feel the system is 30% faster, even if the actual processing time is the same.</p>"},{"location":"office-hours/faq/#what-strategies-can-help-when-there-are-negative-consequences-of-thinking-too-hard-with-reasoning-models","title":"What strategies can help when there are negative consequences of \"thinking too hard\" with reasoning models?","text":"<p>One approach is to predict whether a question is easy or hard and decide when to turn on thinking. You could use a model like BERT to classify this.</p> <p>If that's possible, you can make the decision to think on behalf of the user. The objective would be to maximize customer satisfaction while minimizing token costs.</p> <p>Some companies like have their own proprietary model that tells you which is the best model to route to. You could have a model that's trained so that if you ask \"what's 1+1,\" it sends that to a simpler model, but if you ask about reading a legal document, it routes to an R1 model.</p> <p>For evaluation questions specifically, it really depends on the complexity. Some evaluations are simple yes/no decisions, while others involve complex reasoning like assigning the correct speaker to different comments in a transcript. You'll need to test with your specific use case.</p>"},{"location":"office-hours/faq/#what-advice-would-you-give-for-introducing-llms-into-a-healthcare-company-that-may-not-fully-grasp-their-potential","title":"What advice would you give for introducing LLMs into a healthcare company that may not fully grasp their potential?","text":"<p>First, build a demo and let leadership see the results. Then, clearly identify what types of queries you won't attempt to answer, pre-loading all the risk discussions upfront.</p> <p>Instead of saying \"my model is 80% correct,\" say \"I've identified the 20% of questions that don't work at all, but for the 80% of questions we can solve, the success rate is 99%.\"</p> <p>Do the upfront work to know the failure modes and economically valuable opportunities, then present them clearly. Add guardrails to say what the LLM won't attempt to do. Much of this is about setting expectations for leadership.</p>"},{"location":"office-hours/faq/#are-there-open-source-re-ranking-models-that-come-close-to-coheres-re-rankers-in-quality","title":"Are there open source re-ranking models that come close to Cohere's re-rankers in quality?","text":"<p>There are definitely good cross-encoders available, though some of the top models on leaderboards are 7 billion parameters, which may have high latency.</p> <p>Modern BERT (a new BERT-based embedding model with about 8,000 token sequence length compared to the original 512) will likely lead to more powerful BERT-based re-rankers.</p> <p>However, training your own re-ranker on your specific data will likely beat benchmark models. With just 6,000 examples from your own data, you can train a better embedding model and cross-encoder than what's publicly available, costing around $1.50 and 40 minutes on a laptop.</p>"},{"location":"office-hours/faq/#outside-of-personal-experiments-what-resources-or-mediums-do-you-rely-on-to-stay-up-to-date-on-rag","title":"Outside of personal experiments, what resources or mediums do you rely on to stay up to date on RAG?","text":"<p>Much of the content coming out is very hypey, and many research papers focus on public evaluations that don't mean as much as more fundamental work on data analysis, experimentation, and evaluation.</p> <p>When reading papers, focus more on how they present results and think about experimentation rather than specific methodologies or implementations. The things that work well are often too maintenance-heavy or expensive for production use cases with millions of PDFs.</p> <p>I like Anthropic's blog posts because they're fundamental\u2014discussing how to think about error bars, clustering, and other approaches that everyone can use, not just researchers with 40,000 rows in a database.</p> <p>Outside of that, a lot of information is in private Discords and Twitter. I'll have someone make a summary of the Discords with interesting \"alpha\" or insights.</p>"},{"location":"office-hours/faq/#when-working-with-documents-with-metadata-should-search-and-retrieval-methods-change-based-on-the-level-of-metadata-provided-within-the-queries","title":"When working with documents with metadata, should search and retrieval methods change based on the level of metadata provided within the queries?","text":"<p>Yes, they should. For example, in a construction project, we found people really cared about who made the last edits on legal contracts or who sent particular information. The metadata was very important\u2014queries like \"which contracts did this person send us\" become like SQL queries.</p> <p>We learned that when answering questions about who's doing what, we should include their contact information. These are small details in improving a RAG system that create economic value.</p> <p>Similarly, if you're building information that will be queried across time periods, you probably care about when documents were published and last crawled to determine relevance. A query like \"what is the latest research in physics\" might look at the past 6 months, while \"what is new in AI\" might only look at the past two weeks because it moves so quickly.</p> <p>It comes down to analyzing the queries people are asking and figuring out what creates economic value.</p>"},{"location":"office-hours/faq/#do-you-know-if-anthropic-is-working-on-an-answer-to-o1-or-r1-reasoning-models","title":"Do you know if Anthropic is working on an answer to O1 or R1 (reasoning models)?","text":"<p>Yes and no. If you use Claude's web app, it secretly has thinking tokens. Every time it says \"pondering\" or \"thinking,\" it's actually outputting thinking tokens that you can't see.</p> <p>If you ask Claude to replace the  token with {anyThinking}, you'll start seeing those thinking tokens. You can request this token in the API as well. <p>The real question is whether Anthropic has thinking models that use RLHF, and I'm not fully sure about that. Their CTO has stated they don't do distillation, but there are mixed interpretations of what that means.</p> <p>Claude 3.5 Sonnet is still impressive even without visible reasoning, including its vision capabilities. The bigger issue is that Anthropic is very concerned about safety and has questions about whether thinking tokens could lie to users or follow different policies.</p>"},{"location":"office-hours/faq/#when-working-with-unstructured-data-mostly-pdfs-and-drawings-how-do-you-approach-data-labeling-and-what-models-do-you-use","title":"When working with unstructured data, mostly PDFs and drawings, how do you approach data labeling and what models do you use?","text":"<p>For unprocessed data, I look at companies like Llama Parse, Extend, and Reducto, which parse headers, bodies, tables, and figures so you can work with them separately.</p> <p>For the most part, Claude Sonnet does a very good job\u2014it's just a matter of how much data you need to process. For specific tasks like understanding figures, visual language models like Qwen via Ollama work well for single PDFs, though batch local processing is more challenging as tools like VLLM don't yet support these models.</p>"},{"location":"office-hours/faq/#why-does-this-course-favor-lancedb-versus-other-vector-databases","title":"Why does this course favor LanceDB versus other vector databases?","text":"<p>The main reason is that I want everyone to experience running evaluations on not just embedding search but also full-text search. I want you to try hybrid search with or without a re-ranker.</p> <p>With LanceDB, incorporating these approaches is just one extra line of code. You can do a search with different modes (lexical, vector, hybrid) and easily add a re-ranker. It's the simplest way to try all these combinations and discover what works best.</p> <p>Additionally, LanceDB is backed by DuckDB, which means the same database that supports full-text search, semantic search, and re-rankers also supports SQL. If you want to analyze your queries with SQL, you can do that easily.</p> <p>Another advantage is that LanceDB can be hosted on S3 and is easy to set up for large amounts of data.</p>"},{"location":"office-hours/faq/#which-industry-or-application-domain-do-you-think-is-most-difficult-for-llms","title":"Which industry or application domain do you think is most difficult for LLMs?","text":"<p>It's hard to say definitively, but generally:</p> <ol> <li>Tasks with complex images are difficult</li> <li>Highly regulated industries like legal and healthcare contexts present challenges</li> <li>Financial services, especially ratings agencies, face enormous regulatory hurdles</li> </ol> <p>The fundamental challenge is that anything difficult for humans to collect data on will be hard for an LLM. It's about how much volume of data we have per industry and what kind of feedback loops exist.</p> <p>If an LLM makes a decision that takes weeks to verify, it's going to be hard to improve. The timeline for regulatory approval in some industries (like ratings agencies) can be years, creating a massive barrier to implementing LLM-based solutions.</p>"},{"location":"office-hours/faq/#did-you-find-a-use-case-where-re-rankers-improve-metrics","title":"Did you find a use case where re-rankers improve metrics?","text":"<p>Almost every case I've seen shows improvements with re-rankers, whether it's legal documents, question answering over books, or financial documents. A Cohere re-ranker typically improves performance by 6-12% while adding about 400-500ms of latency.</p> <p>Companies like Cohere are building industry-specific rankers that support financial text, medical text, and code. They're working hard to beat OpenAI embeddings, and they generally succeed.</p> <p>Re-rankers solve problems that embeddings miss, like distinguishing between \"I love coffee\" and \"I hate coffee,\" which look similar in embedding space but are clearly different with cross-attention in a re-ranker.</p>"},{"location":"office-hours/faq/#can-you-share-resources-on-how-to-create-hybrid-embeddings-for-postgresql-vector-databases","title":"Can you share resources on how to create hybrid embeddings for PostgreSQL vector databases?","text":"<p>If you use a library called ParagraphDB, you can set up both sparse BM25 indices and dense embedding-based indices. This allows you to implement rank fusion.</p> <p>Pinecone has good resources about this topic that I can share.</p>"},{"location":"office-hours/faq/#for-medicalhealthcare-administration-how-can-we-get-llms-to-be-something-that-are-trustworthy-with-serious-decisions","title":"For medical/healthcare administration, how can we get LLMs to be something that are trustworthy with serious decisions?","text":"<p>One approach is to use chain of thought models where we can read the reasoning to understand how the model arrived at a decision. Anthropic's concern may be that the chain of thought could be misleading.</p> <p>There's likely a future where we can build UIs that let humans verify not only the decision but also the chain of thought behind it. Then we can train models so that even the reasoning aligns with user preferences. If a model gets the right answer but with faulty reasoning, that's where we'd provide feedback.</p> <p>Another approach is to use ensembles\u2014sample a suite of LLMs and use majority voting on decisions to establish confidence. I often train multiple smaller language models to grade things on a 0-1 scale, then use a classical ML model (like logistic regression) to make the final prediction. This helps with explainability because you can see which features influenced the prediction.</p>"},{"location":"office-hours/faq/#for-multimodal-retrieval-text-images-what-approaches-work-best","title":"For multimodal retrieval (text + images), what approaches work best?","text":"<p>For visual content like photographs, CLIP embeddings work well since they're inherently multimodal\u2014they can represent both images and text in the same embedding space.</p> <p>For instructional manuals with images, I'd pass the images to a language model and ask for a detailed summary of what the image shows, including all text in the image. Then embed that summary instead. This creates a text representation that points to the original image.</p> <p>The approach has two steps:</p> <ol> <li>Given an image, create a synthetic question that would retrieve it</li> <li>Create a summary that would be retrieved for that question</li> </ol> <p>For product marketing scenarios, CLIP embeddings can work well, but you need to define what \"similar\" means in your context. Does a red shirt match other red shirts, or just shirts of the same color? Should expensive silk shirts match inexpensive polyester versions?</p> <p>This is why fine-tuning embedding models to understand your specific definition of similarity is important.</p>"},{"location":"office-hours/faq/#how-do-you-approach-chunking-very-long-documents-1500-2000-pages","title":"How do you approach chunking very long documents (1,500-2,000 pages)?","text":"<p>If you have extremely long documents, I'd first try a page-level approach to determine if answers typically exist on a single page or span multiple pages.</p> <p>One compelling approach is from a paper called RAPTOR. After chunking documents, they recluster the chunks. You embed every page, run a clustering model, and identify concepts that span multiple pages. Then summarize those clusters and use the summaries for retrieval\u2014if the summary is retrieved, you can include all related pages in the context.</p> <p>For metadata, look at your queries to determine what matters. If users frequently ask about publication dates or document authors, those should be included. The needs will become obvious as you analyze user queries\u2014you'll realize what's important and what creates economic value.</p> <p>Generally, if you can reorganize text chunks by clustering and bringing related information together, that's very valuable. For example, with tax law documents where laws are on pages 1-30 and exemptions on page 50, you could process the document once to place exemptions directly below the relevant laws. This preprocessing step might cost $10 of LLM calls per document, but for legal documents that might not change for years, it's worth the investment.</p>"},{"location":"office-hours/faq/#do-you-have-a-go-to-approach-for-visual-document-image-embeddings-like-quarterly-reports-with-tables-images-graphs","title":"Do you have a go-to approach for visual document image embeddings (like quarterly reports with tables, images, graphs)?","text":"<p>For visual documents like quarterly reports full of tables and images:</p> <ol> <li>Dockling is a free library that works quite well, though it might take about 11 seconds per PDF</li> <li>Claude Sonnet also works well for extraction</li> <li>Reducto, Llama Parse, and other commercial tools can be worth the cost to save time</li> <li>For multilingual content, VDR2B-Multi v1 handles multiple languages well</li> </ol> <p>There's an ongoing discussion about using Gemini 2 (with its million-token context window) to convert documents to markdown and extract all the information. This approach is becoming more viable as models improve, potentially reducing the engineering needed for preprocessing.</p> <p>Recent testing shows Reducto still has higher accuracy (0.9 \u00b1 0.1) compared to Gemini (0.84 \u00b1 0.16), but the gap is narrowing. The reason Reducto performs so well is that they have people manually labeling thousands of PDFs to train their models.</p>"},{"location":"office-hours/faq/#why-at-meta-did-you-prefer-sql-databases-over-graph-databases","title":"Why at Meta did you prefer SQL databases over graph databases?","text":"<p>Graph databases are useful when you need complex traversals, like finding all of Jason's followers who follow a specific account, then finding what they like, and sorting by aggregated likes per product.</p> <p>However, what we found is that most use cases are actually simpler\u2014often just requiring 2-3 left joins in SQL rather than complex graph traversals. From a skills perspective, it's easier to hire people who know SQL well than to find graph database experts.</p> <p>At scale, graphs are also hard to manage. Around 2017-2018, only LinkedIn had a true graph database because they needed to compute 3rd-degree friendships very quickly. For most companies, SQL databases offer better performance, easier maintenance, and more familiar tooling.</p> <p>Over a 12-year career, we kept trying different technologies (Hadoop, Spark, etc.) but always ended up returning to SQL. The pattern is consistent across many organizations.</p>"},{"location":"office-hours/faq/#what-have-you-learned-about-prompt-caching","title":"What have you learned about prompt caching?","text":"<p>Prompt caching is a technique where language models can avoid reprocessing the beginning of prompts that are often identical.</p> <p>Different providers handle this differently:</p> <ul> <li>Anthropic caches prompts for 5 minutes; if you make the same request within that time, the entire message is cached</li> <li>OpenAI figures out the optimal prefix to cache automatically</li> </ul> <p>This is valuable because it can save significant processing time and costs, especially when you have many few-shot examples or large system prompts. If you have 50+ examples in your prompt, caching can dramatically improve performance.</p> <p>For models like Claude on Bedrock, prompt caching wasn't available a few months ago but is likely coming soon. It's the kind of feature that rolls out gradually across providers.</p>"},{"location":"office-hours/faq/#for-visual-document-image-processing-whats-the-state-of-the-art","title":"For visual document image processing, what's the state of the art?","text":"<p>There's a recent discussion on Hacker News about using Gemini 2 (with its million-token context window) to process documents and convert them to markdown, extracting tables, layout information, and text.</p> <p>The engineering needed for document pre-processing is getting simpler as these models improve. Recent tests show Reducto still has higher accuracy (0.9 \u00b1 0.1) compared to Gemini (0.84 \u00b1 0.16), but the gap is narrowing.</p> <p>Reducto's performance comes from having people manually label thousands of PDFs, then training models on that high-quality data. This reinforces the point that with 6,000-10,000 high-quality labels from your own data, you can train models that outperform even the biggest general models on your specific tasks.</p>"},{"location":"office-hours/faq/#how-does-brain-trust-work-with-the-notebooks-in-this-course","title":"How does Brain Trust work with the notebooks in this course?","text":"<p>Brain Trust just saves the results that your laptop is running locally. It's not executing anything or using a better database\u2014it's more like an observability tool (similar to Datadog).</p> <p>When we run the notebooks, everything is running on your laptop in LanceDB. The only thing Brain Trust sees is row IDs and scores. Think of it as a powerful UI over a database that's saving your logs, not as a computation service.</p>"},{"location":"office-hours/faq/#whats-the-difference-between-bi-encoders-and-cross-encoders","title":"What's the difference between bi-encoders and cross-encoders?","text":"<p>A bi-encoder converts all documents into numbers (embeddings) first, and then the assumption is that when we compare those numbers, documents that look similar are similar. Because we pre-compute everything, we can search very quickly.</p> <p>A cross-encoder doesn't compare numbers\u2014it compares the actual sentences. This approach can't compare a million documents with a million other documents (too expensive), so instead it takes one question and 50 documents and compares each one individually. That's the \"cross\" part of cross-encoder.</p> <p>The advantage of cross-encoders is that a language model can compare words like \"love\" and \"hate\" in \"I love coffee\" and \"I hate coffee\" and understand they're different, whereas bi-encoders just have lists of numbers that don't capture this nuance.</p> <p>We'll cover this topic more deeply in Week 2, but the key takeaway is that bi-encoders are faster but less accurate, while cross-encoders are slower but better at understanding semantic distinctions.</p>"},{"location":"office-hours/faq/#whats-the-process-for-fine-tuning-embedding-models","title":"What's the process for fine-tuning embedding models?","text":"<p>In Week 2, we'll cover this topic extensively. The overall message is that:</p> <ol> <li>It's probably a bad idea to train your own language model</li> <li>It's a very good idea to train your own embedding model</li> </ol> <p>Fine-tuning embedding models is much less resource-intensive\u2014it typically costs around $1.50 and takes about 40 minutes on a laptop. With just 6,000 examples from your domain, you can train embedding models and cross-encoders that outperform general-purpose models on your specific tasks.</p> <p>This is especially useful when you need embeddings to understand domain-specific concepts or when you're trying to define what \"similar\" means in your particular context (e.g., product recommendations where price range matters).</p>"},{"location":"office-hours/faq/#how-do-you-understand-metrics-like-precision-and-recall-in-one-to-one-answer-scenarios","title":"How do you understand metrics like precision and recall in one-to-one answer scenarios?","text":"<p>For questions with exactly one correct answer, these metrics behave somewhat differently. Recall will be either 0% or 100% depending on whether K is large enough to include the correct answer.</p> <p>For example, if we want to retrieve exactly one document and there's only one correct answer, precision could be either 0% or 100%, and the same for recall.</p> <p>The metrics become more meaningful when:</p> <ol> <li>There are multiple relevant documents</li> <li>We're analyzing trends across many queries</li> <li>We're comparing different retrieval methods</li> </ol> <p>Even with one-to-one mappings, MRR (Mean Reciprocal Rank) is still useful to see where the correct answer appears in your results.</p> <p>What really matters isn't the absolute number but whether we can move these metrics in a positive direction with our interventions. It's like weighing yourself\u2014the absolute number may vary by scale, but if you've gained two pounds, you've definitely gained two pounds.</p> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"office-hours/faq/#how-would-you-evaluate-the-effect-of-different-parsing-strategies-in-rag-notably-on-documents-with-weird-layouts-tables-and-charts","title":"How would you evaluate the effect of different parsing strategies in RAG, notably on documents with weird layouts, tables, and charts?","text":"<p>For documents with complex layouts like tables and charts, there are multiple levels of evaluation:</p> <p>First, you need to evaluate OCR accuracy - checking whether text is being parsed correctly (e.g., is a 0 being parsed as an 8?). Then there's the bounding box detection problem - checking if tables are fully recognized as single bounding boxes using metrics like intersection over union.</p> <p>It's generally safer to evaluate OCR/parsing and retrieval separately because parsing errors can be hard to trace back when they're part of the full RAG pipeline. If you parse an 8 as a 0 and generate synthetic data from that, you won't be able to capture that error in your evaluations.</p> <p>I've leaned on parsing vendors because they're the most incentivized to have good and accurate labels. This lets me focus on retrieval, which is what will create the most value for my specific use case. While there are other businesses focused on PDF processing, no one will focus specifically on your ability to do retrieval well with your data.</p>"},{"location":"office-hours/faq/#when-does-it-make-sense-to-create-a-targeted-summary-for-an-applications-objective-versus-fine-tuning-embedding-models","title":"When does it make sense to create a targeted summary for an application's objective versus fine-tuning embedding models?","text":"<p>This depends on whether you have data to fine-tune and what your embedding should capture. If you're writing the summary yourself, you're essentially making an assumption about what the embedding should look like.</p> <p>For example, with image embeddings, maybe the most common questions aren't just about what's in the photo but about the cinematic mood. In that case, it might make sense to have a language model create a summary describing the mood because I want to be able to search for \"atmospheric\" or \"dark and gloomy\" rather than just \"trees in a forest.\"</p> <p>However, if you have actual user interaction data, it's better to use that data to tell us what's similar rather than creating assumptions. For example, with blueprint images, an image model might just say \"this is a blueprint,\" but what I specifically did was extract information like number of rooms, bathrooms, sizes, and addresses - information that would be harder for a CLIP embedding to capture.</p> <p>In general, I'd much rather use the data my app generates than hard-code these summaries, but summaries can be useful when you need to extract specific structured information that embedding models might miss.</p>"},{"location":"office-hours/faq/#what-are-the-recommended-approaches-for-evaluating-rag-for-single-documents-like-report-generation-for-a-proposal","title":"What are the recommended approaches for evaluating RAG for single documents, like report generation for a proposal?","text":"<p>When working with a single PDF document that might be varying in length (from 10 to 400 pages), semantic chunking can be valuable to separate paragraphs based on their semantic meaning rather than just token-based chunking. This is especially important when requirements for different disciplines might be found in different sections (e.g., structural requirements mentioned within architectural requirements).</p> <p>One approach is to generate synthetic questions per paragraph, asking \"What are the requirements being mentioned in this paragraph?\" rather than \"What question can you ask from this paragraph?\" This helps identify the key information.</p> <p>For retrieval, you can also inject a summary of the page that a paragraph was extracted from and embed them together. This way, when retrieving, you have both the specific chunk and context about where it comes from, which can improve recall.</p> <p>Whether adding summaries improves recall is an empirical question - if it increases recall by 1%, it might not be worth the extra LLM calls, but if it improves recall by 6-8%, it could be worth investigating further.</p>"},{"location":"office-hours/faq/#could-you-distill-key-reasons-when-someone-should-consider-fine-tuning-open-source-embedding-models-over-proprietary-models","title":"Could you distill key reasons when someone should consider fine-tuning open source embedding models over proprietary models?","text":"<p>If you have 6,000-10,000 examples of question-document relevancy pairs, you can likely outperform closed-source models with a fine-tuned model. This is because your tasks can be much more specific than what general models are optimized for.</p> <p>It can also be more valuable if you need to embed massive datasets at scale. By spinning up your own GPUs, you can process much more text per second at a lower cost. For example, embedding 20GB of text data might take only 15 minutes and cost around $20, whereas using OpenAI APIs would be more expensive and much slower.</p> <p>The main downside is the need to maintain your inference server, which adds complexity. It's less about whether the model will perform well and more about whether you have the time and resources to maintain the infrastructure.</p>"},{"location":"office-hours/faq/#is-there-a-reason-to-ever-fine-tune-the-llm-rather-than-or-in-combination-with-fine-tuning-the-retriever-model","title":"Is there a reason to ever fine-tune the LLM rather than or in combination with fine-tuning the retriever model?","text":"<p>I'm pretty open to businesses fine-tuning their retrieval models because companies like OpenAI or Anthropic aren't primarily focused on making retrieval better - they're not launching new embedding models daily. Companies like Cohere, on the other hand, are actually thinking about retrieval.</p> <p>If you spend effort fine-tuning an LLM, you need to consider inference, CUDA drivers, and whether your fine-tuned model will be competitive when the original model provider releases a new version in a few months.</p> <p>It's generally very costly to fine-tune language models, and you often don't get much benefit. However, if there are specific reasons - like tonality, personalization, or access to proprietary data - it might make sense. But for a team of 4-5 people, it's probably not a good idea to spend effort maintaining that kind of infrastructure.</p> <p>In contrast, fine-tuning embedding models can be done on a laptop, run on cloud instances, and be cost-effective. For most teams, the maintenance cost of running your own LLM is just too high to justify.</p>"},{"location":"office-hours/faq/#one-weakness-of-rag-is-difficulty-in-detecting-relationships-between-concepts-because-the-retriever-model-isnt-aware-of-how-concepts-relate-to-each-other-should-we-fine-tune-the-llm-for-this","title":"One weakness of RAG is difficulty in detecting relationships between concepts because the retriever model isn't aware of how concepts relate to each other. Should we fine-tune the LLM for this?","text":"<p>Before considering fine-tuning the language model, I would ask: How much can we put into few-shot examples in the prompt? Can we come up with good chain-of-thought examples that describe these relationships? Can we provide a glossary?</p> <p>The maintenance cost of running an LLM is so high that it's worth really trying to squeeze out as much as possible through prompt engineering, longer system prompts, more few-shot examples, and prompt caching before considering fine-tuning.</p> <p>For example, Bloomberg spent millions on their own model, and within 5-6 months, GPT-4 was better. Instead of fine-tuning, consider using RAG to retrieve relationship information first, put that in the context, and then add the actual question. This is more maintainable and adaptable as new models are released.</p>"},{"location":"office-hours/faq/#what-is-the-main-failure-modes-like-distribution-mismatch-or-biases-that-youve-seen-when-relying-on-synthetic-data-for-retrieval-fine-tuning","title":"What is the main failure modes (like distribution mismatch or biases) that you've seen when relying on synthetic data for retrieval fine-tuning?","text":"<p>The biggest issue is mismatch between user questions in reality versus in the synthetic data. Once you have synthetic data for fine-tuning retrieval models, it's hard to imagine a case where creating more data for your use case would make the model worse.</p> <p>What's more important is figuring out how to intelligently incorporate real-world examples from users into the few-shot examples for synthetic data generation, making it a more diverse process. You can check this by:</p> <ol> <li>Looking at the variance of embeddings against each other to see if they're too similar</li> <li>Checking general statistics like character count variance in questions</li> <li>Ensuring the synthetic data matches user data characteristics</li> </ol> <p>For example, if your customer questions typically have around 30 characters but your synthetic data averages 90 characters because the language model is too verbose, that's a simple distribution mismatch to fix.</p>"},{"location":"office-hours/faq/#can-you-share-the-intuition-for-the-difference-between-a-fine-tuned-embedding-model-and-a-fine-tuned-re-ranker","title":"Can you share the intuition for the difference between a fine-tuned embedding model and a fine-tuned re-ranker?","text":"<p>The embedding model allows you to do search over a large number of documents - given an embedding model, you might retrieve the top 100 text chunks. The re-ranker model then takes these 100 chunks and finds the best 25.</p> <p>We generally want to use both, and the dataset to train these models is actually the same dataset. If you can only afford to fine-tune one, you might choose based on where your bottleneck is:</p> <ol> <li>Is recall at 100 already good (95%) but recall at 10 is poor (50%)? Then focus on the re-ranker.</li> <li>Are you missing relevant documents even in your top 100 results? Then focus on the embedding model.</li> </ol> <p>The key insight is that by having metrics on both stages, you can identify where to focus your improvement efforts.</p>"},{"location":"office-hours/faq/#do-we-need-more-data-to-fine-tune-re-rankers-than-bi-encoders","title":"Do we need more data to fine-tune re-rankers than bi-encoders?","text":"<p>It depends on the model, but generally, Cohere has done a good job of being data-efficient for producing embedding models. The amount of data needed may vary by model and task.</p>"},{"location":"office-hours/faq/#for-collaborative-filtering-models-how-do-you-address-the-cold-start-problem-new-usersitems-the-model-hasnt-seen-without-retraining-the-model","title":"For collaborative filtering models, how do you address the cold start problem (new users/items the model hasn't seen) without retraining the model?","text":"<p>There are multiple approaches to this. Instead of using classical collaborative filtering models, many systems now build models with user embeddings and item embeddings. The question becomes: can we use some other model to predict the embeddings we would have trained using interaction data?</p> <p>For example, in an e-commerce setting, if we trained our item embeddings using purchase data and a new item comes in, we could train a vision model to predict the embedding of the item based on its image and metadata. We can use that as an initial set of recommendations.</p> <p>The core idea is using other available data to predict the embeddings that would have come from interaction data (like checkout data). This approach helps bridge the gap for new items or users.</p>"},{"location":"office-hours/faq/#how-do-you-handle-rag-for-multimodal-content-like-powerpoint-presentations-that-have-complex-layouts","title":"How do you handle RAG for multimodal content like PowerPoint presentations that have complex layouts?","text":"<p>For documents with complex layouts like PowerPoint presentations, the parsing and chunking processes are linked. You might want to evaluate them separately since parsing errors will be hard to detect in the full RAG pipeline.</p> <p>One approach is to use general-purpose parsing tools like Dockling, Claude Sonnet, or commercial tools like Reducto, Llama Parse, and Extend. For multilingual content, models like VDR2B-Multi v1 handle multiple languages well.</p> <p>Recent developments include using models like Gemini 2 (with its million-token context window) to convert documents to markdown and extract information, though specialized tools like Reducto still have higher accuracy (0.9 \u00b1 0.1 vs. 0.84 \u00b1 0.16 for Gemini). These gaps are narrowing as general models improve.</p>"},{"location":"office-hours/faq/#why-did-you-prefer-sql-databases-over-graph-databases-at-metafacebook","title":"Why did you prefer SQL databases over graph databases at Meta/Facebook?","text":"<p>Graph databases are useful when you need complex traversals, but most use cases only require 2-3 left joins in SQL rather than complex graph operations. From a skills perspective, it's easier to hire people who know SQL well than to find graph database experts.</p> <p>At scale, graphs are also hard to manage. Around 2017-2018, only LinkedIn had a true graph database because they needed to compute 3rd-degree friendships very quickly. For most companies, SQL databases offer better performance, easier maintenance, and more familiar tooling.</p> <p>Over a 12-year career, we kept trying different technologies (Hadoop, Spark, etc.) but always ended up returning to SQL. Most cases don't require more than one or two traversals of your graph, making SQL a more practical choice.</p>"},{"location":"office-hours/faq/#what-have-you-learned-about-prompt-caching_1","title":"What have you learned about prompt caching?","text":"<p>Prompt caching is a technique where language models can avoid reprocessing the beginning of prompts that are often identical:</p> <ul> <li>Anthropic caches prompts for 5 minutes; if you make the same request within that time, the entire message is cached</li> <li>OpenAI figures out the optimal prefix to cache automatically</li> </ul> <p>This is valuable because it can save significant processing time and costs, especially when you have many few-shot examples or large system prompts. If you have 50+ examples, caching can dramatically improve performance.</p> <p>For models like Claude on Bedrock, prompt caching wasn't available a few months ago but is likely coming soon. It's the kind of feature that rolls out gradually across providers.</p>"},{"location":"office-hours/faq/#whats-the-difference-between-bi-encoders-and-cross-encoders_1","title":"What's the difference between bi-encoders and cross-encoders?","text":"<p>A bi-encoder converts all documents into numbers (embeddings) first, and then compares those numbers. Because we pre-compute everything, we can search very quickly.</p> <p>A cross-encoder doesn't compare numbers\u2014it compares the actual sentences. This approach can't compare a million documents with a million other documents (too expensive), so instead it takes one question and 50 documents and compares each one individually.</p> <p>The advantage of cross-encoders is that they can understand semantic distinctions like the difference between \"I love coffee\" and \"I hate coffee,\" whereas bi-encoders just have numeric representations that might miss this nuance.</p> <p>Bi-encoders are faster but less accurate, while cross-encoders are slower but better at understanding semantic distinctions.</p>"},{"location":"office-hours/faq/#whats-the-process-for-fine-tuning-embedding-models_1","title":"What's the process for fine-tuning embedding models?","text":"<p>It's probably a bad idea to train your own language model, but it's a very good idea to train your own embedding model.</p> <p>Fine-tuning embedding models is much less resource-intensive\u2014it typically costs around $1.50 and takes about 40 minutes on a laptop. With just 6,000 examples from your domain, you can train embedding models and cross-encoders that outperform general-purpose models on your specific tasks.</p> <p>This is especially useful when you need embeddings to understand domain-specific concepts or when you're trying to define what \"similar\" means in your particular context (e.g., product recommendations where price range matters).</p>"},{"location":"office-hours/faq/#what-non-intuitive-things-have-you-learned-about-recommendation-systems","title":"What non-intuitive things have you learned about recommendation systems?","text":"<p>The big insight about recommendation systems is that inventory matters a lot more than the actual algorithm. While Tiktok's algorithm is good, what really allows it to produce great recommendations is the vast amount of content available. Without those videos, you can't do much - and the same applies to RAG.</p> <p>The metadata you have and the inventory you have are much more important than the algorithm itself. For example, if recommendations for \"Greek restaurants near me\" are bad, the solution might be to add more Greek restaurants to your database, not to tweak the algorithm.</p> <p>Similarly, if queries after 7 PM perform poorly, maybe you're missing information about whether restaurants are open. The solution is to collect that data rather than change your algorithms.</p> <p>The hard work in recommendation systems is often: Do we have enough rows in the database? How much content do we have? And for that content, do we have the right metadata?</p>"},{"location":"office-hours/faq/#when-working-with-documents-with-metadata-should-search-and-retrieval-methods-change-based-on-the-level-of-metadata-provided-within-the-queries_1","title":"When working with documents with metadata, should search and retrieval methods change based on the level of metadata provided within the queries?","text":"<p>Yes, they should. For example, in a construction project, we found people really cared about who made the last edits on legal contracts or who sent particular information. The metadata was very important for queries like \"which contracts did this person send us,\" which function more like SQL queries.</p> <p>Similarly, if you're building information that will be queried across time periods, you probably care about when documents were published and last crawled to determine relevance. A query like \"what is the latest research in physics\" might look at the past 6 months, while \"what is new in AI\" might only look at the past two weeks because it moves so quickly.</p> <p>It comes down to analyzing the queries people are asking and figuring out what creates economic value.</p>"},{"location":"office-hours/faq/#do-you-have-a-go-to-approach-for-visual-document-image-embeddings-like-quarterly-reports-with-tables-images-graphs_1","title":"Do you have a go-to approach for visual document image embeddings (like quarterly reports with tables, images, graphs)?","text":"<p>For visual documents like quarterly reports full of tables and images:</p> <ol> <li>Dockling is a free library that works quite well, though it might take about 11 seconds per PDF</li> <li>Claude Sonnet also works well for extraction</li> <li>Commercial tools like Reducto, Llama Parse, and others can be worth the cost to save time</li> <li>For multilingual content, VDR2B-Multi v1 handles multiple languages well</li> </ol> <p>Recent testing shows Reducto still has higher accuracy (0.9 \u00b1 0.1) compared to Gemini (0.84 \u00b1 0.16), but the gap is narrowing. Reducto performs well because they have people manually labeling thousands of PDFs to train their models.</p>"},{"location":"office-hours/faq/#how-do-you-handle-multilingual-rag","title":"How do you handle multilingual RAG?","text":"<p>Cohere has put the most effort into multilingual models, with both multilingual local LLMs and embedding models.</p> <p>I recommend figuring out which languages appear in your queries and ensuring your evaluation reflects that distribution. Check whether the models you're considering (Cohere, OpenAI) perform well on these languages.</p> <p>While translation might seem like an option, if it worked well, companies like OpenAI and Cohere would already be using synthetic translation data to improve their language models. To evaluate performance across languages, create synthetic questions in multiple languages and verify whether recall rates differ between languages.</p>"},{"location":"office-hours/faq/#how-do-you-approach-chunking-very-long-documents-1500-2000-pages_1","title":"How do you approach chunking very long documents (1,500-2,000 pages)?","text":"<p>If you have extremely long documents, start with a page-level approach to determine if answers typically exist on a single page or span multiple pages.</p> <p>One compelling approach is from the RAPTOR paper. After chunking documents, they recluster the chunks by embedding every page, running a clustering model, and identifying concepts that span multiple pages. Then they summarize those clusters and use the summaries for retrieval\u2014if a summary is retrieved, all related pages are included in the context.</p> <p>For metadata, look at your queries to determine what matters. If users frequently ask about publication dates or document authors, those should be included. The needs will become obvious as you analyze user queries.</p> <p>If you can reorganize text chunks by clustering and bringing related information together, that's very valuable. For example, with tax law documents where laws are on pages 1-30 and exemptions on page 50, you could process the document once to place exemptions directly below the relevant laws. This preprocessing step might cost $10 of LLM calls per document, but for legal documents that might not change for years, it's worth the investment.</p>"},{"location":"office-hours/faq/#how-do-you-understand-metrics-like-precision-and-recall-in-one-to-one-answer-scenarios_1","title":"How do you understand metrics like precision and recall in one-to-one answer scenarios?","text":"<p>For questions with exactly one correct answer, these metrics behave somewhat differently. Recall will be either 0% or 100% depending on whether K is large enough to include the correct answer.</p> <p>For example, if we want to retrieve exactly one document and there's only one correct answer, precision could be either 0% or 100%, and the same for recall.</p> <p>The metrics become more meaningful when:</p> <ol> <li>There are multiple relevant documents</li> <li>We're analyzing trends across many queries</li> <li>We're comparing different retrieval methods</li> </ol> <p>Even with one-to-one mappings, MRR (Mean Reciprocal Rank) is still useful to see where the correct answer appears in your results.</p> <p>What really matters isn't the absolute number but whether we can move these metrics in a positive direction with our interventions.</p>"},{"location":"office-hours/faq/#how-does-a-long-context-window-affect-rag-systems","title":"How does a long context window affect RAG systems?","text":"<p>While having longer context windows allows for more content to be included, there are always tradeoffs with latency and cost. Just because we can fit more in context doesn't mean we should always do so.</p> <p>Like how Amazon could theoretically score every product in their inventory for each user but chooses not to because each 100ms of latency costs them 1% in revenue, we still need to make choices about what to include in context.</p> <p>The battery analogy is apt: iPhone batteries get more powerful every year, but battery life stays the same because we build more power-hungry apps. Similarly, as context windows grow, we'll find ways to use that additional capacity rather than making everything faster or cheaper.</p> <p>There will always be cost, performance, and latency tradeoffs to consider. Having a longer context window doesn't eliminate the need for efficient retrieval - it just changes what problems we can solve.</p>"},{"location":"office-hours/faq/#what-tips-do-you-have-for-making-decisions-about-rag-system-architecture-without-prototyping-everything","title":"What tips do you have for making decisions about RAG system architecture without prototyping everything?","text":"<p>Start by asking for examples of 40-50 questions that customers might ask. Reading these helps build an intuition about what query mechanics need to exist.</p> <p>For example, if questions include \"what's the most recent news?\", you'll need date filters. If queries ask \"who do I talk to about fixing XYZ?\", you need features for finding contacts.</p> <p>This helps identify what metadata you need and whether you can access it. From there, building a demo with tools like LangChain or Llama Index should be quick. You may need to rewrite things later, but if the demo can answer generic questions, that's when you start thinking about synthetic data.</p> <p>The key is getting the system in front of beta testers, collecting feedback, and analyzing what's working and what's not. This helps prioritize the next features to build. If 80% of questions are actually about image search, then that's clearly the next thing to build, regardless of what methodology is trending on Twitter.</p>"},{"location":"office-hours/faq/#how-do-you-optimally-blend-small-pools-of-real-data-with-large-synthetic-data-sets","title":"How do you optimally blend small pools of real data with large synthetic data sets?","text":"<p>Focus on whether blending improves your evaluation suite. If you have 500 real examples, put 250 in your training set and leave 250 for evaluation. Then experiment with different blends of synthetic and real data to see how they perform on your evaluation suite.</p> <p>You might find that as you use more synthetic data, you perform worse on your real user data but better on synthetic data. You can weight these differently - perhaps real data success is worth 1.2 points while synthetic data success is worth 0.9 points - to create a single score for system health.</p> <p>A lot of machine learning is empirical - you can't predict these things ahead of time. You need to run experiments and see what works.</p>"},{"location":"office-hours/faq/#how-do-you-approach-function-calling-for-complex-workflows-that-require-multiple-function-calls","title":"How do you approach function calling for complex workflows that require multiple function calls?","text":"<p>Instead of having the language model immediately execute functions one at a time, prompt it to show the entire plan to the user and potentially ask for confirmation. Have a separate function called \"plan\" where the model says \"Based on this request, I think I'm going to use function 1, then 2, then 3. What do you think?\"</p> <p>When the user clicks yes or no, you've allowed human confirmation of the correct order. Since the plan already exists in the context, it's easier for the model to execute correctly.</p> <p>The second benefit is that user requests, plans, and accept/reject decisions can be used as few-shot examples. You can embed these examples so that next time someone asks a similar question, you can say \"Last time someone asked this, they approved calling functions 1, 2, 3 in this order.\"</p> <p>This helps build a dataset of few-shot examples over plans, making the system more reliable.</p>"},{"location":"office-hours/faq/#how-do-you-constrain-a-rag-system-that-pulls-from-multiple-data-sources","title":"How do you constrain a RAG system that pulls from multiple data sources?","text":"<p>After conducting topic analysis of user questions, you can identify which types of questions you can answer well and which ones you struggle with. For low-percentage, low-performance questions, you might decide to simply decline those queries.</p> <p>For example, if your system doesn't handle contact information well, you could add to your prompt: \"If someone is asking about contact information, say no and tell them to message support.\" This saves face and avoids attempting questions you can't answer well.</p> <p>Conversely, if there are questions you can answer very well (even if they're a small percentage), highlight these as sample questions in your UI to guide users toward queries you're confident in handling.</p> <p>Much of the progress in making systems better comes from improving UI, better educating users about capabilities, and enhancing the quality of your inventory rather than just tweaking algorithms.</p>"},{"location":"office-hours/faq/#do-you-sometimes-use-differently-tuned-embeddings-within-the-same-query","title":"Do you sometimes use differently tuned embeddings within the same query?","text":"<p>Unless you're at massive scale, having multiple embedding models for different content types (like product descriptions vs. comments) probably won't yield enough performance improvement to justify the maintenance cost.</p> <p>There's evidence that having a single unified model trained on all your data performs better than specialized models. In machine translation, we used to train separate models for each language pair, but researchers found that a single model trained to translate all languages performed better than any individual model.</p> <p>The unified model learns something about the underlying system that allows it to handle even rare cases better than specialized models would. The same principle likely applies to embedding models.</p>"},{"location":"office-hours/faq/#how-do-you-reason-about-papers-and-new-research-in-rag-and-llms","title":"How do you reason about papers and new research in RAG and LLMs?","text":"<p>Most papers published weekly aren't that important, and many are just reinventing ideas from decades ago. Instead of flooding yourself with information, focus on running experiments with your own data and solving specific problems.</p> <p>The popular stuff on Twitter is generally reasonable to follow, but even then, much research is a distraction if you're building something. For example, a recent popular paper on \"reasoning powered RAG\" was essentially just using an LLM to judge relevancy pairs in a for loop - something basic that's been around for a while.</p> <p>Rather than chasing the latest research, focus on building strong evaluation suites, analyzing your data, and solving specific problems in your implementation. These are the durable skills that will last throughout your career.</p>"},{"location":"office-hours/faq/#does-a-long-context-window-make-rag-obsolete","title":"Does a long context window make RAG obsolete?","text":"<p>No. Just like Amazon could theoretically score every product in their inventory for each user but chooses not to because each 100ms of latency costs them 1% in revenue, we still need to make choices about what to include in context.</p> <p>The battery analogy is apt: iPhone batteries get more powerful every year, but battery life stays the same because we build more power-hungry apps. Similarly, as context windows grow, we'll find ways to use that additional capacity rather than making everything faster or cheaper.</p> <p>There will always be cost, performance, and latency tradeoffs to consider. Having a longer context window doesn't eliminate the need for efficient retrieval - it just changes what problems we can solve.</p>"},{"location":"office-hours/faq/#how-do-you-handle-the-upkeep-of-documents-that-go-in-and-out-of-scope-or-have-frequent-version-changes","title":"How do you handle the upkeep of documents that go in and out of scope or have frequent version changes?","text":"<p>For \"evergreen\" vs. \"Rhodian\" (frequently changing) documents, include published dates and make sure they're in the context so the language model is aware of them. For example, with HR holiday calendars for different years, include the dates so the model can reason about which is current.</p> <p>Consider implementing both published dates and last modified dates, and be explicit in your function calling to filter on these attributes (e.g., only return documents published or updated in the past year).</p> <p>The key question is how sensitive your model is to low precision, and whether that low precision is mainly happening because of your inability to expire outdated documents.</p>"},{"location":"office-hours/faq/#how-do-you-approach-building-voice-ai-for-outbound-calls-or-structured-conversations","title":"How do you approach building voice AI for outbound calls or structured conversations?","text":"<p>Graph-based models or finite state machines have been very successful in the agent world. In this approach, you're in different states (introduction, data collection, etc.) with different system messages for each state.</p> <p>For example, when collecting payment data to book a meeting, you have logical checks to ensure the date is correct and that you have necessary information like a phone number. The set of function calls available also changes based on the state.</p> <p>Once you have fully successful conversations, you can summarize them and put them back in the system prompt to ensure transitions are more accurate. You can prompt the model with these few-shot examples to improve transition states: \"When I have 5 few shots, my transitions are more accurate. When I have 20 few shots it gets too confused. So now I've picked 15 for now.\"</p> <p>This finite state machine approach has been around for decades and is still very effective, with LLMs improving the transitions between states.</p>"},{"location":"office-hours/faq/#when-working-with-metadata-should-you-include-it-in-the-chunk-or-add-it-separately","title":"When working with metadata, should you include it in the chunk or add it separately?","text":"<p>There are a few approaches:</p> <ol> <li>Embed the string without metadata but add metadata when sending to the LLM</li> <li>Embed the string with metadata included</li> </ol> <p>This is something to test empirically - does including metadata in the embedding hurt retrieval? Cohere's embedding models (like Compass) can embed JSON quite well.</p> <p>Including metadata in chunks is common practice as it allows answering questions like \"who wrote this document\" or \"what's their contact information.\" This metadata can then be used for function calls, such as \"Jason wrote the document 2 weeks ago, it has not been updated since. Here's Jason's email, click to write an email to Jason.\"</p>"},{"location":"office-hours/faq/#how-can-you-best-apply-synthetic-data-generation-to-agent-workflows-with-multiple-tools","title":"How can you best apply synthetic data generation to agent workflows with multiple tools?","text":"<p>Instead of generating synthetic questions, you can generate synthetic queries that would trigger certain function calls. If you have 3-4 different functions, you can create synthetic queries that should call specific functions or combinations of functions.</p> <p>If each individual function call is accurate, then the combined sequence should also be accurate. You can also use planning to improve data generation - create questions that would result in specific functions being called in sequence, then verify that with certain requests, these functions are indeed called by the model.</p> <p>This approach helps ensure reliability across different types of function calling patterns.</p>"},{"location":"office-hours/faq/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Fine-tuning priorities: Fine-tune embedding models, not LLMs. With just 6,000 examples, you can create embedding models that outperform general models on your specific tasks at minimal cost.</p> </li> <li> <p>Inventory matters more than algorithms: Having the right documents and metadata is more important than the algorithm itself. Missing information can't be retrieved no matter how good your algorithm is.</p> </li> <li> <p>Evaluation is empirical: Many decisions about chunking, including metadata, and blending synthetic data should be driven by empirical testing rather than theoretical assumptions.</p> </li> <li> <p>Parsing strategy: For complex documents, consider evaluating parsing/OCR separately from retrieval performance since parsing errors will be difficult to trace in the full pipeline.</p> </li> <li> <p>Function calling with planning: For complex agent workflows, have the model create a plan first and get user confirmation rather than executing functions immediately. This creates training data for future interactions.</p> </li> <li> <p>State machines still work: Graph-based/finite state machine approaches remain effective for structured conversations, with LLMs improving the transitions between states.</p> </li> <li> <p>Metadata inclusion: Include relevant metadata in chunks to answer questions about document properties like authorship, modification dates, and contact information.</p> </li> <li> <p>Long context doesn't eliminate RAG: Despite larger context windows, there will always be latency, cost, and performance tradeoffs that make efficient retrieval necessary.</p> </li> <li> <p>Research pragmatism: Focus on solving specific problems with your data rather than chasing the latest research papers, which often reinvent existing techniques.</p> </li> <li> <p>Cross-encoders vs. bi-encoders: Cross-encoders (re-rankers) understand semantic distinctions better but are slower; bi-encoders (embedding models) are faster but less nuanced. Use both for optimal performance.</p> </li> </ol> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"office-hours/faq/#when-gathering-negative-feedback-from-documents-not-being-found-how-do-we-use-and-validate-the-reliability-of-an-llm-labeler","title":"When gathering negative feedback from documents not being found, how do we use and validate the reliability of an LLM labeler?","text":"<p>When it comes to getting negative feedback that documents were not found, I'd assume we're running into issues around low recall. This might manifest as low re-ranker scores or low cosine similarities with embedding models.</p> <p>What I would do is identify whether the language model itself can identify if documents are irrelevant. We'll need some manual labeling step. With our clients, we generally find questions that emit flags - maybe you tell a language model to always say \"we couldn't find relevant documents\" when it can't find anything.</p> <p>You can then label that as traffic is being processed. We might sample 1% of traffic, and some percentage of that might have that message. That's one level of detection.</p> <p>The second level would be building a streamlit UI where we can manually label whether we agree with the irrelevancy assessment. The hard task is determining if any of 10 text chunks are relevant to a question. The easier task is determining if a single text chunk is relevant to a question. That's easy for a human to do and also pretty easy to prompt for.</p> <p>This approach helps ensure the judgment we're using is aligned with human preferences. There's obviously a big difference between 60% alignment and 95% alignment, but this is a good start for figuring out whether low relevancy is causing the lack of documents.</p>"},{"location":"office-hours/faq/#in-the-segmentation-topic-we-talked-about-inventories-and-capabilities-is-it-realistic-to-do-this-automatically-or-is-it-something-we-have-to-do-manually","title":"In the segmentation topic, we talked about inventories and capabilities. Is it realistic to do this automatically or is it something we have to do manually?","text":"<p>I would generally recommend doing this manually, because it's so important for what your business is trying to do that you need to actually think about these problems.</p> <p>We've delegated so much thinking to language models. If we just think a bit harder about our problem, we often find very specific issues.</p> <p>For example, with a client doing tax law resolution, the first 20 pages were massive articles, and then pages 30-40 were the exemptions to those articles. We spent maybe $20 of LLM calls to rewrite the documents so that the exemptions were close to the relevant articles. Now we have a single page/chunk covering an article and all its exemptions, with references to related articles.</p> <p>We run that job once a week when new tax laws come in. Since we only have about 45 documents we really care about, I'd rather spend the money upfront to get the process right rather than waste customer time requerying data.</p> <p>The real goal isn't to get a number right - it's to figure out what to do next. The AI can't tell us that. Your job isn't to automate this process; you're being paid to figure out what the next intervention should be.</p>"},{"location":"office-hours/faq/#can-you-elaborate-on-your-view-on-rag-versus-recommendations-how-would-you-approach-the-use-case-of-friend-suggestions","title":"Can you elaborate on your view on RAG versus recommendations? How would you approach the use case of friend suggestions?","text":"<p>When you build a recommendation system, there are several steps:</p> <ol> <li>Sourcing - What inventory can I show my customer? In the friends case, this would be all users on the platform.</li> <li>Query - Either your user ID or a question embedding.</li> <li>Scoring - For simple RAG, this is cosine distance of embeddings and maybe re-ranker distance. For friends, it might include mutual connections, location, etc.</li> <li>Filtering - In RAG this might be top 10 results or embeddings greater than a threshold. For friends, filters might include having at least 3 mutual friends, same zip code, etc.</li> <li>Rendering the results</li> </ol> <p>When users take actions (adding/removing friends, opening files, deleting citations), you collect feedback to improve your system. When a language model sees 10 documents but only cites 3, those 3 are likely more relevant. You can use that signal to improve your re-ranker or embedding model.</p> <p>If the user deletes one of those citations, you have a triplet: documents the model thinks are important, plus a negative example. When training, these need to be adjusted accordingly.</p> <p>It's like different levels of signals in e-commerce: liking a product is a weaker signal than adding it to cart, which is weaker than buying it, which is different from buying and returning it. That's your portfolio of data collected over time.</p>"},{"location":"office-hours/faq/#in-the-4th-lecture-you-mentioned-the-formula-expected-value-as-impact-times-the-number-of-queries-times-the-probability-of-success-can-you-explain-more-what-you-mean-by-impact","title":"In the 4th lecture you mentioned the formula expected value as impact times the number of queries, times the probability of success. Can you explain more what you mean by impact?","text":"<p>Impact here is a general term that the Facebook folks like to use. I generally think of impact as economic value.</p> <p>In the construction example I often mention, about 70% of questions were simple things like \"Where do I show up today?\" or \"How thick is the drywall?\" These weren't individually valuable.</p> <p>But we also found a set of questions that were super valuable - around scheduling and figuring out if contracts were signed. These were only about 10% of queries but extremely important. When we asked our clients, they said that preventing one missed contract could save $60,000 in delays.</p> <p>This told us these queries had high economic value, even though they were less frequent. So we invested resources in making sure we could query contracts and schedules to answer that segment.</p> <p>Impact is about how valuable a problem is and how much it's worth, rather than just how frequently it occurs. Every metric you track should enable you to take follow-up action afterward - it's not just about knowing the number.</p>"},{"location":"office-hours/faq/#what-is-the-lifecycle-of-feedback-if-we-improve-the-ui-old-labels-might-be-out-of-date-and-new-data-will-be-labeled-differently-what-is-good-to-keep-versus-letting-go","title":"What is the lifecycle of feedback? If we improve the UI, old labels might be out of date and new data will be labeled differently. What is good to keep versus letting go?","text":"<p>This depends on how much data we have and the blend of that data. If we had a million labels before changing the UI, I'd push hard to keep the new UI somewhat similar to ensure the data we collect remains comparable.</p> <p>If we're really changing things dramatically, there are modeling techniques to control for this. You might pre-train on the old data for your embedding model, then use that as a foundation for training a newer model. You can also control for the source of data in your modeling.</p> <p>You can have different evaluations to verify performance on the old data versus the new data, then choose how to weight those scores. Generally, I'd try to keep things as generic as possible - you don't want a dataset that's too specific and won't generalize.</p> <p>For embedding models specifically, I'd typically include everything, as more data is generally better.</p>"},{"location":"office-hours/faq/#is-it-interesting-to-collect-feedback-not-only-as-thumbs-up-or-thumbs-down-but-let-users-explain-in-text-what-is-wrong-with-the-answer","title":"Is it interesting to collect feedback not only as thumbs up or thumbs down, but let users explain in text what is wrong with the answer?","text":"<p>Yes and no. Thumbs up/down is super useful, and it would be hard to convince me not to use these binary labels. Going to a 5-star scale creates issues where you don't know if users consider 3 or 4 stars to be \"average.\"</p> <p>With free text feedback, you'll face two issues:</p> <ol> <li>Probably less than 10% of users will give a text response. If only 1% of users leave feedback at all, and only 10% of those leave text, you get very little text data, and you don't know how biased that sample is.</li> <li>You likely won't be able to read all the free text, so you'll build clustering models to analyze the feedback - in which case, you might as well just have 5 buttons for the most common issues (too slow, answer too long, format incorrect, etc.).</li> </ol> <p>It's about maximizing data per label. Having buttons for common issues will get you more useful data than open text fields.</p> <p>That said, free text can help you figure out what those buttons should be. For enterprise situations, we include the default buttons plus free text, and when users enter text, we post it to Slack where the team and customer can see it. This shows users their feedback is seen, making them more likely to provide it.</p> <p>But think about how often you've thumbs-downed a ChatGPT response, let alone written why. Most users simply won't take the time.</p>"},{"location":"office-hours/faq/#how-do-you-handle-recall-when-dealing-with-large-knowledge-bases-with-a-messy-topology-near-identical-documents-overlapping-content-hub-pages-etc","title":"How do you handle recall when dealing with large knowledge bases with a messy topology (near-identical documents, overlapping content, hub pages, etc.)?","text":"<p>This is challenging, especially with something like a large software product knowledge base (44,000+ documents) where many people have been adding content, creating overlap and interstitial hub pages.</p> <p>One approach is to build a system where if you retrieve a subset of pages, you can reference the connections. Similar to how e-commerce sites show \"people who viewed this also viewed\" suggestions.</p> <p>As context windows get larger, you could implement a system where if you pull in a page that references other documents, you traverse one level and bring in those referenced documents too.</p> <p>You could also do clustering and summarization. If your repository is very valuable, maybe it costs 10 cents to process a page, but with a budget of 50 cents per query, you could chunk everything, cluster similar content, and then summarize the clusters. This essentially rewrites the knowledge base in a less duplicated way.</p> <p>The more fundamental question is about how you define relevance. Do you have a policy document on what makes a document relevant? Google has a detailed document on what makes a good search result. You need to establish and document your criteria so everyone has the same understanding.</p>"},{"location":"office-hours/faq/#have-you-compared-the-effectiveness-of-classical-and-agent-based-rag-systems-with-capabilities-offered-by-models-like-gemini-flashlight-for-real-projects","title":"Have you compared the effectiveness of classical and agent-based RAG systems with capabilities offered by models like Gemini Flashlight for real projects?","text":"<p>I prefer not to think about systems as \"classical\" versus \"agent-based\" RAG systems. Most RAG systems are essentially function calling in a for-loop or while-loop.</p> <p>The goal is to provide the language model with two things:</p> <ol> <li>Good functions</li> <li>Good indices for each function to query that are well-defined</li> </ol> <p>You want to ensure each index has good recall, each function is useful for the system, and you have good prompts to help the model choose the right function.</p> <p>For real projects, it's not just about question answering but also about tool rendering. Some tool calls define UX elements - like a fitness company chatbot that renders modals for booking calendar events and following up with payment links. This becomes the economically valuable work - not just answering questions but helping the company make money.</p>"},{"location":"office-hours/faq/#whats-the-moat-for-companies-building-rag-systems-when-so-much-is-being-open-sourced","title":"What's the moat for companies building RAG systems when so much is being open-sourced?","text":"<p>I generally think the moat is your labeled data. There probably isn't much difference between various newsfeed algorithms, but the moat is the inventory - the content that's already out there.</p> <p>If you have relationships in a specific sector like construction and can be the first to build connectors and bring in that data, that's an easy moat (though not as defensible).</p> <p>After that, it's about analyzing that data to understand what questions people are asking and building specialized tools for those needs. This is software that LLMs won't replace anytime soon.</p> <p>Then it's understanding what relevance actually means - fine-tuning re-ranking models, training custom embedding models. These are aspects that LLM companies won't compete against.</p> <p>The moat becomes your data - both relevancy data and access to the content itself - plus your understanding of customer needs and workflows. The more you understand what customers are truly trying to do (beyond just answering questions about PDFs), the better your product will be.</p>"},{"location":"office-hours/faq/#in-the-ux-lectures-you-mentioned-that-explicit-copy-instead-of-just-thumbs-updown-can-impact-whether-people-give-feedback-have-you-observed-an-impact-based-on-what-the-copy-actually-says","title":"In the UX lectures, you mentioned that explicit copy instead of just thumbs up/down can impact whether people give feedback. Have you observed an impact based on what the copy actually says?","text":"<p>Absolutely. At Zapier, they asked \"How did we do?\" which was a very vague question that didn't get much feedback.</p> <p>When we A/B tested copy, the version that got 5x more feedback was \"Did we answer your question?\" This was much more specific and focused on the core value proposition, not about latency or formatting. If users said no, we'd follow up with \"Do you have any other feedback? Was it too slow? Was the formatting wrong?\" since we knew those were common failure modes.</p> <p>This not only got more feedback but also correlated better with customer satisfaction. The previous vague question made it hard to identify what was a good or bad answer - some might say we did poorly because we answered correctly but too slowly.</p> <p>At Raycast, our copy now is \"Did we take the correct actions?\" since we're showing function calls like \"Set a 1-hour lunch on my calendar and update my Slack status.\" We show users the sequence of function calls and ask if we're taking the correct actions.</p> <p>The key is that every metric you track should lead to a follow-up action. It's not just about knowing the number.</p>"},{"location":"office-hours/faq/#how-can-we-extract-value-from-templatepre-filled-questions-in-chatbots","title":"How can we extract value from template/pre-filled questions in chatbots?","text":"<p>For a situation like a lawn care subscription company's chatbot where 70% of conversations start with template questions, I'd be curious to understand what the follow-up questions look like. This helps determine if we could create complete guides for common paths.</p> <p>If people start with a certain template question, do their follow-ups cluster in a specific domain? This can help you understand if your example questions are actually helpful or if you should be writing better content to answer these questions more comprehensively.</p> <p>One approach is to use a language model to summarize conversations, identifying what topics come after the template questions. This gives you insight into actual user intents that might be hidden behind that initial templated interaction.</p> <p>You should analyze which topics are economically important by looking at metrics like thumbs up/down data. For instance, we found that many negative ratings come from users who want to talk to a real person but can't easily figure out how to do that.</p> <p>It's also valuable to analyze what products you should recommend based on question patterns. If you're seeing thumbs-down ratings, analyze whether it's because you don't have the right content in your knowledge base, or if there are capabilities you're missing. Often, the solution might be as simple as hiring someone to write targeted content for frequently asked questions.</p>"},{"location":"office-hours/faq/#how-do-you-handle-business-knowledge-translation-like-acronyms-in-rag","title":"How do you handle business knowledge translation (like acronyms) in RAG?","text":"<p>When you have documents that spell everything out formally but users want to query using acronyms (like \"What's the deal with ABC?\"), I'd generally just put this translation knowledge in the prompt unless you have an enormous number of acronyms.</p> <p>If you have fewer than 80 acronyms or terms that need translation, putting them directly in the prompt is the simplest and most effective approach. You only need to explore more complex approaches when you have evidence that this simple solution isn't working.</p> <p>You can also create synthetic data to test how well your system handles these acronym queries, which is usually straightforward to generate.</p>"},{"location":"office-hours/faq/#what-are-the-best-practices-for-chunking-in-rag-systems","title":"What are the best practices for chunking in RAG systems?","text":"<p>The general advice from companies like OpenAI and Anthropic is to start with around 800 tokens with 50% overlap using a sliding window approach. That should be enough to get you started.</p> <p>After that initial setup, the real improvements come from understanding what kinds of questions are being asked and what the answers look like. If most questions can be answered by a single document, focus on improving document search and relevancy rather than chunking. If answers typically come from small paragraphs across many documents, then experiment more with chunking.</p> <p>We've spent weeks doing chunking experiments and often haven't seen significant improvements. It's rarely the case that changing from 500 to 800 tokens suddenly makes everything work better - that would suggest most answers require just a few more sentences in the same document, which is usually not the issue.</p> <p>What's been more helpful is looking at the questions and working backward: What are people trying to do, and what design assumptions can I make to better serve that? For instance, if users are searching for blueprints, maybe summarizing blueprints first would help, or perhaps including text above and below the blueprint, or even applying OCR and building a bounding box model to count rooms.</p> <p>Solve specific problems where you can justify that \"this is 20% of our questions\" - if you make those 20% twice as good, you've improved overall performance by 8%, which is meaningful.</p>"},{"location":"office-hours/faq/#are-xml-tags-still-best-practice-for-prompting-models","title":"Are XML tags still best practice for prompting models?","text":"<p>Yes, we've learned that even the GPT-4 models now perform better with XML formatting. We have internal evaluations from Zenbase showing that XML is good not just for Anthropic models but also for ChatGPT models.</p> <p>The second thing we've found is that you generally want to have all the long context information at the beginning of the prompt - first the goal, then all the documents, with the actual questions at the bottom.</p> <p>Claude's prompt rewriter has been very helpful for showing how to write better prompts. I almost always run my prompts through it first before setting up evaluation suites, as it's a free way to get useful feedback.</p>"},{"location":"office-hours/faq/#how-do-you-handle-tokenization-concerns-with-things-like-wallet-addresses","title":"How do you handle tokenization concerns with things like wallet addresses?","text":"<p>When dealing with data that contains wallet addresses (which are 52 characters of what looks like nonsense), I'd worry less about the tokenization itself and focus more on validation.</p> <p>For example, in situations where we use UUIDs, we reference content with a UUID, and we tell the model to cite everything. We then have an allowlist of valid UUIDs from our data, and we check that any UUID the model outputs exists in that allowlist.</p> <p>So if you have a use case where users ask about wallet IDs, focus on making sure the model can only reference valid wallet IDs from your dataset rather than worrying about how they're tokenized.</p> <p>These days, models aren't typically off by a few characters - they'll either get it right or completely make up new identifiers. Having logical checks in your code is more important than the tokenization strategy.</p> <p>You can also generate synthetic test data where you know which wallet addresses should appear in the answers and ensure there are no hallucinations.</p>"},{"location":"office-hours/faq/#should-we-transform-content-from-narrative-format-to-qa-format-for-better-retrieval","title":"Should we transform content from narrative format to Q&amp;A format for better retrieval?","text":"<p>Yes, massively. This can be very beneficial, especially for a question-answering chatbot.</p> <p>It's already an assumption to think that everything is going to be in the form of a question. For some assistants, it might be more about conversations or past memories. If you know your use case is primarily Q&amp;A, then extracting question-answer pairs from your documents is valuable.</p> <p>You can build a system where when you embed a question, you retrieve the embedding of similar questions, but pull in both the question and its answer. This makes sense if your use cases are mostly Q&amp;A-based rather than narrative requests like \"tell me a story.\"</p> <p>One of the big assumptions in RAG is that the embedding of a question is similar to the embedding of a relevant document, which is actually a massive assumption that doesn't always hold true.</p> <p>To prevent retrieving too many similar question-answer pairs (which could be redundant when getting top-K results), consider doing clustering. You could extract 10 questions per document, then cluster similar questions together and rewrite them to create a more concise, focused knowledge base.</p>"},{"location":"office-hours/faq/#can-you-recommend-any-open-source-libraries-or-tools-for-streaming-uis-and-interstitials","title":"Can you recommend any open source libraries or tools for streaming UIs and interstitials?","text":"<p>I can't necessarily recommend a specific library too strongly because most companies I've worked with have built these themselves. However, if you're in the Python world, using something like FastAPI and server-side events (SSE) API is probably the simplest approach. In the slides, we give an example of what this looks like - you're basically using the yield keyword from Python generators to emit events.</p> <p>If you're using JavaScript and part of the Vercel/React ecosystem, I think Vercel's AI library does a great job of handling structured streaming. Other libraries like LangChain, LlamaIndex, and Instructor also support partial streaming where you can send incomplete JSON to a frontend, which can then rerender it.</p> <p>For interstitials, I've been impressed with what Ankur from BrainTrust has done in their playground. I've reached out to him to ask about recommendations for this.</p> <p>With these tools, the implementation is fairly straightforward. The bigger challenge is often designing a UX that communicates progress effectively. Notion's approach is a good example - when you enter a search query, it shows \"making a search request,\" rewrites the request, then renders documents one by one, and finally shows steps like \"carefully reading documents,\" \"thinking,\" and \"formulating an answer.\" This is really just buying time while showing progress, but it dramatically improves the perceived responsiveness.</p>"},{"location":"office-hours/faq/#why-arent-data-labeling-companies-a-bigger-focus-in-current-ai-discussions","title":"Why aren't data labeling companies a bigger focus in current AI discussions?","text":"<p>This is an interesting historical shift. Around 2018, data labeling was a huge focus because the biggest models were vision models that required massive amounts of labeled data. Vision models aren't very data-efficient - training ImageNet required labeling a million JPEGs. Companies like Scale AI won by excelling at tasks like self-driving car LiDAR labeling.</p> <p>As we've moved to LLMs, two things have changed:</p> <ol> <li>The big winners (like Scale AI) have already established themselves and now focus on large contracts. Smaller players either grew or struggled to find viable business models on smaller contracts.</li> <li>LLMs are much more data-efficient.</li> </ol> <p>The data efficiency of modern LLMs is remarkable. You're better off having 1,000 very high-quality labels to fine-tune a model than 10,000 mediocre labels. This means that instead of outsourcing labeling work, it often makes more sense to have subject matter experts do a one-month project to create the data you need.</p> <p>We're so sample-efficient now that offshore labeling doesn't make economic sense for many use cases, especially when LLMs have been shown to match or exceed the quality of offshore labeling for many tasks. If you have specific legal workflows, you're better off asking the lawyer on your team to do the labeling.</p> <p>The real challenge now is: how do you find people who are smarter than GPT-4 to label data to train the next generation of models? That hiring problem is different from the traditional labeling company approach.</p>"},{"location":"office-hours/faq/#how-do-you-see-re-rankers-evolving-beyond-just-measuring-relevancy","title":"How do you see re-rankers evolving beyond just measuring relevancy?","text":"<p>Right now, most RAG systems only rank based on relevancy between a query and a document. But I think re-rankers will soon incorporate much more side information, similar to what we see in e-commerce recommendation systems.</p> <p>In e-commerce, we have additional rankers for things like price sensitivity, seasonality, and product age to determine if customers prefer trendy or timeless items. This hasn't really happened in the RAG world yet.</p> <p>As AI systems accumulate multiple years of memories about users, figuring out what information to put in context will become much more interesting. Re-rankers won't just measure string similarity between a question and document - they'll likely incorporate user features, environmental features, and contextual information to determine relevance.</p> <p>For example:</p> <ul> <li>Security constraints (only searching documents you have access to)</li> <li>Time/recency components for memories</li> <li>Domain authority when sources disagree</li> <li>User preferences based on past interactions</li> </ul> <p>Even systems like Deep Research might evolve to pull from sources you tend to agree with, or deliberately include sources that challenge your viewpoint. These personalized relevancy signals could dramatically improve RAG systems beyond simple semantic matching.</p>"},{"location":"office-hours/faq/#key-takeaways-and-additional-resources","title":"Key Takeaways and Additional Resources","text":""},{"location":"office-hours/faq/#key-takeaways_1","title":"Key Takeaways:","text":"<ul> <li>Data quality is becoming more important than ever - good models make data quality the differentiator</li> <li>When collecting feedback, be specific with your questions to increase response rates</li> <li>Focus on economically valuable workflows, not just answering questions</li> <li>For messy knowledge bases, consider clustering and summarization approaches</li> <li>The moat for RAG companies is proprietary data and domain expertise, not algorithms</li> <li>Binary feedback (thumbs up/down) generally gets more responses than free text</li> <li>Always have a clear next action from any metric you collect</li> <li>Focus on impact (economic value) rather than just query volume</li> </ul>"},{"location":"office-hours/faq/#additional-resources","title":"Additional Resources:","text":"<ul> <li>Google Search Relevancy document/policy is a good reference for defining relevance</li> <li>RAPTOR paper for document summarization approaches</li> <li>Week 3-4 content in the course covers more on these topics</li> <li>For prompt rewriting, Claude's prompt rewriter is highly recommended</li> <li>When dealing with streaming UIs and latencies, Notion's approach of showing steps visually is a good reference</li> <li>For friends example in recommendation systems, consider platforms like Facebook's friend recommendation system as reference implementations</li> </ul> <p>Note: I'll continue to add resources and notes from future office hours sessions</p> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"office-hours/faq/#what-can-segments-mean-beyond-query-volume-and-satisfaction-values-in-a-rag-system","title":"What can \"segments\" mean beyond query volume and satisfaction values in a RAG system?","text":"<p>Segmentation really depends on who your customers are and what they're trying to do. With a generic chatbot, it's hard to figure out what segmentation means. But if you think about intents of a specific application, you can uncover different patterns.</p> <p>For example, with a nutrition company chatbot, you might discover segments within product search \u2013 different capabilities around understanding deliveries, rescheduling, recurring orders, etc. Data analysis helps figure out what's important to build for the customer.</p> <p>In a construction context, we found segments around:</p> <ul> <li>Users inputting specific project IDs (e.g., \"Tell me about RFC 1257\")</li> <li>Questions about time windows (\"What do I have due today?\" or \"What's happening this week?\")</li> <li>Counting items in documents</li> </ul> <p>The goal of segmentation is to help you figure out what new function tools to build and what workflows might be viable. Another example: for a product that takes screenshots of users' computers, we found 10% of customers asking \"How much time did I spend in this application?\" That's impossible to answer with just screenshots, but we realized we had a Postgres database of all screenshots with timestamps, so we built a specific tool to query, group, and sum that data to answer the question.</p> <p>The key is to find external understanding of your data \u2013 what are you worried about, and if you discover certain properties, what can you do about it?</p>"},{"location":"office-hours/faq/#how-should-we-approach-segmentation-for-chatbots-where-the-output-is-a-whole-conversation-rather-than-just-a-query-response","title":"How should we approach segmentation for chatbots where the output is a whole conversation rather than just a query response?","text":"<p>If you have the compute resources, do similar classification and segmentation over your conversations. You'll uncover different insights beyond just tools.</p> <p>When analyzing queries alone, we're basically asking how well we can execute tools to answer in one generation. By analyzing conversations, we might find segments that tell us:</p> <ul> <li>Users think the chatbot talks too much or not enough</li> <li>Users are frustrated with responses</li> <li>Common patterns in how conversations progress</li> </ul> <p>The general idea is to gain an external understanding of your data \u2013 what properties are you concerned about, and if you discover X% of conversations have a certain property, what action can you take?</p> <p>For example, if you find many users asking the language model to rewrite answers in their own words, should that be part of your system prompt? Analysis might show only 10% want tone matching, while most users actually prefer the AI voice.</p>"},{"location":"office-hours/faq/#what-approaches-do-you-recommend-for-topic-clustering-and-have-you-tried-using-thinking-models-to-generate-clusters","title":"What approaches do you recommend for topic clustering, and have you tried using thinking models to generate clusters?","text":"<p>I generally use what I call \"old school\" approaches \u2013 K-means and DBSCAN. I typically start with the default settings in BERTTopic, which has been very good. The topic modeling goal isn't to uncover topics for production use but to do data analysis that helps you understand your data better.</p> <p>For example, I might take Ada 2 embeddings, use K-means to pick 10-30 clusters, and look at 100 questions per cluster. That might take 2-3 days but teaches you a lot about your data. It's rarely the case that you run topic models and can just use them directly in your business.</p> <p>When working with thinking models for clustering, I still do the initial clustering first because I might have 20 million questions to analyze. I'll cluster that data, find good and bad examples across clusters, and put that into Claude 3.7 or similar models, asking them to:</p> <ul> <li>Name each cluster</li> <li>Provide a short description</li> <li>Give good examples of what belongs in the cluster</li> <li>Provide nuanced examples of what's not in the cluster</li> </ul> <p>This produces a YAML file that I can then use for classification. The language model helps expand our understanding, especially when we can't easily enumerate all possibilities ourselves.</p>"},{"location":"office-hours/faq/#what-are-your-thoughts-on-chunk-size-and-chunk-overlap-is-it-worth-trying-out-different-chunking-strategies","title":"What are your thoughts on chunk size and chunk overlap? Is it worth trying out different chunking strategies?","text":"<p>I generally use 800 tokens with 50% overlap, which is what OpenAI recommends in their blog posts. In my experience, chunking strategies rarely make a significant difference compared to other improvements.</p> <p>There's only a small subset of questions where chunk size makes a difference \u2013 you would need a question that can only be answered by a paragraph where two concepts are exactly 500 tokens apart. Performance gains usually come from better re-ranking, contextual retrieval (where you rewrite text chunks given the entire document), or better filtering and metadata capabilities.</p> <p>I've rarely seen chunk size be the 10% improvement win \u2013 it might be a 1-2% improvement, which could just be noise. I would focus more on contextual retrieval if you have the compute budget for it.</p> <p>For semantic chunking (using an LLM to determine good chunking points), I'm actually pretty convinced that contextual retrieval is better than dynamically chunking. The real question is whether you need to cite things word-for-word (in which case you shouldn't rewrite chunks) or if you just need general question answering.</p> <p>I'd always spend more compute upfront to improve data quality. For example, I worked with a company doing Brazilian tax law with 50 documents, each 600 pages long. I asked, \"Why are you only spending 70 cents to process this PDF? Why not spend $30?\" If you're processing billions of dollars through the system, you should invest in good ingestion.</p>"},{"location":"office-hours/faq/#what-strategies-can-improve-experimentation-speed-when-working-with-rag-systems","title":"What strategies can improve experimentation speed when working with RAG systems?","text":"<p>If you feel like you're not running enough experiments, focus on improving your infrastructure:</p> <ol> <li> <p>Write parallelized code: Many teams are still doing all their tests using for loops. Spending 1-2 hours learning to write parallelized code can dramatically reduce your experimentation time, going from days to hours. Using tools like multiprocessing to hit multiple endpoints simultaneously is much better than having code break on iteration 2,000.</p> </li> <li> <p>Improve data access and understanding: Document how to query your data effectively. It's a waste of time if you write a query to prepare data, and someone comes back a day later saying, \"That's wrong, we actually need to include only last week's data.\"</p> </li> <li> <p>Build modular pipelines: If your entire RAG application is a giant Python file, it will be hard to test. But if each search index is a separate POST request, you can test them individually. This allows you to focus on one component (like an image retriever system) and improve it from 30% to 80% accuracy in one afternoon before integrating it back into your router.</p> </li> <li> <p>Test locally when possible: Create smaller synthetic datasets for quick iteration before running larger tests.</p> </li> </ol> <p>Being able to test components in isolation is crucial for rapid experimentation. A lot of this comes down to good software engineering practices and thoughtful system design.</p>"},{"location":"office-hours/faq/#how-do-you-handle-multiple-languages-in-a-rag-system-especially-when-job-titles-may-be-similar-but-written-differently-across-languages","title":"How do you handle multiple languages in a RAG system, especially when job titles may be similar but written differently across languages?","text":"<p>For multilingual challenges like job titles across different languages, I recommend two approaches:</p> <ol> <li> <p>Metadata extraction and filtering: Build classifiers to add more metadata to your ontology. For example, \"software engineering recruiter\" and \"software engineer\" go into two different classes, allowing you to filter for one and not the other. This improves search precision.</p> </li> <li> <p>Fine-tune embedding models with triplets: Create a dataset with examples like \"software engineer\" (query), \"python developer\" (positive example), and \"software engineering recruiter\" (hard negative). This teaches your model to separate similar-looking job titles that have different meanings.</p> </li> </ol> <p>For handling multiple languages, run tests to see whether translation improves performance. For instance, does your classifier perform better if you translate everything to English first, or if you use the original languages? If translating provides only a 1-2% improvement but requires complex infrastructure to maintain, it might make sense to accept slightly lower performance.</p> <p>If you lack training data for certain languages, consider using synthetic data creation. Use $2,000 of API credits to generate examples that cover edge cases in your domain, like distinguishing between \"real estate developer\" and \"python developer\" across languages.</p>"},{"location":"office-hours/faq/#what-are-your-thoughts-on-vision-rag-and-what-databases-would-you-recommend-for-multimodal-embeddings","title":"What are your thoughts on vision RAG, and what databases would you recommend for multimodal embeddings?","text":"<p>Vision RAG isn't talked about as much because it's more expensive and most of the important data is typically in text. That said, there are valuable use cases \u2013 like a company that does RAG over video clips to help movie producers find content, using Gemini Flash to describe what's happening in scenes.</p> <p>For databases, I'd recommend looking at:</p> <ul> <li>ChromaDB</li> <li>LanceDB</li> <li>TurboBuffer (used by Notion and Cursor)</li> <li>PgVector with Scale (for relational data with many reads/writes)</li> </ul> <p>However, I'm finding that pure multimodal embeddings aren't always the best approach anymore. Often it's better to generate a text summary of the image data. For example, when trying to embed images and text in the same space, CLIP embeddings often work worse than just doing image captioning and then embedding that text.</p> <p>In week 5, I'll talk more about this \u2013 there are many things you can't do with multimodal embeddings. They're trained mostly with caption data, which limits their capabilities for certain tasks.</p>"},{"location":"office-hours/faq/#what-are-your-experiences-with-the-model-context-protocol-mcp-and-how-might-it-change-rag-systems","title":"What are your experiences with the Model Context Protocol (MCP) and how might it change RAG systems?","text":"<p>MCP is becoming increasingly important because it allows different systems to connect with each other. When you own all the code, you don't really need MCP since you can just use function calling. But the ability to connect different systems is very compelling.</p> <p>Some interesting examples of MCP usage:</p> <ul> <li>Having an MCP server in Cursor to do image generation while building a video game</li> <li>Creating an MCP server to access network logs for debugging web applications</li> <li>Building MCP servers that connect to production databases so Cursor can understand your schema and write SQL</li> <li>Setting up an MCP server that writes conversation notes to Notion automatically</li> </ul> <p>What makes MCP powerful is that it standardizes these integrations and reduces boilerplate code. The protocol founders explain that it's easy to integrate with other servers when building your own client or server. Instead of rebuilding connectors with databases or services, you can reuse patterns and implementations.</p> <p>Claude 3.7 with Claude Code, for instance, has impressive agent functionality using MCP. It features better context management through commands like \"/compact\" which summarizes conversation history effectively without bloating the context window.</p>"},{"location":"office-hours/faq/#how-can-we-use-synthetic-data-generation-for-summarization-tasks","title":"How can we use synthetic data generation for summarization tasks?","text":"<p>There are many creative ways to generate synthetic data. For summarization, you can:</p> <ol> <li> <p>Create reverse tasks: For example, start with the outcomes you care about (like action items) and ask an LLM to generate a transcript that would produce those items. Then you can verify if your summarization system correctly extracts the original action items from this synthetic transcript.</p> </li> <li> <p>Use data augmentation techniques: Look at techniques from other domains like speech detection, where researchers combine clean audio samples to create more complex scenarios (like overlapping speakers). You can apply similar principles to text.</p> </li> <li> <p>Apply transformations similar to image processing: In computer vision, we've long used techniques like converting color photos to black and white, then training models to predict the original colors. Similarly, we convert high-resolution images to low-resolution and train models to predict the original resolution. We can apply similar transformations to text data.</p> </li> </ol> <p>The key is to think about ways to go from your desired output backward to input data, or to systematically transform existing data in ways that preserve the information you care about while changing other aspects.</p>"},{"location":"office-hours/faq/#when-using-structured-outputs-with-few-shot-prompts-should-the-examples-use-the-exact-same-json-schema-or-can-they-be-plain-text","title":"When using structured outputs with few-shot prompts, should the examples use the exact same JSON schema or can they be plain text?","text":"<p>I would almost always try to keep the JSON format consistent in your few-shot examples. This is somewhat superstitious, but I feel like the attention mechanism will always attend better to similar tokens.</p> <p>The schema itself is probably not what's going to break things these days. More likely, problems will arise from unintended properties of your examples. For instance, if all your action items in the few-shot examples are very short (under 4 words), your outputs will tend to be very short too. The examples communicate that these properties are correlated.</p> <p>I'd rather keep everything in JSON because there will be other random issues that come up. The only caution is to make sure you have checks in place so that when the language model has nothing in the context, it won't just automatically recite the few-shot examples.</p> <p>For complex contexts (like insurance claims that require understanding policies and forms), if including the context for each few-shot example would make your context window explode, consider few-shotting the thinking more importantly. Show examples of the reasoning process: \"I noticed the customer said they had 28 people, and our pricing page has different pricing for teams with less than 30 employees, so I'll use that pricing tier and mention they could get a better price with more employees...\"</p>"},{"location":"office-hours/faq/#how-do-you-approach-rag-when-you-have-transcripts-or-unstructured-text-without-clear-paragraph-markers","title":"How do you approach RAG when you have transcripts or unstructured text without clear paragraph markers?","text":"<p>For transcripts without clear paragraph markers, a few approaches work well:</p> <ol> <li> <p>Use diarization models to get speaker tags, which can serve as natural boundaries (each dialog line becomes a chunk)</p> </li> <li> <p>Detect silences in the audio and chunk on those silences</p> </li> <li> <p>Consider the structure of your content - for instance, if it's an interview format, you might know it's always question-answer pairs, so you can embed those pairs together</p> </li> </ol> <p>It ultimately depends on your specific use case. For a general conversation, chunking on silences or using diarization with a sliding window over dialog will work. For job interviews or expert interviews, understanding the structure (question followed by answer) lets you optimize your chunking strategy.</p> <p>If you have mixed domains and raw transcripts without access to the original source, you might need to default to generic approaches like 800 tokens with 40% overlap, then rely more on contextual retrieval techniques.</p>"},{"location":"office-hours/faq/#what-are-your-recommendations-for-building-slide-presentations-with-ai-tools","title":"What are your recommendations for building slide presentations with AI tools?","text":"<p>I've been using AI tools to build academic-style slides with LaTeX and Beamer. My process is:</p> <ol> <li>Load all relevant content into Cursor (in my case, all 6 hours of course transcripts)</li> <li>Create an outline for the presentation</li> <li>Use Claude to extract key case studies and insights from the transcripts</li> <li>Have the LLM generate slides using LaTeX Beamer format</li> <li>Use a simple auto-compiler (built with Watchdog) that recompiles the PDF whenever the file changes</li> </ol> <p>The advantages of this approach:</p> <ul> <li>You can create both slides and a detailed document from the same source</li> <li>The LLM can generate diagrams using TikZ (a graphics creation library)</li> <li>Everything is vector-based so it looks clean at any resolution</li> <li>You can have the LLM add callouts, highlights, and formatting</li> </ul> <p>This approach lets me essentially talk to my slides and have them update in real-time. I can say \"make this section shorter\" or \"add an example about X\" and see the changes immediately in the PDF preview.</p> <p>For those who prefer different formats, you could also try reveal.js for web-based presentations. The key is finding a workflow that lets you focus on content while the AI handles formatting and details.</p>"},{"location":"office-hours/faq/#how-do-ai-coding-tools-compare-claude-code-aider-cursor-windsurf","title":"How do AI coding tools compare (Claude Code, Aider, Cursor, Windsurf)?","text":"<p>There's been significant evolution in AI coding tools, with different strengths and approaches:</p> <ul> <li> <p>Claude Code has impressive agent functionality with excellent context management. It features a \"/compact\" command that summarizes conversation history effectively without bloating the context window. Some users report it's more capable than Cursor for certain tasks, particularly with how it handles context and managing complexity.</p> </li> <li> <p>Aider is a CLI-based tool that gives very low-level control over the files you can edit. It's open source and allows granular control over which models you use at specific points. Some users have migrated from Cursor to Aider due to its flexibility, though it has a steeper learning curve.</p> </li> <li> <p>Cursor is widely used for its UI and integrations. It works well for incremental changes to code and has good MCP integrations, but some find its context management becomes less effective over time on complex projects.</p> </li> <li> <p>Windsurf is particularly good at handling projects with good requirements and system design. It excels at context management over time and keeping track of multiple files in a repository. It's especially valuable for staff engineers and system architects who start with clear system designs.</p> </li> </ul> <p>The key differentiation often comes down to context management - how well the tool maintains an understanding of your entire codebase and project requirements as you work. For complex projects, tools that help document the goals and requirements (like adding branch goals in comments) tend to perform better.</p>"},{"location":"office-hours/faq/#how-do-you-use-deep-research-and-other-search-tools-effectively","title":"How do you use Deep Research and other search tools effectively?","text":"<p>Different search tools serve different purposes depending on context:</p> <ul> <li> <p>Claude's Deep Research works well for technical documentation, business-level competitive analysis, and generating comprehensive memos. Its tone is particularly well-suited for business communications that need minimal editing. Many users leverage it to materialize blog posts or analyses they want to read (e.g., \"Write me a blog post on why someone should look at MCP versus just using the Open API spec\").</p> </li> <li> <p>Grok's Deep Search has different strengths, with some users preferring it for timely news or quick research questions. Interestingly, usage patterns often split between mobile (Grok) and desktop (Claude/OpenAI) platforms based on when and where research is being done.</p> </li> <li> <p>Perplexity offers another approach to deep research, useful for generating product specs and learning resource reports, especially for colleagues without AI engineering backgrounds.</p> </li> </ul> <p>The quality of these tools has advanced to the point where they can effectively replace traditional research methods for many use cases, saving significant time for competitive analyses and technical investigations.</p>"},{"location":"office-hours/faq/#what-makes-lovable-stand-out-for-no-code-app-generation","title":"What makes Lovable stand out for no-code app generation?","text":"<p>Lovable has emerged as a powerful tool for no-code app generation:</p> <ul> <li>It excels at creating fully functional applications with modern UIs from scratch, going beyond simple prototypes to production-ready systems</li> <li>Its deep integration with Supabase provides authentication, real-time features, and database capabilities out of the box</li> <li>Every code change gets pushed to GitHub, allowing developers to fix issues locally in tools like Cursor or Windsurf when needed</li> <li>Each commit creates a preview deployment on Cloudflare, streamlining the development and testing process</li> <li>The tool can implement complex features like row-level security, push notifications, and real-time commenting systems using websockets</li> </ul> <p>Users report that Lovable outperforms alternatives like V0 and Bolt for creating complete applications, though it can be expensive ($200+ for complex projects). The tight integration with Supabase is particularly valuable, with many users becoming paid Supabase customers after using Lovable to build their applications.</p>"},{"location":"office-hours/faq/#what-emerging-techniques-are-promising-for-handling-long-documents-in-rag","title":"What emerging techniques are promising for handling long documents in RAG?","text":"<p>Handling long documents effectively is still evolving, with several promising approaches:</p> <ol> <li> <p>Hierarchical retrieval: Create summary or header-level embeddings for entire documents/chapters, then more granular embeddings for sections/paragraphs. This allows multi-stage retrieval that narrows down from document to specific passages.</p> </li> <li> <p>Graph-based approaches: Build knowledge graphs connecting concepts across documents, enabling retrieval that follows conceptual relationships rather than just lexical similarity.</p> </li> <li> <p>Hybrid sparse-dense retrieval: Combine embedding-based retrieval with keyword/BM25 approaches to capture both semantic and lexical matches, which is particularly valuable for documents with specialized terminology.</p> </li> <li> <p>Learning to rewrite: Train models to rewrite retrieved chunks into more coherent contexts that preserve the key information while eliminating redundancy.</p> </li> <li> <p>Recursive summarization: For extremely long documents, apply recursive summarization techniques that gradually compress information while maintaining key details.</p> </li> </ol> <p>Projects like LangChain's Document Transformer framework and repositories focusing on document processing show significant advances in these areas. The most effective systems often combine multiple approaches based on the specific characteristics of their document collections.</p>"},{"location":"office-hours/faq/#how-can-i-approach-rag-for-messy-knowledge-bases-with-duplicate-documents","title":"How can I approach RAG for messy knowledge bases with duplicate documents?","text":"<p>When dealing with messy knowledge bases that contain duplicate or near-duplicate documents:</p> <ol> <li> <p>Pre-processing pipeline: Implement de-duplication strategies during ingestion. This could involve computing similarity scores between documents and merging or filtering based on a threshold.</p> </li> <li> <p>Metadata extraction and filtering: Add more metadata to your ontology by building classifiers for different document types or topics. This allows you to filter for specific categories during retrieval.</p> </li> <li> <p>Query classification: For ambiguous queries, implement both pre-retrieval and post-retrieval classification to identify query intent and determine when clarification is needed.</p> </li> <li> <p>Progressive disclosure: Consider displaying intermediate results with summarized information about potential topics before generating a complete answer. This helps users navigate ambiguity, especially for queries that could refer to multiple topics.</p> </li> <li> <p>Dynamic presentation: For high-latency requirements (e.g., responses needed in under 6 seconds), consider showing retrieved documents first while the full answer is being generated, allowing users to see some results immediately.</p> </li> </ol> <p>Remember that the goal isn't perfect retrieval but helping users find the information they need. Sometimes showing multiple possible interpretations of a query is more helpful than trying to guess the single \"right\" answer.</p>"},{"location":"office-hours/faq/#when-is-it-better-to-use-dags-versus-agentic-approaches","title":"When is it better to use DAGs versus agentic approaches?","text":"<p>For specific workflows with well-defined steps, DAGs (Directed Acyclic Graphs) often provide more reliable and predictable results than fully agentic approaches:</p> <ol> <li> <p>Use DAGs when:</p> </li> <li> <p>The workflow has clear, sequential steps</p> </li> <li>You know the process is correct and just need to choose the right workflow</li> <li>You're implementing established protocols (like therapy approaches or compliance processes)</li> <li> <p>Predictability and consistency are critical</p> </li> <li> <p>Use agentic approaches when:</p> </li> <li> <p>The problem space is exploratory</p> </li> <li>Tasks require adaptation to unpredictable user input</li> <li>The workflow needs to evolve based on intermediate results</li> <li>You need to handle a wide variety of open-ended requests</li> </ol> <p>The distinction often comes down to control versus flexibility. DAGs provide more control over the exact process, while agentic approaches offer more flexibility but less predictability.</p> <p>For example, in a therapeutic chatbot following an established CBT protocol, a DAG approach ensures the conversation follows the correct therapeutic sequence. However, for an open-ended research assistant, an agentic approach allows for more dynamic problem-solving.</p>"},{"location":"office-hours/faq/#how-do-i-create-effective-negative-examples-for-training-retrieval-models","title":"How do I create effective negative examples for training retrieval models?","text":"<p>Creating effective negative examples for training retrieval models involves several strategies:</p> <ol> <li> <p>Hard negative mining: Find examples that are semantically similar but actually irrelevant. For job listings, \"software engineer recruiter\" is a hard negative for \"software engineer\" queries - they look similar textually but represent different job categories.</p> </li> <li> <p>Top-K analysis: Run retrieval with your current model, then have an LLM evaluate which results in the top K are actually irrelevant. These make excellent negative examples because they expose weaknesses in your current model.</p> </li> <li> <p>Controlled random sampling: While pure random sampling provides some signal, it's often too easy for the model to distinguish. Instead, use controlled randomization that preserves some properties of the positive examples.</p> </li> </ol> <p>When working with triplet learning (query, positive example, negative example), the quality of your negative examples often has more impact on model performance than adding more positive examples. Focus on finding negative examples that are difficult to distinguish from positive ones.</p> <p>For multimodal or multilingual applications, you may need to create synthetic data, especially for languages with limited training data. This can be done by using LLMs to generate examples that explore edge cases in your domain.</p>"},{"location":"office-hours/faq/#what-strategies-can-improve-response-time-in-rag-systems-with-tight-latency-requirements","title":"What strategies can improve response time in RAG systems with tight latency requirements?","text":"<p>For applications requiring responses in just a few seconds:</p> <ol> <li> <p>Progressive rendering: Show retrieved documents first (which can be returned in 150-400ms) while the LLM generates the complete answer in the background. This gives users immediate results while they wait for the full response.</p> </li> <li> <p>Caching: Implement aggressive caching for common queries. When a question-answer pair receives positive feedback (like being forwarded, shared, or rated highly), save it as a new document that can be quickly retrieved for similar questions.</p> </li> <li> <p>Response type classification: Use a lightweight classifier to determine if a query needs full retrieval and generation or if it can be answered with a simpler approach.</p> </li> <li> <p>Contextual snippet generation: During retrieval, generate quick summaries of each chunk that can be displayed alongside search results before the complete answer is ready.</p> </li> <li> <p>Parallel processing: Run multiple retrieval strategies in parallel and combine the results, rather than using sequential processing that adds to the total latency.</p> </li> </ol> <p>The key insight is to avoid an all-or-nothing approach to response generation. By decomposing the process into steps that can be displayed incrementally, you can significantly improve perceived latency even when the complete answer takes longer to generate.</p>"},{"location":"office-hours/faq/#what-are-your-experiences-with-the-model-context-protocol-mcp","title":"What are your experiences with the Model Context Protocol (MCP)?","text":"<p>MCP (Model Context Protocol) is becoming increasingly important as it allows different AI systems to connect with each other:</p> <ol> <li> <p>Key benefits:</p> </li> <li> <p>Standardizes integrations between AI systems</p> </li> <li>Reduces boilerplate code when connecting to different services</li> <li> <p>Allows models to access data and functionality they wouldn't normally have permission to use</p> </li> <li> <p>Practical examples:</p> </li> <li> <p>Image generation servers in Cursor for creating assets while building applications</p> </li> <li>Servers that connect to network logs for debugging web applications</li> <li>Connectors to production databases that help models understand schemas and write SQL</li> <li> <p>Automation tools that write conversation notes directly to Notion or other note-taking systems</p> </li> <li> <p>Comparison to function calling:</p> </li> <li> <p>When you own all the code, function calling may be simpler</p> </li> <li>MCP becomes valuable when connecting separate systems with different permission models</li> <li>Provides a standardized way to expose capabilities across different AI platforms</li> </ol> <p>The protocol is still evolving but shows promise for creating more powerful AI systems by composing specialized components. Some implementations like Claude 3.7 with Claude Code demonstrate how MCP can enable better context management and more sophisticated agent capabilities.</p>"},{"location":"office-hours/faq/#key-takeaways-and-additional-resources_1","title":"Key Takeaways and Additional Resources","text":""},{"location":"office-hours/faq/#key-takeaways_2","title":"Key Takeaways:","text":"<ul> <li>The goal of segmentation is to understand customer needs and determine what tools to build next</li> <li>Chunking strategy (800 tokens, 50% overlap) is rarely the bottleneck - focus on contextual retrieval instead</li> <li>For topic modeling, start with BERTTopic defaults and then use thinking models to better understand clusters</li> <li>Spend more compute upfront to improve data quality - particularly for high-value documents</li> <li>Write parallelized code to dramatically speed up experimentation</li> <li>For multilingual RAG, test whether translation improves performance enough to justify the added complexity</li> <li>Consider transforming image content to text summaries rather than using pure multimodal embeddings</li> <li>MCP is becoming increasingly important for connecting different AI systems together</li> <li>Use structured JSON consistently in few-shot examples rather than plain text</li> <li>For slide creation, AI tools can generate both content and formatting in vector-based formats</li> <li>For long documents, consider hierarchical retrieval, graph-based approaches, hybrid sparse-dense retrieval, learning to rewrite, and recursive summarization</li> <li>For messy knowledge bases, implement pre-processing pipeline, metadata extraction and filtering, query classification, progressive disclosure, and dynamic presentation</li> <li>For DAGs versus agentic approaches, use DAGs when the workflow has clear, sequential steps, and use agentic approaches when the problem space is exploratory</li> <li>For negative examples, use hard negative mining, top-K analysis, and controlled random sampling</li> <li>For response time, implement progressive rendering, caching, response type classification, contextual snippet generation, and parallel processing</li> </ul>"},{"location":"office-hours/faq/#additional-resources_1","title":"Additional Resources:","text":"<ul> <li>BERTTopic: https://maartengr.github.io/BERTopic/index.html</li> <li>MCP Agent: https://github.com/lastmile-ai/mcp-agent</li> <li>Claude Code: https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview</li> <li>RepoPrompt: https://repoprompt.com/</li> <li>Aider CLI coding tool: https://aider.chat/</li> <li>Lovable for no-code app generation with Supabase integration</li> <li>Cursor and Windsurf for AI-assisted coding environments</li> </ul> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"office-hours/faq/#how-should-we-handle-excel-files-with-multiple-sheets-and-tables","title":"How should we handle Excel files with multiple sheets and tables?","text":"<p>Handling Excel files with multiple sheets and tables is challenging, and few companies have solved this problem well. Experience from companies like Zapier shows that connecting Excel spreadsheets to automation tools requires many controls to work properly.</p> <p>The recommended approach is implementing checks on uploads to ensure files meet certain criteria. These checks might verify if an Excel file contains a single table, stays within size limits, or contains a specific number of tables. For simpler Excel files with single tables, implementing validation and processing works well, but for more complex files with multiple sheets and scattered tables, exporting to PDF might allow for easier parsing.</p> <p>It's important to segment data and build specific extractors based on the data's structure. Single-table files can go through a dedicated single-table pipeline, while multi-table files might work better through a PDF parsing pipeline.</p> <p>The most practical advice is to focus on simpler problems first and reject more complex ones until better solutions are developed. Solving for the simpler 40% of use cases while avoiding the most complex scenarios can be an effective strategy. Exporting Excel files as CSVs might also provide better compatibility with processing tools in many situations.</p>"},{"location":"office-hours/faq/#what-tools-are-recommended-for-sql-generation","title":"What tools are recommended for SQL generation?","text":"<p>Claude Sonnet has proven effective for generating SQL queries. Success depends heavily on whether the system can retrieve the correct tables and CREATE statements.</p> <p>The key to successful SQL generation is having good descriptions and CREATE statements for the tables, as well as ensuring that embedding and search capabilities properly identify the right tables when needed.</p> <p>A recommended approach from Timescale involves first retrieving relevant tables, then retrieving pre-existing, approved SQL snippets. When both the correct tables and appropriate SQL patterns are in context, the generation process becomes significantly more reliable.</p> <p>The complexity increases with many tables and columns, but focusing on retrieving the correct tables first, then incorporating approved SQL snippets to guide the generation process creates a two-step approach that significantly reduces errors in SQL generation.</p>"},{"location":"office-hours/faq/#what-is-the-linear-adapter-for-embeddings-and-how-does-it-work","title":"What is the Linear Adapter for embeddings and how does it work?","text":"<p>Linear adapters provide a cost-effective way to fine-tune embeddings. An embedding model takes data and produces a vector, with the dot product of two vectors indicating how similar they are. A linear adapter learns how to \"rotate\" these vectors slightly to better align with specific queries.</p> <p>The approach is very economical - if a vector has 500 dimensions, the linear adapter is just a 500 by 500 matrix that multiplies the original vector. This allows for significant improvements in embedding quality with minimal computational cost.</p> <p>Linear adapters can be compared to LoRA (Low-Rank Adaptation), but with key differences. LoRA works between many layers of a neural network, while a linear adapter works only at the end. Additionally, linear adapters can be applied to pre-trained embeddings like those from OpenAI without needing access to the original model weights.</p> <p>This approach enables domain-specific adaptations - for example, creating different adapters for marketing versus sales questions, or specialized adapters for legal, marketing, or tax information. The cost benefit is significant - training a linear adapter typically costs around $12 and can be done quickly, making it much more accessible than full model fine-tuning.</p> <p>Implementation uses the standard fine-tuning process with triplets (question, positive example, negative example), but specifically changes the embedding function for the query. This rotation of vectors into more effective alignments can significantly improve retrieval performance for domain-specific applications.</p>"},{"location":"office-hours/faq/#how-does-partitioning-work-in-retrieval-systems","title":"How does partitioning work in retrieval systems?","text":"<p>Partitioning in retrieval systems refers to how data is organized and segmented, rather than being about individual users. In applications like Cursor, a \"user\" might represent a documentation page. When working with code libraries like Requests, there might be a dedicated Requests index that multiple users access, but the partition is organized around the library package, documentation URL, or codebase.</p> <p>Similarly, in applications like Notion, a \"user\" isn't an individual email account but represents an entire workspace. This means a company's complete Notion workspace would exist in a single index.</p> <p>An interesting approach is partition-specific fine-tuning, which involves using different models to embed questions versus text chunks. Standard fine-tuning uses one model for both the question and the text chunk, but it's possible to fine-tune only one side of that equation. This might involve using the same model to embed all text chunks but having a different model to embed the question.</p> <p>This technique proves particularly valuable in e-commerce settings. One embedding model might identify products that are similar to each other (creating a \"similar products\" carousel), while a different embedding model could identify complementary products (for \"frequently bought together\" recommendations). Both embeddings operate on the same product data but serve different retrieval purposes.</p>"},{"location":"office-hours/faq/#what-is-the-model-context-protocol-mcp-and-how-does-it-differ-from-regular-apis","title":"What is the Model Context Protocol (MCP) and how does it differ from regular APIs?","text":"<p>The Model Context Protocol (MCP) functions like a USB-C connector for AI systems - it's about standardization rather than specific functionality. While the devices that connect may vary, the connection method itself is standardized.</p> <p>The key advantage of MCP over traditional APIs is the separation of client from backend. With a REST API, developers must write specific code to interact with each API, and each application needs to implement these integrations. MCP creates a standardized way for different systems to connect without custom code for each integration.</p> <p>Consider an enterprise example where different teams might focus on different functionality: one team might work on email search while another builds CRM search tools. Both teams can develop their MCP clients, and the chatbot developers can easily integrate both without writing extensive custom code for each system.</p> <p>This standardization means tools built with MCP work across multiple platforms without requiring custom integration code. A tool that works in one MCP-compatible environment (like Cursor) will work in others (like Claude desktop app) with minimal additional effort.</p> <p>Beyond just function execution, MCP also supports resources and prompts. The MCP developer can provide not just functionality but also the prompts needed to use that functionality effectively. This means client developers don't need to write their own prompts for common operations like summarization or action item extraction.</p> <p>This approach significantly reduces the need for glue code and allows developers to build applications without having to own all the client code, making integrations between different AI systems much more seamless.</p>"},{"location":"office-hours/faq/#what-ai-tools-are-recommended-for-daily-work","title":"What AI tools are recommended for daily work?","text":"<p>Claude Code stands out among AI coding tools, particularly for its high-quality prompts. What makes it exceptional is how it handles context and continuity - when asked to write blog posts, it will first analyze existing posts to create a style guide, then reference that guide for every new post it writes.</p> <p>This attention to existing style and consistency makes it particularly effective for content creation tasks. Despite higher costs compared to some alternatives (potentially $100+ per weekend of heavy use), many users find the value justifies the expense, with some noting they would willingly pay $200 monthly for the service.</p> <p>For report generation specifically, there's significant value in tools that provide templated outputs. Ideally, a report generation tool would allow users to standardize formats across different reports - ensuring all market analysis reports follow the same structure, or that candidate evaluation reports maintain consistent formatting rather than varying significantly in format and depth.</p> <p>This points to a broader trend in AI tool development - the need for tools that not only generate content but do so in consistent, predictable formats that align with existing workflows and style guidelines.</p>"},{"location":"office-hours/faq/#what-approaches-are-being-used-for-multimodal-applications","title":"What approaches are being used for multimodal applications?","text":"<p>Multimodal applications combining vision and language models are expanding into specialized domains. One example shared during office hours involved food image analysis, where a system extracts structured data from food photographs.</p> <p>These systems can identify cuisine type, restaurant information, nutritional content, and dietary characteristics like whether a food item is vegan. While acknowledging practical limitations (\"there's a limit to what you can effectively do\"), early experiments show promising results in extracting valuable information from visual food content.</p> <p>This example demonstrates how multimodal AI applications are moving beyond basic image recognition to extract detailed, structured information from visual content. The integration of vision models with language models allows systems to interpret and categorize visual information in ways that support practical applications like dietary tracking or restaurant recommendations.</p>"},{"location":"office-hours/faq/#what-emerging-trends-should-we-be-aware-of","title":"What emerging trends should we be aware of?","text":"<p>Report generation emerges as a particularly important trend in AI applications. The ability to automatically generate structured reports from unstructured data represents significant economic value for organizations.</p> <p>Structured outputs and templates are increasingly valuable, especially for business use cases where standardized formats are essential. The ideal scenario allows for consistency in outputs - ensuring all reports of a specific type follow the same structure rather than varying significantly in format and organization.</p> <p>Several organizations are developing report generation capabilities for internal use, with teams requiring standardized reports on a regular basis. This trend spans multiple industries, with financial due diligence being one area where automated report generation from multiple PDF sources shows particular promise.</p> <p>The growing importance of fine-tuning approaches for handling data from multiple teams or domains also represents a significant trend. As organizations deploy AI systems across different business units, finding ways to effectively fine-tune models while maintaining performance becomes crucial.</p> <p>Report generation capabilities demonstrate how AI can move beyond simple question answering to create significant economic value through structured information synthesis - transforming unstructured data into formatted reports that follow organizational templates and standards.</p>"},{"location":"office-hours/faq/#how-should-we-handle-retrieval-across-multiple-queries-in-a-conversation","title":"How should we handle retrieval across multiple queries in a conversation?","text":"<p>When handling retrieval across multiple queries within the same conversation, the simplest approach is often the most effective. Using function calls for retrieval is recommended, where each call retrieves text chunks and includes information in XML format specifying the context and the question.</p> <p>A key consideration is whether to keep retrieved context from previous queries in the message history for subsequent queries. This requires careful balancing, as including all previous context can consume tokens quickly.</p> <p>The recommended practice is to prompt the retrieval system to always generate fully specified queries. For example, if the first question is \"Where does Jason live?\" and the follow-up is \"What's the population of that city?\", the retrieval system should be prompted to expand the second query to \"What is the population of New York City?\" This approach can be implemented through few-shot examples that demonstrate how to handle conversational context.</p> <p>This strategy works because the model has access to both the previous question and answer in its context, allowing it to formulate complete, self-contained queries even when the user's input is ambiguous or relies on previous context.</p> <p>An additional benefit of this approach is the ability to generate follow-up questions based on retrieved content. For instance, if a text chunk mentions that \"Jason lived in different places when he was younger versus when he was older,\" the system can suggest follow-up questions like \"Where did Jason live when he was younger?\" This not only improves information discovery but also demonstrates to users that there are more interesting questions they could ask about the topic.</p>"},{"location":"office-hours/faq/#what-innovations-are-happening-in-memory-and-context-management-for-agents","title":"What innovations are happening in memory and context management for agents?","text":"<p>Recent innovations in memory and context management for agents focus on creating more dynamic and self-improving systems. Rather than simply saving memories at the end of a conversation, newer approaches incorporate real-time memory creation and utilization during interactions.</p> <p>Frameworks like Letta are incorporating self-editing capabilities alongside memory management. This integration allows agents to refactor their understanding and approach during a conversation rather than only learning from interactions after they conclude.</p> <p>Implementation of these advances requires significant infrastructure changes, as memory layers affect prompt construction and the overall flow of agent interactions. The approach involves creating memories as the conversation progresses, which in turn influences the prompts used in subsequent exchanges.</p> <p>With newer models like Claude 3.7 Sonnet, there's a shift in how tools are used by agents. Similar to the adaptation period seen with Claude Opus or GPT-4, these models require different prompting patterns to effectively utilize tools. Studying how systems like Cloud Code implement their tool use can provide valuable insights for optimizing agent performance with newer models.</p>"},{"location":"office-hours/faq/#what-can-we-learn-from-examining-cloud-codes-approach-to-prompts-and-tools","title":"What can we learn from examining Cloud Code's approach to prompts and tools?","text":"<p>Cloud Code's approach to prompts and tools offers valuable insights for designing effective AI systems. Analysis of the Cloud Code source (extracted from their minified code) reveals highly detailed and carefully structured prompts for various tools.</p> <p>Cloud Code implements a robust workflow where tools are designed to work together cohesively. Each tool's prompt contains specific instructions about how to use other tools first, creating a well-defined sequence of operations. For example, tools may include instructions to check for uniqueness before proceeding or to use specific validation approaches.</p> <p>The prompts are remarkably detailed, with extensive instructions for common operations. For instance, batch tools contain comprehensive guidelines just for creating pull requests. This level of specificity helps ensure consistent and reliable performance.</p> <p>Another notable aspect is Cloud Code's implementation of specialized tools like a notebook tool, showing how diverse functionality can be incorporated into a unified system. The prompts demonstrate that Claude is capable of working with numerous tools simultaneously when the system is properly designed.</p> <p>This examination highlights the importance of thoughtful prompt engineering and tool design in building effective AI systems. By providing clear, detailed instructions and establishing well-defined workflows between tools, systems can achieve more reliable and sophisticated functionality.</p>"},{"location":"office-hours/faq/#how-can-we-create-automated-evaluation-reports-and-insights-for-rag-systems","title":"How can we create automated evaluation reports and insights for RAG systems?","text":"<p>Automated evaluation reports can significantly enhance RAG system development by providing structured insights and clear next steps. A comprehensive approach involves building a pipeline that:</p> <ol> <li>Takes validation datasets and runs them through the RAG system</li> <li>Computes various metrics (correctness, citation accuracy, URL validity)</li> <li>Generates visualizations segmented by topic, question type, and other dimensions</li> <li>Uses LLMs to analyze metrics and provide insights</li> <li>Creates recommendations for system improvements</li> </ol> <p>The reports can include both detailed analyses (15+ pages) and condensed slides for easier consumption in meetings. Key components include:</p> <ul> <li>Methodology explanations for stakeholders who may not be familiar with technical details</li> <li>System architecture diagrams</li> <li>Performance visualizations broken down by different segments</li> <li>Statistical analysis of which topics or question types perform well or poorly</li> <li>LLM-generated insights that explain patterns in the data</li> <li>Specific recommendations tied to the codebase</li> </ul> <p>This process can effectively \"close the loop\" in the flywheel of development by identifying specific areas for improvement. For example, the system might recommend improving schema handling, enhancing retrieval tools, or adding more data for underrepresented topics.</p> <p>The insights generated by LLMs analyzing the metrics can often align well with developer intuitions, but having them formally documented provides better clarity for prioritization and communication with stakeholders. These insights can be directly translated into development tickets, creating a streamlined workflow from evaluation to implementation.</p> <p>The ultimate goal is to use these insights to guide the next iteration of development, run the evaluation again, and continue improving through this structured feedback loop.</p>"},{"location":"office-hours/faq/#what-strategies-help-maintain-context-when-working-with-ai-coding-assistants","title":"What strategies help maintain context when working with AI coding assistants?","text":"<p>When working with AI coding assistants like Cursor, maintaining context throughout a development session can be challenging. Several effective strategies have emerged from practical experience:</p> <p>Creating and maintaining to-do lists within project documentation provides an effective way to preserve context. By instructing the AI to update the to-do list after completing each task, you ensure the progress remains in context for subsequent interactions. This approach creates a record of what has been accomplished and what remains to be done.</p> <p>Templates within documentation files help maintain consistent structure across generated content. For example, having templates for different documentation sections ensures the AI follows established patterns when creating new content. This approach allows you to simply prompt the AI to \"make more concept templates that generally look like this,\" maintaining consistency through visual examples rather than complex rules.</p> <p>For more structured workflows, some developers create development plans using models like Claude Opus, which provide a roadmap the AI can follow. This helps prevent the AI from getting lost during implementation or going down unproductive paths.</p> <p>Many developers find that keeping AI agents away from certain code areas (particularly tests) helps maintain structure. This can be accomplished either through explicit instructions or by adding files to the cursor.ignore configuration, which prevents them from being indexed while still allowing the AI to run commands like pytest.</p> <p>Follow-up prompts at the end of interactions help maintain momentum. By asking what else needs to be done or what the next steps are, you encourage the AI to reference the to-do list and continue working on remaining tasks, creating a more cohesive development experience.</p> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"office-hours/faq/#what-is-deep-research-and-how-does-it-relate-to-rag","title":"What is Deep Research and how does it relate to RAG?","text":"<p>Deep Research is essentially a model fine-tuned for tool use that leverages RAG and iteration in a loop to produce reports. It can be viewed as RAG with solid data sources and strong reasoning capabilities on top. Deep Research is distinct from standard RAG applications because it typically produces more comprehensive outputs like reports rather than just answering specific questions.</p> <p>While Deep Research generates reports that might differ in structure between runs, more advanced approaches like those used by Vantage aim to create standardized, deterministic reports. The ideal approach is to define a specific structure for reports, particularly when you know exactly what questions need to be answered for your domain.</p> <p>There's significant economic value in creating structured reports rather than just answering ad-hoc questions. For example, instead of building a system that allows recruiters to query interview transcripts individually, creating a standardized hiring report that distills key information from all interviews provides greater business value. This approach helps stakeholders make better decisions rather than just saving time on information retrieval.</p> <p>The techniques taught in the RAG course are directly applicable to building Deep Research-style systems, particularly when focused on specific domains rather than general-purpose research.</p>"},{"location":"office-hours/faq/#should-we-use-long-context-windows-or-rag-for-complex-questions","title":"Should we use long context windows or RAG for complex questions?","text":"<p>Long context windows should be leveraged first when possible, as they generally produce better results than relying solely on chunk retrieval. The ideal approach is often to use document-level retrieval rather than chunk-level retrieval when working with long context models.</p> <p>When faced with specific tasks that require processing lengthy documents (like generating pricing emails based on policy documents), consider creating dedicated tools that use the full context window rather than breaking documents into chunks. This can be implemented as a function that uses a very long prompt containing all relevant policy documents.</p> <p>This approach simplifies the retrieval problem from needing good chunk retrieval to just needing good document retrieval, which can be accomplished with simpler techniques like full-text search. The decision becomes not about whether you have good chunk retrieval but rather if you have good document retrieval capabilities.</p> <p>As models' context windows continue to expand, this approach becomes increasingly viable for more use cases, potentially reducing the complexity of some RAG implementations.</p>"},{"location":"office-hours/faq/#how-important-is-human-labeled-data-for-rag-systems","title":"How important is human-labeled data for RAG systems?","text":"<p>Human-labeled data remains essential for building high-quality RAG systems, though many teams underestimate its importance. Teams that are reluctant to invest in data labeling often struggle to achieve meaningful performance improvements.</p> <p>From a consulting perspective, one effective approach is to demonstrate the impact of data quality through experimentation. Show how model performance improves with synthetic data, then demonstrate how it plateaus. This creates a data-driven argument that once synthetic data reaches diminishing returns, real human-labeled data becomes necessary for further improvement.</p> <p>For high-value applications, the investment in human labeling is justified. Companies like Vantage, which produces due diligence reports for investment decisions, dedicate staff to labeling and evaluating the quality of question-answer pairs. This reflects the understanding that without at least one human producing high-quality data, systems will struggle to achieve meaningful differentiation in output quality.</p> <p>The economic argument is compelling: if a model is helping make decisions that involve millions or billions of dollars (as in investment due diligence or hiring), the cost of high-quality human labeling is minimal compared to the value it creates.</p>"},{"location":"office-hours/faq/#how-do-you-handle-model-evaluation-when-generating-reports-rather-than-simple-answers","title":"How do you handle model evaluation when generating reports rather than simple answers?","text":"<p>Evaluating report generation presents different challenges than evaluating direct question answering. While individual components can be measured with standard metrics, evaluating complete reports often requires human judgment against a defined rubric.</p> <p>Language models can perform reasonably well as judges against a rubric, but they primarily assess whether all required elements are present rather than providing nuanced feedback on quality or analysis. Human evaluation remains important for assessing whether the analysis itself is valuable and meets business needs.</p> <p>This challenge mirrors broader evaluation difficulties in the generative AI space, where outputs become more complex and subjective. The solution often involves creating clear rubrics for what constitutes a good report in your specific domain, then combining automated checks with strategic human review.</p> <p>Teams should focus on defining what makes a report valuable to their specific users rather than pursuing generic quality metrics. This might involve understanding whether users need comprehensive information, specific recommendations, or particular formatting that helps with decision-making.</p>"},{"location":"office-hours/faq/#what-broader-trends-are-emerging-in-ai-consulting","title":"What broader trends are emerging in AI consulting?","text":"<p>The AI consulting landscape is evolving rapidly, with several key trends emerging:</p> <ol> <li> <p>Shift from implementation to experimentation: More consulting work now involves helping teams design and run effective experiments rather than just implementing specific techniques. This includes teaching scientific methods, hypothesis formation, and systematic testing.</p> </li> <li> <p>Focus on data quality over algorithms: Successful consultants emphasize improving data quality and data collection processes rather than just applying newer algorithms. Many organizations still lack basic data infrastructure for effective AI work.</p> </li> <li> <p>Organizational change management: A significant portion of AI consulting now involves helping teams adapt to new workflows and develop the right skills. This includes teaching software engineers to approach problems more like data scientists.</p> </li> <li> <p>Economic value alignment: The most successful AI implementations focus on creating decision-making value rather than just time savings. Products that help customers make better decisions (like hiring recommendations or investment analysis) can command higher prices than those that merely save time.</p> </li> </ol> <p>The role of consultants remains valuable even as AI tools become more accessible because they bring expertise in experiment design, data quality improvement, and aligning AI capabilities with business value.</p>"},{"location":"office-hours/faq/#how-will-ai-impact-the-consulting-industry-itself","title":"How will AI impact the consulting industry itself?","text":"<p>The consulting industry will continue to evolve alongside AI advancements, but consultants who adapt will remain valuable. The core value of consulting is increasingly about bringing expertise in scientific methods, data analysis, and business process transformation rather than simply implementing technology.</p> <p>Several shifts are occurring in the consulting space:</p> <ol> <li> <p>Distribution becomes more important: Consultants who can effectively share their insights through content creation (blogs, videos, courses) will have advantages in attracting clients.</p> </li> <li> <p>Process expertise over pure technical knowledge: As technical implementation becomes easier with AI tools, consultants who understand how to change organizational processes and workflows become more valuable.</p> </li> <li> <p>Organization and workflow design: Consultants who can help structure workflows and processes that leverage AI effectively will remain in demand, even as some technical implementation work becomes automated.</p> </li> <li> <p>Connection to economic value: Consultants who can clearly connect AI capabilities to business value and ROI will continue to thrive, focusing less on technology and more on business outcomes.</p> </li> </ol> <p>While AI will automate some aspects of consulting work, it simultaneously creates new opportunities for consultants who can help organizations navigate the complex landscape of AI implementation and business transformation.</p>"},{"location":"office-hours/faq/#how-should-we-handle-training-data-contamination-from-ai-generated-content","title":"How should we handle training data contamination from AI-generated content?","text":"<p>As more content on the internet becomes AI-generated, concerns about training data contamination and potential \"model collapse\" are valid but may be overstated for several reasons:</p> <ol> <li> <p>Unexplored modalities: Even if text data becomes saturated with AI-generated content, there are many other modalities (video, computer interaction data, etc.) that remain largely untapped for training.</p> </li> <li> <p>Mode covering vs. mode collapse: Advanced research at organizations like OpenAI focuses on developing models that can identify multiple solution modes rather than collapsing to the lowest-resistance path. Models that are \"mode covering\" can maintain diversity in their outputs even when trained on some low-quality data.</p> </li> <li> <p>Real-world data sources: For many specialized applications, the most valuable data isn't from the public internet but from proprietary sources or human interaction with systems. This data remains largely uncontaminated.</p> </li> <li> <p>Post-training refinement: Much of the current improvement in AI models comes from post-training techniques like RLHF rather than pre-training alone. This allows models to improve based on high-quality human feedback even if pre-training data becomes noisier.</p> </li> </ol> <p>OpenAI researchers reportedly maintain confidence that there's still significant high-quality data available, suggesting that concerns about running out of training data may be premature.</p>"},{"location":"office-hours/faq/#what-are-emerging-trends-in-ai-tool-development","title":"What are emerging trends in AI tool development?","text":"<p>Several noteworthy trends are emerging in AI tool development:</p> <ol> <li> <p>Advanced agents like Manus: New tools like Manus are providing powerful capabilities by combining foundation models (like Claude Sonnet) with extensive tooling. While details are limited, these systems represent a new generation of AI assistants with enhanced capabilities.</p> </li> <li> <p>Cloud Code improvements: Cloud Code has shown impressive performance for specific tasks, sometimes outperforming tools like Cursor for certain types of development work. However, success often depends on the user's expertise in the domain they're working in - users still need significant knowledge to effectively guide AI tools.</p> </li> <li> <p>Context management evolution: Newer AI tools are improving how they manage context over time, creating better continuity between sessions and maintaining understanding of project requirements.</p> </li> <li> <p>Focus on expert augmentation: The most successful AI tools are those that augment human expertise rather than trying to replace it entirely. Tools work best when users have clear goals and domain knowledge, with the AI handling implementation details.</p> </li> </ol> <p>Despite significant advances in AI capabilities, domain expertise remains crucial for effective use of these tools. The relationship between user expertise and AI capabilities creates a complex dynamic where both need to evolve together for optimal results.</p>"},{"location":"office-hours/faq/#how-will-data-collection-evolve-for-ai-applications","title":"How will data collection evolve for AI applications?","text":"<p>Data collection for AI is shifting in several important ways:</p> <ol> <li> <p>Purposeful logging: Companies are moving beyond debugging-focused logging to capturing data specifically designed for model training. This requires engineers to think about what signals might be useful for future models rather than just for troubleshooting.</p> </li> <li> <p>Structured feedback collection: More companies are implementing systematic ways to collect user feedback and interactions, recognizing these signals as valuable training data rather than just product metrics.</p> </li> <li> <p>Data quality over quantity: There's growing recognition that having smaller amounts of high-quality, well-labeled data is often more valuable than vast amounts of noisy data.</p> </li> <li> <p>Economic value alignment: Organizations are increasingly evaluating what data to collect based on economic value rather than technical feasibility alone. This means focusing data collection efforts on areas where improved model performance translates directly to business outcomes.</p> </li> </ol> <p>Many companies still struggle with basic data collection infrastructure, often lacking the systems needed to capture useful signals from user interactions. Building these foundations remains a critical first step before more advanced AI applications can be developed.</p>"},{"location":"office-hours/faq/#how-should-we-think-about-distribution-and-economic-viability-in-ai-products","title":"How should we think about distribution and economic viability in AI products?","text":"<p>The most successful AI applications focus on creating decision-making value rather than just time savings. This fundamental shift in value proposition affects pricing, distribution, and product design:</p> <ol> <li> <p>Value-based pricing: Products that help customers make better decisions (like hiring recommendations or investment analysis) can command higher prices than those that merely save time. For example, recruiters charge 25% of a hire's salary not because they save time but because they help make better hiring decisions.</p> </li> <li> <p>Structured outputs: There's increasing value in AI systems that produce standardized, structured outputs (like reports) rather than just answering ad-hoc questions. This creates more consistent value and makes the outputs more directly usable in business processes.</p> </li> <li> <p>Domain specialization: Applications focused on specific domains with clear economic value (financial analysis, legal research, specialized technical fields) can support higher pricing than general-purpose AI tools.</p> </li> <li> <p>Content as marketing: For many AI consultants and product builders, content creation (blog posts, courses, etc.) derived from their expertise serves as efficient marketing. This \"sawdust\" from their core work helps attract clients and build credibility.</p> </li> </ol> <p>The most economically viable AI products are those that align directly with high-value business decisions rather than just providing generalized capabilities or incremental efficiency improvements.</p>"},{"location":"office-hours/faq/#what-recommendations-do-you-have-for-structuring-the-course-and-its-content","title":"What recommendations do you have for structuring the course and its content?","text":"<p>Several suggestions emerged for improving the course structure and content:</p> <ol> <li> <p>Better content organization: Ensure core videos and tutorials are prominently featured in the main menu rather than buried under multiple links. This would improve discoverability and help students stay on track.</p> </li> <li> <p>Standardized office hours format: Implement a consistent format for office hours, with the first 10-20 minutes dedicated to setting context about the week's material before moving to questions. This helps orient participants who may be joining different sessions.</p> </li> <li> <p>Email reminders with direct links: Send regular emails with direct links to the week's core videos and tutorials to ensure students know exactly what to watch and when.</p> </li> <li> <p>Calendar integration: Consider adding placeholder calendar events for self-study time to help students schedule time to watch asynchronous content.</p> </li> <li> <p>Expanded coverage of enterprise tools: While OpenAI tools were featured prominently for practical reasons, more coverage of enterprise platforms (Azure, AWS, Google Vertex) would be valuable for many students working in corporate environments.</p> </li> <li> <p>Open-source alternatives: Include more examples using open-source tools alongside commercial offerings, especially for cases where data residency requirements make cloud services challenging.</p> </li> </ol> <p>The feedback emphasized that while the course content was valuable, improvements to structure and discoverability would help students manage the significant amount of material more effectively.</p>"},{"location":"office-hours/faq/#how-can-we-use-slack-effectively-after-the-course-ends","title":"How can we use Slack effectively after the course ends?","text":"<p>The Slack channel will remain available as an ongoing resource for students after the course concludes. Several recommendations for effective use include:</p> <ol> <li> <p>Specific questions get better answers: When asking questions in Slack, provide specific details about your use case, what you've already tried, and exactly what you're trying to accomplish. This allows for more targeted and helpful responses.</p> </li> <li> <p>Share real-world applications: Sharing how you're applying concepts from the course to real projects provides valuable context for others and creates learning opportunities for everyone.</p> </li> <li> <p>Ongoing community learning: The Slack channel offers an opportunity to continue learning from peers who are implementing RAG systems across different industries and use cases.</p> </li> <li> <p>Access to course materials: All course materials will remain accessible through Maven, and the Slack community provides a way to discuss those materials as you continue to review them.</p> </li> </ol> <p>The instructor emphasized that students will get as much value from the community as they put in through specific, thoughtful questions and sharing their own experiences.</p>"},{"location":"office-hours/faq/#what-future-trends-do-you-anticipate-in-ai-development","title":"What future trends do you anticipate in AI development?","text":"<p>Several key trends are likely to shape AI development in the near future:</p> <ol> <li> <p>Structured output generation: The ability to generate consistent, structured reports and analyses will become increasingly valuable, particularly for business applications where standardized formats are essential.</p> </li> <li> <p>Report generation workflows: Building on the structured output trend, more sophisticated workflows for generating comprehensive reports from multiple data sources will become mainstream.</p> </li> <li> <p>Scientific approach to AI development: Organizations that adopt rigorous experimentation, hypothesis testing, and data analysis will pull ahead of those that simply implement the latest techniques without careful evaluation.</p> </li> <li> <p>Economic alignment: AI applications that directly support high-value decision making will see stronger adoption and commercial success compared to those that merely provide incremental efficiency improvements.</p> </li> <li> <p>Integration of multiple modalities: While still evolving, the ability to reason across text, images, video, and interactive data will create new application possibilities, though many practical applications will still focus on extracting structured information from these inputs rather than general understanding.</p> </li> </ol> <p>The most successful organizations will be those that develop systematic processes for continuous improvement of their AI systems rather than chasing the latest models or techniques without a clear evaluation framework.</p>"},{"location":"office-hours/faq/#how-do-you-balance-providing-generic-ai-solutions-versus-domain-specific-implementations","title":"How do you balance providing generic AI solutions versus domain-specific implementations?","text":"<p>The balance between generic AI solutions and domain-specific implementations depends on both economic factors and technical feasibility:</p> <ol> <li> <p>Start with domain specificity: Focusing on specific domains allows for more valuable outputs, better evaluation, and clearer value propositions. This approach makes it easier to create systems that provide significant value.</p> </li> <li> <p>Specialize by intent rather than content: Even within a domain, segmenting by user intent (what they're trying to accomplish) rather than just content type creates more focused and effective solutions.</p> </li> <li> <p>Economic viability: Domain-specific solutions can often command higher prices because they solve specific high-value problems rather than providing general capabilities. This makes them more economically viable despite smaller potential market size.</p> </li> <li> <p>Technical feasibility: Creating effective general-purpose AI systems remains technically challenging, while domain-specific implementations can achieve high performance by narrowing the scope of what they need to handle.</p> </li> </ol> <p>For most organizations building AI applications, starting with a specific domain and set of well-defined use cases is likely to produce better results than attempting to build general-purpose systems. This focus allows for better data collection, more effective evaluation, and clearer alignment with business value.</p>"},{"location":"office-hours/faq/#key-takeaways-and-additional-resources_2","title":"Key Takeaways and Additional Resources","text":""},{"location":"office-hours/faq/#key-takeaways_3","title":"Key Takeaways:","text":"<ul> <li>Deep Research can be understood as RAG with high-quality data sources and strong reasoning capabilities</li> <li>Structured reports often provide more business value than ad-hoc question answering</li> <li>Long context windows should be leveraged first when possible before falling back to chunking</li> <li>Human-labeled data remains essential for high-quality RAG systems, especially as systems reach the limits of improvement from synthetic data</li> <li>Evaluating report generation often requires human judgment against defined rubrics</li> <li>AI consulting is shifting toward experimental design and process transformation rather than just implementation</li> <li>Data collection is evolving to focus more on purposeful logging and structured feedback collection</li> <li>The most economically viable AI products align with high-value business decisions rather than just providing efficiency improvements</li> <li>Content organization and standardized formats for course materials can significantly improve the learning experience</li> <li>Domain-specific AI implementations typically provide better economic and technical outcomes than general-purpose solutions</li> </ul>"},{"location":"office-hours/faq/#additional-resources_2","title":"Additional Resources:","text":"<ul> <li>The Future of RAG - Jason Liu's blog post on where RAG is heading</li> <li>Deep Research - OpenAI's introduction to Deep Research</li> <li>Vantage - Company mentioned as an example of advanced report generation</li> <li>Claude 3.7 Sonnet - Latest model referenced in discussions</li> <li>Cloud Code - AI coding tool discussed in the sessions</li> <li>Manus - Emerging AI agent mentioned in the discussions</li> </ul> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"office-hours/faq/#how-should-we-understand-precision-in-the-context-of-llms-and-rag","title":"How should we understand precision in the context of LLMs and RAG?","text":"<p>Precision has become increasingly important as most people are comfortable adding substantial information to the context window. The key question is how sensitive these models are to ambiguous or irrelevant information.</p> <p>Most current models are very good at recall because they're optimized for \"needle in the haystack\" tests. Due to the attention mechanism with sliding windows, they can typically catch important information even when it's buried among other content. However, the sensitivity to irrelevant information is less well-studied.</p> <p>As we increase the amount of data in the context, we need to understand what precision looks like and how it correlates with factuality or answer quality. Earlier models like GPT-3.5 were quite sensitive to long context with low precision - if you gave them too much information, they would \"overthink\" because of the volume of content to process.</p> <p>This is why it's valuable to experiment with different retrieval settings: \"For the top 10 documents, this is my precision and recall; for my top 25 documents, this is my precision and recall. If my recall is not increasing as a function of K (the number of documents), that's where I decide to set a threshold.\"</p> <p>Some people set thresholds based on re-ranker scores, but that can be dangerous since these scores aren't true probabilities. You can't just set 0.5 as a universal threshold - you need to understand the precision-recall tradeoffs for your specific application.</p> <p>Key Takeaway: Models can be sensitive to low-precision context, where irrelevant information causes them to incorporate red herrings into answers. Testing different document count thresholds is more reliable than using arbitrary re-ranker score cutoffs.</p> <p>Key Takeaway</p> <p>Models can be sensitive to low-precision context, where irrelevant information causes them to incorporate red herrings into answers. Testing different document count thresholds is more reliable than using arbitrary re-ranker score cutoffs.</p>"},{"location":"office-hours/faq/#what-role-can-small-language-models-play-in-a-rag-architecture","title":"What role can small language models play in a RAG architecture?","text":"<p>Small language models can serve several valuable functions within a RAG system:</p> <p>First, they can rewrite queries quickly and efficiently. For example, if a user asks \"What is happening this week?\", a small model could convert this into a structured query like a JSON object specifying \"search all documents where the datetime is between today and today minus 7 days.\" This type of entity resolution and query parsing doesn't require the full knowledge of a large model but benefits from the lower latency of smaller models.</p> <p>Second, small models can build better embedding spaces. Most current embedding models are relatively simple, but a small language model fine-tuned on your specific task could significantly improve embedding quality or re-ranking performance.</p> <p>In this context, I think of \"small\" as meaning lower latency with less world knowledge - models that can perform specific tasks efficiently without needing to understand everything about the world.</p> <p>Key Takeaway: Small language models can enhance RAG systems through query rewriting and improved embeddings, offering lower latency for specific tasks that don't require comprehensive world knowledge.</p> <p>Key Takeaway</p> <p>Small language models can enhance RAG systems through query rewriting and improved embeddings, offering lower latency for specific tasks that don't require comprehensive world knowledge.</p>"},{"location":"office-hours/faq/#how-can-we-measure-quality-in-multi-turn-conversations","title":"How can we measure quality in multi-turn conversations?","text":"<p>When evaluating multi-turn exchanges like simulated customer interactions or teacher training scenarios, there are several approaches to consider.</p> <p>One approach is to model the interaction as a state machine or \"LangDraft\" type model where there are defined states that can be traversed. For example, in a customer support scenario, you might have an intake state, followed by various question states, triage states, and resolution states.</p> <p>We've used this with \"LLM therapists\" where the system might say, \"I can tell you're angry. Let me transition you to this sub-draft that deals with negative emotions.\" The user remains in that state until they've acknowledged something, then returns to the main flow.</p> <p>Another approach is to use rubrics and data mining. We extract examples from historical transactions that match specific rubric criteria, then have experts score these examples. These scored examples become few-shot examples for future evaluations.</p> <p>For instance, with venture capital funding requests, we might extract 200 examples of founders discussing how well they know each other, then have experts grade these as good, bad, or great. This creates a training set for evaluating future conversations.</p> <p>The model we build with these rubrics typically extracts scores for 30+ criteria, with LLMs giving scores from 0-4, which then feed into a logistic regression model. This makes the evaluation somewhat explainable - if a candidate gets prioritized unexpectedly, we can see which features drove that decision.</p> <p>Key Takeaway: For evaluating multi-turn conversations, combine state machines to enforce guardrails with human-labeled examples to create scoring rubrics. Using a simple model like logistic regression on top of LLM-generated feature scores maintains interpretability.</p> <p>Key Takeaway</p> <p>For evaluating multi-turn conversations, combine state machines to enforce guardrails with human-labeled examples to create scoring rubrics. Using a simple model like logistic regression on top of LLM-generated feature scores maintains interpretability.</p>"},{"location":"office-hours/faq/#how-do-you-approach-data-analysis-to-find-business-value-in-ai-applications","title":"How do you approach data analysis to find business value in AI applications?","text":"<p>My favorite content in the course is actually weeks 4 and 5, where I cover my process for data analysis and uncovering new capabilities needed in AI systems.</p> <p>When analyzing data from AI applications, I look for two main types of issues:</p> <ol> <li> <p>Inventory issues: These occur when the system lacks the necessary data to fulfill user requests. For example, if users search for content that doesn't exist in your database, the solution isn't to improve the AI - it's to add the missing content. Many companies don't realize their inventory might be the problem rather than their AI.</p> </li> <li> <p>Capabilities issues: These involve functionality gaps where the system can't perform certain types of queries or filters. For instance, you might need to add metadata filters or specialized search capabilities to handle specific user needs.</p> </li> </ol> <p>I've found tremendous business value by identifying these issues through data analysis. In one case with a restaurant voice AI system, we discovered that when the AI attempted upselling, it generated 20% more revenue 50% of the time - a 10% overall increase. However, the agent only tried upselling in 9% of calls.</p> <p>The solution wasn't to improve the AI's core capabilities but to add a simple check ensuring the agent always asks if the customer wants anything else before ending the call. This small change could generate an additional $2 million in revenue by increasing upselling attempts from 9% to 40%.</p> <p>For me, the most enjoyable work is identifying these business opportunities that don't necessarily require complex AI improvements. Software engineers often aren't trained to think this way, but my background in data science makes this approach natural.</p> <p>Key Takeaway: The biggest business value often comes from analyzing usage patterns to identify inventory gaps or missing capabilities, rather than improving core AI performance. Simple changes like adding missing data or implementing basic business rules can deliver millions in value.</p> <p>Key Takeaway</p> <p>The biggest business value often comes from analyzing usage patterns to identify inventory gaps or missing capabilities, rather than improving core AI performance. Simple changes like adding missing data or implementing basic business rules can deliver millions in value.</p>"},{"location":"office-hours/faq/#how-do-you-balance-technical-implementation-with-business-outcomes","title":"How do you balance technical implementation with business outcomes?","text":"<p>I've worked with many companies where they think they want me to make their AI better, but my actual job is to make their business better. There's often substantial money to be captured by focusing on business outcomes rather than technical improvements.</p> <p>For example, with a construction project, I spoke with contractors to understand their actual pain points. While they initially thought they needed better document search, the real issue was tracking delays and identifying who was causing them. This led us to implement contact search with metadata filters - a solution that addressed a $100,000/month problem.</p> <p>Similarly, with Netflix, if users search for \"Oscar-nominated\" movies but get results about Oscar Wilde or actors named Oscar, the solution might not be more sophisticated AI. It could be as simple as paying IMDB for better awards metadata.</p> <p>I'm constantly looking for these opportunities where a relatively simple technical solution can unlock significant business value. This approach has been much more impactful than pursuing technical sophistication for its own sake.</p> <p>Key Takeaway: Focus on business outcomes over technical sophistication. Often the highest-value solutions involve simple changes that address real user needs rather than complex AI improvements.</p> <p>Key Takeaway</p> <p>Focus on business outcomes over technical sophistication. Often the highest-value solutions involve simple changes that address real user needs rather than complex AI improvements.</p>"},{"location":"office-hours/faq/#what-are-your-thoughts-on-the-latest-ai-developments-like-claude-3","title":"What are your thoughts on the latest AI developments like Claude 3?","text":"<p>I'm currently reviewing the entire Instructor codebase to adapt it for Claude 3. The model is making about 15 pull requests for me, so we'll see how that goes.</p> <p>Regarding the guest speakers we've had, I found the Chroma presentation particularly valuable for its hands-on, practical approach. While the Exa presentation was more high-level and story-focused, both offered valuable perspectives.</p> <p>I try to balance technical depth with accessibility in these sessions. When Nils gave his talk, it quickly became very technical with neural network diagrams and mathematical equations, and I could see people leaving the call. It's challenging to find the right balance between technical content and storytelling.</p> <p>Key Takeaway: Balancing technical depth with accessibility is crucial when presenting AI concepts. Different audiences require different approaches to effectively communicate complex ideas.</p> <p>Key Takeaway</p> <p>Balancing technical depth with accessibility is crucial when presenting AI concepts. Different audiences require different approaches to effectively communicate complex ideas.</p>"},{"location":"office-hours/faq/#how-should-we-approach-building-rag-applications-for-course-materials","title":"How should we approach building RAG applications for course materials?","text":"<p>If someone wanted to build a RAG application over all the course transcripts and office hours, I'd love to see that. However, this would quickly reveal the limitations of simple chunking approaches.</p> <p>You'd discover that people have different capability requests - like wanting to know who asked specific questions or what was discussed in a particular week. This would require metadata filters for cohort numbers, transcript types (lectures vs. office hours vs. lightning lessons), and speaker identification.</p> <p>You might also need to handle requests for information about guest speakers, like their LinkedIn profiles. All of these are inventory issues that could be solved by ensuring you have the right metadata alongside your content.</p> <p>For a dataset as small as course transcripts, long-context models like Claude 3 might work well without complex RAG. It's really the enterprise use cases with massive document collections that need sophisticated RAG approaches.</p> <p>Key Takeaway: Even simple datasets like course transcripts reveal the importance of metadata and structured information for effective retrieval. Many issues are inventory problems rather than AI problems.</p> <p>Key Takeaway</p> <p>Even simple datasets like course transcripts reveal the importance of metadata and structured information for effective retrieval. Many issues are inventory problems rather than AI problems.</p>"},{"location":"office-hours/faq/#how-do-you-handle-uiux-development-for-ai-applications","title":"How do you handle UI/UX development for AI applications?","text":"<p>I try to write most things as command line tools - I'm a \"filthy machine learning Python engineer\" who finds any UI to be too much work. Even Streamlit feels excessive to me when a command line interface would suffice.</p> <p>That said, Claude has demonstrated how well you can do with thoughtful UX patterns. In Week 3, I'll talk about UX patterns that make applications feel responsive - like how Claude shows progress counters as it's uploading and downloading tokens, ensuring something on the page is always moving to indicate work is happening.</p> <p>For those who need to build UIs but lack JavaScript skills, LLMs are remarkably good at writing JavaScript. I've built many bespoke data labeling applications in just 5 hours by prompting models to convert JSON structures to PostgreSQL databases and build the corresponding UIs.</p> <p>The software can be ephemeral enough that I don't worry about long-term maintenance. For more polished applications, I recommend checking out Lovable.dev - I've built about 20 apps with them that work quite well.</p> <p>Key Takeaway: Focus on learning the concepts rather than specific implementation details. Modern LLMs can generate high-quality UI code, making it easier than ever to build functional applications without deep frontend expertise.</p> <p>Key Takeaway</p> <p>Focus on learning the concepts rather than specific implementation details. Modern LLMs can generate high-quality UI code, making it easier than ever to build functional applications without deep frontend expertise.</p>"},{"location":"office-hours/faq/#whats-been-your-most-rewarding-project-in-the-ai-space","title":"What's been your most rewarding project in the AI space?","text":"<p>My background is in physics - I initially thought the universe would generate the most interesting datasets. Then I went to Facebook because I believed people-to-people interactions would be the most fascinating data. Now I'm focused on people-to-AI interactions, and in the future, it will likely be AI-to-AI interactions. I'm essentially chasing the most interesting datasets I can analyze.</p> <p>The most rewarding projects have been those where data analysis revealed clear business opportunities. For instance, with the restaurant voice AI system, identifying the upselling opportunity was straightforward but incredibly valuable.</p> <p>I enjoy working with teams that have access to subject matter experts who can help interpret the data. For the construction project, I spoke with contractors wearing hard hats on Zoom to understand why certain questions were valuable and what problems they were trying to solve.</p> <p>This approach of combining data analysis with domain expertise has consistently led to high-impact solutions that address real business needs rather than just technical challenges.</p> <p>Key Takeaway: The most rewarding AI projects combine data analysis with domain expertise to identify high-impact business opportunities rather than pursuing technical sophistication for its own sake.</p> <p>Key Takeaway</p> <p>The most rewarding AI projects combine data analysis with domain expertise to identify high-impact business opportunities rather than pursuing technical sophistication for its own sake.</p>"},{"location":"office-hours/faq/#final-thoughts-on-balancing-course-workload","title":"Final thoughts on balancing course workload","text":"<p>I recognize that the course material can be overwhelming, especially for those balancing it with full-time jobs. We'll have no notebooks in Week 3, which should provide a buffer, and you'll always have access to the Slack channel even after the 6 weeks are over.</p> <p>For those feeling overwhelmed, remember that many people take multiple cohorts to fully absorb the material. The flexible structure is intentional - unlike more prescriptive courses, this approach allows you to focus on what's most relevant to your specific needs.</p> <p>As one participant noted, they've found at least one \"golden nugget\" from each session so far, including the introduction where I presented the \"sandwich view\" of RAG systems. These conceptual frameworks can provide clarity when you're deep in implementation details.</p> <p>Remember that the AI field is moving incredibly quickly, and none of us can absorb everything. The goal isn't to become an expert on everything but to get really good at leveraging AI to stay ahead of everyone else.</p> <p>Key Takeaway: Learning complex technical skills requires finding the right balance between depth of content and time for absorption. Focus on what's most relevant to your needs and remember that continuous learning is more important than perfect comprehension.</p> <p>FAQs</p> <p>Key Takeaway</p> <p>Learning complex technical skills requires finding the right balance between depth of content and time for absorption. Focus on what's most relevant to your needs and remember that continuous learning is more important than perfect comprehension.</p>"},{"location":"office-hours/faq/#how-can-i-balance-this-course-with-my-day-job","title":"How can I balance this course with my day job?","text":"<p>Managing this course alongside your regular work can be challenging. Many students find success by aligning the course with existing work projects, allowing them to apply what they're learning directly to their professional tasks. If you don't have a relevant project, the course notebooks provide boilerplate code you can use as a starting point. Remember that Week 3 has no notebooks, which gives you a buffer to catch up if needed. The course is designed with some flexibility, so you can prioritize the most relevant content for your needs.</p>"},{"location":"office-hours/faq/#what-should-i-do-if-i-dont-have-a-specific-project-to-apply-the-course-material-to","title":"What should I do if I don't have a specific project to apply the course material to?","text":"<p>You can start with the boilerplate code provided in the notebooks. These are designed to demonstrate key concepts even without a specific application in mind. Additionally, consider looking for datasets from colleagues or within your organization that might benefit from the techniques taught in the course. Many people have conversation data or other information they're not sure how to leverage effectively. The course materials are structured to help you experiment with these techniques regardless of whether you have a specific project.</p>"},{"location":"office-hours/faq/#how-are-the-course-materials-structured","title":"How are the course materials structured?","text":"<p>The course includes lecture videos, notebooks with code examples, office hours, and summary notes. Each set of notebooks focuses on a specific theme or concept, such as synthetic data generation or evaluation metrics. The notebooks are designed to be practical and applicable to real-world scenarios. Week 3 has no notebooks, providing a buffer period. Weeks 4-5 focus on data analysis processes and building specific tools based on identified needs. The course also includes guest lectures from industry experts to provide different perspectives.</p>"},{"location":"office-hours/faq/#where-can-i-find-the-summary-notes-and-faqs","title":"Where can I find the summary notes and FAQs?","text":"<p>Currently, summary notes are posted in Slack, but they will eventually be available in Notion or another website format. Many students find these notes helpful as they allow them to focus more on understanding the content rather than taking extensive notes during lectures.</p>"},{"location":"office-hours/faq/#whats-the-instructors-approach-to-evaluating-rag-applications","title":"What's the instructor's approach to evaluating RAG applications?","text":"<p>The instructor emphasizes a data-driven approach to evaluations rather than relying on subjective assessments. This includes measuring precision and recall for different numbers of retrieved documents, understanding how models respond to ambiguous information, and using metrics to make informed decisions about system design. The instructor discourages using adjectives to describe performance and instead encourages teams to use numbers, plots, and quantifiable metrics to evaluate their systems.</p>"},{"location":"office-hours/faq/#how-can-small-language-models-be-used-in-a-rag-architecture","title":"How can small language models be used in a RAG architecture?","text":"<p>Small language models can serve several purposes in a RAG architecture. They can be used to quickly rewrite queries, breaking them down into more structured formats. They can help build better embedding spaces or re-rankers that are fine-tuned for specific tasks. Small language models generally offer lower latency with less world knowledge, making them suitable for specific components of a RAG system where full context understanding isn't necessary.</p>"},{"location":"office-hours/faq/#what-are-the-most-valuable-insights-from-the-course-so-far","title":"What are the most valuable insights from the course so far?","text":"<p>Many students highlight the \"sandwich view\" of RAG systems (where RAG is presented as a recommendation system between LLM layers) as particularly insightful. The course provides practical \"golden nuggets\" in each session, including frameworks for thinking about RAG applications, evaluation techniques, and implementation strategies. The balance between technical details and storytelling across different guest lectures has been valuable for understanding both theoretical concepts and practical applications.</p>"},{"location":"office-hours/faq/#whats-the-instructors-perspective-on-building-uiux-for-llm-applications","title":"What's the instructor's perspective on building UI/UX for LLM applications?","text":"<p>The instructor suggests focusing on understanding concepts rather than specific UI technologies. Command-line tools can be highly effective for many applications, and modern LLMs are excellent at generating JavaScript and other frontend code when needed. Understanding server-sent events and streaming is particularly important for creating responsive LLM applications. The instructor emphasizes that streaming is essential for good user experience - applications without streaming capabilities are generally considered subpar in the current landscape.</p>"},{"location":"office-hours/faq/#how-does-the-instructor-approach-business-value-in-ai-projects","title":"How does the instructor approach business value in AI projects?","text":"<p>The instructor focuses on identifying business value through data analysis rather than just improving AI capabilities. This involves analyzing user interactions, identifying patterns, and determining whether issues are related to inventory (missing data) or capabilities (features the system can't perform). Often, the most valuable insights come from discovering simple business improvements that don't require complex AI solutions. The instructor recommends working closely with subject matter experts to understand the real business needs behind technical requirements.</p>"},{"location":"office-hours/faq/#will-there-be-opportunities-to-continue-learning-after-the-course-ends","title":"Will there be opportunities to continue learning after the course ends?","text":"<p>Yes, students will still have access to Slack after the 6-week course concludes, and the instructor encourages continued questions. Additionally, students can join future cohorts of the course if they need more time to absorb the material. Many students find they benefit from going through the content multiple times as the field evolves.</p>"},{"location":"office-hours/faq/#how-should-we-understand-precision-in-the-context-of-llms-and-rag_1","title":"How should we understand precision in the context of LLMs and RAG?","text":"<p>Precision has become increasingly important as most people are comfortable adding substantial information to the context window. The key question is how sensitive these models are to ambiguous or irrelevant information.</p> <p>Most current models are very good at recall because they're optimized for \"needle in the haystack\" tests. Due to the attention mechanism with sliding windows, they can typically catch important information even when it's buried among other content. However, the sensitivity to irrelevant information is less well-studied.</p> <p>As we increase the amount of data in the context, we need to understand what precision looks like and how it correlates with factuality or answer quality. Earlier models like GPT-3.5 were quite sensitive to long context with low precision - if you gave them too much information, they would \"overthink\" because of the volume of content to process.</p> <p>This is why it's valuable to experiment with different retrieval settings: \"For the top 10 documents, this is my precision and recall; for my top 25 documents, this is my precision and recall. If my recall is not increasing as a function of K (the number of documents), that's where I decide to set a threshold.\"</p> <p>Some people set thresholds based on re-ranker scores, but that can be dangerous since these scores aren't true probabilities. You can't just set 0.5 as a universal threshold - you need to understand the precision-recall tradeoffs for your specific application.</p> <p>Key Takeaway: Models can be sensitive to low-precision context, where irrelevant information causes them to incorporate red herrings into answers. Testing different document count thresholds is more reliable than using arbitrary re-ranker score cutoffs.</p> <p>Key Takeaway</p> <p>Models can be sensitive to low-precision context, where irrelevant information causes them to incorporate red herrings into answers. Testing different document count thresholds is more reliable than using arbitrary re-ranker score cutoffs.</p>"},{"location":"office-hours/faq/#what-role-can-small-language-models-play-in-a-rag-architecture_1","title":"What role can small language models play in a RAG architecture?","text":"<p>Small language models can serve several valuable functions within a RAG system:</p> <p>First, they can rewrite queries quickly and efficiently. For example, if a user asks \"What is happening this week?\", a small model could convert this into a structured query like a JSON object specifying \"search all documents where the datetime is between today and today minus 7 days.\" This type of entity resolution and query parsing doesn't require the full knowledge of a large model but benefits from the lower latency of smaller models.</p> <p>Second, small models can build better embedding spaces. Most current embedding models are relatively simple, but a small language model fine-tuned on your specific task could significantly improve embedding quality or re-ranking performance.</p> <p>In this context, I think of \"small\" as meaning lower latency with less world knowledge - models that can perform specific tasks efficiently without needing to understand everything about the world.</p>"},{"location":"office-hours/faq/#how-can-we-measure-quality-in-multi-turn-conversations_1","title":"How can we measure quality in multi-turn conversations?","text":"<p>When evaluating multi-turn exchanges like simulated customer interactions or teacher training scenarios, there are several approaches to consider.</p> <p>One approach is to model the interaction as a state machine or \"LangDraft\" type model where there are defined states that can be traversed. For example, in a customer support scenario, you might have an intake state, followed by various question states, triage states, and resolution states.</p> <p>We've used this with \"LLM therapists\" where the system might say, \"I can tell you're angry. Let me transition you to this sub-draft that deals with negative emotions.\" The user remains in that state until they've acknowledged something, then returns to the main flow.</p> <p>Another approach is to use rubrics and data mining. We extract examples from historical transactions that match specific rubric criteria, then have experts score these examples. These scored examples become few-shot examples for future evaluations.</p> <p>For instance, with venture capital funding requests, we might extract 200 examples of founders discussing how well they know each other, then have experts grade these as good, bad, or great. This creates a training set for evaluating future conversations.</p> <p>The model we build with these rubrics typically extracts scores for 30+ criteria, with LLMs giving scores from 0-4, which then feed into a logistic regression model. This makes the evaluation somewhat explainable - if a candidate gets prioritized unexpectedly, we can see which features drove that decision.</p> <p>Key Takeaway: For evaluating multi-turn conversations, combine state machines to enforce guardrails with human-labeled examples to create scoring rubrics. Using a simple model like logistic regression on top of LLM-generated feature scores maintains interpretability.</p> <p>Key Takeaway</p> <p>For evaluating multi-turn conversations, combine state machines to enforce guardrails with human-labeled examples to create scoring rubrics. Using a simple model like logistic regression on top of LLM-generated feature scores maintains interpretability.</p>"},{"location":"office-hours/faq/#how-do-you-approach-data-analysis-to-find-business-value-in-ai-applications_1","title":"How do you approach data analysis to find business value in AI applications?","text":"<p>My favorite content in the course is actually weeks 4 and 5, where I cover my process for data analysis and uncovering new capabilities needed in AI systems.</p> <p>When analyzing data from AI applications, I look for two main types of issues:</p> <ol> <li> <p>Inventory issues: These occur when the system lacks the necessary data to fulfill user requests. For example, if users search for content that doesn't exist in your database, the solution isn't to improve the AI - it's to add the missing content. Many companies don't realize their inventory might be the problem rather than their AI.</p> </li> <li> <p>Capabilities issues: These involve functionality gaps where the system can't perform certain types of queries or filters. For instance, you might need to add metadata filters or specialized search capabilities to handle specific user needs.</p> </li> </ol> <p>I've found tremendous business value by identifying these issues through data analysis. In one case with a restaurant voice AI system, we discovered that when the AI attempted upselling, it generated 20% more revenue 50% of the time - a 10% overall increase. However, the agent only tried upselling in 9% of calls.</p> <p>The solution wasn't to improve the AI's core capabilities but to add a simple check ensuring the agent always asks if the customer wants anything else before ending the call. This small change could generate an additional $2 million in revenue by increasing upselling attempts from 9% to 40%.</p> <p>For me, the most enjoyable work is identifying these business opportunities that don't necessarily require complex AI improvements. Software engineers often aren't trained to think this way, but my background in data science makes this approach natural.</p> <p>Key Takeaway: The biggest business value often comes from analyzing usage patterns to identify inventory gaps or missing capabilities, rather than improving core AI performance. Simple changes like adding missing data or implementing basic business rules can deliver millions in value.</p> <p>Key Takeaway</p> <p>The biggest business value often comes from analyzing usage patterns to identify inventory gaps or missing capabilities, rather than improving core AI performance. Simple changes like adding missing data or implementing basic business rules can deliver millions in value.</p>"},{"location":"office-hours/faq/#how-do-you-balance-technical-implementation-with-business-outcomes_1","title":"How do you balance technical implementation with business outcomes?","text":"<p>I've worked with many companies where they think they want me to make their AI better, but my actual job is to make their business better. There's often substantial money to be captured by focusing on business outcomes rather than technical improvements.</p> <p>For example, with a construction project, I spoke with contractors to understand their actual pain points. While they initially thought they needed better document search, the real issue was tracking delays and identifying who was causing them. This led us to implement contact search with metadata filters - a solution that addressed a $100,000/month problem.</p> <p>Similarly, with Netflix, if users search for \"Oscar-nominated\" movies but get results about Oscar Wilde or actors named Oscar, the solution might not be more sophisticated AI. It could be as simple as paying IMDB for better awards metadata.</p> <p>I'm constantly looking for these opportunities where a relatively simple technical solution can unlock significant business value. This approach has been much more impactful than pursuing technical sophistication for its own sake.</p>"},{"location":"office-hours/faq/#what-are-your-thoughts-on-the-latest-ai-developments-like-claude-3_1","title":"What are your thoughts on the latest AI developments like Claude 3?","text":"<p>I'm currently reviewing the entire Instructor codebase to adapt it for Claude 3. The model is making about 15 pull requests for me, so we'll see how that goes.</p> <p>Regarding the guest speakers we've had, I found the Chroma presentation particularly valuable for its hands-on, practical approach. While the Exa presentation was more high-level and story-focused, both offered valuable perspectives.</p> <p>I try to balance technical depth with accessibility in these sessions. When Nils gave his talk, it quickly became very technical with neural network diagrams and mathematical equations, and I could see people leaving the call. It's challenging to find the right balance between technical content and storytelling.</p>"},{"location":"office-hours/faq/#how-should-we-approach-building-rag-applications-for-course-materials_1","title":"How should we approach building RAG applications for course materials?","text":"<p>If someone wanted to build a RAG application over all the course transcripts and office hours, I'd love to see that. However, this would quickly reveal the limitations of simple chunking approaches.</p> <p>You'd discover that people have different capability requests - like wanting to know who asked specific questions or what was discussed in a particular week. This would require metadata filters for cohort numbers, transcript types (lectures vs. office hours vs. lightning lessons), and speaker identification.</p> <p>You might also need to handle requests for information about guest speakers, like their LinkedIn profiles. All of these are inventory issues that could be solved by ensuring you have the right metadata alongside your content.</p> <p>For a dataset as small as course transcripts, long-context models like Claude 3 might work well without complex RAG. It's really the enterprise use cases with massive document collections that need sophisticated RAG approaches.</p>"},{"location":"office-hours/faq/#how-do-you-handle-uiux-development-for-ai-applications_1","title":"How do you handle UI/UX development for AI applications?","text":"<p>I try to write most things as command line tools - I'm a \"filthy machine learning Python engineer\" who finds any UI to be too much work. Even Streamlit feels excessive to me when a command line interface would suffice.</p> <p>That said, Claude has demonstrated how well you can do with thoughtful UX patterns. In Week 3, I'll talk about UX patterns that make applications feel responsive - like how Claude shows progress counters as it's uploading and downloading tokens, ensuring something on the page is always moving to indicate work is happening.</p> <p>For those who need to build UIs but lack JavaScript skills, LLMs are remarkably good at writing JavaScript. I've built many bespoke data labeling applications in just 5 hours by prompting models to convert JSON structures to PostgreSQL databases and build the corresponding UIs.</p> <p>The software can be ephemeral enough that I don't worry about long-term maintenance. For more polished applications, I recommend checking out Lovable.dev - I've built about 20 apps with them that work quite well.</p> <p>Key Takeaway: Focus on learning the concepts rather than specific implementation details. Modern LLMs can generate high-quality UI code, making it easier than ever to build functional applications without deep frontend expertise.</p> <p>Key Takeaway</p> <p>Focus on learning the concepts rather than specific implementation details. Modern LLMs can generate high-quality UI code, making it easier than ever to build functional applications without deep frontend expertise.</p>"},{"location":"office-hours/faq/#whats-been-your-most-rewarding-project-in-the-ai-space_1","title":"What's been your most rewarding project in the AI space?","text":"<p>My background is in physics - I initially thought the universe would generate the most interesting datasets. Then I went to Facebook because I believed people-to-people interactions would be the most fascinating data. Now I'm focused on people-to-AI interactions, and in the future, it will likely be AI-to-AI interactions. I'm essentially chasing the most interesting datasets I can analyze.</p> <p>The most rewarding projects have been those where data analysis revealed clear business opportunities. For instance, with the restaurant voice AI system, identifying the upselling opportunity was straightforward but incredibly valuable.</p> <p>I enjoy working with teams that have access to subject matter experts who can help interpret the data. For the construction project, I spoke with contractors wearing hard hats on Zoom to understand why certain questions were valuable and what problems they were trying to solve.</p> <p>This approach of combining data analysis with domain expertise has consistently led to high-impact solutions that address real business needs rather than just technical challenges.</p>"},{"location":"office-hours/faq/#final-thoughts-on-balancing-course-workload_1","title":"Final thoughts on balancing course workload","text":"<p>I recognize that the course material can be overwhelming, especially for those balancing it with full-time jobs. We'll have no notebooks in Week 3, which should provide a buffer, and you'll always have access to the Slack channel even after the 6 weeks are over.</p> <p>For those feeling overwhelmed, remember that many people take multiple cohorts to fully absorb the material. The flexible structure is intentional - unlike more prescriptive courses, this approach allows you to focus on what's most relevant to your specific needs.</p> <p>As one participant noted, they've found at least one \"golden nugget\" from each session so far, including the introduction where I presented the \"sandwich view\" of RAG systems. These conceptual frameworks can provide clarity when you're deep in implementation details.</p> <p>Remember that the AI field is moving incredibly quickly, and none of us can absorb everything. The goal isn't to become an expert on everything but to get really good at leveraging AI to stay ahead of everyone else.</p> <p>FAQs</p>"},{"location":"office-hours/faq/#how-can-i-balance-this-course-with-my-day-job_1","title":"How can I balance this course with my day job?","text":"<p>Managing this course alongside your regular work can be challenging. Many students find success by aligning the course with existing work projects, allowing them to apply what they're learning directly to their professional tasks. If you don't have a relevant project, the course notebooks provide boilerplate code you can use as a starting point. Remember that Week 3 has no notebooks, which gives you a buffer to catch up if needed. The course is designed with some flexibility, so you can prioritize the most relevant content for your needs.</p>"},{"location":"office-hours/faq/#what-should-i-do-if-i-dont-have-a-specific-project-to-apply-the-course-material-to_1","title":"What should I do if I don't have a specific project to apply the course material to?","text":"<p>You can start with the boilerplate code provided in the notebooks. These are designed to demonstrate key concepts even without a specific application in mind. Additionally, consider looking for datasets from colleagues or within your organization that might benefit from the techniques taught in the course. Many people have conversation data or other information they're not sure how to leverage effectively. The course materials are structured to help you experiment with these techniques regardless of whether you have a specific project.</p>"},{"location":"office-hours/faq/#how-are-the-course-materials-structured_1","title":"How are the course materials structured?","text":"<p>The course includes lecture videos, notebooks with code examples, office hours, and summary notes. Each set of notebooks focuses on a specific theme or concept, such as synthetic data generation or evaluation metrics. The notebooks are designed to be practical and applicable to real-world scenarios. Week 3 has no notebooks, providing a buffer period. Weeks 4-5 focus on data analysis processes and building specific tools based on identified needs. The course also includes guest lectures from industry experts to provide different perspectives.</p>"},{"location":"office-hours/faq/#where-can-i-find-the-summary-notes-and-faqs_1","title":"Where can I find the summary notes and FAQs?","text":"<p>Currently, summary notes are posted in Slack, but they will eventually be available in Notion or another website format. Many students find these notes helpful as they allow them to focus more on understanding the content rather than taking extensive notes during lectures.</p>"},{"location":"office-hours/faq/#whats-the-instructors-approach-to-evaluating-rag-applications_1","title":"What's the instructor's approach to evaluating RAG applications?","text":"<p>The instructor emphasizes a data-driven approach to evaluations rather than relying on subjective assessments. This includes measuring precision and recall for different numbers of retrieved documents, understanding how models respond to ambiguous information, and using metrics to make informed decisions about system design. The instructor discourages using adjectives to describe performance and instead encourages teams to use numbers, plots, and quantifiable metrics to evaluate their systems.</p>"},{"location":"office-hours/faq/#how-can-small-language-models-be-used-in-a-rag-architecture_1","title":"How can small language models be used in a RAG architecture?","text":"<p>Small language models can serve several purposes in a RAG architecture. They can be used to quickly rewrite queries, breaking them down into more structured formats. They can help build better embedding spaces or re-rankers that are fine-tuned for specific tasks. Small language models generally offer lower latency with less world knowledge, making them suitable for specific components of a RAG system where full context understanding isn't necessary.</p>"},{"location":"office-hours/faq/#what-are-the-most-valuable-insights-from-the-course-so-far_1","title":"What are the most valuable insights from the course so far?","text":"<p>Many students highlight the \"sandwich view\" of RAG systems (where RAG is presented as a recommendation system between LLM layers) as particularly insightful. The course provides practical \"golden nuggets\" in each session, including frameworks for thinking about RAG applications, evaluation techniques, and implementation strategies. The balance between technical details and storytelling across different guest lectures has been valuable for understanding both theoretical concepts and practical applications.</p>"},{"location":"office-hours/faq/#whats-the-instructors-perspective-on-building-uiux-for-llm-applications_1","title":"What's the instructor's perspective on building UI/UX for LLM applications?","text":"<p>The instructor suggests focusing on understanding concepts rather than specific UI technologies. Command-line tools can be highly effective for many applications, and modern LLMs are excellent at generating JavaScript and other frontend code when needed. Understanding server-sent events and streaming is particularly important for creating responsive LLM applications. The instructor emphasizes that streaming is essential for good user experience - applications without streaming capabilities are generally considered subpar in the current landscape.</p>"},{"location":"office-hours/faq/#how-does-the-instructor-approach-business-value-in-ai-projects_1","title":"How does the instructor approach business value in AI projects?","text":"<p>The instructor focuses on identifying business value through data analysis rather than just improving AI capabilities. This involves analyzing user interactions, identifying patterns, and determining whether issues are related to inventory (missing data) or capabilities (features the system can't perform). Often, the most valuable insights come from discovering simple business improvements that don't require complex AI solutions. The instructor recommends working closely with subject matter experts to understand the real business needs behind technical requirements.</p>"},{"location":"office-hours/faq/#will-there-be-opportunities-to-continue-learning-after-the-course-ends_1","title":"Will there be opportunities to continue learning after the course ends?","text":"<p>Yes, students will still have access to Slack after the 6-week course concludes, and the instructor encourages continued questions. Additionally, students can join future cohorts of the course if they need more time to absorb the material. Many students find they benefit from going through the content multiple times as the field evolves.</p>"},{"location":"office-hours/faq/#how-should-i-approach-medical-rag-systems-with-complex-queries","title":"How should I approach medical RAG systems with complex queries?","text":"<p>When dealing with specialized domains like medical records where users ask comprehensive questions (e.g., \"Give a complete medical history of patient X\"), the key is understanding that you can't just throw everything into a generic RAG system and expect good results.</p> <p>I've found that building separate indices for different document categories is essential. For example, with an oil and gas client I'm working with, we're processing millions of PDFs but categorizing them into about five different types: drill logs, specifications, geospatial diagrams, etc.</p> <p>Within each category, we extract specific data structures. For drill logs, we identify that some pages represent different days of logging, so we extract metadata like dates, drill IDs, and personnel information. This allows us to build specialized tools for querying each data type effectively.</p> <p>For medical history queries, you'd want to create structured data that can be queried directly - essentially turning it into a \"SELECT * FROM medical_history WHERE client_id = X\" type of operation rather than relying on semantic search across unstructured text.</p> <p>Key Takeaway: Don't try to build one universal RAG system. Instead, identify the categories of documents in your domain, extract relevant structured data from each category, and build specialized tools to query that structured data effectively.</p> <p>Key Takeaway</p> <p>Don't try to build one universal RAG system. Instead, identify the categories of documents in your domain, extract relevant structured data from each category, and build specialized tools to query that structured data effectively.</p>"},{"location":"office-hours/faq/#whats-the-best-approach-to-handling-citations-in-rag-systems","title":"What's the best approach to handling citations in RAG systems?","text":"<p>When your LLM isn't reliable for generating citations and semantic/fuzzy similarity doesn't work well (particularly in domains with many abbreviations like medicine), you need a more structured approach.</p> <p>I recommend following Claude's citation approach, which uses XML tags to wrap citations. When you create statements, include XML that references the source ID. In your text chunks, you'll have chunk IDs and other metadata alongside the content.</p> <p>To make this more precise, especially with longer contexts, include the first three words and last three words of the cited span. For example, if citing \"similarity isn't reliable either for our use case,\" the citation would include both the chunk ID and \"start is similarity isn't reliable, end is for our use case.\"</p> <p>This approach works well with fine-tuning. We implemented something similar in Instructor, where an answer is structured as a list of facts, each with a substring quote, ensuring alignment between facts and quotes to minimize hallucinations.</p> <p>Key Takeaway: Structure your citations with explicit references to chunk IDs and text spans rather than relying on similarity matching. This approach can be implemented through fine-tuning and provides much more reliable attribution.</p> <p>Key Takeaway</p> <p>Structure your citations with explicit references to chunk IDs and text spans rather than relying on similarity matching. This approach can be implemented through fine-tuning and provides much more reliable attribution.</p>"},{"location":"office-hours/faq/#should-i-use-graph-based-rag-approaches","title":"Should I use graph-based RAG approaches?","text":"<p>I'm generally skeptical about graph-based RAG systems. In my experience with data analysis over many years, graph databases often fall away in favor of embeddings and SQL databases.</p> <p>The main challenge with graph RAG is that building out the taxonomy is often harder than you expect. You think you're avoiding the complexity of embedding models, but you're just substituting it with the problem of modeling out the taxonomy, which can be equally challenging.</p> <p>For most use cases where you might consider a graph, you can achieve similar results with a few SQL joins. Unless you need to do complex traversals (like LinkedIn finding connections-of-connections), the overhead of learning graph query languages and modeling data as graphs usually isn't worth it.</p> <p>Even Facebook, despite being fundamentally a social graph, uses a very large-scale MySQL instance rather than a dedicated graph database. If you only need one-way traversals, a standard relational database is typically sufficient.</p> <p>Key Takeaway: Unless your use case requires complex multi-step graph traversals, you're likely better off using embeddings with SQL databases rather than implementing a graph-based RAG system. The taxonomy development often becomes more complex than the embedding approach you were trying to avoid.</p> <p>Key Takeaway</p> <p>Unless your use case requires complex multi-step graph traversals, you're likely better off using embeddings with SQL databases rather than implementing a graph-based RAG system. The taxonomy development often becomes more complex than the embedding approach you were trying to avoid.</p>"},{"location":"office-hours/faq/#how-do-you-recommend-clustering-and-categorizing-user-queries","title":"How do you recommend clustering and categorizing user queries?","text":"<p>For understanding what users are asking about, we've developed a library called Cura (similar to Anthropic's Clio) that performs population-level analysis of conversation history.</p> <p>The process works like this:</p> <ol> <li>We summarize every conversation</li> <li>We extract key information: languages used, topics, tasks, requests, user complaints, and assistant errors</li> <li>We concatenate everything and create embeddings</li> <li>We perform clustering to identify patterns</li> <li>We use a language model to group and label clusters</li> </ol> <p>This approach gives you insights into what people are asking for, how big each cluster is, and metrics like error rates or user satisfaction for different types of queries. You can then identify which clusters are performing well and which need improvement, helping you decide where to invest in new tools or capabilities.</p> <p>We're releasing a new version of Cura soon with better ergonomics and UI for exploration. This will be covered in more detail in Week 4 of the course.</p> <p>Key Takeaway: Systematic analysis of user queries through summarization, extraction, embedding, and clustering helps identify patterns in how people use your system, allowing you to prioritize improvements where they'll have the most impact.</p> <p>Key Takeaway</p> <p>Systematic analysis of user queries through summarization, extraction, embedding, and clustering helps identify patterns in how people use your system, allowing you to prioritize improvements where they'll have the most impact.</p>"},{"location":"office-hours/faq/#whats-your-recommendation-for-chunking-documentation","title":"What's your recommendation for chunking documentation?","text":"<p>When dealing with documentation PDFs containing tables, definitions, paragraphs, and figures, I challenge the conventional wisdom about chunking. For documentation, I believe the chunk should often be the size of the document page.</p> <p>The right question to ask is \"which page do I need to look on?\" rather than trying to break documents into arbitrary chunks. Modern models are large enough to handle page-sized chunks, and documentation typically uses consistent terminology (unlike cases where semantic search helps bridge vocabulary differences).</p> <p>By combining semantic and lexical search and focusing on page-level retrieval, you can often get better results than with smaller chunks. This approach also respects the semantic boundaries that document authors typically maintain - they rarely split headers from content across pages or break logical sections in awkward places.</p> <p>Key Takeaway: For documentation, consider using page-level chunking rather than arbitrary token-based chunking. This respects the document's inherent structure and works well when combined with both semantic and lexical search approaches.</p> <p>Key Takeaway</p> <p>For documentation, consider using page-level chunking rather than arbitrary token-based chunking. This respects the document's inherent structure and works well when combined with both semantic and lexical search approaches.</p>"},{"location":"office-hours/faq/#what-are-the-trade-offs-between-different-vector-database-options","title":"What are the trade-offs between different vector database options?","text":"<p>I generally prefer using Postgres with pgvector because it allows me to join on different tables, which is extremely valuable. However, pgvector doesn't do exhaustive search by default, which can be a limitation with large datasets.</p> <p>If you're dealing with very large vector collections, consider Timescale's pgvector_scale, which has better streaming methods for exhaustive search. Another advantage of the Postgres approach is that you can install pg_search from PostgresML to get BM25 implementation, giving you both vector search and lexical search in the same database.</p> <p>This combination of vector search and lexical search in a single database that also supports filtering by metadata (like dates or access permissions) is powerful for real-world applications.</p> <p>Key Takeaway: Postgres with pgvector provides a good balance of functionality for most RAG applications, especially when combined with pg_search for lexical search. For very large datasets, consider specialized extensions like pgvector_scale.</p> <p>Key Takeaway</p> <p>Postgres with pgvector provides a good balance of functionality for most RAG applications, especially when combined with pg_search for lexical search. For very large datasets, consider specialized extensions like pgvector_scale.</p>"},{"location":"office-hours/faq/#how-do-you-approach-building-economically-valuable-ai-systems","title":"How do you approach building economically valuable AI systems?","text":"<p>When building AI systems, I focus on economic value rather than just time savings. Time savings is bounded - you can only save as much time as someone currently spends. But economic value can be much larger.</p> <p>For example, with construction blueprints, we realized that simply answering questions about window heights wasn't that valuable - it just saved a worker a few minutes. But by extracting structured data about room counts, building lines, and floor numbers, we could quickly locate the right blueprints when workers were on site, preventing costly delays.</p> <p>In another case, we built voice agents that call car owners to schedule maintenance appointments. Rather than charging by call duration, the system charges a percentage of what the mechanic makes. This aligns incentives - the AI provider is motivated to resolve phone numbers correctly, ensure calendar synchronization works, and get customers to actually show up.</p> <p>The most valuable systems help make better decisions, not just answer questions. If you're building a hiring assistant, don't just price based on tokens used - think about what a bad hire costs a company and how much value your system provides by helping them avoid that outcome.</p> <p>Key Takeaway: Focus on building systems that drive economic value through better decision-making rather than just answering questions or saving time. Structure your pricing to align with the value you create, such as taking a percentage of revenue generated or costs avoided.</p> <p>Key Takeaway</p> <p>Focus on building systems that drive economic value through better decision-making rather than just answering questions or saving time. Structure your pricing to align with the value you create, such as taking a percentage of revenue generated or costs avoided.</p>"},{"location":"office-hours/faq/#how-did-you-handle-blueprint-analysis-for-construction-projects","title":"How did you handle blueprint analysis for construction projects?","text":"<p>For a construction project involving blueprints, we realized through user query analysis that workers needed to find specific documents based on their location in a building. They'd say things like \"I'm on the 40th floor in a room with two bedrooms and a bathroom on the north-facing side - find me the schemas for the windows.\"</p> <p>We built a system that extracted structured data from blueprints: which building, which floor, which \"line\" (position in the building), room counts, and directional orientation. This required a combination of bounding box models and LLMs to identify and extract this information.</p> <p>The challenge was proving that we needed to invest in these specialized models. By analyzing the types of questions being asked, we could justify building tools that could count rooms, identify directions, and extract other key metadata that made retrieval much more effective.</p> <p>For specialized domains like blueprints, it's crucial to understand the specific queries users have and build structured data models that directly address those needs rather than relying on generic text embeddings.</p> <p>Key Takeaway: For specialized visual content like blueprints, invest in extracting structured data that matches the way users think about and query the information. This often requires specialized models beyond general-purpose LLMs, but the investment pays off in much more effective retrieval.</p> <p>Key Takeaway</p> <p>For specialized visual content like blueprints, invest in extracting structured data that matches the way users think about and query the information. This often requires specialized models beyond general-purpose LLMs, but the investment pays off in much more effective retrieval.</p>"},{"location":"office-hours/faq/#final-thoughts-on-building-effective-rag-systems","title":"Final thoughts on building effective RAG systems","text":"<p>The most successful RAG implementations I've seen share a few common characteristics:</p> <ol> <li>They don't try to build one universal system but instead create specialized tools for different document categories and query types</li> <li>They extract structured data that matches the way users think about and query information</li> <li>They combine multiple search approaches - semantic, lexical, and metadata filtering</li> <li>They focus on delivering economic value, not just answering questions</li> <li>They evolve based on systematic analysis of user queries and pain points</li> </ol> <p>As we continue to develop these systems, I expect to see more specialized, domain-specific implementations that go beyond generic question-answering to provide decision support and drive measurable business outcomes. The future of these agents will be selling work and outcomes, not just time and tokens.</p> <p>FAQs</p>"},{"location":"office-hours/faq/#what-approach-should-i-take-for-medical-rag-with-complex-queries","title":"What approach should I take for medical RAG with complex queries?","text":"<p>For complex medical queries like \"give a complete medical history of patient,\" a generic chunking approach isn't sufficient. Instead, build separate indices for different categories of documents and create specific data structures for each type. For example, with medical records, you might create distinct structures for doctor's visits, referral letters, and prescriptions. This allows you to develop targeted tools that can directly query these structures rather than relying on general semantic search across all documents.</p>"},{"location":"office-hours/faq/#how-should-i-handle-citations-in-my-llm-responses","title":"How should I handle citations in my LLM responses?","text":"<p>When implementing citations in LLM responses, consider using an XML-based approach similar to Claude's citation system. This involves wrapping citations with XML tags that reference the source chunk ID along with the first and last few words of the cited span. For fine-tuned models, you can train the model to output citations in this format, which provides more precise references than simple chunk IDs. This approach works well even when the model rephrases information from abbreviation-heavy medical texts.</p>"},{"location":"office-hours/faq/#what-are-your-thoughts-on-graph-based-rag-versus-traditional-approaches","title":"What are your thoughts on graph-based RAG versus traditional approaches?","text":"<p>While graph-based RAG sounds promising, it often substitutes one complex problem (embedding models) with another (taxonomy modeling). For most use cases, a well-structured SQL database with appropriate joins is more practical than implementing a graph database. Graph databases require learning new query languages and modeling approaches, which adds significant overhead. Unless you need complex multi-step traversals (like LinkedIn's connection finder), the benefits rarely outweigh the costs. Most \"graph-like\" relationships can be effectively modeled with standard SQL joins.</p>"},{"location":"office-hours/faq/#how-should-i-approach-chunking-for-documentation-based-rag","title":"How should I approach chunking for documentation-based RAG?","text":"<p>For documentation, consider using page-level chunking rather than arbitrary token-based chunks. This aligns with how documentation is naturally structured and how authors organize information. Combine semantic search with lexical search for better results, as documentation typically uses consistent terminology. Test this approach with evaluations to verify its effectiveness for your specific use case. Remember that document creators are usually aware of page-level semantics and rarely split important concepts across pages.</p>"},{"location":"office-hours/faq/#how-can-i-understand-what-my-users-are-asking-about","title":"How can I understand what my users are asking about?","text":"<p>To analyze user queries effectively, use a conversation analysis tool like Cura. This approach involves:</p> <ol> <li>Summarizing each conversation</li> <li>Extracting key information (language used, topics, tasks, requests, complaints)</li> <li>Embedding this data</li> <li>Clustering similar conversations</li> <li>Using an LLM to label and group these clusters</li> </ol> <p>This gives you insights into what users are asking, which features are performing well, and which need improvement. You can then develop targeted tools to address the most common or high-value query types.</p>"},{"location":"office-hours/faq/#whats-your-experience-with-extracting-data-from-construction-blueprints","title":"What's your experience with extracting data from construction blueprints?","text":"<p>When working with construction blueprints, focus on extracting structured data that answers specific questions users ask. For example, in a condominium project, we extracted data like floor numbers, room counts, directional orientation, and unit identifiers. This required developing specialized bounding box models to identify key elements in the blueprints. The approach was driven by analyzing actual user queries, which revealed they needed to quickly locate specific information like window dimensions or material specifications for particular rooms or floors.</p>"},{"location":"office-hours/faq/#should-i-use-postgres-with-pgvector-for-my-rag-implementation","title":"Should I use Postgres with pgvector for my RAG implementation?","text":"<p>Postgres with pgvector is a good choice for RAG implementations because it allows you to combine vector search with traditional SQL queries, enabling pre-filtering by metadata like dates or access permissions. For better performance, consider pgvector-scale, which provides more efficient exhaustive search capabilities for larger datasets. Adding pg_search from PostgreSQL gives you BM25 implementation, allowing you to combine vector search with lexical search in the same database. This approach gives you flexibility to switch between semantic and lexical search while maintaining the ability to join with other data tables.</p>"},{"location":"office-hours/faq/#how-do-you-determine-which-user-questions-are-worth-optimizing-for","title":"How do you determine which user questions are worth optimizing for?","text":"<p>Focus on identifying which questions deliver the most economic value, not just which are most common. For example, in a construction project, helping workers quickly locate specific blueprint details might save a few minutes, but identifying unsigned contracts that could cause project delays delivers much higher value. Analyze your user conversations to identify these high-value query patterns, then build specialized tools to address them. The goal isn't just to answer questions faster but to help users make better decisions that impact their bottom line.</p>"},{"location":"office-hours/faq/#whats-your-recommendation-on-model-selection-for-rag-applications","title":"What's your recommendation on model selection for RAG applications?","text":"<p>There's no one-size-fits-all model recommendation for RAG. Start by getting your retrieval right, as reasoning over data you can't find is a bigger issue than reasoning capabilities. Then run evaluations to test different models against your specific use cases. Consider your budget constraints across three dimensions: cost, latency, and performance. Your choice will depend on the economic value of the application - a financial analysis tool might justify using GPT-4 at $4 per report if it's still cheaper than human analysis, while a nutritionist website chatbot might need a more cost-effective model.</p>"},{"location":"office-hours/faq/#how-can-i-apply-course-concepts-to-my-actual-project-while-balancing-time-constraints","title":"How can I apply course concepts to my actual project while balancing time constraints?","text":"<p>Several participants expressed the challenge of finding time to apply course concepts to their real-world projects while managing full-time jobs. One participant noted, \"I have a day job with a packed schedule. I already have to make room for lectures and these conversations, which leaves very little time to apply this to my project.\"</p> <p>This is a common challenge when learning new technical skills alongside existing responsibilities. For those in this situation, I recommend focusing on completing the course first and then applying the knowledge afterward. The community will remain active even after the course ends, with the Slack channel staying open and all videos remaining available.</p> <p>For those who need more immediate application, consider reaching out about a consulting engagement after the course. The reality is that deep implementation often requires dedicated time that's difficult to carve out while maintaining other responsibilities.</p> <p>Key Takeaway: Learning and implementation often need to be sequenced rather than parallel when you have limited time. Focus on absorbing the knowledge first, then plan dedicated time for application afterward.</p> <p>Key Takeaway</p> <p>Learning and implementation often need to be sequenced rather than parallel when you have limited time. Focus on absorbing the knowledge first, then plan dedicated time for application afterward.</p>"},{"location":"office-hours/faq/#what-happens-to-the-community-after-the-course-ends","title":"What happens to the community after the course ends?","text":"<p>While we won't have structured bi-weekly meetings after the course concludes, the Slack channel will remain active, and I'll check it regularly to share resources and interesting developments. All course materials, including videos and the Maven pages, will remain accessible.</p> <p>The community's activity level will largely depend on participant engagement - \"it's basically going to be like however much you put in is what you're going to get out.\" We don't have a community manager pushing conversations, as my goal isn't to maximize message volume.</p> <p>Many valuable interactions happen through direct messages rather than in public channels. For example, one participant is about to launch their own company, and we're jumping on calls to discuss their ideas and make introductions.</p> <p>Key Takeaway: The community will continue beyond the formal course structure, but its value will depend on your active participation and willingness to engage with others.</p> <p>Key Takeaway</p> <p>The community will continue beyond the formal course structure, but its value will depend on your active participation and willingness to engage with others.</p>"},{"location":"office-hours/faq/#how-should-i-handle-irrelevant-data-being-pushed-into-my-vector-database","title":"How should I handle irrelevant data being pushed into my vector database?","text":"<p>One participant working on an application with high-performance RAG requirements asked about the impact of irrelevant data in their vector database: \"How much do I need to worry if there's irrelevant data being pushed into our vector database? Is it not that big of a deal because we have metadata filtering and good retrieval, or is it a big deal?\"</p> <p>This concern tends to be model-specific. Foundation model companies have been optimizing for recall after discovering the \"needle in a haystack\" problem, where models struggled to find specific information buried in large contexts. While this improved recall, it made models more sensitive to precision issues.</p> <p>The real risk now is that low precision might hurt your language model's ability to reason correctly. When dealing with irrelevant data, consider whether it's \"adversarially irrelevant\" - is the data actually conflicting rather than just unnecessary?</p> <p>For example, in construction documentation, you might have an email saying a wall is yellow, an architect's note saying it's blue, and a text message claiming it's purple. In these cases, you need to establish authority hierarchies or time-based weighting to resolve conflicts.</p> <p>Key Takeaway: The impact of irrelevant data depends on whether it's merely unnecessary or actively conflicting. Modern models are optimized for high recall but can be sensitive to precision issues, so conflicting information can be particularly problematic.</p> <p>Key Takeaway</p> <p>The impact of irrelevant data depends on whether it's merely unnecessary or actively conflicting. Modern models are optimized for high recall but can be sensitive to precision issues, so conflicting information can be particularly problematic.</p>"},{"location":"office-hours/faq/#what-metrics-should-i-monitor-for-retrieval-quality-in-production","title":"What metrics should I monitor for retrieval quality in production?","text":"<p>When asked about vector databases providing retrieval quality measurements, I recommended focusing on metrics you can monitor yourself rather than trusting vendor-provided metrics.</p> <p>Consider tracking the average cosine distance of your queries over time. If this metric suddenly changes, it could indicate a shift in your data or user behavior. For example, in a previous recommendation system I built, we monitored cosine distance between products and noticed a sudden drop. After investigating by segmenting the data by signup date, gender, and life stage, we discovered we had onboarded many young users through a Super Bowl ad campaign who couldn't afford our $300 clothing items.</p> <p>You might also monitor average re-ranker scores and look for changes over time or across different user segments. These metrics are more valuable than arbitrary tests created by vector database providers.</p> <p>Key Takeaway: Focus on monitoring changes in metrics like average cosine distance rather than absolute values, and segment your analysis by relevant variables to identify the root causes of any shifts.</p> <p>Key Takeaway</p> <p>Focus on monitoring changes in metrics like average cosine distance rather than absolute values, and segment your analysis by relevant variables to identify the root causes of any shifts.</p>"},{"location":"office-hours/faq/#whats-the-best-approach-for-processing-complex-technical-documentation","title":"What's the best approach for processing complex technical documentation?","text":"<p>A participant working on processing technical manuals for question answering described their current approach: \"We're leveraging the internal structure of the document, taking sections, splitting them, but including the hierarchy of titles - section, chapter, and manual title. But it feels naive to me.\"</p> <p>This challenge is common when dealing with structured technical content. One approach is to use traversal rather than pure semantic search - similar to how code-based agents navigate repositories. Instead of embedding everything, the system can navigate the document structure to find relevant information.</p> <p>For example, when working with Brazilian tax codes (400-page PDFs), we implemented a system that traversed the documents using a combination of semantic search, full-text search, and grep-like tools. The system could navigate from main sections to specific appendices to find relevant information.</p> <p>The key insight is that traversal is still a form of retrieval. As you collect traversal data, you can use it to improve your embedding models, potentially reducing the need for complex traversal in the future.</p> <p>Key Takeaway: For complex technical documentation, consider combining semantic search with structural traversal. Use the document's inherent organization to guide your retrieval process, and collect this data to improve your embedding models over time.</p> <p>Key Takeaway</p> <p>For complex technical documentation, consider combining semantic search with structural traversal. Use the document's inherent organization to guide your retrieval process, and collect this data to improve your embedding models over time.</p>"},{"location":"office-hours/faq/#should-i-build-complex-hierarchical-structures-for-document-retrieval","title":"Should I build complex hierarchical structures for document retrieval?","text":"<p>When discussing whether to build sophisticated graph structures for document retrieval, I emphasized the importance of getting to usable data quickly: \"My metric is: whatever I build should be the thing that gets me to 10,000 rows in a CSV file.\"</p> <p>Rather than spending extensive time modeling tax laws as a graph or building complex hierarchical indexes upfront, I recommend chunking everything, getting a working system, understanding the problems, and creating examples. This data-driven approach allows you to identify patterns that can inform more sophisticated solutions later.</p> <p>The better lesson in AI development is that segmenting and solving individual problems can help you make progress now, while preparing unified datasets that will allow you to combine approaches when technology improves. This mirrors the evolution of speech-to-text systems, which initially required separate stages for waveforms, phonemes, and words before end-to-end solutions became viable.</p> <p>Key Takeaway: Focus on collecting data and building working solutions rather than perfect architectures. The insights gained from real usage will guide your more sophisticated implementations later.</p>"},{"location":"office-hours/faq/#how-should-we-think-about-the-relationship-between-rag-and-agents","title":"How should we think about the relationship between RAG and agents?","text":"<p>An interesting perspective emerged during our discussion: \"RAG is like the superpower for AI right now.\" We explored how the boundaries between RAG and other AI capabilities are blurring, with one participant noting \"grep is RAG\" - highlighting that any method of retrieving context for an AI system shares fundamental similarities with RAG.</p> <p>I've been considering whether we should rename the course to focus on \"RAG applications\" since modern AI systems are essentially exposing a portfolio of tools to agents. Whether you're using semantic search or a grep-like function to pull in relevant code, you're still finding information to enhance the context available to the model.</p> <p>The core principle remains the same: \"It has to be put into the context at the right time so that you can get the response correct.\" This perspective frames RAG not just as a specific technique but as a fundamental paradigm for augmenting AI capabilities with relevant information.</p> <p>Key Takeaway: The distinction between RAG and other AI augmentation approaches is increasingly blurred. The fundamental goal is getting the right information into the context at the right time, regardless of the specific retrieval mechanism.</p>"},{"location":"office-hours/faq/#whats-the-value-of-the-office-hours-format-for-learning","title":"What's the value of the office hours format for learning?","text":"<p>Several participants expressed surprise at how valuable they found the office hours sessions. One noted, \"I thought they wouldn't be useful, but I'm surprised with the quality of the questions being asked.\"</p> <p>These interactive sessions provide an opportunity to hear how others are applying the course concepts and to discuss specific challenges that might not be covered in the structured lectures. The questions often reveal practical implementation issues that many participants are facing but might not have articulated themselves.</p> <p>The conversations also help connect theoretical concepts to real-world applications, making the material more concrete and actionable. For example, our discussion about monitoring cosine distances in production systems provided a practical perspective on evaluation that complements the more structured content on evaluation frameworks.</p> <p>Key Takeaway: Interactive learning formats like office hours provide valuable perspectives that complement structured course content, particularly for understanding how concepts apply to diverse real-world scenarios.</p> <p>Key Takeaway</p> <p>Interactive learning formats like office hours provide valuable perspectives that complement structured course content, particularly for understanding how concepts apply to diverse real-world scenarios.</p>"},{"location":"office-hours/faq/#how-should-we-pace-the-course-to-maximize-learning","title":"How should we pace the course to maximize learning?","text":"<p>When asked about the pacing of the course, I acknowledged that many participants are finding it challenging to keep up with all the material. One suggestion was to include a week in the middle with no new material to allow people to catch up, which received positive feedback.</p> <p>I noted that Week 3 is intentionally lighter, with only a 40-minute video and no notebooks, designed as a catch-up week. However, I recognized that I should make this more explicit to help participants plan their time.</p> <p>The six-week format provides more depth than a one-week intensive course would allow, but it requires consistent engagement to get the full benefit. Finding the right balance between comprehensive coverage and manageable pacing remains a challenge.</p> <p>Key Takeaway: Learning complex technical skills requires finding the right balance between depth of content and time for absorption and practice. Building explicit catch-up periods into courses can help participants manage their learning journey more effectively.</p> <p>Key Takeaway</p> <p>Learning complex technical skills requires finding the right balance between depth of content and time for absorption and practice. Building explicit catch-up periods into courses can help participants manage their learning journey more effectively.</p>"},{"location":"office-hours/faq/#what-can-we-learn-from-leaked-system-prompts-like-anthropics-claude","title":"What can we learn from leaked system prompts like Anthropic's Claude?","text":"<p>One participant asked about the recently leaked Anthropic Claude prompt, which was reportedly around 30,000 tokens: \"Where does it leave room for actual content to be processed? Is it even realistic or just hype?\"</p> <p>I wasn't surprised by the size of this prompt, explaining that it makes sense for the Claude web app experience, which includes tools for fetching information. The API version likely has a smaller prompt, but it's still substantial if web search capabilities are included.</p> <p>This reveals how much can be done through prompting without changing model weights. It's remarkable that models can now process 30,000 token system messages when just two years ago, the entire context was limited to 32K tokens.</p> <p>The existence of such extensive system prompts raises questions about where certain capabilities should reside - in the prompt or in the model weights. For example, if a fetch tool were baked into the model weights, what would happen if you named your custom tool \"web_search\" and the model tried to call a hardcoded \"fetch\" function?</p> <p>Key Takeaway: Large system prompts demonstrate how much functionality can be implemented through instructions rather than model training. This creates flexibility but also raises important questions about the boundary between prompt engineering and model architecture.</p> <p>FAQs</p> <p>Key Takeaway</p> <p>Large system prompts demonstrate how much functionality can be implemented through instructions rather than model training. This creates flexibility but also raises important questions about the boundary between prompt engineering and model architecture.</p>"},{"location":"office-hours/faq/#how-can-i-balance-the-course-with-my-regular-work-schedule","title":"How can I balance the course with my regular work schedule?","text":"<p>Many participants find balancing the course with their day job challenging. The course requires time for watching lectures, completing exercises, and participating in discussions. Consider setting aside specific time slots in your schedule for course activities and prioritize what aspects are most valuable to you. Remember that you can always revisit materials after the course ends if you're unable to complete everything during the active weeks.</p>"},{"location":"office-hours/faq/#will-course-materials-remain-available-after-the-course-ends","title":"Will course materials remain available after the course ends?","text":"<p>Yes, all course materials including videos, notebooks, and exercises will remain accessible after the course concludes. The Slack channel will also stay active, allowing you to continue asking questions and collaborating with other participants. While structured bi-weekly meetings won't continue, you'll still have access to all resources and can work through them at your own pace.</p>"},{"location":"office-hours/faq/#how-active-will-the-community-be-after-the-course-ends","title":"How active will the community be after the course ends?","text":"<p>The community's activity level will largely depend on participant engagement. While there won't be formal scheduled sessions after the course, the instructors will check the Slack channel regularly and share relevant resources. The value you get from the community will correlate with how much you contribute to it. Many valuable connections happen through direct messages rather than in public channels.</p>"},{"location":"office-hours/faq/#is-there-a-recommended-approach-to-catching-up-if-im-behind","title":"Is there a recommended approach to catching up if I'm behind?","text":"<p>Week 3 is intentionally lighter with only an hour-long video and no notebooks, providing an opportunity to catch up on previous materials. The course team is considering adding a \"break week\" in future cohorts to give participants more time to process information and complete exercises. Don't worry if you can't complete everything during the course timeframe\u2014the materials will remain available afterward.</p>"},{"location":"office-hours/faq/#how-can-i-apply-what-im-learning-to-my-actual-projects","title":"How can I apply what I'm learning to my actual projects?","text":"<p>The most effective way to apply course concepts to your work is to start with the exercises to build foundational understanding, then gradually incorporate techniques into your projects. Some participants find it helpful to wait until after the course to fully implement what they've learned, as this allows them to focus on understanding the concepts first. For more personalized guidance, reaching out about consulting engagements after the course can be beneficial.</p>"},{"location":"office-hours/faq/#whats-the-best-approach-to-rag-retrieval-augmented-generation-for-technical-documentation","title":"What's the best approach to RAG (Retrieval-Augmented Generation) for technical documentation?","text":"<p>When working with technical documentation, consider these approaches:</p> <ol> <li> <p>Start by focusing on getting retrieval right before worrying about other aspects</p> </li> <li> <p>Use document structure (sections, chapters, titles) to improve chunking</p> </li> <li> <p>Consider a combination of semantic search, full-text search, and traversal approaches</p> </li> <li> <p>Monitor metrics like cosine distance to evaluate retrieval quality</p> </li> <li> <p>Begin with a simple implementation that works for most of your documents rather than trying to solve every edge case immediately</p> </li> </ol>"},{"location":"office-hours/faq/#how-should-i-handle-irrelevant-data-in-my-vector-database","title":"How should I handle irrelevant data in my vector database?","text":"<p>The impact of irrelevant data depends on your specific model and use case. Modern language models are optimized for high recall, which can make them sensitive to low precision issues. Consider whether irrelevant data is merely noise or actually conflicting/adversarial. For conflicting information, you may need to implement authority rules (like prioritizing certain document types) or time-based weighting. Rather than trying to perfect your data filtering upfront, start with a basic implementation, collect examples, and iterate based on actual performance.</p>"},{"location":"office-hours/faq/#are-vector-databases-providing-built-in-retrieval-quality-measurements","title":"Are vector databases providing built-in retrieval quality measurements?","text":"<p>While some vector databases may offer metrics, it's generally better to implement your own monitoring. Focus on tracking metrics like average cosine distance of your queries and monitor how these change over time or across different user segments. This approach allows you to detect shifts in data patterns or user behavior that might affect retrieval quality. Looking at changes in these metrics is often more valuable than the absolute values themselves.</p>"},{"location":"office-hours/faq/#what-open-source-re-ranking-models-work-well-for-fine-tuning","title":"What open-source re-ranking models work well for fine-tuning?","text":"<p>When selecting re-ranking models for fine-tuning, I believe the right approach depends on your specific constraints and data volume. For many scenarios, the BGE models from the Beijing Academy of Artificial Intelligence (BAAI) have proven quite stable and easy to fine-tune with datasets of around 100,000 examples.</p> <p>The model selection process isn't about finding a single \"best\" model, but rather systematically testing different options against your specific requirements. I've found that BAAI models tend to have more predictable loss curves during training compared to some alternatives where parameters might need more careful tuning.</p> <p>Your model selection should consider several factors:</p> <ul> <li>Latency requirements (5-10 seconds for total search and retrieval in your case)</li> <li>Hosting constraints (on-premises deployment for medical applications)</li> <li>The volume of training data available</li> <li>The trade-off between performance and computational cost</li> </ul> <p>For on-premises medical applications requiring self-hosting, I'd recommend starting with the BGE models and systematically testing different configurations. The process is inherently experimental - you'll likely need to train numerous models with different parameters and dataset preparations before finding the optimal combination.</p> <p>Key Takeaway: Don't get fixated on finding the \"perfect\" model architecture. Instead, create a systematic testing framework where you can evaluate multiple models against your specific constraints of latency, hosting requirements, and performance needs.</p> <p>Key Takeaway</p> <p>Don't get fixated on finding the \"perfect\" model architecture. Instead, create a systematic testing framework where you can evaluate multiple models against your specific constraints of latency, hosting requirements, and performance needs.</p>"},{"location":"office-hours/faq/#how-should-i-approach-creating-training-datasets-for-embedding-models-versus-re-rankers","title":"How should I approach creating training datasets for embedding models versus re-rankers?","text":"<p>The fundamental difference between training embedding models and re-rankers lies in how they handle similarity scores. Embedding models typically work with triplets (this is similar to that, different from something else) with binary scores of 1 or -1. Re-rankers, however, can be more nuanced with scores like 0.8 or 0.9, allowing for finer distinctions between results.</p> <p>I've found that focusing first on building a strong dataset for your embedding model is usually the most efficient approach. If you're currently only training on positive examples, incorporating negative examples will dramatically improve performance - we're talking about a 30% improvement rather than just 6%.</p> <p>For creating effective negative examples, I recommend being strategic rather than random:</p> <p>In a financial context I worked with recently, we were distinguishing between \"fuel\" (for employee vehicle reimbursements) and \"equipment fuel\" (for company vehicles like tractors). Simply using random negative examples wouldn't help the model learn this subtle distinction. Instead, we created hard negatives by:</p> <ol> <li> <p>Taking a transaction from one category</p> </li> <li> <p>Finding another transaction in the same category as a positive example</p> </li> <li> <p>Using embedding search to find the most similar transaction from a different category as the negative example</p> </li> </ol> <p>This approach forces the model to learn the meaningful boundaries between similar but distinct concepts. For your medical data with abbreviations that have different meanings in different contexts, you could apply a similar strategy - finding examples where the same abbreviation appears in different contexts to create hard negatives.</p> <p>Key Takeaway: Including well-crafted negative examples in your training data is crucial for model performance. Focus on creating \"hard negatives\" that challenge the model to learn subtle distinctions rather than obvious differences.</p> <p>Key Takeaway</p> <p>Including well-crafted negative examples in your training data is crucial for model performance. Focus on creating \"hard negatives\" that challenge the model to learn subtle distinctions rather than obvious differences.</p>"},{"location":"office-hours/faq/#what-are-effective-sources-of-negative-examples-for-training-data","title":"What are effective sources of negative examples for training data?","text":"<p>Some of the most valuable negative examples come from user interactions that indicate a mismatch between what the system thought was relevant and what the user actually found useful. I've implemented several approaches across different domains:</p> <p>For citation systems:</p> <p>When experts review citations and delete ones they find irrelevant, saying \"regenerate without this document because it's misleading\" - that's a perfect negative example. The question and the deleted chunk form a negative pair for training.</p> <p>For recommendation systems:</p> <ul> <li>In content recommendation, when a salesperson deletes a suggested blog post from an automated email, that's a negative example</li> <li>In music services like Spotify, skipping a song is a weaker negative signal than deleting it from a playlist</li> <li>In e-commerce, items that are purchased together but later returned indicate a false positive that can be used as a negative example</li> </ul> <p>For your medical context, you might consider:</p> <ul> <li>Tracking when users reject or ignore certain retrieved chunks</li> <li>Using expert feedback to identify misleading or irrelevant retrievals</li> <li>Creating synthetic data with language models to generate examples where abbreviations are used in different contexts</li> </ul> <p>The key insight is that these high-signal negative examples often come from cases where the system initially thought it was right but was ultimately wrong - these boundary cases are extremely valuable for training.</p> <p>Key Takeaway: The most valuable negative examples often come from user interactions that indicate a mismatch between system predictions and actual relevance. Design your system to capture these signals and incorporate them into your training data.</p> <p>Key Takeaway</p> <p>The most valuable negative examples often come from user interactions that indicate a mismatch between system predictions and actual relevance. Design your system to capture these signals and incorporate them into your training data.</p>"},{"location":"office-hours/faq/#how-should-i-think-about-compute-allocation-in-retrieval-systems","title":"How should I think about compute allocation in retrieval systems?","text":"<p>When designing retrieval systems, especially for complex documents like legal texts or medical records, I think about it as a fundamental trade-off: where do I want to allocate my compute resources? This is essentially a decision between investing compute at \"write time\" (indexing) versus \"read time\" (retrieval).</p> <p>There are two main approaches to consider:</p> <p>Contextual retrieval (compute-heavy at write time):</p> <ul> <li>Rewrite text chunks during indexing to include all necessary context</li> <li>For example, converting \"He is unhappy with her\" to \"Jason the doctor is unhappy with Patient X\"</li> <li>This makes retrieval simpler but requires more upfront processing</li> <li>Anthropic has published about this approach for their Claude assistant</li> </ul> <p>Tool use and traversal (compute-heavy at read time):</p> <ul> <li>Store minimal context in each chunk</li> <li>Use additional compute during retrieval to navigate between related chunks</li> <li>Similar to how Cursor IDE navigates code by finding functions and then examining surrounding context</li> <li>This approach is more flexible but can feel slower to users</li> </ul> <p>For your medical application where the data is self-contained (not requiring external information), and where you want to minimize user wait time, I'd lean toward investing more compute at indexing time. This is especially true since you can run indexing jobs overnight without affecting user experience.</p> <p>The decision also relates to data normalization - do you want to denormalize data by including related information in each chunk (like adding phone numbers whenever a person is mentioned), or keep information separate and join it at retrieval time? The answer depends on your specific use case and resource constraints.</p> <p>Key Takeaway: Frame your retrieval system design as a strategic decision about compute allocation. For medical applications with self-contained data and latency constraints, investing more compute at indexing time to create context-rich chunks will likely provide a better user experience.</p> <p>Key Takeaway</p> <p>Frame your retrieval system design as a strategic decision about compute allocation. For medical applications with self-contained data and latency constraints, investing more compute at indexing time to create context-rich chunks will likely provide a better user experience.</p>"},{"location":"office-hours/faq/#what-determines-the-complexity-of-the-architecture-i-should-use","title":"What determines the complexity of the architecture I should use?","text":"<p>I believe the volume and quality of your training data should be the primary factor determining architectural complexity. This is a principle I emphasize repeatedly: your dataset size dictates what approaches make sense.</p> <p>As a general guideline:</p> <ul> <li>With ~100 examples: Use few-shot prompting</li> <li>With thousands of examples: Fine-tune embedding models</li> <li>With millions of examples: Fine-tune language models</li> </ul> <p>The data volume determines what's feasible. If you told me you had a million examples, I'd probably just train a language model directly and worry about everything else later. With limited data, you need to be more strategic about targeting specific challenges like medical abbreviations with ambiguous meanings.</p> <p>This is why I'm skeptical when I see engineers celebrating 98% accuracy on their first model - it usually means they've created a test set that's too easy. As your model improves, you should be making your test data harder by finding more challenging examples. If your retrieval dashboard is showing 95% accuracy, that's a sign you need to create harder test cases.</p> <p>Key Takeaway: Let your data volume guide your architectural decisions. With limited data, focus on targeted improvements to specific challenges rather than complex architectures. As your model improves, continuously create harder test cases to drive further improvement.</p> <p>Key Takeaway</p> <p>Let your data volume guide your architectural decisions. With limited data, focus on targeted improvements to specific challenges rather than complex architectures. As your model improves, continuously create harder test cases to drive further improvement.</p>"},{"location":"office-hours/faq/#how-can-i-improve-my-system-when-i-dont-yet-have-real-user-feedback","title":"How can I improve my system when I don't yet have real user feedback?","text":"<p>Without real user feedback, you can still make significant progress through synthetic data generation and expert knowledge. For your medical abbreviation challenge, you could:</p> <ol> <li> <p>Identify known ambiguous abbreviations in medical contexts</p> </li> <li> <p>Use language models like GPT-4 to generate synthetic examples showing these abbreviations in different contexts</p> </li> <li> <p>Have medical experts validate these examples or create additional ones</p> </li> <li> <p>Build a curated dataset of hard negatives focusing on these ambiguities</p> </li> </ol> <p>This approach lets you systematically address known challenges before deployment. Once you have real users, you can implement feedback mechanisms to capture when they reject or modify system outputs, creating a virtuous cycle of improvement.</p> <p>Remember that as your system improves, you need to continuously create harder test cases. If you're scoring 95% accuracy, it's not because your AI is exceptional - it's because your test data isn't challenging enough. The goal is to build a dataset that pushes the boundaries of what your system can handle.</p> <p>Key Takeaway: Before having real users, leverage synthetic data generation and expert knowledge to create challenging test cases. Design your system to capture user feedback from the start, as this will become your most valuable source of training data once deployed.</p> <p>FAQs</p> <p>Key Takeaway</p> <p>Before having real users, leverage synthetic data generation and expert knowledge to create challenging test cases. Design your system to capture user feedback from the start, as this will become your most valuable source of training data once deployed.</p>"},{"location":"office-hours/faq/#what-open-source-re-ranking-models-are-recommended-for-fine-tuning","title":"What open-source re-ranking models are recommended for fine-tuning?","text":"<p>The Beijing Academy of Artificial Intelligence (BAAI) models, such as BGE re-ranker v3, are often good choices for fine-tuning. These models have proven to be stable during the training process and work well with the sentence transformers library. When selecting a model, consider your specific data volume and performance requirements. Testing multiple models with your dataset is ultimately the best approach to finding the optimal solution for your specific use case.</p>"},{"location":"office-hours/faq/#how-should-i-approach-model-selection-for-my-re-ranking-needs","title":"How should I approach model selection for my re-ranking needs?","text":"<p>Start by exploring models that perform well on MTAB benchmarks on Hugging Face. Consider your constraints around latency, hosting requirements, and data volume. With the right dataset, the process becomes more about searching the model space to find what works best for your specific scenario. For medical or specialized domains, you'll want to test various models against your specific data to determine which one provides the best performance-to-cost ratio.</p>"},{"location":"office-hours/faq/#what-are-the-different-types-of-re-ranking-models-available","title":"What are the different types of re-ranking models available?","text":"<p>Re-rankers can be embedding models, cross-encoder models, or even large language models (LLMs). Each has different characteristics: embedding models typically classify results as relevant or not relevant, cross-encoders can provide more nuanced scoring (like 0.8 vs 0.9 relevance), and LLM-based re-rankers (like those used by Exa) can provide sophisticated re-ranking but may have higher latency. Your choice depends on your specific requirements and constraints.</p>"},{"location":"office-hours/faq/#training-data-for-embedding-and-re-ranking-models","title":"Training Data for Embedding and Re-Ranking Models","text":"<p>How important are negative examples when training embedding models?</p> <p>Negative examples are extremely valuable when training embedding models. Including hard negatives (examples that are similar but should be classified differently) can improve performance by 30% or more compared to training with only positive examples. These hard negatives help the model learn subtle distinctions that are crucial for accurate retrieval, especially in specialized domains like medicine or legal text.</p>"},{"location":"office-hours/faq/#whats-an-effective-way-to-generate-hard-negative-examples","title":"What's an effective way to generate hard negative examples?","text":"<p>One effective approach is to find examples that are semantically similar but belong to different categories. For instance, if you're working with medical abbreviations that have different meanings in different contexts, you could create examples showing the same abbreviation used in different medical specialties. Another method is to use embedding search to find the most similar items that should be classified differently, rather than just using random negative examples.</p>"},{"location":"office-hours/faq/#how-can-user-feedback-be-leveraged-to-improve-retrieval-models","title":"How can user feedback be leveraged to improve retrieval models?","text":"<p>User interactions provide valuable signals for creating training data. For example, if users delete a citation or regenerate an answer without a specific document, that's a strong signal that the document was irrelevant (a negative example). Similarly, if users consistently skip or remove certain recommendations, those can be used as negative examples in your training data. These real-world signals often create the most valuable training examples.</p>"},{"location":"office-hours/faq/#what-factors-should-i-consider-when-designing-a-retrieval-system-architecture","title":"What factors should I consider when designing a retrieval system architecture?","text":"<p>Consider where to allocate your compute resources: at indexing/write time or at query/read time. If you invest more in preprocessing your data (like contextual retrieval where you rewrite chunks to include necessary context), your query-time processing can be simpler and faster. Alternatively, you can keep preprocessing minimal and implement more sophisticated query-time processing like document traversal. Your decision should balance user experience requirements, cost constraints, and the specific characteristics of your data.</p>"},{"location":"office-hours/faq/#how-do-i-handle-context-in-long-documents-where-paragraphs-depend-on-previous-content","title":"How do I handle context in long documents where paragraphs depend on previous content?","text":"<p>There are two main approaches: (1) Contextual retrieval, where you rewrite text chunks at indexing time to include all necessary context, making each chunk self-contained; or (2) Document traversal, where your system can navigate through the document at query time to gather needed context. The first approach frontloads the processing cost but enables faster query responses, while the second requires more complex query-time processing but minimizes preprocessing.</p>"},{"location":"office-hours/faq/#what-hosting-considerations-should-i-keep-in-mind-for-medical-applications","title":"What hosting considerations should I keep in mind for medical applications?","text":"<p>For medical applications, especially in European contexts like the Netherlands, you'll likely need to host models on your own hardware or on-premises at hospitals. This requires selecting models that can run efficiently on your available hardware while meeting your latency requirements. Consider models that can be fully self-hosted without external API dependencies, and ensure your architecture complies with relevant healthcare data regulations.</p>"},{"location":"office-hours/faq/#how-should-we-approach-evaluation-data-collection-for-rag-systems","title":"How should we approach evaluation data collection for RAG systems?","text":"<p>One participant was struggling with exporting conversation data from Langsmith for evaluation purposes. This highlights a common challenge with many tracing tools - they're often better at collecting data than exporting it in useful formats.</p> <p>When Langsmith or similar tools create export difficulties, I recommend two alternative approaches:</p> <ol> <li>Direct database storage: Instead of relying on tracing software, consider saving queries directly to your database as the application runs.    \"This is something we do all the time - we just write the question, answer pairs, or chunks to Postgres. That way, we can build UI on top of that database rather than trying to export data out of tools like Langsmith.\"</li> <li>Create a simple, wide database table that includes:</li> <li>Session ID</li> <li>User ID</li> <li>Query text</li> <li>Retrieved chunks</li> <li>Generated answer</li> </ol> <p>This approach gives you direct access to your data without depending on third-party export functionality, which can be unreliable. It's like building your own analytics system rather than trying to export from something like Data Dog for analysis.</p> <p>Key Takeaway: While tracing tools like Langsmith and Log Fire are valuable for telemetry, consider implementing your own database storage for evaluation data to avoid export headaches and gain more control over your analysis process.</p> <p>Key Takeaway</p> <p>While tracing tools like Langsmith and Log Fire are valuable for telemetry, consider implementing your own database storage for evaluation data to avoid export headaches and gain more control over your analysis process.</p>"},{"location":"office-hours/faq/#which-models-should-we-use-for-different-rag-applications","title":"Which models should we use for different RAG applications?","text":"<p>When choosing between models like GPT-4, GPT-4 Turbo, or GPT-3.5, I've observed different selection patterns based on the task's importance and time constraints:</p> <p>For high-value applications where accuracy is critical (like financial due diligence with 44 data rooms generating reports for clients paying $200,000 annually), companies often default to GPT-4 because \"if it is just 2% better, it'll be worth it.\"</p> <p>For applications requiring speed, GPT-3.5 or GPT-4 are common choices.</p> <p>Many developers are now using Gemini for RAG applications because its large context window allows for less precision in retrieval: \"You can just be really frivolous with how much context you use.\"</p> <p>The decision often comes down to the stakes involved rather than technical benchmarks. For example, when helping sales teams craft follow-up emails containing offers, we use GPT-4 because the potential revenue impact justifies the additional cost.</p> <p>Key Takeaway: Model selection should be driven by business value rather than technical specifications alone. For high-stakes applications where even small improvements matter, use the most capable model available. For less critical applications, prioritize speed and cost-efficiency.</p> <p>Key Takeaway</p> <p>Model selection should be driven by business value rather than technical specifications alone. For high-stakes applications where even small improvements matter, use the most capable model available. For less critical applications, prioritize speed and cost-efficiency.</p>"},{"location":"office-hours/faq/#how-can-we-enhance-report-generation-with-visual-elements","title":"How can we enhance report generation with visual elements?","text":"<p>One exciting development in RAG applications is the integration of visual elements into generated reports. I'm currently working with a company on two key improvements:</p> <ol> <li>Supporting mermaid diagrams in reports to visualize relationships and processes</li> <li>Intelligently adding relevant images to reports</li> </ol> <p>For example, in a construction permitting application, this could mean automatically including screenshots of potential errors in blueprints with accompanying explanations: \"If in a report of predicted potential errors that you should pay attention to on your project, it would actually take a screenshot of the error in the PDF of the blueprint, and then have a narrative around it.\"</p> <p>This approach dramatically increases the value of generated reports by combining visual and textual information, making complex issues immediately understandable to users.</p> <p>Key Takeaway: The next frontier in RAG applications involves intelligently incorporating visual elements like diagrams and contextual images to enhance understanding and provide more comprehensive analysis.</p> <p>Key Takeaway</p> <p>The next frontier in RAG applications involves intelligently incorporating visual elements like diagrams and contextual images to enhance understanding and provide more comprehensive analysis.</p>"},{"location":"office-hours/faq/#how-should-we-manage-expectations-around-ai-capabilities","title":"How should we manage expectations around AI capabilities?","text":"<p>Managing expectations is one of the biggest challenges when implementing AI systems, especially with clients who have either unrealistic expectations or excessive skepticism.</p> <p>For construction applications, one participant described their approach: \"We try to explain to people that ultimately in our field, you're an architect or a structural engineer. It's your stamp on the docs. You're making the call. We're just here to provide suggestions and things to look out for.\"</p> <p>This aligns with my experience working with large enterprises, where much of my consulting work involves \"dealing with the personality of the CEO\" who might want AI to be a major theme at the next sales conference without understanding the practical limitations.</p> <p>The most effective approach is focusing on how AI can augment human decision-making rather than replace it. For example, having the LLM run simulations and help humans interpret the results is more realistic than promising fully autonomous systems.</p> <p>Key Takeaway: Set clear boundaries around AI capabilities by positioning your system as a decision support tool rather than an autonomous decision-maker. Be explicit about where human judgment remains essential, especially in high-stakes domains like construction or finance.</p> <p>Key Takeaway</p> <p>Set clear boundaries around AI capabilities by positioning your system as a decision support tool rather than an autonomous decision-maker. Be explicit about where human judgment remains essential, especially in high-stakes domains like construction or finance.</p>"},{"location":"office-hours/faq/#whats-your-vision-for-building-open-source-ai-tools","title":"What's your vision for building open-source AI tools?","text":"<p>When asked about my vision for building AI tools, I explained that my approach differs from the typical venture-backed startup model:</p> <p>\"Before it was okay, consulting can drive revenue that allows us to do open source work. The open source projects don't need to raise venture capital or figure out how to monetize, which changes the nature of the code.\"</p> <p>This model allows me to create a portfolio of small, useful tools without worrying about monetization. The course serves as a way to connect with practitioners across different industries and identify common challenges:</p> <p>\"The most I ever did was like 7 clients in a month, and that was kind of a hazy period of my life where I have no memory of what happened. Whereas with the course, I can do these office hours - 10 people show up, great. I can understand how this permitting thing goes, maybe some architectural things, some construction things, some supply chain stuff.\"</p> <p>This broader exposure helps me identify patterns across industries, like the common need for better report generation or specialized table parsing, which informs both my consulting work and open-source development.</p> <p>Key Takeaway: By funding open-source development through consulting and courses rather than venture capital, I can focus on building genuinely useful tools without the pressure to monetize every component, leading to more sustainable and practical solutions.</p> <p>Key Takeaway</p> <p>By funding open-source development through consulting and courses rather than venture capital, I can focus on building genuinely useful tools without the pressure to monetize every component, leading to more sustainable and practical solutions.</p>"},{"location":"office-hours/faq/#how-should-we-think-about-pricing-and-value-capture-for-ai-systems","title":"How should we think about pricing and value capture for AI systems?","text":"<p>One of the most exciting developments I see in AI is the evolution of pricing models away from usage-based metrics toward outcome-based pricing:</p> <p>\"I'm personally curious about pricing the work that LLMs do. A lot of systems right now are being priced on usage. I'm really excited about what it would mean to have a system that has so much accountability that you can price on the outcome it delivers.\"</p> <p>I shared an example of a company that uses voice AI to make calls to car owners on behalf of dealerships. Under a usage-based model, the calls that make the most money are often those that waste time with confusion and errors. But with an outcome-based model, the incentives change dramatically:</p> <p>\"If you change the model to say, 'We want to take 3% of the mechanic's cost,' then it becomes, 'What if we had systems that are intelligently doing upsells? What if we intelligently figure out the right time and try to load balance the mechanic?'\"</p> <p>This shift changes the fundamental question from \"How much am I willing to pay to process one PDF file?\" (maybe 30 cents) to \"Under what circumstances would I be willing to pay $20 to process a PDF?\" The answer depends on the business value created.</p> <p>Key Takeaway: The future of AI pricing will likely move from usage-based models (tokens, API calls) to outcome-based models where vendors are compensated based on the business value they create. This will drive investment in higher-quality systems that optimize for results rather than minimizing usage.</p> <p>Key Takeaway</p> <p>The future of AI pricing will likely move from usage-based models (tokens, API calls) to outcome-based models where vendors are compensated based on the business value they create. This will drive investment in higher-quality systems that optimize for results rather than minimizing usage.</p>"},{"location":"office-hours/faq/#will-ai-capabilities-eventually-be-built-into-platforms-or-remain-in-applications","title":"Will AI capabilities eventually be built into platforms or remain in applications?","text":"<p>When asked whether AI capabilities will eventually be absorbed into platforms rather than remaining in applications, I suggested it depends on the time horizon:</p> <p>\"On any reasonable time horizon, it will probably just be the applications. The limiting factor is that for any specific application, we don't actually have the training data to bake this back into the model.\"</p> <p>I referenced the \"bitter lesson\" in AI, which shows that when you have enough data and compute, general approaches tend to outperform specialized ones. However, we first need applications to generate the necessary data:</p> <p>\"We still have to build these applications as sort of sensors to create this data. And then, once we do, we can kind of sidestep the next innovation.\"</p> <p>This is similar to how speech recognition evolved from complex phoneme-based systems to end-to-end models, but only after platforms like YouTube created enough data to make this possible.</p> <p>\"We had to build YouTube to produce enough data to get to a world where now we can train the GPT-4 model. So we still have to build these applications as sensors to create this data.\"</p> <p>Key Takeaway: While AI capabilities will eventually be absorbed into platforms, we first need to build applications that generate the necessary training data. This creates a cycle where applications serve as data collection mechanisms that eventually enable more general-purpose AI systems.</p> <p>Key Takeaway</p> <p>While AI capabilities will eventually be absorbed into platforms, we first need to build applications that generate the necessary training data. This creates a cycle where applications serve as data collection mechanisms that eventually enable more general-purpose AI systems.</p>"},{"location":"office-hours/faq/#how-might-ai-transform-business-models-and-value-chains","title":"How might AI transform business models and value chains?","text":"<p>I believe AI will fundamentally change how businesses capture value, potentially shifting from software-as-a-service models to more integrated approaches:</p> <p>\"Everything stops becoming SaaS budget and it's all headcount budget. If you absorb this entire 'dealership calls car owner to get them in the mechanic' thing, at some point you're just a sales guy.\"</p> <p>This could lead to companies expanding vertically to capture more of the value chain:</p> <p>\"Why don't you just own the entire value chain? Because then you can really price on the outcome that you're trying to deliver rather than just tokens.\"</p> <p>While this approach means taking on additional complexity (like owning car mechanics with \"all the pros and cons\"), it allows for capturing more of the value created. This is similar to how I view the difference between writers who charge by word versus those who are paid based on qualified leads that convert.</p> <p>\"If there was an agent that's like, 'We'll just take all your phone calls and turn them into blog posts, and we only get charged a commission of course sales,' I would probably be really happy with that.\"</p> <p>Key Takeaway: AI may drive a shift from software companies selling tools to companies that own entire value chains and are compensated based on business outcomes. This will require building systems that connect previously separate data streams to create end-to-end accountability.</p> <p>Key Takeaway</p> <p>AI may drive a shift from software companies selling tools to companies that own entire value chains and are compensated based on business outcomes. This will require building systems that connect previously separate data streams to create end-to-end accountability.</p>"},{"location":"office-hours/faq/#whats-the-most-valuable-data-for-future-ai-development","title":"What's the most valuable data for future AI development?","text":"<p>The most valuable data for AI development has evolved over time:</p> <p>\"When I started, it was physics. And then it's like, 'Well, we're running out of sensors, but the next sensor is going to cost us 10 billion dollars.' So I went to Facebook - what's every post and comment and Facebook group and the social graph?\"</p> <p>Now, I believe the most valuable data will be how humans interact with AI:</p> <p>\"How humans use AI will be the most interesting dataset. And then in 10 years, it'll be how AI talks to AI. Most of the data produced will just be AI talking to AI.\"</p> <p>This is why I'm particularly interested in working with companies that have large proprietary datasets in specialized domains:</p> <p>\"Someone was like, 'Oh, we have the last 40 years of investment decisions.' I was like, 'What?' Now I'm willing to pay so much to process this. Let's actually think about what the schemas look like and how to design this system.\"</p> <p>These unique datasets offer opportunities to create specialized tools that can extract insights that general models can't access without the proper context and structure.</p> <p>Key Takeaway: The most valuable data is shifting from general internet content to human-AI interactions and eventually AI-to-AI interactions. Companies with large proprietary datasets in specialized domains are particularly well-positioned to create value with AI systems tailored to their unique information.</p> <p>FAQs</p> <p>Key Takeaway</p> <p>The most valuable data is shifting from general internet content to human-AI interactions and eventually AI-to-AI interactions. Companies with large proprietary datasets in specialized domains are particularly well-positioned to create value with AI systems tailored to their unique information.</p>"},{"location":"office-hours/faq/#what-tools-are-recommended-for-tracing-and-evaluations-in-ai-applications","title":"What tools are recommended for tracing and evaluations in AI applications?","text":"<p>While Langsmith is commonly used, some users experience technical issues with data exports. Alternative options include Brain Trust for evaluations and Log Fire for tracing. The choice often depends on your specific needs and existing partnerships. For simpler implementations, consider storing question-answer pairs directly in a database rather than relying on third-party tracing software, which gives you more control and easier access to your data.</p>"},{"location":"office-hours/faq/#how-should-i-approach-data-collection-for-evaluating-my-ai-application","title":"How should I approach data collection for evaluating my AI application?","text":"<p>Start by creating an evaluation dataset from real user interactions. This can be done by exporting traces from tools like Langsmith or by directly storing question-answer pairs in your database. Once you have real data, you can generate synthetic questions to expand your test set. Focus on collecting both the user queries and your system's responses, along with any relevant context like retrieved document chunks, to enable comprehensive evaluation.</p>"},{"location":"office-hours/faq/#which-language-models-are-best-for-rag-retrieval-augmented-generation-applications","title":"Which language models are best for RAG (Retrieval-Augmented Generation) applications?","text":"<p>The choice depends on your specific requirements. GPT-4 is commonly used for standard implementations, while GPT-3.5 may be sufficient for applications where speed is critical. Gemini is popular for RAG applications due to its large context window, allowing you to include more retrieved content without worrying about token limits. For high-stakes applications where accuracy is paramount, GPT-3.5 is sometimes preferred despite being older, as it can be more reliable for certain use cases.</p>"},{"location":"office-hours/faq/#how-should-i-approach-improving-my-ai-applications-performance","title":"How should I approach improving my AI application's performance?","text":"<p>Focus on systematic evaluation before making changes. Create a representative dataset of real user queries, then establish metrics that align with your business goals. Prioritize experiments based on potential impact and resource constraints\u2014you can only run a limited number of experiments in a given timeframe. Remember that improving AI performance is an iterative process requiring continuous testing and refinement rather than a one-time fix.</p>"},{"location":"office-hours/faq/#what-are-effective-ways-to-manage-expectations-when-implementing-ai-solutions","title":"What are effective ways to manage expectations when implementing AI solutions?","text":"<p>Be transparent about both capabilities and limitations. Help stakeholders understand that AI implementation is an iterative process requiring ongoing refinement rather than a one-time deployment. Clearly define the role of AI as a tool to assist humans rather than replace them completely. For specialized fields like architecture or engineering, emphasize that professionals still need to make the final decisions, with AI serving as a support system that provides suggestions and identifies potential issues.</p>"},{"location":"office-hours/faq/#how-can-i-integrate-visuals-and-diagrams-into-ai-generated-reports","title":"How can I integrate visuals and diagrams into AI-generated reports?","text":"<p>This is an emerging area with promising developments. Consider implementing systems that can intelligently select and incorporate relevant images from your existing resources. For technical applications like construction or engineering, the ability to include screenshots of blueprints with annotations highlighting specific areas of concern can significantly enhance the value of AI-generated reports. Libraries like Mermaid for diagram generation are becoming more widely supported and can be integrated into AI workflows.</p>"},{"location":"office-hours/faq/#how-should-ai-applications-be-priced-to-capture-appropriate-value","title":"How should AI applications be priced to capture appropriate value?","text":"<p>Consider moving beyond usage-based pricing (like per-token or per-user) toward outcome-based models that align with the actual business value delivered. For example, charging per resolved customer support ticket rather than per API call creates better alignment between your pricing and the value customers receive. This shift requires building systems with sufficient accountability and measurement capabilities to track outcomes reliably. The most innovative pricing approaches treat AI capabilities as replacements for headcount rather than as traditional software tools.</p>"},{"location":"office-hours/faq/#whats-the-relationship-between-data-collection-and-future-ai-capabilities","title":"What's the relationship between data collection and future AI capabilities?","text":"<p>Every AI application serves as a sensor that generates valuable data. The applications built today create the datasets that will enable more advanced AI capabilities tomorrow. Proprietary datasets from specialized industries (like investment decisions, supply chain operations, or construction projects) are particularly valuable for building domain-specific AI capabilities. The most interesting future developments will likely come from analyzing how humans interact with AI systems, creating a feedback loop of continuous improvement.</p>"},{"location":"office-hours/faq/#how-should-i-approach-dynamic-data-visualization-in-ai-generated-reports","title":"How should I approach dynamic data visualization in AI-generated reports?","text":"<p>When creating AI-generated reports with dynamic visualizations, there are several approaches to consider depending on your specific needs.</p> <p>For deep research-style reports (like those from Gemini, Claude, or OpenAI), the LLM typically decides on a set of subtasks and executes them sequentially. These reports often don't include charts or visualizations by default, though OpenAI's deep research does incorporate images.</p> <p>For more structured reports with visualizations, I see three main approaches:</p> <ol> <li>Post-hoc image addition: You can have the LLM identify places where supplementary images would enhance the text, then add them afterward.</li> <li>Image citations during research: Treat images as another citation source that the LLM can reference while generating text. For example, with a client yesterday, the LLM decided to include an org chart in a leadership report because it had access to an org chart JPEG file during generation.</li> <li>Mermaid diagrams: These are particularly useful for creating dynamic visualizations directly in Markdown. The key challenge is validation - if Claude generates an incorrect Mermaid diagram, it simply fails to render. You need a validation loop or external server to check the diagram code, report errors, and iterate to fix them.</li> </ol> <p>For standard data visualizations, most companies use JavaScript libraries like Recharts, which allow you to pass data as props and generate visualizations.</p> <p>The approach depends on whether your report format is flexible or fixed. If fixed, each header might have its own RAG workflow - for example, every competitor analysis might need a leadership team section, which triggers a subtask to find the leadership team of the target company.</p>"},{"location":"office-hours/faq/#how-can-we-handle-styling-challenges-in-professional-reports","title":"How can we handle styling challenges in professional reports?","text":"<p>One of the biggest challenges in AI report generation is matching the exact styling expectations of professional reports. I work with companies that sell to consultants like McKinsey, and the hardest part isn't generating the content - it's making the slides and plots look exactly like McKinsey-branded material.</p> <p>While it's easy to plug in matplotlib or Recharts, it's extremely difficult to match the precise styling requirements of professional consulting firms. Some clients are literally saying, \"We're not going to pay you any of that $80,000 unless you can make it look like we actually made this.\"</p> <p>These firms often use specialized software from the early 2000s for plot generation, with very specific requirements about legend shapes, marker styles (X's versus T's), and other formatting details. The styling is so challenging that we're considering using computer vision to train systems to use PowerPoint and implement styling changes based on feedback comments.</p> <p>I believe there's a significant market opportunity here - you could easily sell software that generates McKinsey-style plots for $100,000 to an analyst team. The last 5% of styling is what makes the difference between something that looks AI-generated versus professionally produced.</p> <p>Key Takeaway: The styling challenge represents a major opportunity for AI tools that can match the exact visual requirements of professional consulting firms. The technical content generation is often easier than matching the precise styling expectations that make reports look professionally produced.</p> <p>Key Takeaway</p> <p>The styling challenge represents a major opportunity for AI tools that can match the exact visual requirements of professional consulting firms. The technical content generation is often easier than matching the precise styling expectations that make reports look professionally produced.</p>"},{"location":"office-hours/faq/#how-should-i-approach-analyzing-unstructured-customer-feedback-data","title":"How should I approach analyzing unstructured customer feedback data?","text":"<p>For a project like Netflix's customer feedback analysis, where you're collecting unstructured data through a \"report a problem\" feature, I recommend a hybrid approach combining semantic search with structured analysis.</p> <p>First, consider doing hierarchical clustering to build a taxonomy of error categories. This gives you a structured way to analyze the data beyond just semantic search. By tagging all feedback with these hierarchical categories, you can provide accurate counts and faceted navigation.</p> <p>When a user asks \"What are members saying about Seinfeld's aspect ratio?\", you might return 10-20 semantically relevant results, but also show facets like \"200 comments in this category, 80 in that category\" to help them understand the distribution of issues.</p> <p>This approach allows users to traverse the data in interesting ways - they might start with audio issues, discover that 20% of complaints are about Seinfeld, then dig into which season has the most problems. The goal is giving users a portfolio of tools to explore the hierarchy rather than just semantic search alone.</p> <p>For quantitative questions like \"How many audio sync issues were reported in Brazil last month?\", you need structured data. The LLM will hallucinate counts if you rely solely on semantic search. By building lightweight classifiers for common issues, you can provide accurate counts while still allowing semantic exploration of the unstructured text.</p> <p>I worked with a company called Interpret that built something similar - a chatbot that could talk to customer feedback and give realistic counts by combining semantic understanding with structured analysis.</p> <p>Key Takeaway: The most effective approach combines semantic search with structured analysis through hierarchical clustering and classification. This gives users both the flexibility to explore feedback semantically and the accuracy of structured data for quantitative questions.</p> <p>Key Takeaway</p> <p>The most effective approach combines semantic search with structured analysis through hierarchical clustering and classification. This gives users both the flexibility to explore feedback semantically and the accuracy of structured data for quantitative questions.</p>"},{"location":"office-hours/faq/#whats-the-best-way-to-build-fast-classifiers-for-unstructured-data","title":"What's the best way to build fast classifiers for unstructured data?","text":"<p>When you need to quickly classify unstructured data, there are several approaches depending on your requirements.</p> <p>One approach is using embedding-based classification. As Jan mentioned, OpenAI's documentation describes a simple technique where you embed category descriptions and then classify items by finding the closest category embedding. This works well for straightforward classification tasks and is extremely fast to implement.</p> <p>In my previous work, we used a matrix-based approach where we'd embed all products in a matrix, then learn another matrix to multiply by the product embeddings whenever we needed to build a classifier. This allowed us to label about 1,000 examples, learn the weights, and then multiply the entire product space by that vector to get predictions for every product. It was very fast but typically achieved around 85% accuracy.</p> <p>For Netflix's feedback analysis, you might want to combine pre-defined categories from domain experts with data-driven clusters discovered through analysis. There will be common issues like rendering problems or audio sync issues that domain experts can define, plus a longer tail of soft clusters that emerge from the data.</p> <p>The key is building a system that can quickly create and apply these classifiers as new issues emerge. When a new feature launches, you want to detect feedback about it immediately, even if it wasn't in your training data.</p> <p>Key Takeaway: Fast classifier development is essential for responsive feedback analysis. Combining embedding-based approaches with domain expertise allows you to quickly identify both known issues and emerging patterns in user feedback.</p> <p>Key Takeaway</p> <p>Fast classifier development is essential for responsive feedback analysis. Combining embedding-based approaches with domain expertise allows you to quickly identify both known issues and emerging patterns in user feedback.</p>"},{"location":"office-hours/faq/#how-should-we-think-about-tool-based-approaches-versus-semantic-search","title":"How should we think about tool-based approaches versus semantic search?","text":"<p>I believe we're moving toward a world where many RAG applications will use tool-based approaches rather than pure semantic search, especially for structured data.</p> <p>In the coming weeks, we'll have talks from teams building coding agents that use a portfolio of tools rather than semantic search. Their thesis is that for structured data, the right way to prepare context isn't one semantic search request, but an agent using multiple tools to build context incrementally.</p> <p>Think about how you debug an error message - you see the error came from a specific file, so you load that file, then you find the function causing the issue, load that file, and traverse the file tree building context before solving the problem. Coding agents are implementing this approach rather than embedding all code.</p> <p>You can implement this with simple tools like \"ls\" (list files), \"read_file\", and \"grep\". The agent uses these tools to navigate the data, building context as it goes. This approach might cost more at query time but requires less preprocessing of data.</p> <p>I'm curious if this approach would work for traversing complex documents like 1,000-page PDFs. Instead of embedding everything, you could provide tools like \"list_table_of_contents\", \"grep\", \"show_page\", and \"show_page_as_image\". The agent could navigate the document naturally, finding references and following them just as a human would.</p> <p>Key Takeaway: Semantic search is most valuable when the producer and consumer of data don't share vocabulary. For structured data or documents with clear organization, a tool-based approach that mimics human navigation may be more effective and require less preprocessing.</p> <p>Key Takeaway</p> <p>Semantic search is most valuable when the producer and consumer of data don't share vocabulary. For structured data or documents with clear organization, a tool-based approach that mimics human navigation may be more effective and require less preprocessing.</p>"},{"location":"office-hours/faq/#what-are-you-working-on-with-your-cura-project","title":"What are you working on with your Cura project?","text":"<p>We're making progress on building out Cura, which is an open-source project (not quite a product yet) focused on analyzing conversation data. In the next few days, we'll be benchmarking it on about a thousand conversations to see what patterns we discover.</p> <p>The core of the project involves hierarchical clustering, explaining clusters, and generating names for these clusters. We're planning to download every open-source chat conversation dataset and run our analysis on it to see what we find.</p> <p>My philosophy with any product I build is that it should function as a sensor that generates data. I want to \"trick\" users into labeling data for me. If I don't know which chart type works best, I'll generate three options and ask users to delete the ones they don't want. My messaging is that it's important for them to review the data, but I'm actually collecting valuable feedback on what visualizations work best.</p> <p>We apply the same approach to citations in paragraphs - users can mouse over citations to see the source data and delete or regenerate citations they don't trust. This creates a feedback loop that continuously improves the system.</p> <p>Key Takeaway: Building products that function as data collection sensors is a powerful approach. By giving users options and tracking their choices, you can gather valuable feedback that improves your system while providing a better user experience.</p> <p>Key Takeaway</p> <p>Building products that function as data collection sensors is a powerful approach. By giving users options and tracking their choices, you can gather valuable feedback that improves your system while providing a better user experience.</p>"},{"location":"office-hours/faq/#what-upcoming-content-are-you-excited-about-in-the-course","title":"What upcoming content are you excited about in the course?","text":"<p>I'm particularly excited about the second half of the course where we'll dive deeper into data analysis and explore the portfolio of tools approach.</p> <p>In the coming weeks, we'll have talks from Reducto, one of the best PDF parsing libraries available right now. They have contracts with companies like Vanta and government agencies and have achieved impressive results.</p> <p>We'll also be inviting teams building coding agents, including the Augment team and the Klein team. These companies are focusing less on RAG with semantic search and more on RAG using a portfolio of tools. Their thesis is that for structured data like code, the right approach isn't one semantic search request but an agent using multiple tools to build context.</p> <p>Beyond the course, I'm organizing a speaker series with guests from OpenAI's memory team and possibly Claude Code. My goal is to bring in the most interesting speakers in the field to share their insights.</p> <p>Key Takeaway: The future of RAG systems, especially for structured data like code, may involve less semantic search and more tool-based approaches where agents navigate information using a portfolio of tools to build context incrementally.</p> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p> <p>Key Takeaway</p> <p>The future of RAG systems, especially for structured data like code, may involve less semantic search and more tool-based approaches where agents navigate information using a portfolio of tools to build context incrementally.</p>"},{"location":"office-hours/faq/#how-effective-is-fine-tuning-for-improving-citation-accuracy","title":"How effective is fine-tuning for improving citation accuracy?","text":"<p>When working with citation requirements, fine-tuning can dramatically reduce error rates. In one project, we used OpenAI's fine-tuning API with about 1,000 examples to improve our marketing content generation system.</p> <p>The results were impressive - our error rates dropped from around 4% to essentially 0% on our test set of 200 examples. We didn't need complex frameworks like Fluoro since we weren't hosting local models, just using OpenAI's API directly.</p> <p>The key was having evaluators validate our offline data, filtering out incorrectly formatted examples before using them in the fine-tuning process. This approach worked particularly well because we weren't trying to change the model's knowledge - just its formatting behavior.</p> <p>When determining how much data you need, I recommend experimenting with different sample sizes:</p> <p>\"What I would often do is try to use a subset of my data for fine-tuning, then increase the sample size and figure out what that curve looks like. It's going to be performance versus volume.\"</p> <p>Different models will have different learning curves - a 1.3 billion parameter model might flatten out at 10,000 data points, while larger models might show different patterns. Adjusting learning rates can also affect these curves.</p> <p>Key Takeaway: Fine-tuning can be remarkably effective for formatting tasks like citation, often requiring less data than you might expect. Start with small batches, measure performance, and increase data volume until you reach your desired accuracy level.</p> <p>Key Takeaway</p> <p>Fine-tuning can be remarkably effective for formatting tasks like citation, often requiring less data than you might expect. Start with small batches, measure performance, and increase data volume until you reach your desired accuracy level.</p>"},{"location":"office-hours/faq/#should-we-shuffle-citation-sources-during-fine-tuning","title":"Should we shuffle citation sources during fine-tuning?","text":"<p>When fine-tuning models to cite sources correctly, shuffling the order of retrieved sources can be beneficial to prevent position bias. This approach makes the model invariant to the order of sources, which is particularly important if you're not sorting by relevance.</p> <p>However, if you are sorting by relevance, maintaining the original order might actually be preferable: \"Maybe it is important for the model to know that the first text chunk is the most relevant text chunk.\"</p> <p>The need for shuffling may also depend on the context length of your model. With older, smaller context models (like 4K token models), position bias was more pronounced due to the \"lost in the middle\" effect. Newer models with better attention mechanisms have improved recall across their context window.</p> <p>\"If you look at the newer models, they just have way better lost-in-the-middle sensitivity in general, and I would expect that when you fine-tune these things, they also preserve some of that ability to attend over long contexts.\"</p> <p>The real challenge that remains is reasoning over multiple \"needles\" of information scattered throughout a document - connecting facts from different sections remains difficult for most models.</p> <p>Key Takeaway: Consider shuffling citation sources during fine-tuning if you want position-invariant citations, but if you're sorting by relevance, maintaining order may be beneficial. Newer models have better attention across their context window, reducing the need for this technique.</p> <p>Key Takeaway</p> <p>Consider shuffling citation sources during fine-tuning if you want position-invariant citations, but if you're sorting by relevance, maintaining order may be beneficial. Newer models have better attention across their context window, reducing the need for this technique.</p>"},{"location":"office-hours/faq/#how-should-we-approach-tool-design-for-specialized-retrieval-tasks","title":"How should we approach tool design for specialized retrieval tasks?","text":"<p>When designing tools for retrieval systems, focus on creating a portfolio of specialized tools rather than just distinguishing between semantic and structured data. The key question isn't \"Am I searching semantic or structured data?\" but rather \"What is the portfolio of tools I want to expose to my system?\"</p> <p>For example, in a construction use case, we implemented several specialized tools:</p> <ul> <li>Generic document search that searches everything</li> <li>Contact search for finding people</li> <li>RFI (Request for Information) search that takes specific RFI codes</li> <li>Contract search that returns not just text chunks but also responsible parties</li> </ul> <p>The implementation details (whether it's semantic search or structured data) matter less than how you present these tools to the language model. Your focus should be on making sure the model understands what tool to use and when.</p> <p>For evaluating tool selection, I recommend having the model \"write a plan of all the tools it might want to use\" for a given query, then evaluating that plan first. You can even present this plan to users for approval before execution, which creates valuable training data based on acceptance rates.</p> <p>\"That gets you a pretty good dataset in terms of customer plan acceptance rates, and then you can look at the ones that are not accepted and figure out what you need to do afterwards.\"</p> <p>The naming of tools significantly impacts how models use them. In coding agents, for example, providing a specific \"grep\" tool versus just mentioning grep in the command line instructions can change execution patterns by 2% in evaluations.</p> <p>Key Takeaway: Design a portfolio of specialized tools based on specific use cases rather than general data types. Focus on clear tool descriptions and evaluate how well the model selects the appropriate tools for different queries.</p> <p>Key Takeaway</p> <p>Design a portfolio of specialized tools based on specific use cases rather than general data types. Focus on clear tool descriptions and evaluate how well the model selects the appropriate tools for different queries.</p>"},{"location":"office-hours/faq/#how-can-we-handle-temporal-reasoning-in-medical-data","title":"How can we handle temporal reasoning in medical data?","text":"<p>One of the most challenging aspects of working with medical data is reasoning about information across a timeline. When retrieving documents about medications, for example, you might get 20 documents all describing medications, but understanding what changed over time requires special handling.</p> <p>\"You might want to know what changed over time, or you have to always see it in the context of time. And you also need to find relationships like 'there's this medication and the patient became worse' or 'this medication went up' - that all needs to be conceived in the system.\"</p> <p>For presenting temporal data effectively to models, I recommend structuring it as a markdown table whenever possible. In our testing, markdown tables performed 12% better than CSV, JSON, or YAML formats for complex lookup tasks across large datasets.</p> <p>\"We've done tests where I put like 6,000 rows, 50 columns as CSV, as markdown, as JSON, as YAML - and markdown tables is like 12% better in terms of identifying on row X where the value is Y, find me the row that's one above and one to the left.\"</p> <p>The ordering of temporal data also matters significantly. You might get different results if you order events in ascending versus descending time. This affects how the model scans and reasons about cause and effect relationships.</p> <p>For building better temporal reasoning capabilities, consider:</p> <ol> <li> <p>Ordering retrieved documents chronologically</p> </li> <li> <p>Presenting data in markdown table format with clear timestamps</p> </li> <li> <p>Having the model first extract and reorganize relevant information before reasoning about it</p> </li> <li> <p>Mining reasoning chains from expert users to create training data</p> </li> </ol> <p>Key Takeaway: For temporal reasoning, structure data chronologically in markdown tables and implement a two-stage approach where the model first extracts and organizes relevant timeline information before reasoning about it.</p> <p>Key Takeaway</p> <p>For temporal reasoning, structure data chronologically in markdown tables and implement a two-stage approach where the model first extracts and organizes relevant timeline information before reasoning about it.</p>"},{"location":"office-hours/faq/#whats-the-difference-between-multi-agent-and-single-agent-approaches","title":"What's the difference between multi-agent and single-agent approaches?","text":"<p>The debate between multi-agent and single-agent systems often comes down to context coordination challenges. For coding tasks, Devin (from Cognition) chose a single-threaded approach because coordinating between agents modifying different parts of a codebase is extremely difficult.</p> <p>\"If one agent modifies one directory and another agent modifies another directory, that communication channel is sort of not well defined yet.\"</p> <p>In contrast, Claude's Deep Research uses multiple agents, but they're all read-only - they don't need to coordinate changes because they're just retrieving information that will later be combined:</p> <p>\"In that multi-agent system, the agents are all read-only, so they don't need to manage that communication overhead because they're all going to be reduced. If I search about who I am, one agent searches childhood, one agent searches career, and once they bring all the information back, they can be reduced.\"</p> <p>The primary benefit of multi-agent systems appears to be token efficiency - you can use more tokens across multiple agents than with a single agent. \"The performance just increases with the amount of tokens each sub-agent is able to consume. If you have 10 sub-agents, you can use more tokens, and your research quality is better.\"</p> <p>For medical data applications that are primarily read-only, a multi-agent approach might work, but the challenge remains in ensuring no context is missed when combining information from different agents.</p> <p>Key Takeaway: Choose multi-agent approaches for read-only tasks where you need to process more tokens than a single context window allows. For tasks requiring coordination of changes, single-agent approaches remain more practical until better coordination mechanisms are developed.</p> <p>Key Takeaway</p> <p>Choose multi-agent approaches for read-only tasks where you need to process more tokens than a single context window allows. For tasks requiring coordination of changes, single-agent approaches remain more practical until better coordination mechanisms are developed.</p>"},{"location":"office-hours/faq/#how-can-we-use-document-summarization-to-improve-retrieval","title":"How can we use document summarization to improve retrieval?","text":"<p>Generating summaries during document ingestion can be a cost-effective approach to improving retrieval. Summaries function as a form of compression and can be particularly valuable when working with smaller context window models.</p> <p>\"In general, this is a good idea because that's almost in some ways just a more cost-effective way of doing this contextual retrieval stuff. Summary is just compression.\"</p> <p>The key is designing your summarization prompt based on the specific tasks your system needs to perform. For example, with architectural blueprints, we knew users would ask about room counts and dimensions, so we created summaries that explicitly counted and listed these elements:</p> <p>\"Because we know that our tasks involve things like extracting the names of rooms and counting things, if our language model can have a summary that counts everything, then it becomes much easier to think about 'the place with 4 bedrooms and 2 bathrooms.'\"</p> <p>We implemented this as a separate document search tool that only hits the summaries. Through iteration and evaluation, we improved our summary generation from 16% recall to 85% recall in just a few days.</p> <p>For implementation, you can:</p> <ol> <li> <p>Create a separate \"search summaries\" tool</p> </li> <li> <p>Design summary prompts that extract the specific types of information users will query</p> </li> <li> <p>Evaluate and iterate on summary quality using test queries</p> </li> <li> <p>Use summaries as synthetic text chunks that supplement your existing text chunks</p> </li> </ol> <p>This approach works particularly well for documents like financial reports, where structured information can be extracted, or for multimedia content where describing images or videos in text makes them searchable.</p> <p>Key Takeaway: Document summarization during ingestion creates valuable synthetic text chunks that can dramatically improve retrieval performance. Design summary prompts based on the specific information needs of your application and iterate based on evaluation results.</p> <p>Key Takeaway</p> <p>Document summarization during ingestion creates valuable synthetic text chunks that can dramatically improve retrieval performance. Design summary prompts based on the specific information needs of your application and iterate based on evaluation results.</p>"},{"location":"office-hours/faq/#how-can-we-implement-price-quote-generation-using-rag","title":"How can we implement price quote generation using RAG?","text":"<p>One practical application we've built is an automated price quote system for sales teams. After multiple calls with a prospect, the system generates personalized pricing options and potential upsells.</p> <p>The process works like this:</p> <ol> <li> <p>We have 16 pages of pricing information (per-seat pricing, volume discounts, prepayment options)</p> </li> <li> <p>We have transcripts from 6 phone calls with the prospect</p> </li> <li> <p>We ask the language model to:</p> </li> <li> <p>Read the transcripts and list all relevant variables</p> </li> <li>Extract the values of those variables</li> <li>Reason about the pricing document</li> <li>Propose options and upsells</li> <li>Write an email to the prospect</li> </ol> <p>\"The email's like 'Great talking to you, Tim. It sounds like for a company your size, you can probably commit to 15 seats. This will get you a 20% discount. If you don't use it, we'll move it to next year, and if you pay upfront, we can give you another 20-25% discount because I know that's something your CTO really values.'\"</p> <p>Our evaluation method is simple but effective - we have salespeople review the generated emails before sending them, and we track whether they make edits. When edits are needed, we analyze what went wrong in the reasoning step.</p> <p>This approach of extracting variables, reasoning about them, and then generating output could be applied to medical data as well. For example, if a patient shows drowsiness, the system could first extract all timeline information about drowsiness, then reason about potential causes.</p> <p>Key Takeaway: For complex reasoning tasks, implement a multi-step process where the model first extracts and organizes relevant information, then reasons about it, and finally generates output. This structured approach makes the reasoning more transparent and easier to evaluate.</p> <p>Key Takeaway</p> <p>For complex reasoning tasks, implement a multi-step process where the model first extracts and organizes relevant information, then reasons about it, and finally generates output. This structured approach makes the reasoning more transparent and easier to evaluate.</p>"},{"location":"office-hours/faq/#whats-the-best-way-to-format-data-for-language-models","title":"What's the best way to format data for language models?","text":"<p>When presenting structured data to language models, markdown tables consistently outperform other formats like CSV, JSON, or YAML. In our testing, markdown tables were 12% more effective for complex lookup tasks.</p> <p>\"We've done tests where I put like 6,000 rows, 50 columns as CSV, as markdown, as JSON, as YAML - and markdown tables is like 12% better in terms of identifying on row X where the value is Y, find me the row that's one above and one to the left.\"</p> <p>The formatting details matter significantly. For example, having spaces between tokens in markdown tables (like \"| data |\" instead of \"|data|\") affects how the model processes the information.</p> <p>\"If I search for the word Jason, the token is 'space Jason'. But if it's Jason in JSON, it's actually 'quote Jason' - those are different tokens. And so those things end up affecting the lookup a little bit.\"</p> <p>These seemingly minor formatting choices can have meaningful impacts on model performance, especially for tasks requiring precise information retrieval or table navigation.</p> <p>For temporal data specifically, presenting information in chronological order (either ascending or descending) can significantly affect how models reason about cause and effect. Testing both approaches is worthwhile, as one may work better than the other depending on your specific use case.</p> <p>Markdown tables consistently outperform other data formats for structured information. Pay attention to spacing and formatting details, as they affect tokenization and retrieval performance. For temporal data, experiment with both chronological and reverse-chronological ordering.</p>"},{"location":"office-hours/faq/#how-should-we-approach-end-to-end-evaluation-of-complex-rag-systems","title":"How should we approach end-to-end evaluation of complex RAG systems?","text":"<p>End-to-end evaluation of complex retrieval systems remains challenging, especially when there isn't a single correct answer or when the system needs to perform multi-step reasoning.</p> <p>\"The end-to-end evaluation of these kinds of things are still pretty challenging, unless it really is the case that there are just certain text chunks that we're trying to achieve or certain answers we already know ahead of time.\"</p> <p>For tool selection, one effective approach is evaluating the system's planning capabilities:</p> <ol> <li> <p>Ask the model to write a plan of which tools it would use for a query</p> </li> <li> <p>Evaluate the plan before executing it</p> </li> <li> <p>Allow users to approve or reject the plan</p> </li> <li> <p>Track plan acceptance rates and analyze rejected plans</p> </li> </ol> <p>For reasoning tasks, breaking evaluation into steps can be helpful:</p> <ol> <li> <p>Evaluate information extraction (did the system find the relevant information?)</p> </li> <li> <p>Evaluate reasoning (given the correct information, did it reach valid conclusions?)</p> </li> <li> <p>Evaluate output generation (was the final response clear and actionable?)</p> </li> </ol> <p>In some domains like coding, the evaluation metrics are clearer - does the code pass tests? In other domains like medical reasoning, evaluation may require expert review or comparison to known outcomes.</p> <p>For systems like our price quote generator, we use a practical metric - do salespeople edit the generated emails before sending them? This real-world usage metric helps us identify where the system's reasoning falls short.</p> <p>Key Takeaway: Break evaluation into component parts rather than relying solely on end-to-end metrics. Incorporate user feedback into your evaluation process, and track how often outputs require human editing or intervention.</p> <p>Key Takeaway</p> <p>Break evaluation into component parts rather than relying solely on end-to-end metrics. Incorporate user feedback into your evaluation process, and track how often outputs require human editing or intervention.</p>"},{"location":"office-hours/faq/#how-does-fine-tuning-improve-citation-accuracy-in-llms","title":"How does fine-tuning improve citation accuracy in LLMs?","text":"<p>Fine-tuning can dramatically reduce error rates when teaching models to properly cite sources. In one example, fine-tuning reduced citation errors from 4% to nearly 0% for marketing content generation. The process involves collecting properly formatted examples, validating them, filtering out incorrect formats, and using them in the fine-tuning process.</p>"},{"location":"office-hours/faq/#how-many-examples-are-typically-needed-for-effective-fine-tuning","title":"How many examples are typically needed for effective fine-tuning?","text":"<p>Around 1,000 high-quality examples can be sufficient for format-related fine-tuning tasks. However, the exact number depends on your specific use case. It's recommended to experiment with increasing sample sizes to determine the optimal amount for your needs. Start with a smaller subset and gradually increase to identify where performance improvements begin to plateau.</p>"},{"location":"office-hours/faq/#should-i-shuffle-the-order-of-retrieved-sources-in-my-fine-tuning-dataset","title":"Should I shuffle the order of retrieved sources in my fine-tuning dataset?","text":"<p>Shuffling retrieved sources can be beneficial to make your model invariant to the order of information. This approach helps prevent the model from developing biases toward information presented first. However, if your retrieval system sorts by relevance, maintaining that order might be important as the first chunk would genuinely contain the most relevant information.</p>"},{"location":"office-hours/faq/#how-should-i-approach-tool-selection-for-my-llm-application","title":"How should I approach tool selection for my LLM application?","text":"<p>Focus on developing a portfolio of specialized tools rather than simply categorizing between semantic and structured data searches. Consider what specific capabilities would benefit your use case, such as date-range filtering, categorical filters, or metadata tag filtering. The implementation details (whether semantic or structured) matter less than ensuring your model understands when to use each tool.</p>"},{"location":"office-hours/faq/#whats-an-effective-way-to-evaluate-tool-selection-by-the-model","title":"What's an effective way to evaluate tool selection by the model?","text":"<p>A practical approach is to have the model write a plan listing all tools it would use for a given query, then evaluate that plan before execution. You can present this plan to users for approval or rejection, which generates valuable feedback data. Analyzing rejected plans helps identify improvements needed in your tool selection and routing logic.</p>"},{"location":"office-hours/faq/#how-do-coding-agents-approach-tool-integration","title":"How do coding agents approach tool integration?","text":"<p>Coding agents have made significant progress with tool integration. One key insight is that providing named tools for specific functions (rather than general capabilities) significantly changes how frequently these functions are used. For example, providing a dedicated \"grep\" tool versus expecting the model to remember to use grep through a general command line interface can improve performance by several percentage points.</p>"},{"location":"office-hours/faq/#how-should-i-organize-timeline-based-data-for-llm-processing","title":"How should I organize timeline-based data for LLM processing?","text":"<p>For timeline data, consider presenting information in a markdown table format, which models tend to process effectively. Order the data chronologically (either ascending or descending) and include clear date markers. This organization helps the model understand temporal relationships and reason about cause and effect. Testing both ascending and descending time orders may yield different results depending on your use case.</p>"},{"location":"office-hours/faq/#why-are-markdown-tables-particularly-effective-for-structured-data","title":"Why are markdown tables particularly effective for structured data?","text":"<p>Markdown tables have shown superior performance (approximately 12% better) compared to other formats like CSV, JSON, or YAML when models need to perform lookup tasks or understand relationships between data points. The spacing between tokens in markdown tables appears to be particularly well-suited to how models process information.</p>"},{"location":"office-hours/faq/#how-can-i-help-models-reason-across-complex-information","title":"How can I help models reason across complex information?","text":"<p>For complex reasoning tasks, consider implementing a two-step approach: first have the model extract and reorganize all relevant information from different sources, then reason about this reorganized information. This approach works well for tasks requiring synthesis across multiple data points, such as analyzing medical timelines or generating pricing quotes based on multiple conversations.</p>"},{"location":"office-hours/faq/#is-it-beneficial-to-generate-summaries-during-data-ingestion","title":"Is it beneficial to generate summaries during data ingestion?","text":"<p>Creating summaries during data ingestion can be very effective, especially for longer documents. Summaries act as compressed versions of your data that can be more efficiently processed. For specific use cases like blueprints or financial documents, you can design summarization prompts that extract the most relevant information (like room counts or key financial figures) to make subsequent queries more efficient.</p>"},{"location":"office-hours/faq/#how-can-i-handle-reasoning-across-multiple-documents","title":"How can I handle reasoning across multiple documents?","text":"<p>For reasoning across multiple documents, consider having the model first extract all relevant information related to the query, reorganize it (possibly chronologically or thematically), and then reason about the reorganized information. This approach helps manage context limitations and focuses the model's attention on the most pertinent details.</p>"},{"location":"office-hours/faq/#whats-the-best-way-to-handle-long-context-windows","title":"What's the best way to handle long context windows?","text":"<p>Newer models with improved attention mechanisms handle long contexts better than older models. However, for complex reasoning tasks involving multiple \"needles\" of information spread throughout a document, consider using tools that first organize the relevant information before reasoning about it. This approach remains effective even with models that have long context windows.</p> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"office-hours/faq/#how-should-i-approach-dynamically-generating-and-handling-metadata-for-documents","title":"How should I approach dynamically generating and handling metadata for documents?","text":"<p>When dealing with the need to extract new metadata from existing documents, the architectural approach depends largely on your current infrastructure. Most companies I work with already have some existing setup, so we're rarely building from scratch.</p> <p>In essence, this is just like any ETL (Extract, Transform, Load) job where a process creates a new database artifact. The key question is: what makes backfilling this data challenging in your specific context? Is it the cost of reprocessing millions of documents? Is it the unpredictability of expenses?</p> <p>For cost estimation, I recommend calculating the token volume of your data. We had a task to summarize a million conversations, and we made sure to calculate the expected input and output tokens. This allowed us to make informed decisions about model selection - for instance, we discovered that using open source models was only 8 times cheaper than using OpenAI's API.</p> <p>\"I was really disappointed to realize that the open source models are only 8 times cheaper. We're putting all this effort to save $60. And that was for a million conversations - it cost $60 to summarize a million conversations. These models are just so cheap now.\"</p> <p>For specialized extraction tasks, consider using smaller, purpose-built models. At Stitch Fix, we built a suite of small models doing specific extractions. For example, we realized we were selling belts with pants that had no belt loops, so we created a simple computer vision model to detect belt loops. This approach was efficient and solved a specific business problem worth millions of dollars.</p> <p>Key Takeaway: Calculate token volumes and costs before deciding on your extraction approach. Sometimes the cost difference between APIs and self-hosted models is smaller than expected, making the engineering effort to switch questionable. For specialized extractions, consider purpose-built models that solve specific business problems rather than trying to do everything with one large model.</p> <p>Key Takeaway</p> <p>Calculate token volumes and costs before deciding on your extraction approach. Sometimes the cost difference between APIs and self-hosted models is smaller than expected, making the engineering effort to switch questionable. For specialized extractions, consider purpose-built models that solve specific business problems rather than trying to do everything with one large model.</p>"},{"location":"office-hours/faq/#what-are-the-challenges-with-extracting-multiple-attributes-in-a-single-api-call","title":"What are the challenges with extracting multiple attributes in a single API call?","text":"<p>When extracting multiple attributes from documents, be aware that prompts for some attributes can affect the extraction of other attributes. We found this when processing transcripts - when we asked for shorter action items, the summaries would also get shorter.</p> <p>To address this, we split our extraction into separate jobs: one for action items and another for summary and memo generation. This separation gave us better control over each component. We made this approach cost-effective by leveraging prompt caching - the transcript only needed to be processed once, with multiple outputs generated from that single input.</p> <p>Key Takeaway: Be cautious about extracting too many attributes in a single API call, as they can influence each other in unexpected ways. Consider splitting extractions into separate jobs with specific focuses, and use techniques like prompt caching to maintain cost efficiency.</p> <p>Key Takeaway</p> <p>Be cautious about extracting too many attributes in a single API call, as they can influence each other in unexpected ways. Consider splitting extractions into separate jobs with specific focuses, and use techniques like prompt caching to maintain cost efficiency.</p>"},{"location":"office-hours/faq/#how-should-i-approach-recommendation-systems-with-llms","title":"How should I approach recommendation systems with LLMs?","text":"<p>For recommendation systems like predicting product purchases, I wouldn't use an LLM directly in the recommendation system. Companies like Stitch Fix and YouTube use LLMs primarily to create better embeddings, not for the core recommendation logic.</p> <p>The approach I'd recommend is building item embeddings using historical data, where the inputs might include product images, descriptions, user comments, and checkout rates. Similarly, user embeddings would incorporate their feedback, fit comments, and other behavioral signals.</p> <p>One valuable application of LLMs is creating synthetic users to run simulations, particularly for addressing cold-start problems. When a new item appears, there's no transaction or impression data to train on. An LLM can simulate transaction data and returns, helping predict success rates for the first orders.</p> <p>\"At Stitch Fix we needed about 400 shipments of a single SKU before we had a good embedding for it. So our only job was: how do we get to a world where we either can simulate the SKUs or need less data?\"</p> <p>We addressed this by building a \"Tinder for clothes\" where users could swipe left or right on clothing items. This generated 6,000 labels much faster than waiting for 400 actual shipments, as users would label 30 items a day versus receiving only 5 items a month.</p> <p>Key Takeaway: Rather than using LLMs directly for recommendations, use them to generate better embeddings and synthetic data to address cold-start problems. Consider creative ways to gather user preferences at scale, as the velocity of data collection is often the limiting factor in recommendation quality.</p> <p>Key Takeaway</p> <p>Rather than using LLMs directly for recommendations, use them to generate better embeddings and synthetic data to address cold-start problems. Consider creative ways to gather user preferences at scale, as the velocity of data collection is often the limiting factor in recommendation quality.</p>"},{"location":"office-hours/faq/#how-can-i-blend-traditional-ml-with-unstructured-data-from-llms","title":"How can I blend traditional ML with unstructured data from LLMs?","text":"<p>The most promising approach I've seen is using LLMs for synthetic data generation and feature engineering. The challenge with many recommendation systems is the low velocity of data - unlike Spotify or Netflix where users consume content quickly, physical product recommendations might take weeks to validate through purchases and returns.</p> <p>Our focus at Stitch Fix was making each sample more efficient. Instead of building general-purpose computer vision models, we created specialized models for specific attributes (like detecting belt loops). These targeted models were more data-efficient and could directly drive business decisions (like upselling belts with pants that have belt loops).</p> <p>The workflow we found effective was:</p> <ol> <li>Use smaller, data-efficient models for specific extractions</li> <li>Use these models to generate simulations and synthetic data</li> <li>Feed this expanded dataset into larger, more powerful models</li> </ol> <p>\"Can we use LLMs for feature engineering and then use traditional models because they're gonna absorb the data faster? And then, once those cap out, how can we use the traditional models to create more data for the larger models to take in more capacity?\"</p> <p>This approach recognizes that different models have different data efficiency profiles, and leveraging their strengths in combination yields better results than trying to solve everything with a single approach.</p> <p>Key Takeaway: Blend traditional ML with LLMs by using LLMs for feature engineering and synthetic data generation. Build specialized, data-efficient models for specific attributes, then use these to feed larger models. This creates a virtuous cycle where each type of model enhances the capabilities of the others.</p> <p>Key Takeaway</p> <p>Blend traditional ML with LLMs by using LLMs for feature engineering and synthetic data generation. Build specialized, data-efficient models for specific attributes, then use these to feed larger models. This creates a virtuous cycle where each type of model enhances the capabilities of the others.</p>"},{"location":"office-hours/faq/#are-there-good-tools-for-data-engineering-in-the-llm-ecosystem","title":"Are there good tools for data engineering in the LLM ecosystem?","text":"<p>The data engineering landscape for LLMs is still developing, with most early-stage companies using relatively simple approaches like \"data to JSON\" pipelines. One company worth looking at is Tensor Lake, which provides sophisticated data processing for tensors.</p> <p>A critical area that's often overlooked is managing evaluation datasets. Many companies have inconsistent approaches where individual team members export data in ad-hoc ways:</p> <p>\"Almost every company I work with has datasets for evals, but they're all kind of like one guy wrote a SQL query to export things, saved it as a CSV file on their laptop and started working with it. And then they wrote this to Brain Trust, and that's what they're working on. But the other guy on a different team is using a different dataset.\"</p> <p>This creates problems when metrics improve - does anyone trust the results? Was the test data recent or old? Did it cover multiple organizations or just one customer? Proper data engineering for evaluation is a substantial undertaking that requires careful planning and coordination across teams.</p> <p>At Facebook, defining a new table for newsfeed views would involve a data engineer interviewing 20 teams, designing columns to support various query patterns, and ensuring everyone could write consistent SQL queries against the database. This level of rigor is often missing in LLM evaluation setups.</p> <p>Key Takeaway: The data engineering ecosystem for LLMs is still maturing. Pay special attention to how you organize evaluation datasets, as inconsistent approaches lead to unreliable metrics. Consider investing in proper data engineering for your evaluation pipeline, similar to how established companies handle critical data infrastructure.</p> <p>Key Takeaway</p> <p>The data engineering ecosystem for LLMs is still maturing. Pay special attention to how you organize evaluation datasets, as inconsistent approaches lead to unreliable metrics. Consider investing in proper data engineering for your evaluation pipeline, similar to how established companies handle critical data infrastructure.</p>"},{"location":"office-hours/faq/#whats-your-approach-to-topic-modeling-and-specialized-indices","title":"What's your approach to topic modeling and specialized indices?","text":"<p>For topic modeling and specialized indices, we've been developing tools like Kora, which helps with topic extraction from documents. This approach is becoming increasingly valuable as managing knowledge bases becomes more complex.</p> <p>The fundamental issue is that embeddings alone aren't sufficient for many complex queries. If someone asks \"Who is the best basketball player under 25 years old from Europe?\", embeddings might not find a direct answer unless that exact information exists in a paragraph somewhere.</p> <p>This is why we need to build a portfolio of tools rather than relying solely on embeddings. For the basketball player example, you might need:</p> <ol> <li>A structured player database with extracted attributes</li> <li>Specialized extractors that pull out statements about people</li> <li>Tools that can perform semantic search combined with structured filtering</li> </ol> <p>\"It's not that the tools are one-to-one with the retriever. It's actually gonna be the case that we probably have multiple tools hitting the same index.\"</p> <p>This is similar to how command-line tools interact with a file system - you have commands like \"list directories\" and \"view files,\" but also more specialized commands like \"list files sorted by last modified\" or \"list files by editor.\" A smart model can learn to use these various tools rather than trying to build one mega-search tool that works for all cases.</p> <p>Key Takeaway: Don't rely solely on embeddings for complex information retrieval. Build a portfolio of specialized tools that can work with your data in different ways. This approach is gaining traction in code generation and will likely become standard across other domains as well.</p> <p>Key Takeaway</p> <p>Don't rely solely on embeddings for complex information retrieval. Build a portfolio of specialized tools that can work with your data in different ways. This approach is gaining traction in code generation and will likely become standard across other domains as well.</p>"},{"location":"office-hours/faq/#will-reasoning-models-eliminate-the-need-for-specialized-indices","title":"Will reasoning models eliminate the need for specialized indices?","text":"<p>Even with advanced reasoning models that can perform multi-step thinking, I don't believe they'll eliminate the need for specialized indices and tools. Instead, the focus should be on exposing a wide range of tools that these models can leverage.</p> <p>The key insight is that tools aren't necessarily one-to-one with retrievers. You might have multiple tools hitting the same index, similar to how command-line tools interact with a file system. For example, you might have tools for listing directories, viewing files, sorting by modification date, or filtering by editor.</p> <p>\"A smart enough model might just be able to reason about how to use all five tools rather than trying to build a mega search tool that will work in all cases.\"</p> <p>This is the direction that code generation tools are taking - they're finding that embedding your codebase isn't the right approach. Instead, they're building portfolios of tools, and I believe this pattern will spread to other domains as well.</p> <p>Key Takeaway: Even with advanced reasoning capabilities, models benefit from having access to specialized tools rather than trying to do everything through a single approach. The future lies in building portfolios of tools that models can intelligently select and combine, not in creating a single universal solution.</p> <p>Key Takeaway</p> <p>Even with advanced reasoning capabilities, models benefit from having access to specialized tools rather than trying to do everything through a single approach. The future lies in building portfolios of tools that models can intelligently select and combine, not in creating a single universal solution.</p>"},{"location":"office-hours/faq/#how-do-you-approach-cost-calculations-for-ai-processing","title":"How do you approach cost calculations for AI processing?","text":"<p>When calculating costs for AI processing, focus on understanding your token volumes. For any extraction or processing task, calculate the expected input and output tokens to make informed decisions about model selection.</p> <p>We had a surprising discovery when comparing OpenAI's API to open source models for summarizing a million conversations. The open source approach was only 8 times cheaper, saving just $60 total. While it was 26 times faster, the absolute cost was so low that it wasn't worth the engineering effort to switch.</p> <p>\"I was gonna write a blog post on how to use open source models to do the data extraction. I was like, 'Oh, it's not worth writing the blog post because 8 times cheaper for $60? Well, unless I'm doing this a hundred times, I don't need to save $50.'\"</p> <p>These calculations help you make rational decisions about where to invest your engineering time. Sometimes the cost difference between approaches is so small that it's not worth optimizing further, especially when the absolute costs are already low.</p> <p>Key Takeaway: Calculate token volumes and costs before investing in optimization. Modern AI models are often surprisingly affordable at scale, making some optimizations unnecessary. Focus your engineering efforts where they'll have meaningful impact rather than chasing small percentage improvements.</p> <p>Key Takeaway</p> <p>Calculate token volumes and costs before investing in optimization. Modern AI models are often surprisingly affordable at scale, making some optimizations unnecessary. Focus your engineering efforts where they'll have meaningful impact rather than chasing small percentage improvements.</p>"},{"location":"office-hours/faq/#how-should-i-approach-dynamically-generating-and-handling-metadata-for-documents_1","title":"How should I approach dynamically generating and handling metadata for documents?","text":"<p>When building metadata extraction systems that need to evolve over time, consider treating each extraction as a separate ETL (Extract, Transform, Load) job. This approach allows you to add new extraction tasks without redoing everything. Before implementing, calculate the token volume to estimate costs - you might find that even with millions of records, the cost is surprisingly manageable (often just tens of dollars). For specialized extractions, consider using smaller, focused models rather than trying to extract everything in a single pass, as this can provide better control over individual attributes.</p>"},{"location":"office-hours/faq/#is-it-worth-using-open-source-models-for-data-extraction-tasks","title":"Is it worth using open source models for data extraction tasks?","text":"<p>It depends on your specific needs. In many cases, the cost difference between using open source models versus API models like GPT-4 may be smaller than expected - sometimes only 8x cheaper. For a job that costs $60 with an API model, saving $50 might not justify the engineering effort required to implement an open source solution. Always calculate the token volume and expected costs before making this decision, and consider factors beyond cost such as latency and maintenance requirements.</p>"},{"location":"office-hours/faq/#how-can-i-estimate-the-cost-of-running-extraction-jobs-on-large-datasets","title":"How can I estimate the cost of running extraction jobs on large datasets?","text":"<p>Create a table that tracks input token counts for your documents and calculate the expected costs based on current API pricing. This simple exercise can provide valuable insights that inform your architecture decisions. For many extraction tasks, you might find that using models like GPT-4 Mini or similar smaller models is cost-effective enough, especially for straightforward extraction tasks.</p>"},{"location":"office-hours/faq/#should-i-extract-multiple-attributes-in-a-single-api-call-or-separate-them","title":"Should I extract multiple attributes in a single API call or separate them?","text":"<p>It's often better to separate extraction tasks into multiple focused API calls rather than trying to extract everything at once. When multiple attributes are extracted in a single prompt, changes to one attribute's extraction can unintentionally affect others. For example, requesting shorter action items might inadvertently make summaries shorter as well. Breaking these into separate jobs gives you better control, and techniques like prompt caching can help manage costs by avoiding redundant processing of the same input text.</p>"},{"location":"office-hours/faq/#how-can-i-blend-traditional-ml-with-llms-for-recommendation-systems","title":"How can I blend traditional ML with LLMs for recommendation systems?","text":"<p>Rather than using LLMs directly in recommendation systems, consider using them to:</p> <ol> <li>Generate better embeddings for items and users</li> <li>Create synthetic data to help with cold-start problems</li> <li>Simulate user behavior for new items that lack transaction data</li> <li>Extract structured attributes that can feed into traditional recommendation models</li> </ol> <p>At companies like Stitch Fix, the approach has been to use a cascade of models (vision, text, feedback, factorization) that build different scores, then blend these scores into a final probability-of-sale model.</p>"},{"location":"office-hours/faq/#what-are-effective-strategies-for-specialized-indices-versus-general-embeddings","title":"What are effective strategies for specialized indices versus general embeddings?","text":"<p>For complex queries like \"Who is the best European basketball player under 25 years old?\", general embeddings often fall short. Instead, consider:</p> <ol> <li>Building structured data extractors that pull out specific attributes (age, nationality, sport)</li> <li>Creating a portfolio of specialized tools rather than relying on a single embedding approach</li> <li>Using different representations for different types of data</li> <li>Exposing multiple tools that might access the same index in different ways</li> </ol> <p>The trend is moving toward having multiple specialized tools rather than trying to build a single \"mega search tool\" that works for all cases.</p>"},{"location":"office-hours/faq/#how-are-companies-handling-data-engineering-for-llm-applications","title":"How are companies handling data engineering for LLM applications?","text":"<p>Data engineering remains a significant challenge. Many companies are still figuring out best practices for:</p> <ol> <li>Creating and maintaining evaluation datasets</li> <li>Building extraction pipelines that can be easily updated</li> <li>Managing backfills when new attributes need to be extracted</li> <li>Ensuring consistency across teams using the same data</li> </ol> <p>For companies exploring this space, tools like Tensor Lake might be worth investigating, as they're designed for tensor-based data processing at scale.</p>"},{"location":"office-hours/faq/#will-better-reasoning-models-eliminate-the-need-for-specialized-indices","title":"Will better reasoning models eliminate the need for specialized indices?","text":"<p>Not entirely. Even as models improve at reasoning, having a portfolio of specialized tools remains valuable. The approach is shifting toward giving models access to multiple tools that can retrieve and process data in different ways, rather than expecting a single model to handle everything. For example, instead of one mega-search tool, you might have tools for listing directories, viewing files, filtering by metadata, semantic search, and full-text search - all potentially accessing the same underlying data but in different ways.</p> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"office-hours/faq/#additional-resources_3","title":"Additional Resources","text":"<ul> <li>Office Hours Overview</li> <li>Workshop Materials</li> <li>Talks and Presentations</li> </ul>"},{"location":"office-hours/faq/#contributing","title":"Contributing","text":"<p>Found an error or want to suggest improvements to these FAQs? The source files are located in the office hours documentation and can be regenerated using the <code>generate_faq_md.py</code> script.</p>"},{"location":"office-hours/cohort2/week1-summary/","title":"RAG Office Hours Q&amp;A Summary - Week 1","text":""},{"location":"office-hours/cohort2/week1-summary/#what-is-your-take-on-dspy-should-we-use-it","title":"What is your take on DSpy? Should we use it?","text":"<p>Generally, I think DSpy allows you to do some kind of prompt optimization by synthetically creating a bunch of few-shot examples and then identifying which of these examples could improve the performance of your system.</p> <p>Personally, I feel like most of the time you should be spending a lot of time actually just tweaking those prompts yourself. The most valuable part of looking at data, few-shots, and examples is you building an intuition of what customers are looking for and what mistakes the system is making.</p> <p>Your product isn't just a prompt\u2014it includes how you collect feedback, how you set expectations in the UI, how you think about data extraction, and how you represent chunks in the context. If you spend the time to look at how the model is making mistakes and what users are asking for, you'll make much more progress in improving the product as a whole.</p> <p>DSpy is fine especially when you have very specific evaluations. For example, maybe you have a 35-class classification task where all you care about is accuracy. Then DSpy really works because you can figure out which of the 10 examples you need to maximize your accuracy.</p> <p>But most of the time, that's not the case. If I'm building a model to extract sales insights from a transcript, I don't have a dataset of \"here's all the sales insights.\" The real work might be extracting everything and hand-labeling some stuff. Because these tasks are very hard to hill-climb (when metrics aren't just classification accuracy), tools like DSpy don't work as well.</p> <p>Another good use of DSpy is around using LLMs as judges. If you have a tonality or factuality evaluation you really care about, it makes sense to label a hundred examples yourself and then use prompt optimization tools to create your own judge that aligns with your grades.</p>"},{"location":"office-hours/cohort2/week1-summary/#is-it-useful-to-prompt-language-models-with-an-understanding-of-structure-and-rationale-for-their-actions","title":"Is it useful to prompt language models with an understanding of structure and rationale for their actions?","text":"<p>Yes, absolutely. Understanding structure and rationale is critical because your product includes the ways you collect feedback, set expectations in the UI, perform data extraction, and represent chunks in the context.</p> <p>It's not just about the prompt\u2014it's a whole system. And if you can spend time looking at how the model makes mistakes and what users are asking for, you'll make much more progress in improving the product holistically.</p> <p>When you build an intuition for what's happening, you can make smarter design decisions across the entire product experience.</p>"},{"location":"office-hours/cohort2/week1-summary/#how-do-we-introduce-a-concept-of-time-and-vector-search-to-answer-questions-like-whats-the-latest-news-without-needing-to-move-to-a-graph-database","title":"How do we introduce a concept of time and vector search to answer questions like \"What's the latest news?\" without needing to move to a graph database?","text":"<p>The answer is to use a SQL database. If you use something like Timescale or PostgreSQL, there are many ways of doing time filtering.</p> <p>One specific thing to note is the difference between pgvector and pgvector-scale. Pgvector does not do exhaustive search, so there's a chance you don't recall all information because of how the database segments things. With pgvector-scale, it will exhaustively search every single row in your database if required. This small difference means a lot if you're trying to find very specific details.</p> <p>The general idea is to use structured extraction to identify start and end dates, prompt your language model with an understanding of what those dates are, and then use filtering. You would do an embedding search plus a BETWEEN statement in your time query. This works pretty well.</p>"},{"location":"office-hours/cohort2/week1-summary/#is-knowledge-graph-rag-production-ready-by-now-do-you-recommend-it","title":"Is knowledge graph RAG production ready by now? Do you recommend it?","text":"<p>In my 10 years of doing data science and machine learning, I generally stay away from any kind of graph modeling. The reason is that every time I've seen a company go into this graph-based world, within 4-5 years they decide to move back to a PostgreSQL database.</p> <p>There are several issues with graph databases:</p> <ol> <li>They're really hard to learn - it's much easier to hire talent that knows PostgreSQL than graph databases.</li> <li>Defining schemas in PostgreSQL and joins is well-defined, whereas in graph databases there's often too much debate and not enough best practices.</li> <li>Most cases don't require more than one or two traversals of your graph.</li> </ol> <p>When I was at Facebook, their graph was actually just a very large MySQL database. This makes me cautious about using graph databases unless you have expert users.</p> <p>The only company I really believe could effectively use a graph database is LinkedIn, because they need to compute things like nearest neighbors up to three or five degrees away.</p> <p>Even for cases like Microsoft's approach where you build a document graph with entities and relationships, I'd prefer to use fine-tuned embeddings. A graph can be defined as an adjacency matrix, and fine-tuning your embeddings can get you pretty close to the similarity definition that a graph could maintain.</p> <p>I'd rather start with data and say, \"There are certain kinds of queries that really need a graph structure\" and let that justify the graph structure. Most technology needs to be justified by what the product needs to deliver rather than thinking about technology first.</p>"},{"location":"office-hours/cohort2/week1-summary/#would-you-recommend-using-colbert-models-or-other-specialized-retrieval-approaches","title":"Would you recommend using Colbert models or other specialized retrieval approaches?","text":"<p>All of these models do similar things at their core. To decide what to use, we should start with a synthetic dataset to measure precision and recall. Then the real question becomes: do any of these interventions (graph RAG, Colbert models, embeddings, re-rankers) beat the baseline in terms of precision and recall?</p> <p>It might be that graph for a certain problem is only 2% better, in which case it might not be worth the complexity. But if you found that, for parsing hospital records, graph RAG is 40% better on recall and precision, then it doesn't matter what I think\u2014the data would speak for itself.</p> <p>For Colbert specifically, it probably does very well for certain tasks. For example, statements like \"I love coffee\" and \"I hate coffee\" would be very similar in embedding space because embeddings don't fully understand negation. With a Colbert model, the cross-attention mechanism can figure out that these statements are different.</p> <p>But you need to tell the model what's important in your context. Without enough tests to guide us, it's hard to know if these interventions work. Usually, it's hard to beat the baseline of embedding search with a good re-ranker. Colbert might do 4-5% better, but you need to justify that improvement against the added complexity.</p>"},{"location":"office-hours/cohort2/week1-summary/#when-working-with-legal-documents-that-have-multi-level-outlines-and-reference-sections-from-other-documents-what-approach-would-you-recommend","title":"When working with legal documents that have multi-level outlines and reference sections from other documents, what approach would you recommend?","text":"<p>This could be done with a graph, but it could also be done with a simpler pointer system. When you load data, you can pull in other references. For example, in a construction project, whenever we pull up an image, we also pull up the paragraph above and below the image, augmenting the context.</p> <p>We can do the same for legal documents\u2014if it references another page or citation, we pull in that citation. Technically, this is a graph, but it's often easier to build this as a few LEFT JOINs in a PostgreSQL table.</p> <p>When we pull in text chunks, if there are references, we just do another left join back to the original chunk. These systems tend to be much simpler to reason about than dealing with reference types in a graph. Usually, that level of complexity really needs to be earned when building bigger systems.</p>"},{"location":"office-hours/cohort2/week1-summary/#are-we-going-to-cover-any-fundamentals-of-how-to-systematically-do-generation","title":"Are we going to cover any fundamentals of how to systematically do generation?","text":"<p>In terms of generation, a lot comes down to prompting and using LLMs as judges, which we'll talk about in Week 3 when discussing product experience.</p> <p>If you have specific aspects of generation you want to explore, it's mostly about ensuring formatting is correct and chain of thought is reasonable. The challenge is that you can't systematically improve generation primarily because generation evaluations are much more subjective.</p> <p>If it's just formatting, that can be very explicit. But challenges with generation will mostly be addressed through LLM-as-judge approaches and different levels of regular expressions.</p> <p>For example, we have an evaluation for summarization that simply measures what percentage shorter the summary is relative to the original input. These are very basic evaluations for summarization.</p>"},{"location":"office-hours/cohort2/week1-summary/#whats-your-take-on-using-rag-for-report-generation-in-response-to-requests-for-proposals","title":"What's your take on using RAG for report generation in response to requests for proposals?","text":"<p>The expert on report generation will talk in Week 4. Look out for a talk from Vantager, who does this for financial due diligence. Companies can give them existing reports, which they parse into a spec, and then when you upload new PDFs, it automatically generates a report for you.</p> <p>There's a lot of economic value that can come from report generation, and it's probably more valuable than just doing generic question answering.</p>"},{"location":"office-hours/cohort2/week1-summary/#what-is-your-experience-using-reasoning-models-as-the-answer-generator-model","title":"What is your experience using reasoning models as the answer generator model?","text":"<p>Before there were specific reasoning models, I've been pushing everyone to at least have thinking tokens and a reasoning block in the output. This gives language models time to think and allows you to render in a way that minimizes perceived latency.</p> <p>Now that O1 and DeepSeek are available, unless latency is a concern, I would try to use these reasoning models. O3 Mini is fairly affordable, and O1 is very affordable. You can render the product in a way that makes users feel it's faster.</p> <p>DeepSeek's reasoning capability is one reason it stood out to people\u2014they can actually see it think. For many practitioners, we've been asking language models to think step by step for quite a while.</p>"},{"location":"office-hours/cohort2/week1-summary/#how-do-we-set-user-expectations-on-the-delay-while-using-reasoning-models","title":"How do we set user expectations on the delay while using reasoning models?","text":"<p>The first UI tip is to stream out the thinking part of the model to the customer. Things will feel about 45% faster just because something is moving on the page.</p> <p>The second approach, which DeepSeek does well, is to have a button called \"Think harder\" or \"Reasoning.\" If users don't use it, they get the faster V3 model, but if they press reasoning, it switches to the R1 model. This both tells users you want the model to think (which they know will be slower) and, by rendering the thought tokens, improves the perceived latency.</p>"},{"location":"office-hours/cohort2/week1-summary/#how-should-we-handle-multiple-rag-sources-with-different-levels-of-information","title":"How should we handle multiple RAG sources with different levels of information?","text":"<p>When you have multiple RAG sources (like a calendar and a news site with more detailed event information), it can slow down the system when you want to use an LLM to act as a judge and provide a holistic answer.</p> <p>One approach is to predict what types of questions are easy versus hard and route them effectively. Another approach is to improve the user experience by rendering sources before rendering the text. Show an animation like \"I am thinking\" and have document 1, 2, and 3 appear, then \"I'm reading,\" and finally the answer.</p> <p>Notion AI's UX does this well\u2014it says \"thinking about your question,\" \"searching documents,\" animates the documents coming in, and then starts talking. The key is to keep the screen moving to make users believe something is happening.</p> <p>Adding a loading screen that moves can make users feel the system is 30% faster, even if the actual processing time is the same.</p>"},{"location":"office-hours/cohort2/week1-summary/#what-strategies-can-help-when-there-are-negative-consequences-of-thinking-too-hard-with-reasoning-models","title":"What strategies can help when there are negative consequences of \"thinking too hard\" with reasoning models?","text":"<p>One approach is to predict whether a question is easy or hard and decide when to turn on thinking. You could use a model like BERT to classify this.</p> <p>If that's possible, you can make the decision to think on behalf of the user. The objective would be to maximize customer satisfaction while minimizing token costs.</p> <p>Some companies like have their own proprietary model that tells you which is the best model to route to. You could have a model that's trained so that if you ask \"what's 1+1,\" it sends that to a simpler model, but if you ask about reading a legal document, it routes to an R1 model.</p> <p>For evaluation questions specifically, it really depends on the complexity. Some evaluations are simple yes/no decisions, while others involve complex reasoning like assigning the correct speaker to different comments in a transcript. You'll need to test with your specific use case.</p>"},{"location":"office-hours/cohort2/week1-summary/#what-advice-would-you-give-for-introducing-llms-into-a-healthcare-company-that-may-not-fully-grasp-their-potential","title":"What advice would you give for introducing LLMs into a healthcare company that may not fully grasp their potential?","text":"<p>First, build a demo and let leadership see the results. Then, clearly identify what types of queries you won't attempt to answer, pre-loading all the risk discussions upfront.</p> <p>Instead of saying \"my model is 80% correct,\" say \"I've identified the 20% of questions that don't work at all, but for the 80% of questions we can solve, the success rate is 99%.\"</p> <p>Do the upfront work to know the failure modes and economically valuable opportunities, then present them clearly. Add guardrails to say what the LLM won't attempt to do. Much of this is about setting expectations for leadership.</p>"},{"location":"office-hours/cohort2/week1-summary/#are-there-open-source-re-ranking-models-that-come-close-to-coheres-re-rankers-in-quality","title":"Are there open source re-ranking models that come close to Cohere's re-rankers in quality?","text":"<p>There are definitely good cross-encoders available, though some of the top models on leaderboards are 7 billion parameters, which may have high latency.</p> <p>Modern BERT (a new BERT-based embedding model with about 8,000 token sequence length compared to the original 512) will likely lead to more powerful BERT-based re-rankers.</p> <p>However, training your own re-ranker on your specific data will likely beat benchmark models. With just 6,000 examples from your own data, you can train a better embedding model and cross-encoder than what's publicly available, costing around $1.50 and 40 minutes on a laptop.</p>"},{"location":"office-hours/cohort2/week1-summary/#outside-of-personal-experiments-what-resources-or-mediums-do-you-rely-on-to-stay-up-to-date-on-rag","title":"Outside of personal experiments, what resources or mediums do you rely on to stay up to date on RAG?","text":"<p>Much of the content coming out is very hypey, and many research papers focus on public evaluations that don't mean as much as more fundamental work on data analysis, experimentation, and evaluation.</p> <p>When reading papers, focus more on how they present results and think about experimentation rather than specific methodologies or implementations. The things that work well are often too maintenance-heavy or expensive for production use cases with millions of PDFs.</p> <p>I like Anthropic's blog posts because they're fundamental\u2014discussing how to think about error bars, clustering, and other approaches that everyone can use, not just researchers with 40,000 rows in a database.</p> <p>Outside of that, a lot of information is in private Discords and Twitter. I'll have someone make a summary of the Discords with interesting \"alpha\" or insights.</p>"},{"location":"office-hours/cohort2/week1-summary/#when-working-with-documents-with-metadata-should-search-and-retrieval-methods-change-based-on-the-level-of-metadata-provided-within-the-queries","title":"When working with documents with metadata, should search and retrieval methods change based on the level of metadata provided within the queries?","text":"<p>Yes, they should. For example, in a construction project, we found people really cared about who made the last edits on legal contracts or who sent particular information. The metadata was very important\u2014queries like \"which contracts did this person send us\" become like SQL queries.</p> <p>We learned that when answering questions about who's doing what, we should include their contact information. These are small details in improving a RAG system that create economic value.</p> <p>Similarly, if you're building information that will be queried across time periods, you probably care about when documents were published and last crawled to determine relevance. A query like \"what is the latest research in physics\" might look at the past 6 months, while \"what is new in AI\" might only look at the past two weeks because it moves so quickly.</p> <p>It comes down to analyzing the queries people are asking and figuring out what creates economic value.</p>"},{"location":"office-hours/cohort2/week1-summary/#do-you-know-if-anthropic-is-working-on-an-answer-to-o1-or-r1-reasoning-models","title":"Do you know if Anthropic is working on an answer to O1 or R1 (reasoning models)?","text":"<p>Yes and no. If you use Claude's web app, it secretly has thinking tokens. Every time it says \"pondering\" or \"thinking,\" it's actually outputting thinking tokens that you can't see.</p> <p>If you ask Claude to replace the  token with {anyThinking}, you'll start seeing those thinking tokens. You can request this token in the API as well. <p>The real question is whether Anthropic has thinking models that use RLHF, and I'm not fully sure about that. Their CTO has stated they don't do distillation, but there are mixed interpretations of what that means.</p> <p>Claude 3.5 Sonnet is still impressive even without visible reasoning, including its vision capabilities. The bigger issue is that Anthropic is very concerned about safety and has questions about whether thinking tokens could lie to users or follow different policies.</p>"},{"location":"office-hours/cohort2/week1-summary/#when-working-with-unstructured-data-mostly-pdfs-and-drawings-how-do-you-approach-data-labeling-and-what-models-do-you-use","title":"When working with unstructured data, mostly PDFs and drawings, how do you approach data labeling and what models do you use?","text":"<p>For unprocessed data, I look at companies like Llama Parse, Extend, and Reducto, which parse headers, bodies, tables, and figures so you can work with them separately.</p> <p>For the most part, Claude Sonnet does a very good job\u2014it's just a matter of how much data you need to process. For specific tasks like understanding figures, visual language models like Qwen via Ollama work well for single PDFs, though batch local processing is more challenging as tools like VLLM don't yet support these models.</p>"},{"location":"office-hours/cohort2/week1-summary/#why-does-this-course-favor-lancedb-versus-other-vector-databases","title":"Why does this course favor LanceDB versus other vector databases?","text":"<p>The main reason is that I want everyone to experience running evaluations on not just embedding search but also full-text search. I want you to try hybrid search with or without a re-ranker.</p> <p>With LanceDB, incorporating these approaches is just one extra line of code. You can do a search with different modes (lexical, vector, hybrid) and easily add a re-ranker. It's the simplest way to try all these combinations and discover what works best.</p> <p>Additionally, LanceDB is backed by DuckDB, which means the same database that supports full-text search, semantic search, and re-rankers also supports SQL. If you want to analyze your queries with SQL, you can do that easily.</p> <p>Another advantage is that LanceDB can be hosted on S3 and is easy to set up for large amounts of data.</p>"},{"location":"office-hours/cohort2/week1-summary/#which-industry-or-application-domain-do-you-think-is-most-difficult-for-llms","title":"Which industry or application domain do you think is most difficult for LLMs?","text":"<p>It's hard to say definitively, but generally:</p> <ol> <li>Tasks with complex images are difficult</li> <li>Highly regulated industries like legal and healthcare contexts present challenges</li> <li>Financial services, especially ratings agencies, face enormous regulatory hurdles</li> </ol> <p>The fundamental challenge is that anything difficult for humans to collect data on will be hard for an LLM. It's about how much volume of data we have per industry and what kind of feedback loops exist.</p> <p>If an LLM makes a decision that takes weeks to verify, it's going to be hard to improve. The timeline for regulatory approval in some industries (like ratings agencies) can be years, creating a massive barrier to implementing LLM-based solutions.</p>"},{"location":"office-hours/cohort2/week1-summary/#did-you-find-a-use-case-where-re-rankers-improve-metrics","title":"Did you find a use case where re-rankers improve metrics?","text":"<p>Almost every case I've seen shows improvements with re-rankers, whether it's legal documents, question answering over books, or financial documents. A Cohere re-ranker typically improves performance by 6-12% while adding about 400-500ms of latency.</p> <p>Companies like Cohere are building industry-specific rankers that support financial text, medical text, and code. They're working hard to beat OpenAI embeddings, and they generally succeed.</p> <p>Re-rankers solve problems that embeddings miss, like distinguishing between \"I love coffee\" and \"I hate coffee,\" which look similar in embedding space but are clearly different with cross-attention in a re-ranker.</p>"},{"location":"office-hours/cohort2/week1-summary/#can-you-share-resources-on-how-to-create-hybrid-embeddings-for-postgresql-vector-databases","title":"Can you share resources on how to create hybrid embeddings for PostgreSQL vector databases?","text":"<p>If you use a library called ParagraphDB, you can set up both sparse BM25 indices and dense embedding-based indices. This allows you to implement rank fusion.</p> <p>Pinecone has good resources about this topic that I can share.</p>"},{"location":"office-hours/cohort2/week1-summary/#for-medicalhealthcare-administration-how-can-we-get-llms-to-be-something-that-are-trustworthy-with-serious-decisions","title":"For medical/healthcare administration, how can we get LLMs to be something that are trustworthy with serious decisions?","text":"<p>One approach is to use chain of thought models where we can read the reasoning to understand how the model arrived at a decision. Anthropic's concern may be that the chain of thought could be misleading.</p> <p>There's likely a future where we can build UIs that let humans verify not only the decision but also the chain of thought behind it. Then we can train models so that even the reasoning aligns with user preferences. If a model gets the right answer but with faulty reasoning, that's where we'd provide feedback.</p> <p>Another approach is to use ensembles\u2014sample a suite of LLMs and use majority voting on decisions to establish confidence. I often train multiple smaller language models to grade things on a 0-1 scale, then use a classical ML model (like logistic regression) to make the final prediction. This helps with explainability because you can see which features influenced the prediction.</p>"},{"location":"office-hours/cohort2/week1-summary/#for-multimodal-retrieval-text-images-what-approaches-work-best","title":"For multimodal retrieval (text + images), what approaches work best?","text":"<p>For visual content like photographs, CLIP embeddings work well since they're inherently multimodal\u2014they can represent both images and text in the same embedding space.</p> <p>For instructional manuals with images, I'd pass the images to a language model and ask for a detailed summary of what the image shows, including all text in the image. Then embed that summary instead. This creates a text representation that points to the original image.</p> <p>The approach has two steps:</p> <ol> <li>Given an image, create a synthetic question that would retrieve it</li> <li>Create a summary that would be retrieved for that question</li> </ol> <p>For product marketing scenarios, CLIP embeddings can work well, but you need to define what \"similar\" means in your context. Does a red shirt match other red shirts, or just shirts of the same color? Should expensive silk shirts match inexpensive polyester versions?</p> <p>This is why fine-tuning embedding models to understand your specific definition of similarity is important.</p>"},{"location":"office-hours/cohort2/week1-summary/#how-do-you-approach-chunking-very-long-documents-1500-2000-pages","title":"How do you approach chunking very long documents (1,500-2,000 pages)?","text":"<p>If you have extremely long documents, I'd first try a page-level approach to determine if answers typically exist on a single page or span multiple pages.</p> <p>One compelling approach is from a paper called RAPTOR. After chunking documents, they recluster the chunks. You embed every page, run a clustering model, and identify concepts that span multiple pages. Then summarize those clusters and use the summaries for retrieval\u2014if the summary is retrieved, you can include all related pages in the context.</p> <p>For metadata, look at your queries to determine what matters. If users frequently ask about publication dates or document authors, those should be included. The needs will become obvious as you analyze user queries\u2014you'll realize what's important and what creates economic value.</p> <p>Generally, if you can reorganize text chunks by clustering and bringing related information together, that's very valuable. For example, with tax law documents where laws are on pages 1-30 and exemptions on page 50, you could process the document once to place exemptions directly below the relevant laws. This preprocessing step might cost $10 of LLM calls per document, but for legal documents that might not change for years, it's worth the investment.</p>"},{"location":"office-hours/cohort2/week1-summary/#do-you-have-a-go-to-approach-for-visual-document-image-embeddings-like-quarterly-reports-with-tables-images-graphs","title":"Do you have a go-to approach for visual document image embeddings (like quarterly reports with tables, images, graphs)?","text":"<p>For visual documents like quarterly reports full of tables and images:</p> <ol> <li>Dockling is a free library that works quite well, though it might take about 11 seconds per PDF</li> <li>Claude Sonnet also works well for extraction</li> <li>Reducto, Llama Parse, and other commercial tools can be worth the cost to save time</li> <li>For multilingual content, VDR2B-Multi v1 handles multiple languages well</li> </ol> <p>There's an ongoing discussion about using Gemini 2 (with its million-token context window) to convert documents to markdown and extract all the information. This approach is becoming more viable as models improve, potentially reducing the engineering needed for preprocessing.</p> <p>Recent testing shows Reducto still has higher accuracy (0.9 \u00b1 0.1) compared to Gemini (0.84 \u00b1 0.16), but the gap is narrowing. The reason Reducto performs so well is that they have people manually labeling thousands of PDFs to train their models.</p>"},{"location":"office-hours/cohort2/week1-summary/#why-at-meta-did-you-prefer-sql-databases-over-graph-databases","title":"Why at Meta did you prefer SQL databases over graph databases?","text":"<p>Graph databases are useful when you need complex traversals, like finding all of Jason's followers who follow a specific account, then finding what they like, and sorting by aggregated likes per product.</p> <p>However, what we found is that most use cases are actually simpler\u2014often just requiring 2-3 left joins in SQL rather than complex graph traversals. From a skills perspective, it's easier to hire people who know SQL well than to find graph database experts.</p> <p>At scale, graphs are also hard to manage. Around 2017-2018, only LinkedIn had a true graph database because they needed to compute 3rd-degree friendships very quickly. For most companies, SQL databases offer better performance, easier maintenance, and more familiar tooling.</p> <p>Over a 12-year career, we kept trying different technologies (Hadoop, Spark, etc.) but always ended up returning to SQL. The pattern is consistent across many organizations.</p>"},{"location":"office-hours/cohort2/week1-summary/#what-have-you-learned-about-prompt-caching","title":"What have you learned about prompt caching?","text":"<p>Prompt caching is a technique where language models can avoid reprocessing the beginning of prompts that are often identical.</p> <p>Different providers handle this differently:</p> <ul> <li>Anthropic caches prompts for 5 minutes; if you make the same request within that time, the entire message is cached</li> <li>OpenAI figures out the optimal prefix to cache automatically</li> </ul> <p>This is valuable because it can save significant processing time and costs, especially when you have many few-shot examples or large system prompts. If you have 50+ examples in your prompt, caching can dramatically improve performance.</p> <p>For models like Claude on Bedrock, prompt caching wasn't available a few months ago but is likely coming soon. It's the kind of feature that rolls out gradually across providers.</p>"},{"location":"office-hours/cohort2/week1-summary/#for-visual-document-image-processing-whats-the-state-of-the-art","title":"For visual document image processing, what's the state of the art?","text":"<p>There's a recent discussion on Hacker News about using Gemini 2 (with its million-token context window) to process documents and convert them to markdown, extracting tables, layout information, and text.</p> <p>The engineering needed for document pre-processing is getting simpler as these models improve. Recent tests show Reducto still has higher accuracy (0.9 \u00b1 0.1) compared to Gemini (0.84 \u00b1 0.16), but the gap is narrowing.</p> <p>Reducto's performance comes from having people manually label thousands of PDFs, then training models on that high-quality data. This reinforces the point that with 6,000-10,000 high-quality labels from your own data, you can train models that outperform even the biggest general models on your specific tasks.</p>"},{"location":"office-hours/cohort2/week1-summary/#how-does-brain-trust-work-with-the-notebooks-in-this-course","title":"How does Brain Trust work with the notebooks in this course?","text":"<p>Brain Trust just saves the results that your laptop is running locally. It's not executing anything or using a better database\u2014it's more like an observability tool (similar to Datadog).</p> <p>When we run the notebooks, everything is running on your laptop in LanceDB. The only thing Brain Trust sees is row IDs and scores. Think of it as a powerful UI over a database that's saving your logs, not as a computation service.</p>"},{"location":"office-hours/cohort2/week1-summary/#whats-the-difference-between-bi-encoders-and-cross-encoders","title":"What's the difference between bi-encoders and cross-encoders?","text":"<p>A bi-encoder converts all documents into numbers (embeddings) first, and then the assumption is that when we compare those numbers, documents that look similar are similar. Because we pre-compute everything, we can search very quickly.</p> <p>A cross-encoder doesn't compare numbers\u2014it compares the actual sentences. This approach can't compare a million documents with a million other documents (too expensive), so instead it takes one question and 50 documents and compares each one individually. That's the \"cross\" part of cross-encoder.</p> <p>The advantage of cross-encoders is that a language model can compare words like \"love\" and \"hate\" in \"I love coffee\" and \"I hate coffee\" and understand they're different, whereas bi-encoders just have lists of numbers that don't capture this nuance.</p> <p>We'll cover this topic more deeply in Week 2, but the key takeaway is that bi-encoders are faster but less accurate, while cross-encoders are slower but better at understanding semantic distinctions.</p>"},{"location":"office-hours/cohort2/week1-summary/#whats-the-process-for-fine-tuning-embedding-models","title":"What's the process for fine-tuning embedding models?","text":"<p>In Week 2, we'll cover this topic extensively. The overall message is that:</p> <ol> <li>It's probably a bad idea to train your own language model</li> <li>It's a very good idea to train your own embedding model</li> </ol> <p>Fine-tuning embedding models is much less resource-intensive\u2014it typically costs around $1.50 and takes about 40 minutes on a laptop. With just 6,000 examples from your domain, you can train embedding models and cross-encoders that outperform general-purpose models on your specific tasks.</p> <p>This is especially useful when you need embeddings to understand domain-specific concepts or when you're trying to define what \"similar\" means in your particular context (e.g., product recommendations where price range matters).</p>"},{"location":"office-hours/cohort2/week1-summary/#how-do-you-understand-metrics-like-precision-and-recall-in-one-to-one-answer-scenarios","title":"How do you understand metrics like precision and recall in one-to-one answer scenarios?","text":"<p>For questions with exactly one correct answer, these metrics behave somewhat differently. Recall will be either 0% or 100% depending on whether K is large enough to include the correct answer.</p> <p>For example, if we want to retrieve exactly one document and there's only one correct answer, precision could be either 0% or 100%, and the same for recall.</p> <p>The metrics become more meaningful when:</p> <ol> <li>There are multiple relevant documents</li> <li>We're analyzing trends across many queries</li> <li>We're comparing different retrieval methods</li> </ol> <p>Even with one-to-one mappings, MRR (Mean Reciprocal Rank) is still useful to see where the correct answer appears in your results.</p> <p>What really matters isn't the absolute number but whether we can move these metrics in a positive direction with our interventions. It's like weighing yourself\u2014the absolute number may vary by scale, but if you've gained two pounds, you've definitely gained two pounds.</p> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"office-hours/cohort2/week2-summary/","title":"RAG Office Hours Q&amp;A Summary - Week 2","text":""},{"location":"office-hours/cohort2/week2-summary/#how-would-you-evaluate-the-effect-of-different-parsing-strategies-in-rag-notably-on-documents-with-weird-layouts-tables-and-charts","title":"How would you evaluate the effect of different parsing strategies in RAG, notably on documents with weird layouts, tables, and charts?","text":"<p>For documents with complex layouts like tables and charts, there are multiple levels of evaluation:</p> <p>First, you need to evaluate OCR accuracy - checking whether text is being parsed correctly (e.g., is a 0 being parsed as an 8?). Then there's the bounding box detection problem - checking if tables are fully recognized as single bounding boxes using metrics like intersection over union.</p> <p>It's generally safer to evaluate OCR/parsing and retrieval separately because parsing errors can be hard to trace back when they're part of the full RAG pipeline. If you parse an 8 as a 0 and generate synthetic data from that, you won't be able to capture that error in your evaluations.</p> <p>I've leaned on parsing vendors because they're the most incentivized to have good and accurate labels. This lets me focus on retrieval, which is what will create the most value for my specific use case. While there are other businesses focused on PDF processing, no one will focus specifically on your ability to do retrieval well with your data.</p>"},{"location":"office-hours/cohort2/week2-summary/#when-does-it-make-sense-to-create-a-targeted-summary-for-an-applications-objective-versus-fine-tuning-embedding-models","title":"When does it make sense to create a targeted summary for an application's objective versus fine-tuning embedding models?","text":"<p>This depends on whether you have data to fine-tune and what your embedding should capture. If you're writing the summary yourself, you're essentially making an assumption about what the embedding should look like.</p> <p>For example, with image embeddings, maybe the most common questions aren't just about what's in the photo but about the cinematic mood. In that case, it might make sense to have a language model create a summary describing the mood because I want to be able to search for \"atmospheric\" or \"dark and gloomy\" rather than just \"trees in a forest.\"</p> <p>However, if you have actual user interaction data, it's better to use that data to tell us what's similar rather than creating assumptions. For example, with blueprint images, an image model might just say \"this is a blueprint,\" but what I specifically did was extract information like number of rooms, bathrooms, sizes, and addresses - information that would be harder for a CLIP embedding to capture.</p> <p>In general, I'd much rather use the data my app generates than hard-code these summaries, but summaries can be useful when you need to extract specific structured information that embedding models might miss.</p>"},{"location":"office-hours/cohort2/week2-summary/#what-are-the-recommended-approaches-for-evaluating-rag-for-single-documents-like-report-generation-for-a-proposal","title":"What are the recommended approaches for evaluating RAG for single documents, like report generation for a proposal?","text":"<p>When working with a single PDF document that might be varying in length (from 10 to 400 pages), semantic chunking can be valuable to separate paragraphs based on their semantic meaning rather than just token-based chunking. This is especially important when requirements for different disciplines might be found in different sections (e.g., structural requirements mentioned within architectural requirements).</p> <p>One approach is to generate synthetic questions per paragraph, asking \"What are the requirements being mentioned in this paragraph?\" rather than \"What question can you ask from this paragraph?\" This helps identify the key information.</p> <p>For retrieval, you can also inject a summary of the page that a paragraph was extracted from and embed them together. This way, when retrieving, you have both the specific chunk and context about where it comes from, which can improve recall.</p> <p>Whether adding summaries improves recall is an empirical question - if it increases recall by 1%, it might not be worth the extra LLM calls, but if it improves recall by 6-8%, it could be worth investigating further.</p>"},{"location":"office-hours/cohort2/week2-summary/#could-you-distill-key-reasons-when-someone-should-consider-fine-tuning-open-source-embedding-models-over-proprietary-models","title":"Could you distill key reasons when someone should consider fine-tuning open source embedding models over proprietary models?","text":"<p>If you have 6,000-10,000 examples of question-document relevancy pairs, you can likely outperform closed-source models with a fine-tuned model. This is because your tasks can be much more specific than what general models are optimized for.</p> <p>It can also be more valuable if you need to embed massive datasets at scale. By spinning up your own GPUs, you can process much more text per second at a lower cost. For example, embedding 20GB of text data might take only 15 minutes and cost around $20, whereas using OpenAI APIs would be more expensive and much slower.</p> <p>The main downside is the need to maintain your inference server, which adds complexity. It's less about whether the model will perform well and more about whether you have the time and resources to maintain the infrastructure.</p>"},{"location":"office-hours/cohort2/week2-summary/#is-there-a-reason-to-ever-fine-tune-the-llm-rather-than-or-in-combination-with-fine-tuning-the-retriever-model","title":"Is there a reason to ever fine-tune the LLM rather than or in combination with fine-tuning the retriever model?","text":"<p>I'm pretty open to businesses fine-tuning their retrieval models because companies like OpenAI or Anthropic aren't primarily focused on making retrieval better - they're not launching new embedding models daily. Companies like Cohere, on the other hand, are actually thinking about retrieval.</p> <p>If you spend effort fine-tuning an LLM, you need to consider inference, CUDA drivers, and whether your fine-tuned model will be competitive when the original model provider releases a new version in a few months.</p> <p>It's generally very costly to fine-tune language models, and you often don't get much benefit. However, if there are specific reasons - like tonality, personalization, or access to proprietary data - it might make sense. But for a team of 4-5 people, it's probably not a good idea to spend effort maintaining that kind of infrastructure.</p> <p>In contrast, fine-tuning embedding models can be done on a laptop, run on cloud instances, and be cost-effective. For most teams, the maintenance cost of running your own LLM is just too high to justify.</p>"},{"location":"office-hours/cohort2/week2-summary/#one-weakness-of-rag-is-difficulty-in-detecting-relationships-between-concepts-because-the-retriever-model-isnt-aware-of-how-concepts-relate-to-each-other-should-we-fine-tune-the-llm-for-this","title":"One weakness of RAG is difficulty in detecting relationships between concepts because the retriever model isn't aware of how concepts relate to each other. Should we fine-tune the LLM for this?","text":"<p>Before considering fine-tuning the language model, I would ask: How much can we put into few-shot examples in the prompt? Can we come up with good chain-of-thought examples that describe these relationships? Can we provide a glossary?</p> <p>The maintenance cost of running an LLM is so high that it's worth really trying to squeeze out as much as possible through prompt engineering, longer system prompts, more few-shot examples, and prompt caching before considering fine-tuning.</p> <p>For example, Bloomberg spent millions on their own model, and within 5-6 months, GPT-4 was better. Instead of fine-tuning, consider using RAG to retrieve relationship information first, put that in the context, and then add the actual question. This is more maintainable and adaptable as new models are released.</p>"},{"location":"office-hours/cohort2/week2-summary/#what-is-the-main-failure-modes-like-distribution-mismatch-or-biases-that-youve-seen-when-relying-on-synthetic-data-for-retrieval-fine-tuning","title":"What is the main failure modes (like distribution mismatch or biases) that you've seen when relying on synthetic data for retrieval fine-tuning?","text":"<p>The biggest issue is mismatch between user questions in reality versus in the synthetic data. Once you have synthetic data for fine-tuning retrieval models, it's hard to imagine a case where creating more data for your use case would make the model worse.</p> <p>What's more important is figuring out how to intelligently incorporate real-world examples from users into the few-shot examples for synthetic data generation, making it a more diverse process. You can check this by:</p> <ol> <li>Looking at the variance of embeddings against each other to see if they're too similar</li> <li>Checking general statistics like character count variance in questions</li> <li>Ensuring the synthetic data matches user data characteristics</li> </ol> <p>For example, if your customer questions typically have around 30 characters but your synthetic data averages 90 characters because the language model is too verbose, that's a simple distribution mismatch to fix.</p>"},{"location":"office-hours/cohort2/week2-summary/#can-you-share-the-intuition-for-the-difference-between-a-fine-tuned-embedding-model-and-a-fine-tuned-re-ranker","title":"Can you share the intuition for the difference between a fine-tuned embedding model and a fine-tuned re-ranker?","text":"<p>The embedding model allows you to do search over a large number of documents - given an embedding model, you might retrieve the top 100 text chunks. The re-ranker model then takes these 100 chunks and finds the best 25.</p> <p>We generally want to use both, and the dataset to train these models is actually the same dataset. If you can only afford to fine-tune one, you might choose based on where your bottleneck is:</p> <ol> <li>Is recall at 100 already good (95%) but recall at 10 is poor (50%)? Then focus on the re-ranker.</li> <li>Are you missing relevant documents even in your top 100 results? Then focus on the embedding model.</li> </ol> <p>The key insight is that by having metrics on both stages, you can identify where to focus your improvement efforts.</p>"},{"location":"office-hours/cohort2/week2-summary/#do-we-need-more-data-to-fine-tune-re-rankers-than-bi-encoders","title":"Do we need more data to fine-tune re-rankers than bi-encoders?","text":"<p>It depends on the model, but generally, Cohere has done a good job of being data-efficient for producing embedding models. The amount of data needed may vary by model and task.</p>"},{"location":"office-hours/cohort2/week2-summary/#for-collaborative-filtering-models-how-do-you-address-the-cold-start-problem-new-usersitems-the-model-hasnt-seen-without-retraining-the-model","title":"For collaborative filtering models, how do you address the cold start problem (new users/items the model hasn't seen) without retraining the model?","text":"<p>There are multiple approaches to this. Instead of using classical collaborative filtering models, many systems now build models with user embeddings and item embeddings. The question becomes: can we use some other model to predict the embeddings we would have trained using interaction data?</p> <p>For example, in an e-commerce setting, if we trained our item embeddings using purchase data and a new item comes in, we could train a vision model to predict the embedding of the item based on its image and metadata. We can use that as an initial set of recommendations.</p> <p>The core idea is using other available data to predict the embeddings that would have come from interaction data (like checkout data). This approach helps bridge the gap for new items or users.</p>"},{"location":"office-hours/cohort2/week2-summary/#how-do-you-handle-rag-for-multimodal-content-like-powerpoint-presentations-that-have-complex-layouts","title":"How do you handle RAG for multimodal content like PowerPoint presentations that have complex layouts?","text":"<p>For documents with complex layouts like PowerPoint presentations, the parsing and chunking processes are linked. You might want to evaluate them separately since parsing errors will be hard to detect in the full RAG pipeline.</p> <p>One approach is to use general-purpose parsing tools like Dockling, Claude Sonnet, or commercial tools like Reducto, Llama Parse, and Extend. For multilingual content, models like VDR2B-Multi v1 handle multiple languages well.</p> <p>Recent developments include using models like Gemini 2 (with its million-token context window) to convert documents to markdown and extract information, though specialized tools like Reducto still have higher accuracy (0.9 \u00b1 0.1 vs. 0.84 \u00b1 0.16 for Gemini). These gaps are narrowing as general models improve.</p>"},{"location":"office-hours/cohort2/week2-summary/#why-did-you-prefer-sql-databases-over-graph-databases-at-metafacebook","title":"Why did you prefer SQL databases over graph databases at Meta/Facebook?","text":"<p>Graph databases are useful when you need complex traversals, but most use cases only require 2-3 left joins in SQL rather than complex graph operations. From a skills perspective, it's easier to hire people who know SQL well than to find graph database experts.</p> <p>At scale, graphs are also hard to manage. Around 2017-2018, only LinkedIn had a true graph database because they needed to compute 3rd-degree friendships very quickly. For most companies, SQL databases offer better performance, easier maintenance, and more familiar tooling.</p> <p>Over a 12-year career, we kept trying different technologies (Hadoop, Spark, etc.) but always ended up returning to SQL. Most cases don't require more than one or two traversals of your graph, making SQL a more practical choice.</p>"},{"location":"office-hours/cohort2/week2-summary/#what-have-you-learned-about-prompt-caching","title":"What have you learned about prompt caching?","text":"<p>Prompt caching is a technique where language models can avoid reprocessing the beginning of prompts that are often identical:</p> <ul> <li>Anthropic caches prompts for 5 minutes; if you make the same request within that time, the entire message is cached</li> <li>OpenAI figures out the optimal prefix to cache automatically</li> </ul> <p>This is valuable because it can save significant processing time and costs, especially when you have many few-shot examples or large system prompts. If you have 50+ examples, caching can dramatically improve performance.</p> <p>For models like Claude on Bedrock, prompt caching wasn't available a few months ago but is likely coming soon. It's the kind of feature that rolls out gradually across providers.</p>"},{"location":"office-hours/cohort2/week2-summary/#whats-the-difference-between-bi-encoders-and-cross-encoders","title":"What's the difference between bi-encoders and cross-encoders?","text":"<p>A bi-encoder converts all documents into numbers (embeddings) first, and then compares those numbers. Because we pre-compute everything, we can search very quickly.</p> <p>A cross-encoder doesn't compare numbers\u2014it compares the actual sentences. This approach can't compare a million documents with a million other documents (too expensive), so instead it takes one question and 50 documents and compares each one individually.</p> <p>The advantage of cross-encoders is that they can understand semantic distinctions like the difference between \"I love coffee\" and \"I hate coffee,\" whereas bi-encoders just have numeric representations that might miss this nuance.</p> <p>Bi-encoders are faster but less accurate, while cross-encoders are slower but better at understanding semantic distinctions.</p>"},{"location":"office-hours/cohort2/week2-summary/#whats-the-process-for-fine-tuning-embedding-models","title":"What's the process for fine-tuning embedding models?","text":"<p>It's probably a bad idea to train your own language model, but it's a very good idea to train your own embedding model.</p> <p>Fine-tuning embedding models is much less resource-intensive\u2014it typically costs around $1.50 and takes about 40 minutes on a laptop. With just 6,000 examples from your domain, you can train embedding models and cross-encoders that outperform general-purpose models on your specific tasks.</p> <p>This is especially useful when you need embeddings to understand domain-specific concepts or when you're trying to define what \"similar\" means in your particular context (e.g., product recommendations where price range matters).</p>"},{"location":"office-hours/cohort2/week2-summary/#what-non-intuitive-things-have-you-learned-about-recommendation-systems","title":"What non-intuitive things have you learned about recommendation systems?","text":"<p>The big insight about recommendation systems is that inventory matters a lot more than the actual algorithm. While Tiktok's algorithm is good, what really allows it to produce great recommendations is the vast amount of content available. Without those videos, you can't do much - and the same applies to RAG.</p> <p>The metadata you have and the inventory you have are much more important than the algorithm itself. For example, if recommendations for \"Greek restaurants near me\" are bad, the solution might be to add more Greek restaurants to your database, not to tweak the algorithm.</p> <p>Similarly, if queries after 7 PM perform poorly, maybe you're missing information about whether restaurants are open. The solution is to collect that data rather than change your algorithms.</p> <p>The hard work in recommendation systems is often: Do we have enough rows in the database? How much content do we have? And for that content, do we have the right metadata?</p>"},{"location":"office-hours/cohort2/week2-summary/#when-working-with-documents-with-metadata-should-search-and-retrieval-methods-change-based-on-the-level-of-metadata-provided-within-the-queries","title":"When working with documents with metadata, should search and retrieval methods change based on the level of metadata provided within the queries?","text":"<p>Yes, they should. For example, in a construction project, we found people really cared about who made the last edits on legal contracts or who sent particular information. The metadata was very important for queries like \"which contracts did this person send us,\" which function more like SQL queries.</p> <p>Similarly, if you're building information that will be queried across time periods, you probably care about when documents were published and last crawled to determine relevance. A query like \"what is the latest research in physics\" might look at the past 6 months, while \"what is new in AI\" might only look at the past two weeks because it moves so quickly.</p> <p>It comes down to analyzing the queries people are asking and figuring out what creates economic value.</p>"},{"location":"office-hours/cohort2/week2-summary/#do-you-have-a-go-to-approach-for-visual-document-image-embeddings-like-quarterly-reports-with-tables-images-graphs","title":"Do you have a go-to approach for visual document image embeddings (like quarterly reports with tables, images, graphs)?","text":"<p>For visual documents like quarterly reports full of tables and images:</p> <ol> <li>Dockling is a free library that works quite well, though it might take about 11 seconds per PDF</li> <li>Claude Sonnet also works well for extraction</li> <li>Commercial tools like Reducto, Llama Parse, and others can be worth the cost to save time</li> <li>For multilingual content, VDR2B-Multi v1 handles multiple languages well</li> </ol> <p>Recent testing shows Reducto still has higher accuracy (0.9 \u00b1 0.1) compared to Gemini (0.84 \u00b1 0.16), but the gap is narrowing. Reducto performs well because they have people manually labeling thousands of PDFs to train their models.</p>"},{"location":"office-hours/cohort2/week2-summary/#how-do-you-handle-multilingual-rag","title":"How do you handle multilingual RAG?","text":"<p>Cohere has put the most effort into multilingual models, with both multilingual local LLMs and embedding models.</p> <p>I recommend figuring out which languages appear in your queries and ensuring your evaluation reflects that distribution. Check whether the models you're considering (Cohere, OpenAI) perform well on these languages.</p> <p>While translation might seem like an option, if it worked well, companies like OpenAI and Cohere would already be using synthetic translation data to improve their language models. To evaluate performance across languages, create synthetic questions in multiple languages and verify whether recall rates differ between languages.</p>"},{"location":"office-hours/cohort2/week2-summary/#how-do-you-approach-chunking-very-long-documents-1500-2000-pages","title":"How do you approach chunking very long documents (1,500-2,000 pages)?","text":"<p>If you have extremely long documents, start with a page-level approach to determine if answers typically exist on a single page or span multiple pages.</p> <p>One compelling approach is from the RAPTOR paper. After chunking documents, they recluster the chunks by embedding every page, running a clustering model, and identifying concepts that span multiple pages. Then they summarize those clusters and use the summaries for retrieval\u2014if a summary is retrieved, all related pages are included in the context.</p> <p>For metadata, look at your queries to determine what matters. If users frequently ask about publication dates or document authors, those should be included. The needs will become obvious as you analyze user queries.</p> <p>If you can reorganize text chunks by clustering and bringing related information together, that's very valuable. For example, with tax law documents where laws are on pages 1-30 and exemptions on page 50, you could process the document once to place exemptions directly below the relevant laws. This preprocessing step might cost $10 of LLM calls per document, but for legal documents that might not change for years, it's worth the investment.</p>"},{"location":"office-hours/cohort2/week2-summary/#how-do-you-understand-metrics-like-precision-and-recall-in-one-to-one-answer-scenarios","title":"How do you understand metrics like precision and recall in one-to-one answer scenarios?","text":"<p>For questions with exactly one correct answer, these metrics behave somewhat differently. Recall will be either 0% or 100% depending on whether K is large enough to include the correct answer.</p> <p>For example, if we want to retrieve exactly one document and there's only one correct answer, precision could be either 0% or 100%, and the same for recall.</p> <p>The metrics become more meaningful when:</p> <ol> <li>There are multiple relevant documents</li> <li>We're analyzing trends across many queries</li> <li>We're comparing different retrieval methods</li> </ol> <p>Even with one-to-one mappings, MRR (Mean Reciprocal Rank) is still useful to see where the correct answer appears in your results.</p> <p>What really matters isn't the absolute number but whether we can move these metrics in a positive direction with our interventions.</p>"},{"location":"office-hours/cohort2/week2-summary/#how-does-a-long-context-window-affect-rag-systems","title":"How does a long context window affect RAG systems?","text":"<p>While having longer context windows allows for more content to be included, there are always tradeoffs with latency and cost. Just because we can fit more in context doesn't mean we should always do so.</p> <p>Like how Amazon could theoretically score every product in their inventory for each user but chooses not to because each 100ms of latency costs them 1% in revenue, we still need to make choices about what to include in context.</p> <p>The battery analogy is apt: iPhone batteries get more powerful every year, but battery life stays the same because we build more power-hungry apps. Similarly, as context windows grow, we'll find ways to use that additional capacity rather than making everything faster or cheaper.</p> <p>There will always be cost, performance, and latency tradeoffs to consider. Having a longer context window doesn't eliminate the need for efficient retrieval - it just changes what problems we can solve.</p>"},{"location":"office-hours/cohort2/week2-summary/#what-tips-do-you-have-for-making-decisions-about-rag-system-architecture-without-prototyping-everything","title":"What tips do you have for making decisions about RAG system architecture without prototyping everything?","text":"<p>Start by asking for examples of 40-50 questions that customers might ask. Reading these helps build an intuition about what query mechanics need to exist.</p> <p>For example, if questions include \"what's the most recent news?\", you'll need date filters. If queries ask \"who do I talk to about fixing XYZ?\", you need features for finding contacts.</p> <p>This helps identify what metadata you need and whether you can access it. From there, building a demo with tools like LangChain or Llama Index should be quick. You may need to rewrite things later, but if the demo can answer generic questions, that's when you start thinking about synthetic data.</p> <p>The key is getting the system in front of beta testers, collecting feedback, and analyzing what's working and what's not. This helps prioritize the next features to build. If 80% of questions are actually about image search, then that's clearly the next thing to build, regardless of what methodology is trending on Twitter.</p>"},{"location":"office-hours/cohort2/week2-summary/#how-do-you-optimally-blend-small-pools-of-real-data-with-large-synthetic-data-sets","title":"How do you optimally blend small pools of real data with large synthetic data sets?","text":"<p>Focus on whether blending improves your evaluation suite. If you have 500 real examples, put 250 in your training set and leave 250 for evaluation. Then experiment with different blends of synthetic and real data to see how they perform on your evaluation suite.</p> <p>You might find that as you use more synthetic data, you perform worse on your real user data but better on synthetic data. You can weight these differently - perhaps real data success is worth 1.2 points while synthetic data success is worth 0.9 points - to create a single score for system health.</p> <p>A lot of machine learning is empirical - you can't predict these things ahead of time. You need to run experiments and see what works.</p>"},{"location":"office-hours/cohort2/week2-summary/#how-do-you-approach-function-calling-for-complex-workflows-that-require-multiple-function-calls","title":"How do you approach function calling for complex workflows that require multiple function calls?","text":"<p>Instead of having the language model immediately execute functions one at a time, prompt it to show the entire plan to the user and potentially ask for confirmation. Have a separate function called \"plan\" where the model says \"Based on this request, I think I'm going to use function 1, then 2, then 3. What do you think?\"</p> <p>When the user clicks yes or no, you've allowed human confirmation of the correct order. Since the plan already exists in the context, it's easier for the model to execute correctly.</p> <p>The second benefit is that user requests, plans, and accept/reject decisions can be used as few-shot examples. You can embed these examples so that next time someone asks a similar question, you can say \"Last time someone asked this, they approved calling functions 1, 2, 3 in this order.\"</p> <p>This helps build a dataset of few-shot examples over plans, making the system more reliable.</p>"},{"location":"office-hours/cohort2/week2-summary/#how-do-you-constrain-a-rag-system-that-pulls-from-multiple-data-sources","title":"How do you constrain a RAG system that pulls from multiple data sources?","text":"<p>After conducting topic analysis of user questions, you can identify which types of questions you can answer well and which ones you struggle with. For low-percentage, low-performance questions, you might decide to simply decline those queries.</p> <p>For example, if your system doesn't handle contact information well, you could add to your prompt: \"If someone is asking about contact information, say no and tell them to message support.\" This saves face and avoids attempting questions you can't answer well.</p> <p>Conversely, if there are questions you can answer very well (even if they're a small percentage), highlight these as sample questions in your UI to guide users toward queries you're confident in handling.</p> <p>Much of the progress in making systems better comes from improving UI, better educating users about capabilities, and enhancing the quality of your inventory rather than just tweaking algorithms.</p>"},{"location":"office-hours/cohort2/week2-summary/#do-you-sometimes-use-differently-tuned-embeddings-within-the-same-query","title":"Do you sometimes use differently tuned embeddings within the same query?","text":"<p>Unless you're at massive scale, having multiple embedding models for different content types (like product descriptions vs. comments) probably won't yield enough performance improvement to justify the maintenance cost.</p> <p>There's evidence that having a single unified model trained on all your data performs better than specialized models. In machine translation, we used to train separate models for each language pair, but researchers found that a single model trained to translate all languages performed better than any individual model.</p> <p>The unified model learns something about the underlying system that allows it to handle even rare cases better than specialized models would. The same principle likely applies to embedding models.</p>"},{"location":"office-hours/cohort2/week2-summary/#how-do-you-reason-about-papers-and-new-research-in-rag-and-llms","title":"How do you reason about papers and new research in RAG and LLMs?","text":"<p>Most papers published weekly aren't that important, and many are just reinventing ideas from decades ago. Instead of flooding yourself with information, focus on running experiments with your own data and solving specific problems.</p> <p>The popular stuff on Twitter is generally reasonable to follow, but even then, much research is a distraction if you're building something. For example, a recent popular paper on \"reasoning powered RAG\" was essentially just using an LLM to judge relevancy pairs in a for loop - something basic that's been around for a while.</p> <p>Rather than chasing the latest research, focus on building strong evaluation suites, analyzing your data, and solving specific problems in your implementation. These are the durable skills that will last throughout your career.</p>"},{"location":"office-hours/cohort2/week2-summary/#does-a-long-context-window-make-rag-obsolete","title":"Does a long context window make RAG obsolete?","text":"<p>No. Just like Amazon could theoretically score every product in their inventory for each user but chooses not to because each 100ms of latency costs them 1% in revenue, we still need to make choices about what to include in context.</p> <p>The battery analogy is apt: iPhone batteries get more powerful every year, but battery life stays the same because we build more power-hungry apps. Similarly, as context windows grow, we'll find ways to use that additional capacity rather than making everything faster or cheaper.</p> <p>There will always be cost, performance, and latency tradeoffs to consider. Having a longer context window doesn't eliminate the need for efficient retrieval - it just changes what problems we can solve.</p>"},{"location":"office-hours/cohort2/week2-summary/#how-do-you-handle-the-upkeep-of-documents-that-go-in-and-out-of-scope-or-have-frequent-version-changes","title":"How do you handle the upkeep of documents that go in and out of scope or have frequent version changes?","text":"<p>For \"evergreen\" vs. \"Rhodian\" (frequently changing) documents, include published dates and make sure they're in the context so the language model is aware of them. For example, with HR holiday calendars for different years, include the dates so the model can reason about which is current.</p> <p>Consider implementing both published dates and last modified dates, and be explicit in your function calling to filter on these attributes (e.g., only return documents published or updated in the past year).</p> <p>The key question is how sensitive your model is to low precision, and whether that low precision is mainly happening because of your inability to expire outdated documents.</p>"},{"location":"office-hours/cohort2/week2-summary/#how-do-you-approach-building-voice-ai-for-outbound-calls-or-structured-conversations","title":"How do you approach building voice AI for outbound calls or structured conversations?","text":"<p>Graph-based models or finite state machines have been very successful in the agent world. In this approach, you're in different states (introduction, data collection, etc.) with different system messages for each state.</p> <p>For example, when collecting payment data to book a meeting, you have logical checks to ensure the date is correct and that you have necessary information like a phone number. The set of function calls available also changes based on the state.</p> <p>Once you have fully successful conversations, you can summarize them and put them back in the system prompt to ensure transitions are more accurate. You can prompt the model with these few-shot examples to improve transition states: \"When I have 5 few shots, my transitions are more accurate. When I have 20 few shots it gets too confused. So now I've picked 15 for now.\"</p> <p>This finite state machine approach has been around for decades and is still very effective, with LLMs improving the transitions between states.</p>"},{"location":"office-hours/cohort2/week2-summary/#when-working-with-metadata-should-you-include-it-in-the-chunk-or-add-it-separately","title":"When working with metadata, should you include it in the chunk or add it separately?","text":"<p>There are a few approaches:</p> <ol> <li>Embed the string without metadata but add metadata when sending to the LLM</li> <li>Embed the string with metadata included</li> </ol> <p>This is something to test empirically - does including metadata in the embedding hurt retrieval? Cohere's embedding models (like Compass) can embed JSON quite well.</p> <p>Including metadata in chunks is common practice as it allows answering questions like \"who wrote this document\" or \"what's their contact information.\" This metadata can then be used for function calls, such as \"Jason wrote the document 2 weeks ago, it has not been updated since. Here's Jason's email, click to write an email to Jason.\"</p>"},{"location":"office-hours/cohort2/week2-summary/#how-can-you-best-apply-synthetic-data-generation-to-agent-workflows-with-multiple-tools","title":"How can you best apply synthetic data generation to agent workflows with multiple tools?","text":"<p>Instead of generating synthetic questions, you can generate synthetic queries that would trigger certain function calls. If you have 3-4 different functions, you can create synthetic queries that should call specific functions or combinations of functions.</p> <p>If each individual function call is accurate, then the combined sequence should also be accurate. You can also use planning to improve data generation - create questions that would result in specific functions being called in sequence, then verify that with certain requests, these functions are indeed called by the model.</p> <p>This approach helps ensure reliability across different types of function calling patterns.</p>"},{"location":"office-hours/cohort2/week2-summary/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Fine-tuning priorities: Fine-tune embedding models, not LLMs. With just 6,000 examples, you can create embedding models that outperform general models on your specific tasks at minimal cost.</p> </li> <li> <p>Inventory matters more than algorithms: Having the right documents and metadata is more important than the algorithm itself. Missing information can't be retrieved no matter how good your algorithm is.</p> </li> <li> <p>Evaluation is empirical: Many decisions about chunking, including metadata, and blending synthetic data should be driven by empirical testing rather than theoretical assumptions.</p> </li> <li> <p>Parsing strategy: For complex documents, consider evaluating parsing/OCR separately from retrieval performance since parsing errors will be difficult to trace in the full pipeline.</p> </li> <li> <p>Function calling with planning: For complex agent workflows, have the model create a plan first and get user confirmation rather than executing functions immediately. This creates training data for future interactions.</p> </li> <li> <p>State machines still work: Graph-based/finite state machine approaches remain effective for structured conversations, with LLMs improving the transitions between states.</p> </li> <li> <p>Metadata inclusion: Include relevant metadata in chunks to answer questions about document properties like authorship, modification dates, and contact information.</p> </li> <li> <p>Long context doesn't eliminate RAG: Despite larger context windows, there will always be latency, cost, and performance tradeoffs that make efficient retrieval necessary.</p> </li> <li> <p>Research pragmatism: Focus on solving specific problems with your data rather than chasing the latest research papers, which often reinvent existing techniques.</p> </li> <li> <p>Cross-encoders vs. bi-encoders: Cross-encoders (re-rankers) understand semantic distinctions better but are slower; bi-encoders (embedding models) are faster but less nuanced. Use both for optimal performance.</p> </li> </ol> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"office-hours/cohort2/week3-summary/","title":"RAG Office Hours Q&amp;A Summary - Week 3","text":""},{"location":"office-hours/cohort2/week3-summary/#when-gathering-negative-feedback-from-documents-not-being-found-how-do-we-use-and-validate-the-reliability-of-an-llm-labeler","title":"When gathering negative feedback from documents not being found, how do we use and validate the reliability of an LLM labeler?","text":"<p>When it comes to getting negative feedback that documents were not found, I'd assume we're running into issues around low recall. This might manifest as low re-ranker scores or low cosine similarities with embedding models.</p> <p>What I would do is identify whether the language model itself can identify if documents are irrelevant. We'll need some manual labeling step. With our clients, we generally find questions that emit flags - maybe you tell a language model to always say \"we couldn't find relevant documents\" when it can't find anything.</p> <p>You can then label that as traffic is being processed. We might sample 1% of traffic, and some percentage of that might have that message. That's one level of detection.</p> <p>The second level would be building a streamlit UI where we can manually label whether we agree with the irrelevancy assessment. The hard task is determining if any of 10 text chunks are relevant to a question. The easier task is determining if a single text chunk is relevant to a question. That's easy for a human to do and also pretty easy to prompt for.</p> <p>This approach helps ensure the judgment we're using is aligned with human preferences. There's obviously a big difference between 60% alignment and 95% alignment, but this is a good start for figuring out whether low relevancy is causing the lack of documents.</p>"},{"location":"office-hours/cohort2/week3-summary/#in-the-segmentation-topic-we-talked-about-inventories-and-capabilities-is-it-realistic-to-do-this-automatically-or-is-it-something-we-have-to-do-manually","title":"In the segmentation topic, we talked about inventories and capabilities. Is it realistic to do this automatically or is it something we have to do manually?","text":"<p>I would generally recommend doing this manually, because it's so important for what your business is trying to do that you need to actually think about these problems.</p> <p>We've delegated so much thinking to language models. If we just think a bit harder about our problem, we often find very specific issues.</p> <p>For example, with a client doing tax law resolution, the first 20 pages were massive articles, and then pages 30-40 were the exemptions to those articles. We spent maybe $20 of LLM calls to rewrite the documents so that the exemptions were close to the relevant articles. Now we have a single page/chunk covering an article and all its exemptions, with references to related articles.</p> <p>We run that job once a week when new tax laws come in. Since we only have about 45 documents we really care about, I'd rather spend the money upfront to get the process right rather than waste customer time requerying data.</p> <p>The real goal isn't to get a number right - it's to figure out what to do next. The AI can't tell us that. Your job isn't to automate this process; you're being paid to figure out what the next intervention should be.</p>"},{"location":"office-hours/cohort2/week3-summary/#can-you-elaborate-on-your-view-on-rag-versus-recommendations-how-would-you-approach-the-use-case-of-friend-suggestions","title":"Can you elaborate on your view on RAG versus recommendations? How would you approach the use case of friend suggestions?","text":"<p>When you build a recommendation system, there are several steps:</p> <ol> <li>Sourcing - What inventory can I show my customer? In the friends case, this would be all users on the platform.</li> <li>Query - Either your user ID or a question embedding.</li> <li>Scoring - For simple RAG, this is cosine distance of embeddings and maybe re-ranker distance. For friends, it might include mutual connections, location, etc.</li> <li>Filtering - In RAG this might be top 10 results or embeddings greater than a threshold. For friends, filters might include having at least 3 mutual friends, same zip code, etc.</li> <li>Rendering the results</li> </ol> <p>When users take actions (adding/removing friends, opening files, deleting citations), you collect feedback to improve your system. When a language model sees 10 documents but only cites 3, those 3 are likely more relevant. You can use that signal to improve your re-ranker or embedding model.</p> <p>If the user deletes one of those citations, you have a triplet: documents the model thinks are important, plus a negative example. When training, these need to be adjusted accordingly.</p> <p>It's like different levels of signals in e-commerce: liking a product is a weaker signal than adding it to cart, which is weaker than buying it, which is different from buying and returning it. That's your portfolio of data collected over time.</p>"},{"location":"office-hours/cohort2/week3-summary/#in-the-4th-lecture-you-mentioned-the-formula-expected-value-as-impact-times-the-number-of-queries-times-the-probability-of-success-can-you-explain-more-what-you-mean-by-impact","title":"In the 4th lecture you mentioned the formula expected value as impact times the number of queries, times the probability of success. Can you explain more what you mean by impact?","text":"<p>Impact here is a general term that the Facebook folks like to use. I generally think of impact as economic value.</p> <p>In the construction example I often mention, about 70% of questions were simple things like \"Where do I show up today?\" or \"How thick is the drywall?\" These weren't individually valuable.</p> <p>But we also found a set of questions that were super valuable - around scheduling and figuring out if contracts were signed. These were only about 10% of queries but extremely important. When we asked our clients, they said that preventing one missed contract could save $60,000 in delays.</p> <p>This told us these queries had high economic value, even though they were less frequent. So we invested resources in making sure we could query contracts and schedules to answer that segment.</p> <p>Impact is about how valuable a problem is and how much it's worth, rather than just how frequently it occurs. Every metric you track should enable you to take follow-up action afterward - it's not just about knowing the number.</p>"},{"location":"office-hours/cohort2/week3-summary/#what-is-the-lifecycle-of-feedback-if-we-improve-the-ui-old-labels-might-be-out-of-date-and-new-data-will-be-labeled-differently-what-is-good-to-keep-versus-letting-go","title":"What is the lifecycle of feedback? If we improve the UI, old labels might be out of date and new data will be labeled differently. What is good to keep versus letting go?","text":"<p>This depends on how much data we have and the blend of that data. If we had a million labels before changing the UI, I'd push hard to keep the new UI somewhat similar to ensure the data we collect remains comparable.</p> <p>If we're really changing things dramatically, there are modeling techniques to control for this. You might pre-train on the old data for your embedding model, then use that as a foundation for training a newer model. You can also control for the source of data in your modeling.</p> <p>You can have different evaluations to verify performance on the old data versus the new data, then choose how to weight those scores. Generally, I'd try to keep things as generic as possible - you don't want a dataset that's too specific and won't generalize.</p> <p>For embedding models specifically, I'd typically include everything, as more data is generally better.</p>"},{"location":"office-hours/cohort2/week3-summary/#is-it-interesting-to-collect-feedback-not-only-as-thumbs-up-or-thumbs-down-but-let-users-explain-in-text-what-is-wrong-with-the-answer","title":"Is it interesting to collect feedback not only as thumbs up or thumbs down, but let users explain in text what is wrong with the answer?","text":"<p>Yes and no. Thumbs up/down is super useful, and it would be hard to convince me not to use these binary labels. Going to a 5-star scale creates issues where you don't know if users consider 3 or 4 stars to be \"average.\"</p> <p>With free text feedback, you'll face two issues:</p> <ol> <li>Probably less than 10% of users will give a text response. If only 1% of users leave feedback at all, and only 10% of those leave text, you get very little text data, and you don't know how biased that sample is.</li> <li>You likely won't be able to read all the free text, so you'll build clustering models to analyze the feedback - in which case, you might as well just have 5 buttons for the most common issues (too slow, answer too long, format incorrect, etc.).</li> </ol> <p>It's about maximizing data per label. Having buttons for common issues will get you more useful data than open text fields.</p> <p>That said, free text can help you figure out what those buttons should be. For enterprise situations, we include the default buttons plus free text, and when users enter text, we post it to Slack where the team and customer can see it. This shows users their feedback is seen, making them more likely to provide it.</p> <p>But think about how often you've thumbs-downed a ChatGPT response, let alone written why. Most users simply won't take the time.</p>"},{"location":"office-hours/cohort2/week3-summary/#how-do-you-handle-recall-when-dealing-with-large-knowledge-bases-with-a-messy-topology-near-identical-documents-overlapping-content-hub-pages-etc","title":"How do you handle recall when dealing with large knowledge bases with a messy topology (near-identical documents, overlapping content, hub pages, etc.)?","text":"<p>This is challenging, especially with something like a large software product knowledge base (44,000+ documents) where many people have been adding content, creating overlap and interstitial hub pages.</p> <p>One approach is to build a system where if you retrieve a subset of pages, you can reference the connections. Similar to how e-commerce sites show \"people who viewed this also viewed\" suggestions.</p> <p>As context windows get larger, you could implement a system where if you pull in a page that references other documents, you traverse one level and bring in those referenced documents too.</p> <p>You could also do clustering and summarization. If your repository is very valuable, maybe it costs 10 cents to process a page, but with a budget of 50 cents per query, you could chunk everything, cluster similar content, and then summarize the clusters. This essentially rewrites the knowledge base in a less duplicated way.</p> <p>The more fundamental question is about how you define relevance. Do you have a policy document on what makes a document relevant? Google has a detailed document on what makes a good search result. You need to establish and document your criteria so everyone has the same understanding.</p>"},{"location":"office-hours/cohort2/week3-summary/#have-you-compared-the-effectiveness-of-classical-and-agent-based-rag-systems-with-capabilities-offered-by-models-like-gemini-flashlight-for-real-projects","title":"Have you compared the effectiveness of classical and agent-based RAG systems with capabilities offered by models like Gemini Flashlight for real projects?","text":"<p>I prefer not to think about systems as \"classical\" versus \"agent-based\" RAG systems. Most RAG systems are essentially function calling in a for-loop or while-loop.</p> <p>The goal is to provide the language model with two things:</p> <ol> <li>Good functions</li> <li>Good indices for each function to query that are well-defined</li> </ol> <p>You want to ensure each index has good recall, each function is useful for the system, and you have good prompts to help the model choose the right function.</p> <p>For real projects, it's not just about question answering but also about tool rendering. Some tool calls define UX elements - like a fitness company chatbot that renders modals for booking calendar events and following up with payment links. This becomes the economically valuable work - not just answering questions but helping the company make money.</p>"},{"location":"office-hours/cohort2/week3-summary/#whats-the-moat-for-companies-building-rag-systems-when-so-much-is-being-open-sourced","title":"What's the moat for companies building RAG systems when so much is being open-sourced?","text":"<p>I generally think the moat is your labeled data. There probably isn't much difference between various newsfeed algorithms, but the moat is the inventory - the content that's already out there.</p> <p>If you have relationships in a specific sector like construction and can be the first to build connectors and bring in that data, that's an easy moat (though not as defensible).</p> <p>After that, it's about analyzing that data to understand what questions people are asking and building specialized tools for those needs. This is software that LLMs won't replace anytime soon.</p> <p>Then it's understanding what relevance actually means - fine-tuning re-ranking models, training custom embedding models. These are aspects that LLM companies won't compete against.</p> <p>The moat becomes your data - both relevancy data and access to the content itself - plus your understanding of customer needs and workflows. The more you understand what customers are truly trying to do (beyond just answering questions about PDFs), the better your product will be.</p>"},{"location":"office-hours/cohort2/week3-summary/#in-the-ux-lectures-you-mentioned-that-explicit-copy-instead-of-just-thumbs-updown-can-impact-whether-people-give-feedback-have-you-observed-an-impact-based-on-what-the-copy-actually-says","title":"In the UX lectures, you mentioned that explicit copy instead of just thumbs up/down can impact whether people give feedback. Have you observed an impact based on what the copy actually says?","text":"<p>Absolutely. At Zapier, they asked \"How did we do?\" which was a very vague question that didn't get much feedback.</p> <p>When we A/B tested copy, the version that got 5x more feedback was \"Did we answer your question?\" This was much more specific and focused on the core value proposition, not about latency or formatting. If users said no, we'd follow up with \"Do you have any other feedback? Was it too slow? Was the formatting wrong?\" since we knew those were common failure modes.</p> <p>This not only got more feedback but also correlated better with customer satisfaction. The previous vague question made it hard to identify what was a good or bad answer - some might say we did poorly because we answered correctly but too slowly.</p> <p>At Raycast, our copy now is \"Did we take the correct actions?\" since we're showing function calls like \"Set a 1-hour lunch on my calendar and update my Slack status.\" We show users the sequence of function calls and ask if we're taking the correct actions.</p> <p>The key is that every metric you track should lead to a follow-up action. It's not just about knowing the number.</p>"},{"location":"office-hours/cohort2/week3-summary/#how-can-we-extract-value-from-templatepre-filled-questions-in-chatbots","title":"How can we extract value from template/pre-filled questions in chatbots?","text":"<p>For a situation like a lawn care subscription company's chatbot where 70% of conversations start with template questions, I'd be curious to understand what the follow-up questions look like. This helps determine if we could create complete guides for common paths.</p> <p>If people start with a certain template question, do their follow-ups cluster in a specific domain? This can help you understand if your example questions are actually helpful or if you should be writing better content to answer these questions more comprehensively.</p> <p>One approach is to use a language model to summarize conversations, identifying what topics come after the template questions. This gives you insight into actual user intents that might be hidden behind that initial templated interaction.</p> <p>You should analyze which topics are economically important by looking at metrics like thumbs up/down data. For instance, we found that many negative ratings come from users who want to talk to a real person but can't easily figure out how to do that.</p> <p>It's also valuable to analyze what products you should recommend based on question patterns. If you're seeing thumbs-down ratings, analyze whether it's because you don't have the right content in your knowledge base, or if there are capabilities you're missing. Often, the solution might be as simple as hiring someone to write targeted content for frequently asked questions.</p>"},{"location":"office-hours/cohort2/week3-summary/#how-do-you-handle-business-knowledge-translation-like-acronyms-in-rag","title":"How do you handle business knowledge translation (like acronyms) in RAG?","text":"<p>When you have documents that spell everything out formally but users want to query using acronyms (like \"What's the deal with ABC?\"), I'd generally just put this translation knowledge in the prompt unless you have an enormous number of acronyms.</p> <p>If you have fewer than 80 acronyms or terms that need translation, putting them directly in the prompt is the simplest and most effective approach. You only need to explore more complex approaches when you have evidence that this simple solution isn't working.</p> <p>You can also create synthetic data to test how well your system handles these acronym queries, which is usually straightforward to generate.</p>"},{"location":"office-hours/cohort2/week3-summary/#what-are-the-best-practices-for-chunking-in-rag-systems","title":"What are the best practices for chunking in RAG systems?","text":"<p>The general advice from companies like OpenAI and Anthropic is to start with around 800 tokens with 50% overlap using a sliding window approach. That should be enough to get you started.</p> <p>After that initial setup, the real improvements come from understanding what kinds of questions are being asked and what the answers look like. If most questions can be answered by a single document, focus on improving document search and relevancy rather than chunking. If answers typically come from small paragraphs across many documents, then experiment more with chunking.</p> <p>We've spent weeks doing chunking experiments and often haven't seen significant improvements. It's rarely the case that changing from 500 to 800 tokens suddenly makes everything work better - that would suggest most answers require just a few more sentences in the same document, which is usually not the issue.</p> <p>What's been more helpful is looking at the questions and working backward: What are people trying to do, and what design assumptions can I make to better serve that? For instance, if users are searching for blueprints, maybe summarizing blueprints first would help, or perhaps including text above and below the blueprint, or even applying OCR and building a bounding box model to count rooms.</p> <p>Solve specific problems where you can justify that \"this is 20% of our questions\" - if you make those 20% twice as good, you've improved overall performance by 8%, which is meaningful.</p>"},{"location":"office-hours/cohort2/week3-summary/#are-xml-tags-still-best-practice-for-prompting-models","title":"Are XML tags still best practice for prompting models?","text":"<p>Yes, we've learned that even the GPT-4 models now perform better with XML formatting. We have internal evaluations from Zenbase showing that XML is good not just for Anthropic models but also for ChatGPT models.</p> <p>The second thing we've found is that you generally want to have all the long context information at the beginning of the prompt - first the goal, then all the documents, with the actual questions at the bottom.</p> <p>Claude's prompt rewriter has been very helpful for showing how to write better prompts. I almost always run my prompts through it first before setting up evaluation suites, as it's a free way to get useful feedback.</p>"},{"location":"office-hours/cohort2/week3-summary/#how-do-you-handle-tokenization-concerns-with-things-like-wallet-addresses","title":"How do you handle tokenization concerns with things like wallet addresses?","text":"<p>When dealing with data that contains wallet addresses (which are 52 characters of what looks like nonsense), I'd worry less about the tokenization itself and focus more on validation.</p> <p>For example, in situations where we use UUIDs, we reference content with a UUID, and we tell the model to cite everything. We then have an allowlist of valid UUIDs from our data, and we check that any UUID the model outputs exists in that allowlist.</p> <p>So if you have a use case where users ask about wallet IDs, focus on making sure the model can only reference valid wallet IDs from your dataset rather than worrying about how they're tokenized.</p> <p>These days, models aren't typically off by a few characters - they'll either get it right or completely make up new identifiers. Having logical checks in your code is more important than the tokenization strategy.</p> <p>You can also generate synthetic test data where you know which wallet addresses should appear in the answers and ensure there are no hallucinations.</p>"},{"location":"office-hours/cohort2/week3-summary/#should-we-transform-content-from-narrative-format-to-qa-format-for-better-retrieval","title":"Should we transform content from narrative format to Q&amp;A format for better retrieval?","text":"<p>Yes, massively. This can be very beneficial, especially for a question-answering chatbot.</p> <p>It's already an assumption to think that everything is going to be in the form of a question. For some assistants, it might be more about conversations or past memories. If you know your use case is primarily Q&amp;A, then extracting question-answer pairs from your documents is valuable.</p> <p>You can build a system where when you embed a question, you retrieve the embedding of similar questions, but pull in both the question and its answer. This makes sense if your use cases are mostly Q&amp;A-based rather than narrative requests like \"tell me a story.\"</p> <p>One of the big assumptions in RAG is that the embedding of a question is similar to the embedding of a relevant document, which is actually a massive assumption that doesn't always hold true.</p> <p>To prevent retrieving too many similar question-answer pairs (which could be redundant when getting top-K results), consider doing clustering. You could extract 10 questions per document, then cluster similar questions together and rewrite them to create a more concise, focused knowledge base.</p>"},{"location":"office-hours/cohort2/week3-summary/#can-you-recommend-any-open-source-libraries-or-tools-for-streaming-uis-and-interstitials","title":"Can you recommend any open source libraries or tools for streaming UIs and interstitials?","text":"<p>I can't necessarily recommend a specific library too strongly because most companies I've worked with have built these themselves. However, if you're in the Python world, using something like FastAPI and server-side events (SSE) API is probably the simplest approach. In the slides, we give an example of what this looks like - you're basically using the yield keyword from Python generators to emit events.</p> <p>If you're using JavaScript and part of the Vercel/React ecosystem, I think Vercel's AI library does a great job of handling structured streaming. Other libraries like LangChain, LlamaIndex, and Instructor also support partial streaming where you can send incomplete JSON to a frontend, which can then rerender it.</p> <p>For interstitials, I've been impressed with what Ankur from BrainTrust has done in their playground. I've reached out to him to ask about recommendations for this.</p> <p>With these tools, the implementation is fairly straightforward. The bigger challenge is often designing a UX that communicates progress effectively. Notion's approach is a good example - when you enter a search query, it shows \"making a search request,\" rewrites the request, then renders documents one by one, and finally shows steps like \"carefully reading documents,\" \"thinking,\" and \"formulating an answer.\" This is really just buying time while showing progress, but it dramatically improves the perceived responsiveness.</p>"},{"location":"office-hours/cohort2/week3-summary/#why-arent-data-labeling-companies-a-bigger-focus-in-current-ai-discussions","title":"Why aren't data labeling companies a bigger focus in current AI discussions?","text":"<p>This is an interesting historical shift. Around 2018, data labeling was a huge focus because the biggest models were vision models that required massive amounts of labeled data. Vision models aren't very data-efficient - training ImageNet required labeling a million JPEGs. Companies like Scale AI won by excelling at tasks like self-driving car LiDAR labeling.</p> <p>As we've moved to LLMs, two things have changed:</p> <ol> <li>The big winners (like Scale AI) have already established themselves and now focus on large contracts. Smaller players either grew or struggled to find viable business models on smaller contracts.</li> <li>LLMs are much more data-efficient.</li> </ol> <p>The data efficiency of modern LLMs is remarkable. You're better off having 1,000 very high-quality labels to fine-tune a model than 10,000 mediocre labels. This means that instead of outsourcing labeling work, it often makes more sense to have subject matter experts do a one-month project to create the data you need.</p> <p>We're so sample-efficient now that offshore labeling doesn't make economic sense for many use cases, especially when LLMs have been shown to match or exceed the quality of offshore labeling for many tasks. If you have specific legal workflows, you're better off asking the lawyer on your team to do the labeling.</p> <p>The real challenge now is: how do you find people who are smarter than GPT-4 to label data to train the next generation of models? That hiring problem is different from the traditional labeling company approach.</p>"},{"location":"office-hours/cohort2/week3-summary/#how-do-you-see-re-rankers-evolving-beyond-just-measuring-relevancy","title":"How do you see re-rankers evolving beyond just measuring relevancy?","text":"<p>Right now, most RAG systems only rank based on relevancy between a query and a document. But I think re-rankers will soon incorporate much more side information, similar to what we see in e-commerce recommendation systems.</p> <p>In e-commerce, we have additional rankers for things like price sensitivity, seasonality, and product age to determine if customers prefer trendy or timeless items. This hasn't really happened in the RAG world yet.</p> <p>As AI systems accumulate multiple years of memories about users, figuring out what information to put in context will become much more interesting. Re-rankers won't just measure string similarity between a question and document - they'll likely incorporate user features, environmental features, and contextual information to determine relevance.</p> <p>For example:</p> <ul> <li>Security constraints (only searching documents you have access to)</li> <li>Time/recency components for memories</li> <li>Domain authority when sources disagree</li> <li>User preferences based on past interactions</li> </ul> <p>Even systems like Deep Research might evolve to pull from sources you tend to agree with, or deliberately include sources that challenge your viewpoint. These personalized relevancy signals could dramatically improve RAG systems beyond simple semantic matching.</p>"},{"location":"office-hours/cohort2/week3-summary/#key-takeaways-and-additional-resources","title":"Key Takeaways and Additional Resources","text":""},{"location":"office-hours/cohort2/week3-summary/#key-takeaways","title":"Key Takeaways:","text":"<ul> <li>Data quality is becoming more important than ever - good models make data quality the differentiator</li> <li>When collecting feedback, be specific with your questions to increase response rates</li> <li>Focus on economically valuable workflows, not just answering questions</li> <li>For messy knowledge bases, consider clustering and summarization approaches</li> <li>The moat for RAG companies is proprietary data and domain expertise, not algorithms</li> <li>Binary feedback (thumbs up/down) generally gets more responses than free text</li> <li>Always have a clear next action from any metric you collect</li> <li>Focus on impact (economic value) rather than just query volume</li> </ul>"},{"location":"office-hours/cohort2/week3-summary/#additional-resources","title":"Additional Resources:","text":"<ul> <li>Google Search Relevancy document/policy is a good reference for defining relevance</li> <li>RAPTOR paper for document summarization approaches</li> <li>Week 3-4 content in the course covers more on these topics</li> <li>For prompt rewriting, Claude's prompt rewriter is highly recommended</li> <li>When dealing with streaming UIs and latencies, Notion's approach of showing steps visually is a good reference</li> <li>For friends example in recommendation systems, consider platforms like Facebook's friend recommendation system as reference implementations</li> </ul> <p>Note: I'll continue to add resources and notes from future office hours sessions</p> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"office-hours/cohort2/week4-summary/","title":"RAG Office Hours Q&amp;A Summary - Week 4","text":""},{"location":"office-hours/cohort2/week4-summary/#what-can-segments-mean-beyond-query-volume-and-satisfaction-values-in-a-rag-system","title":"What can \"segments\" mean beyond query volume and satisfaction values in a RAG system?","text":"<p>Segmentation really depends on who your customers are and what they're trying to do. With a generic chatbot, it's hard to figure out what segmentation means. But if you think about intents of a specific application, you can uncover different patterns.</p> <p>For example, with a nutrition company chatbot, you might discover segments within product search \u2013 different capabilities around understanding deliveries, rescheduling, recurring orders, etc. Data analysis helps figure out what's important to build for the customer.</p> <p>In a construction context, we found segments around:</p> <ul> <li>Users inputting specific project IDs (e.g., \"Tell me about RFC 1257\")</li> <li>Questions about time windows (\"What do I have due today?\" or \"What's happening this week?\")</li> <li>Counting items in documents</li> </ul> <p>The goal of segmentation is to help you figure out what new function tools to build and what workflows might be viable. Another example: for a product that takes screenshots of users' computers, we found 10% of customers asking \"How much time did I spend in this application?\" That's impossible to answer with just screenshots, but we realized we had a Postgres database of all screenshots with timestamps, so we built a specific tool to query, group, and sum that data to answer the question.</p> <p>The key is to find external understanding of your data \u2013 what are you worried about, and if you discover certain properties, what can you do about it?</p>"},{"location":"office-hours/cohort2/week4-summary/#how-should-we-approach-segmentation-for-chatbots-where-the-output-is-a-whole-conversation-rather-than-just-a-query-response","title":"How should we approach segmentation for chatbots where the output is a whole conversation rather than just a query response?","text":"<p>If you have the compute resources, do similar classification and segmentation over your conversations. You'll uncover different insights beyond just tools.</p> <p>When analyzing queries alone, we're basically asking how well we can execute tools to answer in one generation. By analyzing conversations, we might find segments that tell us:</p> <ul> <li>Users think the chatbot talks too much or not enough</li> <li>Users are frustrated with responses</li> <li>Common patterns in how conversations progress</li> </ul> <p>The general idea is to gain an external understanding of your data \u2013 what properties are you concerned about, and if you discover X% of conversations have a certain property, what action can you take?</p> <p>For example, if you find many users asking the language model to rewrite answers in their own words, should that be part of your system prompt? Analysis might show only 10% want tone matching, while most users actually prefer the AI voice.</p>"},{"location":"office-hours/cohort2/week4-summary/#what-approaches-do-you-recommend-for-topic-clustering-and-have-you-tried-using-thinking-models-to-generate-clusters","title":"What approaches do you recommend for topic clustering, and have you tried using thinking models to generate clusters?","text":"<p>I generally use what I call \"old school\" approaches \u2013 K-means and DBSCAN. I typically start with the default settings in BERTTopic, which has been very good. The topic modeling goal isn't to uncover topics for production use but to do data analysis that helps you understand your data better.</p> <p>For example, I might take Ada 2 embeddings, use K-means to pick 10-30 clusters, and look at 100 questions per cluster. That might take 2-3 days but teaches you a lot about your data. It's rarely the case that you run topic models and can just use them directly in your business.</p> <p>When working with thinking models for clustering, I still do the initial clustering first because I might have 20 million questions to analyze. I'll cluster that data, find good and bad examples across clusters, and put that into Claude 3.7 or similar models, asking them to:</p> <ul> <li>Name each cluster</li> <li>Provide a short description</li> <li>Give good examples of what belongs in the cluster</li> <li>Provide nuanced examples of what's not in the cluster</li> </ul> <p>This produces a YAML file that I can then use for classification. The language model helps expand our understanding, especially when we can't easily enumerate all possibilities ourselves.</p>"},{"location":"office-hours/cohort2/week4-summary/#what-are-your-thoughts-on-chunk-size-and-chunk-overlap-is-it-worth-trying-out-different-chunking-strategies","title":"What are your thoughts on chunk size and chunk overlap? Is it worth trying out different chunking strategies?","text":"<p>I generally use 800 tokens with 50% overlap, which is what OpenAI recommends in their blog posts. In my experience, chunking strategies rarely make a significant difference compared to other improvements.</p> <p>There's only a small subset of questions where chunk size makes a difference \u2013 you would need a question that can only be answered by a paragraph where two concepts are exactly 500 tokens apart. Performance gains usually come from better re-ranking, contextual retrieval (where you rewrite text chunks given the entire document), or better filtering and metadata capabilities.</p> <p>I've rarely seen chunk size be the 10% improvement win \u2013 it might be a 1-2% improvement, which could just be noise. I would focus more on contextual retrieval if you have the compute budget for it.</p> <p>For semantic chunking (using an LLM to determine good chunking points), I'm actually pretty convinced that contextual retrieval is better than dynamically chunking. The real question is whether you need to cite things word-for-word (in which case you shouldn't rewrite chunks) or if you just need general question answering.</p> <p>I'd always spend more compute upfront to improve data quality. For example, I worked with a company doing Brazilian tax law with 50 documents, each 600 pages long. I asked, \"Why are you only spending 70 cents to process this PDF? Why not spend $30?\" If you're processing billions of dollars through the system, you should invest in good ingestion.</p>"},{"location":"office-hours/cohort2/week4-summary/#what-strategies-can-improve-experimentation-speed-when-working-with-rag-systems","title":"What strategies can improve experimentation speed when working with RAG systems?","text":"<p>If you feel like you're not running enough experiments, focus on improving your infrastructure:</p> <ol> <li> <p>Write parallelized code: Many teams are still doing all their tests using for loops. Spending 1-2 hours learning to write parallelized code can dramatically reduce your experimentation time, going from days to hours. Using tools like multiprocessing to hit multiple endpoints simultaneously is much better than having code break on iteration 2,000.</p> </li> <li> <p>Improve data access and understanding: Document how to query your data effectively. It's a waste of time if you write a query to prepare data, and someone comes back a day later saying, \"That's wrong, we actually need to include only last week's data.\"</p> </li> <li> <p>Build modular pipelines: If your entire RAG application is a giant Python file, it will be hard to test. But if each search index is a separate POST request, you can test them individually. This allows you to focus on one component (like an image retriever system) and improve it from 30% to 80% accuracy in one afternoon before integrating it back into your router.</p> </li> <li> <p>Test locally when possible: Create smaller synthetic datasets for quick iteration before running larger tests.</p> </li> </ol> <p>Being able to test components in isolation is crucial for rapid experimentation. A lot of this comes down to good software engineering practices and thoughtful system design.</p>"},{"location":"office-hours/cohort2/week4-summary/#how-do-you-handle-multiple-languages-in-a-rag-system-especially-when-job-titles-may-be-similar-but-written-differently-across-languages","title":"How do you handle multiple languages in a RAG system, especially when job titles may be similar but written differently across languages?","text":"<p>For multilingual challenges like job titles across different languages, I recommend two approaches:</p> <ol> <li> <p>Metadata extraction and filtering: Build classifiers to add more metadata to your ontology. For example, \"software engineering recruiter\" and \"software engineer\" go into two different classes, allowing you to filter for one and not the other. This improves search precision.</p> </li> <li> <p>Fine-tune embedding models with triplets: Create a dataset with examples like \"software engineer\" (query), \"python developer\" (positive example), and \"software engineering recruiter\" (hard negative). This teaches your model to separate similar-looking job titles that have different meanings.</p> </li> </ol> <p>For handling multiple languages, run tests to see whether translation improves performance. For instance, does your classifier perform better if you translate everything to English first, or if you use the original languages? If translating provides only a 1-2% improvement but requires complex infrastructure to maintain, it might make sense to accept slightly lower performance.</p> <p>If you lack training data for certain languages, consider using synthetic data creation. Use $2,000 of API credits to generate examples that cover edge cases in your domain, like distinguishing between \"real estate developer\" and \"python developer\" across languages.</p>"},{"location":"office-hours/cohort2/week4-summary/#what-are-your-thoughts-on-vision-rag-and-what-databases-would-you-recommend-for-multimodal-embeddings","title":"What are your thoughts on vision RAG, and what databases would you recommend for multimodal embeddings?","text":"<p>Vision RAG isn't talked about as much because it's more expensive and most of the important data is typically in text. That said, there are valuable use cases \u2013 like a company that does RAG over video clips to help movie producers find content, using Gemini Flash to describe what's happening in scenes.</p> <p>For databases, I'd recommend looking at:</p> <ul> <li>ChromaDB</li> <li>LanceDB</li> <li>TurboBuffer (used by Notion and Cursor)</li> <li>PgVector with Scale (for relational data with many reads/writes)</li> </ul> <p>However, I'm finding that pure multimodal embeddings aren't always the best approach anymore. Often it's better to generate a text summary of the image data. For example, when trying to embed images and text in the same space, CLIP embeddings often work worse than just doing image captioning and then embedding that text.</p> <p>In week 5, I'll talk more about this \u2013 there are many things you can't do with multimodal embeddings. They're trained mostly with caption data, which limits their capabilities for certain tasks.</p>"},{"location":"office-hours/cohort2/week4-summary/#what-are-your-experiences-with-the-model-context-protocol-mcp-and-how-might-it-change-rag-systems","title":"What are your experiences with the Model Context Protocol (MCP) and how might it change RAG systems?","text":"<p>MCP is becoming increasingly important because it allows different systems to connect with each other. When you own all the code, you don't really need MCP since you can just use function calling. But the ability to connect different systems is very compelling.</p> <p>Some interesting examples of MCP usage:</p> <ul> <li>Having an MCP server in Cursor to do image generation while building a video game</li> <li>Creating an MCP server to access network logs for debugging web applications</li> <li>Building MCP servers that connect to production databases so Cursor can understand your schema and write SQL</li> <li>Setting up an MCP server that writes conversation notes to Notion automatically</li> </ul> <p>What makes MCP powerful is that it standardizes these integrations and reduces boilerplate code. The protocol founders explain that it's easy to integrate with other servers when building your own client or server. Instead of rebuilding connectors with databases or services, you can reuse patterns and implementations.</p> <p>Claude 3.7 with Claude Code, for instance, has impressive agent functionality using MCP. It features better context management through commands like \"/compact\" which summarizes conversation history effectively without bloating the context window.</p>"},{"location":"office-hours/cohort2/week4-summary/#how-can-we-use-synthetic-data-generation-for-summarization-tasks","title":"How can we use synthetic data generation for summarization tasks?","text":"<p>There are many creative ways to generate synthetic data. For summarization, you can:</p> <ol> <li> <p>Create reverse tasks: For example, start with the outcomes you care about (like action items) and ask an LLM to generate a transcript that would produce those items. Then you can verify if your summarization system correctly extracts the original action items from this synthetic transcript.</p> </li> <li> <p>Use data augmentation techniques: Look at techniques from other domains like speech detection, where researchers combine clean audio samples to create more complex scenarios (like overlapping speakers). You can apply similar principles to text.</p> </li> <li> <p>Apply transformations similar to image processing: In computer vision, we've long used techniques like converting color photos to black and white, then training models to predict the original colors. Similarly, we convert high-resolution images to low-resolution and train models to predict the original resolution. We can apply similar transformations to text data.</p> </li> </ol> <p>The key is to think about ways to go from your desired output backward to input data, or to systematically transform existing data in ways that preserve the information you care about while changing other aspects.</p>"},{"location":"office-hours/cohort2/week4-summary/#when-using-structured-outputs-with-few-shot-prompts-should-the-examples-use-the-exact-same-json-schema-or-can-they-be-plain-text","title":"When using structured outputs with few-shot prompts, should the examples use the exact same JSON schema or can they be plain text?","text":"<p>I would almost always try to keep the JSON format consistent in your few-shot examples. This is somewhat superstitious, but I feel like the attention mechanism will always attend better to similar tokens.</p> <p>The schema itself is probably not what's going to break things these days. More likely, problems will arise from unintended properties of your examples. For instance, if all your action items in the few-shot examples are very short (under 4 words), your outputs will tend to be very short too. The examples communicate that these properties are correlated.</p> <p>I'd rather keep everything in JSON because there will be other random issues that come up. The only caution is to make sure you have checks in place so that when the language model has nothing in the context, it won't just automatically recite the few-shot examples.</p> <p>For complex contexts (like insurance claims that require understanding policies and forms), if including the context for each few-shot example would make your context window explode, consider few-shotting the thinking more importantly. Show examples of the reasoning process: \"I noticed the customer said they had 28 people, and our pricing page has different pricing for teams with less than 30 employees, so I'll use that pricing tier and mention they could get a better price with more employees...\"</p>"},{"location":"office-hours/cohort2/week4-summary/#how-do-you-approach-rag-when-you-have-transcripts-or-unstructured-text-without-clear-paragraph-markers","title":"How do you approach RAG when you have transcripts or unstructured text without clear paragraph markers?","text":"<p>For transcripts without clear paragraph markers, a few approaches work well:</p> <ol> <li> <p>Use diarization models to get speaker tags, which can serve as natural boundaries (each dialog line becomes a chunk)</p> </li> <li> <p>Detect silences in the audio and chunk on those silences</p> </li> <li> <p>Consider the structure of your content - for instance, if it's an interview format, you might know it's always question-answer pairs, so you can embed those pairs together</p> </li> </ol> <p>It ultimately depends on your specific use case. For a general conversation, chunking on silences or using diarization with a sliding window over dialog will work. For job interviews or expert interviews, understanding the structure (question followed by answer) lets you optimize your chunking strategy.</p> <p>If you have mixed domains and raw transcripts without access to the original source, you might need to default to generic approaches like 800 tokens with 40% overlap, then rely more on contextual retrieval techniques.</p>"},{"location":"office-hours/cohort2/week4-summary/#what-are-your-recommendations-for-building-slide-presentations-with-ai-tools","title":"What are your recommendations for building slide presentations with AI tools?","text":"<p>I've been using AI tools to build academic-style slides with LaTeX and Beamer. My process is:</p> <ol> <li>Load all relevant content into Cursor (in my case, all 6 hours of course transcripts)</li> <li>Create an outline for the presentation</li> <li>Use Claude to extract key case studies and insights from the transcripts</li> <li>Have the LLM generate slides using LaTeX Beamer format</li> <li>Use a simple auto-compiler (built with Watchdog) that recompiles the PDF whenever the file changes</li> </ol> <p>The advantages of this approach:</p> <ul> <li>You can create both slides and a detailed document from the same source</li> <li>The LLM can generate diagrams using TikZ (a graphics creation library)</li> <li>Everything is vector-based so it looks clean at any resolution</li> <li>You can have the LLM add callouts, highlights, and formatting</li> </ul> <p>This approach lets me essentially talk to my slides and have them update in real-time. I can say \"make this section shorter\" or \"add an example about X\" and see the changes immediately in the PDF preview.</p> <p>For those who prefer different formats, you could also try reveal.js for web-based presentations. The key is finding a workflow that lets you focus on content while the AI handles formatting and details.</p>"},{"location":"office-hours/cohort2/week4-summary/#how-do-ai-coding-tools-compare-claude-code-aider-cursor-windsurf","title":"How do AI coding tools compare (Claude Code, Aider, Cursor, Windsurf)?","text":"<p>There's been significant evolution in AI coding tools, with different strengths and approaches:</p> <ul> <li> <p>Claude Code has impressive agent functionality with excellent context management. It features a \"/compact\" command that summarizes conversation history effectively without bloating the context window. Some users report it's more capable than Cursor for certain tasks, particularly with how it handles context and managing complexity.</p> </li> <li> <p>Aider is a CLI-based tool that gives very low-level control over the files you can edit. It's open source and allows granular control over which models you use at specific points. Some users have migrated from Cursor to Aider due to its flexibility, though it has a steeper learning curve.</p> </li> <li> <p>Cursor is widely used for its UI and integrations. It works well for incremental changes to code and has good MCP integrations, but some find its context management becomes less effective over time on complex projects.</p> </li> <li> <p>Windsurf is particularly good at handling projects with good requirements and system design. It excels at context management over time and keeping track of multiple files in a repository. It's especially valuable for staff engineers and system architects who start with clear system designs.</p> </li> </ul> <p>The key differentiation often comes down to context management - how well the tool maintains an understanding of your entire codebase and project requirements as you work. For complex projects, tools that help document the goals and requirements (like adding branch goals in comments) tend to perform better.</p>"},{"location":"office-hours/cohort2/week4-summary/#how-do-you-use-deep-research-and-other-search-tools-effectively","title":"How do you use Deep Research and other search tools effectively?","text":"<p>Different search tools serve different purposes depending on context:</p> <ul> <li> <p>Claude's Deep Research works well for technical documentation, business-level competitive analysis, and generating comprehensive memos. Its tone is particularly well-suited for business communications that need minimal editing. Many users leverage it to materialize blog posts or analyses they want to read (e.g., \"Write me a blog post on why someone should look at MCP versus just using the Open API spec\").</p> </li> <li> <p>Grok's Deep Search has different strengths, with some users preferring it for timely news or quick research questions. Interestingly, usage patterns often split between mobile (Grok) and desktop (Claude/OpenAI) platforms based on when and where research is being done.</p> </li> <li> <p>Perplexity offers another approach to deep research, useful for generating product specs and learning resource reports, especially for colleagues without AI engineering backgrounds.</p> </li> </ul> <p>The quality of these tools has advanced to the point where they can effectively replace traditional research methods for many use cases, saving significant time for competitive analyses and technical investigations.</p>"},{"location":"office-hours/cohort2/week4-summary/#what-makes-lovable-stand-out-for-no-code-app-generation","title":"What makes Lovable stand out for no-code app generation?","text":"<p>Lovable has emerged as a powerful tool for no-code app generation:</p> <ul> <li>It excels at creating fully functional applications with modern UIs from scratch, going beyond simple prototypes to production-ready systems</li> <li>Its deep integration with Supabase provides authentication, real-time features, and database capabilities out of the box</li> <li>Every code change gets pushed to GitHub, allowing developers to fix issues locally in tools like Cursor or Windsurf when needed</li> <li>Each commit creates a preview deployment on Cloudflare, streamlining the development and testing process</li> <li>The tool can implement complex features like row-level security, push notifications, and real-time commenting systems using websockets</li> </ul> <p>Users report that Lovable outperforms alternatives like V0 and Bolt for creating complete applications, though it can be expensive ($200+ for complex projects). The tight integration with Supabase is particularly valuable, with many users becoming paid Supabase customers after using Lovable to build their applications.</p>"},{"location":"office-hours/cohort2/week4-summary/#what-emerging-techniques-are-promising-for-handling-long-documents-in-rag","title":"What emerging techniques are promising for handling long documents in RAG?","text":"<p>Handling long documents effectively is still evolving, with several promising approaches:</p> <ol> <li> <p>Hierarchical retrieval: Create summary or header-level embeddings for entire documents/chapters, then more granular embeddings for sections/paragraphs. This allows multi-stage retrieval that narrows down from document to specific passages.</p> </li> <li> <p>Graph-based approaches: Build knowledge graphs connecting concepts across documents, enabling retrieval that follows conceptual relationships rather than just lexical similarity.</p> </li> <li> <p>Hybrid sparse-dense retrieval: Combine embedding-based retrieval with keyword/BM25 approaches to capture both semantic and lexical matches, which is particularly valuable for documents with specialized terminology.</p> </li> <li> <p>Learning to rewrite: Train models to rewrite retrieved chunks into more coherent contexts that preserve the key information while eliminating redundancy.</p> </li> <li> <p>Recursive summarization: For extremely long documents, apply recursive summarization techniques that gradually compress information while maintaining key details.</p> </li> </ol> <p>Projects like LangChain's Document Transformer framework and repositories focusing on document processing show significant advances in these areas. The most effective systems often combine multiple approaches based on the specific characteristics of their document collections.</p>"},{"location":"office-hours/cohort2/week4-summary/#how-can-i-approach-rag-for-messy-knowledge-bases-with-duplicate-documents","title":"How can I approach RAG for messy knowledge bases with duplicate documents?","text":"<p>When dealing with messy knowledge bases that contain duplicate or near-duplicate documents:</p> <ol> <li> <p>Pre-processing pipeline: Implement de-duplication strategies during ingestion. This could involve computing similarity scores between documents and merging or filtering based on a threshold.</p> </li> <li> <p>Metadata extraction and filtering: Add more metadata to your ontology by building classifiers for different document types or topics. This allows you to filter for specific categories during retrieval.</p> </li> <li> <p>Query classification: For ambiguous queries, implement both pre-retrieval and post-retrieval classification to identify query intent and determine when clarification is needed.</p> </li> <li> <p>Progressive disclosure: Consider displaying intermediate results with summarized information about potential topics before generating a complete answer. This helps users navigate ambiguity, especially for queries that could refer to multiple topics.</p> </li> <li> <p>Dynamic presentation: For high-latency requirements (e.g., responses needed in under 6 seconds), consider showing retrieved documents first while the full answer is being generated, allowing users to see some results immediately.</p> </li> </ol> <p>Remember that the goal isn't perfect retrieval but helping users find the information they need. Sometimes showing multiple possible interpretations of a query is more helpful than trying to guess the single \"right\" answer.</p>"},{"location":"office-hours/cohort2/week4-summary/#when-is-it-better-to-use-dags-versus-agentic-approaches","title":"When is it better to use DAGs versus agentic approaches?","text":"<p>For specific workflows with well-defined steps, DAGs (Directed Acyclic Graphs) often provide more reliable and predictable results than fully agentic approaches:</p> <ol> <li> <p>Use DAGs when:</p> </li> <li> <p>The workflow has clear, sequential steps</p> </li> <li>You know the process is correct and just need to choose the right workflow</li> <li>You're implementing established protocols (like therapy approaches or compliance processes)</li> <li> <p>Predictability and consistency are critical</p> </li> <li> <p>Use agentic approaches when:</p> </li> <li> <p>The problem space is exploratory</p> </li> <li>Tasks require adaptation to unpredictable user input</li> <li>The workflow needs to evolve based on intermediate results</li> <li>You need to handle a wide variety of open-ended requests</li> </ol> <p>The distinction often comes down to control versus flexibility. DAGs provide more control over the exact process, while agentic approaches offer more flexibility but less predictability.</p> <p>For example, in a therapeutic chatbot following an established CBT protocol, a DAG approach ensures the conversation follows the correct therapeutic sequence. However, for an open-ended research assistant, an agentic approach allows for more dynamic problem-solving.</p>"},{"location":"office-hours/cohort2/week4-summary/#how-do-i-create-effective-negative-examples-for-training-retrieval-models","title":"How do I create effective negative examples for training retrieval models?","text":"<p>Creating effective negative examples for training retrieval models involves several strategies:</p> <ol> <li> <p>Hard negative mining: Find examples that are semantically similar but actually irrelevant. For job listings, \"software engineer recruiter\" is a hard negative for \"software engineer\" queries - they look similar textually but represent different job categories.</p> </li> <li> <p>Top-K analysis: Run retrieval with your current model, then have an LLM evaluate which results in the top K are actually irrelevant. These make excellent negative examples because they expose weaknesses in your current model.</p> </li> <li> <p>Controlled random sampling: While pure random sampling provides some signal, it's often too easy for the model to distinguish. Instead, use controlled randomization that preserves some properties of the positive examples.</p> </li> </ol> <p>When working with triplet learning (query, positive example, negative example), the quality of your negative examples often has more impact on model performance than adding more positive examples. Focus on finding negative examples that are difficult to distinguish from positive ones.</p> <p>For multimodal or multilingual applications, you may need to create synthetic data, especially for languages with limited training data. This can be done by using LLMs to generate examples that explore edge cases in your domain.</p>"},{"location":"office-hours/cohort2/week4-summary/#what-strategies-can-improve-response-time-in-rag-systems-with-tight-latency-requirements","title":"What strategies can improve response time in RAG systems with tight latency requirements?","text":"<p>For applications requiring responses in just a few seconds:</p> <ol> <li> <p>Progressive rendering: Show retrieved documents first (which can be returned in 150-400ms) while the LLM generates the complete answer in the background. This gives users immediate results while they wait for the full response.</p> </li> <li> <p>Caching: Implement aggressive caching for common queries. When a question-answer pair receives positive feedback (like being forwarded, shared, or rated highly), save it as a new document that can be quickly retrieved for similar questions.</p> </li> <li> <p>Response type classification: Use a lightweight classifier to determine if a query needs full retrieval and generation or if it can be answered with a simpler approach.</p> </li> <li> <p>Contextual snippet generation: During retrieval, generate quick summaries of each chunk that can be displayed alongside search results before the complete answer is ready.</p> </li> <li> <p>Parallel processing: Run multiple retrieval strategies in parallel and combine the results, rather than using sequential processing that adds to the total latency.</p> </li> </ol> <p>The key insight is to avoid an all-or-nothing approach to response generation. By decomposing the process into steps that can be displayed incrementally, you can significantly improve perceived latency even when the complete answer takes longer to generate.</p>"},{"location":"office-hours/cohort2/week4-summary/#what-are-your-experiences-with-the-model-context-protocol-mcp","title":"What are your experiences with the Model Context Protocol (MCP)?","text":"<p>MCP (Model Context Protocol) is becoming increasingly important as it allows different AI systems to connect with each other:</p> <ol> <li> <p>Key benefits:</p> </li> <li> <p>Standardizes integrations between AI systems</p> </li> <li>Reduces boilerplate code when connecting to different services</li> <li> <p>Allows models to access data and functionality they wouldn't normally have permission to use</p> </li> <li> <p>Practical examples:</p> </li> <li> <p>Image generation servers in Cursor for creating assets while building applications</p> </li> <li>Servers that connect to network logs for debugging web applications</li> <li>Connectors to production databases that help models understand schemas and write SQL</li> <li> <p>Automation tools that write conversation notes directly to Notion or other note-taking systems</p> </li> <li> <p>Comparison to function calling:</p> </li> <li> <p>When you own all the code, function calling may be simpler</p> </li> <li>MCP becomes valuable when connecting separate systems with different permission models</li> <li>Provides a standardized way to expose capabilities across different AI platforms</li> </ol> <p>The protocol is still evolving but shows promise for creating more powerful AI systems by composing specialized components. Some implementations like Claude 3.7 with Claude Code demonstrate how MCP can enable better context management and more sophisticated agent capabilities.</p>"},{"location":"office-hours/cohort2/week4-summary/#key-takeaways-and-additional-resources","title":"Key Takeaways and Additional Resources","text":""},{"location":"office-hours/cohort2/week4-summary/#key-takeaways","title":"Key Takeaways:","text":"<ul> <li>The goal of segmentation is to understand customer needs and determine what tools to build next</li> <li>Chunking strategy (800 tokens, 50% overlap) is rarely the bottleneck - focus on contextual retrieval instead</li> <li>For topic modeling, start with BERTTopic defaults and then use thinking models to better understand clusters</li> <li>Spend more compute upfront to improve data quality - particularly for high-value documents</li> <li>Write parallelized code to dramatically speed up experimentation</li> <li>For multilingual RAG, test whether translation improves performance enough to justify the added complexity</li> <li>Consider transforming image content to text summaries rather than using pure multimodal embeddings</li> <li>MCP is becoming increasingly important for connecting different AI systems together</li> <li>Use structured JSON consistently in few-shot examples rather than plain text</li> <li>For slide creation, AI tools can generate both content and formatting in vector-based formats</li> <li>For long documents, consider hierarchical retrieval, graph-based approaches, hybrid sparse-dense retrieval, learning to rewrite, and recursive summarization</li> <li>For messy knowledge bases, implement pre-processing pipeline, metadata extraction and filtering, query classification, progressive disclosure, and dynamic presentation</li> <li>For DAGs versus agentic approaches, use DAGs when the workflow has clear, sequential steps, and use agentic approaches when the problem space is exploratory</li> <li>For negative examples, use hard negative mining, top-K analysis, and controlled random sampling</li> <li>For response time, implement progressive rendering, caching, response type classification, contextual snippet generation, and parallel processing</li> </ul>"},{"location":"office-hours/cohort2/week4-summary/#additional-resources","title":"Additional Resources:","text":"<ul> <li>BERTTopic: https://maartengr.github.io/BERTopic/index.html</li> <li>MCP Agent: https://github.com/lastmile-ai/mcp-agent</li> <li>Claude Code: https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview</li> <li>RepoPrompt: https://repoprompt.com/</li> <li>Aider CLI coding tool: https://aider.chat/</li> <li>Lovable for no-code app generation with Supabase integration</li> <li>Cursor and Windsurf for AI-assisted coding environments</li> </ul> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"office-hours/cohort2/week5-summary/","title":"RAG Office Hours Q&amp;A Summary - Week 5","text":""},{"location":"office-hours/cohort2/week5-summary/#how-should-we-handle-excel-files-with-multiple-sheets-and-tables","title":"How should we handle Excel files with multiple sheets and tables?","text":"<p>Handling Excel files with multiple sheets and tables is challenging, and few companies have solved this problem well. Experience from companies like Zapier shows that connecting Excel spreadsheets to automation tools requires many controls to work properly.</p> <p>The recommended approach is implementing checks on uploads to ensure files meet certain criteria. These checks might verify if an Excel file contains a single table, stays within size limits, or contains a specific number of tables. For simpler Excel files with single tables, implementing validation and processing works well, but for more complex files with multiple sheets and scattered tables, exporting to PDF might allow for easier parsing.</p> <p>It's important to segment data and build specific extractors based on the data's structure. Single-table files can go through a dedicated single-table pipeline, while multi-table files might work better through a PDF parsing pipeline.</p> <p>The most practical advice is to focus on simpler problems first and reject more complex ones until better solutions are developed. Solving for the simpler 40% of use cases while avoiding the most complex scenarios can be an effective strategy. Exporting Excel files as CSVs might also provide better compatibility with processing tools in many situations.</p>"},{"location":"office-hours/cohort2/week5-summary/#what-tools-are-recommended-for-sql-generation","title":"What tools are recommended for SQL generation?","text":"<p>Claude Sonnet has proven effective for generating SQL queries. Success depends heavily on whether the system can retrieve the correct tables and CREATE statements.</p> <p>The key to successful SQL generation is having good descriptions and CREATE statements for the tables, as well as ensuring that embedding and search capabilities properly identify the right tables when needed.</p> <p>A recommended approach from Timescale involves first retrieving relevant tables, then retrieving pre-existing, approved SQL snippets. When both the correct tables and appropriate SQL patterns are in context, the generation process becomes significantly more reliable.</p> <p>The complexity increases with many tables and columns, but focusing on retrieving the correct tables first, then incorporating approved SQL snippets to guide the generation process creates a two-step approach that significantly reduces errors in SQL generation.</p>"},{"location":"office-hours/cohort2/week5-summary/#what-is-the-linear-adapter-for-embeddings-and-how-does-it-work","title":"What is the Linear Adapter for embeddings and how does it work?","text":"<p>Linear adapters provide a cost-effective way to fine-tune embeddings. An embedding model takes data and produces a vector, with the dot product of two vectors indicating how similar they are. A linear adapter learns how to \"rotate\" these vectors slightly to better align with specific queries.</p> <p>The approach is very economical - if a vector has 500 dimensions, the linear adapter is just a 500 by 500 matrix that multiplies the original vector. This allows for significant improvements in embedding quality with minimal computational cost.</p> <p>Linear adapters can be compared to LoRA (Low-Rank Adaptation), but with key differences. LoRA works between many layers of a neural network, while a linear adapter works only at the end. Additionally, linear adapters can be applied to pre-trained embeddings like those from OpenAI without needing access to the original model weights.</p> <p>This approach enables domain-specific adaptations - for example, creating different adapters for marketing versus sales questions, or specialized adapters for legal, marketing, or tax information. The cost benefit is significant - training a linear adapter typically costs around $12 and can be done quickly, making it much more accessible than full model fine-tuning.</p> <p>Implementation uses the standard fine-tuning process with triplets (question, positive example, negative example), but specifically changes the embedding function for the query. This rotation of vectors into more effective alignments can significantly improve retrieval performance for domain-specific applications.</p>"},{"location":"office-hours/cohort2/week5-summary/#how-does-partitioning-work-in-retrieval-systems","title":"How does partitioning work in retrieval systems?","text":"<p>Partitioning in retrieval systems refers to how data is organized and segmented, rather than being about individual users. In applications like Cursor, a \"user\" might represent a documentation page. When working with code libraries like Requests, there might be a dedicated Requests index that multiple users access, but the partition is organized around the library package, documentation URL, or codebase.</p> <p>Similarly, in applications like Notion, a \"user\" isn't an individual email account but represents an entire workspace. This means a company's complete Notion workspace would exist in a single index.</p> <p>An interesting approach is partition-specific fine-tuning, which involves using different models to embed questions versus text chunks. Standard fine-tuning uses one model for both the question and the text chunk, but it's possible to fine-tune only one side of that equation. This might involve using the same model to embed all text chunks but having a different model to embed the question.</p> <p>This technique proves particularly valuable in e-commerce settings. One embedding model might identify products that are similar to each other (creating a \"similar products\" carousel), while a different embedding model could identify complementary products (for \"frequently bought together\" recommendations). Both embeddings operate on the same product data but serve different retrieval purposes.</p>"},{"location":"office-hours/cohort2/week5-summary/#what-is-the-model-context-protocol-mcp-and-how-does-it-differ-from-regular-apis","title":"What is the Model Context Protocol (MCP) and how does it differ from regular APIs?","text":"<p>The Model Context Protocol (MCP) functions like a USB-C connector for AI systems - it's about standardization rather than specific functionality. While the devices that connect may vary, the connection method itself is standardized.</p> <p>The key advantage of MCP over traditional APIs is the separation of client from backend. With a REST API, developers must write specific code to interact with each API, and each application needs to implement these integrations. MCP creates a standardized way for different systems to connect without custom code for each integration.</p> <p>Consider an enterprise example where different teams might focus on different functionality: one team might work on email search while another builds CRM search tools. Both teams can develop their MCP clients, and the chatbot developers can easily integrate both without writing extensive custom code for each system.</p> <p>This standardization means tools built with MCP work across multiple platforms without requiring custom integration code. A tool that works in one MCP-compatible environment (like Cursor) will work in others (like Claude desktop app) with minimal additional effort.</p> <p>Beyond just function execution, MCP also supports resources and prompts. The MCP developer can provide not just functionality but also the prompts needed to use that functionality effectively. This means client developers don't need to write their own prompts for common operations like summarization or action item extraction.</p> <p>This approach significantly reduces the need for glue code and allows developers to build applications without having to own all the client code, making integrations between different AI systems much more seamless.</p>"},{"location":"office-hours/cohort2/week5-summary/#what-ai-tools-are-recommended-for-daily-work","title":"What AI tools are recommended for daily work?","text":"<p>Claude Code stands out among AI coding tools, particularly for its high-quality prompts. What makes it exceptional is how it handles context and continuity - when asked to write blog posts, it will first analyze existing posts to create a style guide, then reference that guide for every new post it writes.</p> <p>This attention to existing style and consistency makes it particularly effective for content creation tasks. Despite higher costs compared to some alternatives (potentially $100+ per weekend of heavy use), many users find the value justifies the expense, with some noting they would willingly pay $200 monthly for the service.</p> <p>For report generation specifically, there's significant value in tools that provide templated outputs. Ideally, a report generation tool would allow users to standardize formats across different reports - ensuring all market analysis reports follow the same structure, or that candidate evaluation reports maintain consistent formatting rather than varying significantly in format and depth.</p> <p>This points to a broader trend in AI tool development - the need for tools that not only generate content but do so in consistent, predictable formats that align with existing workflows and style guidelines.</p>"},{"location":"office-hours/cohort2/week5-summary/#what-approaches-are-being-used-for-multimodal-applications","title":"What approaches are being used for multimodal applications?","text":"<p>Multimodal applications combining vision and language models are expanding into specialized domains. One example shared during office hours involved food image analysis, where a system extracts structured data from food photographs.</p> <p>These systems can identify cuisine type, restaurant information, nutritional content, and dietary characteristics like whether a food item is vegan. While acknowledging practical limitations (\"there's a limit to what you can effectively do\"), early experiments show promising results in extracting valuable information from visual food content.</p> <p>This example demonstrates how multimodal AI applications are moving beyond basic image recognition to extract detailed, structured information from visual content. The integration of vision models with language models allows systems to interpret and categorize visual information in ways that support practical applications like dietary tracking or restaurant recommendations.</p>"},{"location":"office-hours/cohort2/week5-summary/#what-emerging-trends-should-we-be-aware-of","title":"What emerging trends should we be aware of?","text":"<p>Report generation emerges as a particularly important trend in AI applications. The ability to automatically generate structured reports from unstructured data represents significant economic value for organizations.</p> <p>Structured outputs and templates are increasingly valuable, especially for business use cases where standardized formats are essential. The ideal scenario allows for consistency in outputs - ensuring all reports of a specific type follow the same structure rather than varying significantly in format and organization.</p> <p>Several organizations are developing report generation capabilities for internal use, with teams requiring standardized reports on a regular basis. This trend spans multiple industries, with financial due diligence being one area where automated report generation from multiple PDF sources shows particular promise.</p> <p>The growing importance of fine-tuning approaches for handling data from multiple teams or domains also represents a significant trend. As organizations deploy AI systems across different business units, finding ways to effectively fine-tune models while maintaining performance becomes crucial.</p> <p>Report generation capabilities demonstrate how AI can move beyond simple question answering to create significant economic value through structured information synthesis - transforming unstructured data into formatted reports that follow organizational templates and standards.</p>"},{"location":"office-hours/cohort2/week5-summary/#how-should-we-handle-retrieval-across-multiple-queries-in-a-conversation","title":"How should we handle retrieval across multiple queries in a conversation?","text":"<p>When handling retrieval across multiple queries within the same conversation, the simplest approach is often the most effective. Using function calls for retrieval is recommended, where each call retrieves text chunks and includes information in XML format specifying the context and the question.</p> <p>A key consideration is whether to keep retrieved context from previous queries in the message history for subsequent queries. This requires careful balancing, as including all previous context can consume tokens quickly.</p> <p>The recommended practice is to prompt the retrieval system to always generate fully specified queries. For example, if the first question is \"Where does Jason live?\" and the follow-up is \"What's the population of that city?\", the retrieval system should be prompted to expand the second query to \"What is the population of New York City?\" This approach can be implemented through few-shot examples that demonstrate how to handle conversational context.</p> <p>This strategy works because the model has access to both the previous question and answer in its context, allowing it to formulate complete, self-contained queries even when the user's input is ambiguous or relies on previous context.</p> <p>An additional benefit of this approach is the ability to generate follow-up questions based on retrieved content. For instance, if a text chunk mentions that \"Jason lived in different places when he was younger versus when he was older,\" the system can suggest follow-up questions like \"Where did Jason live when he was younger?\" This not only improves information discovery but also demonstrates to users that there are more interesting questions they could ask about the topic.</p>"},{"location":"office-hours/cohort2/week5-summary/#what-innovations-are-happening-in-memory-and-context-management-for-agents","title":"What innovations are happening in memory and context management for agents?","text":"<p>Recent innovations in memory and context management for agents focus on creating more dynamic and self-improving systems. Rather than simply saving memories at the end of a conversation, newer approaches incorporate real-time memory creation and utilization during interactions.</p> <p>Frameworks like Letta are incorporating self-editing capabilities alongside memory management. This integration allows agents to refactor their understanding and approach during a conversation rather than only learning from interactions after they conclude.</p> <p>Implementation of these advances requires significant infrastructure changes, as memory layers affect prompt construction and the overall flow of agent interactions. The approach involves creating memories as the conversation progresses, which in turn influences the prompts used in subsequent exchanges.</p> <p>With newer models like Claude 3.7 Sonnet, there's a shift in how tools are used by agents. Similar to the adaptation period seen with Claude Opus or GPT-4, these models require different prompting patterns to effectively utilize tools. Studying how systems like Cloud Code implement their tool use can provide valuable insights for optimizing agent performance with newer models.</p>"},{"location":"office-hours/cohort2/week5-summary/#what-can-we-learn-from-examining-cloud-codes-approach-to-prompts-and-tools","title":"What can we learn from examining Cloud Code's approach to prompts and tools?","text":"<p>Cloud Code's approach to prompts and tools offers valuable insights for designing effective AI systems. Analysis of the Cloud Code source (extracted from their minified code) reveals highly detailed and carefully structured prompts for various tools.</p> <p>Cloud Code implements a robust workflow where tools are designed to work together cohesively. Each tool's prompt contains specific instructions about how to use other tools first, creating a well-defined sequence of operations. For example, tools may include instructions to check for uniqueness before proceeding or to use specific validation approaches.</p> <p>The prompts are remarkably detailed, with extensive instructions for common operations. For instance, batch tools contain comprehensive guidelines just for creating pull requests. This level of specificity helps ensure consistent and reliable performance.</p> <p>Another notable aspect is Cloud Code's implementation of specialized tools like a notebook tool, showing how diverse functionality can be incorporated into a unified system. The prompts demonstrate that Claude is capable of working with numerous tools simultaneously when the system is properly designed.</p> <p>This examination highlights the importance of thoughtful prompt engineering and tool design in building effective AI systems. By providing clear, detailed instructions and establishing well-defined workflows between tools, systems can achieve more reliable and sophisticated functionality.</p>"},{"location":"office-hours/cohort2/week5-summary/#how-can-we-create-automated-evaluation-reports-and-insights-for-rag-systems","title":"How can we create automated evaluation reports and insights for RAG systems?","text":"<p>Automated evaluation reports can significantly enhance RAG system development by providing structured insights and clear next steps. A comprehensive approach involves building a pipeline that:</p> <ol> <li>Takes validation datasets and runs them through the RAG system</li> <li>Computes various metrics (correctness, citation accuracy, URL validity)</li> <li>Generates visualizations segmented by topic, question type, and other dimensions</li> <li>Uses LLMs to analyze metrics and provide insights</li> <li>Creates recommendations for system improvements</li> </ol> <p>The reports can include both detailed analyses (15+ pages) and condensed slides for easier consumption in meetings. Key components include:</p> <ul> <li>Methodology explanations for stakeholders who may not be familiar with technical details</li> <li>System architecture diagrams</li> <li>Performance visualizations broken down by different segments</li> <li>Statistical analysis of which topics or question types perform well or poorly</li> <li>LLM-generated insights that explain patterns in the data</li> <li>Specific recommendations tied to the codebase</li> </ul> <p>This process can effectively \"close the loop\" in the flywheel of development by identifying specific areas for improvement. For example, the system might recommend improving schema handling, enhancing retrieval tools, or adding more data for underrepresented topics.</p> <p>The insights generated by LLMs analyzing the metrics can often align well with developer intuitions, but having them formally documented provides better clarity for prioritization and communication with stakeholders. These insights can be directly translated into development tickets, creating a streamlined workflow from evaluation to implementation.</p> <p>The ultimate goal is to use these insights to guide the next iteration of development, run the evaluation again, and continue improving through this structured feedback loop.</p>"},{"location":"office-hours/cohort2/week5-summary/#what-strategies-help-maintain-context-when-working-with-ai-coding-assistants","title":"What strategies help maintain context when working with AI coding assistants?","text":"<p>When working with AI coding assistants like Cursor, maintaining context throughout a development session can be challenging. Several effective strategies have emerged from practical experience:</p> <p>Creating and maintaining to-do lists within project documentation provides an effective way to preserve context. By instructing the AI to update the to-do list after completing each task, you ensure the progress remains in context for subsequent interactions. This approach creates a record of what has been accomplished and what remains to be done.</p> <p>Templates within documentation files help maintain consistent structure across generated content. For example, having templates for different documentation sections ensures the AI follows established patterns when creating new content. This approach allows you to simply prompt the AI to \"make more concept templates that generally look like this,\" maintaining consistency through visual examples rather than complex rules.</p> <p>For more structured workflows, some developers create development plans using models like Claude Opus, which provide a roadmap the AI can follow. This helps prevent the AI from getting lost during implementation or going down unproductive paths.</p> <p>Many developers find that keeping AI agents away from certain code areas (particularly tests) helps maintain structure. This can be accomplished either through explicit instructions or by adding files to the cursor.ignore configuration, which prevents them from being indexed while still allowing the AI to run commands like pytest.</p> <p>Follow-up prompts at the end of interactions help maintain momentum. By asking what else needs to be done or what the next steps are, you encourage the AI to reference the to-do list and continue working on remaining tasks, creating a more cohesive development experience.</p> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"office-hours/cohort2/week6-summary/","title":"RAG Office Hours Q&amp;A Summary - Week 6","text":""},{"location":"office-hours/cohort2/week6-summary/#what-is-deep-research-and-how-does-it-relate-to-rag","title":"What is Deep Research and how does it relate to RAG?","text":"<p>Deep Research is essentially a model fine-tuned for tool use that leverages RAG and iteration in a loop to produce reports. It can be viewed as RAG with solid data sources and strong reasoning capabilities on top. Deep Research is distinct from standard RAG applications because it typically produces more comprehensive outputs like reports rather than just answering specific questions.</p> <p>While Deep Research generates reports that might differ in structure between runs, more advanced approaches like those used by Vantage aim to create standardized, deterministic reports. The ideal approach is to define a specific structure for reports, particularly when you know exactly what questions need to be answered for your domain.</p> <p>There's significant economic value in creating structured reports rather than just answering ad-hoc questions. For example, instead of building a system that allows recruiters to query interview transcripts individually, creating a standardized hiring report that distills key information from all interviews provides greater business value. This approach helps stakeholders make better decisions rather than just saving time on information retrieval.</p> <p>The techniques taught in the RAG course are directly applicable to building Deep Research-style systems, particularly when focused on specific domains rather than general-purpose research.</p>"},{"location":"office-hours/cohort2/week6-summary/#should-we-use-long-context-windows-or-rag-for-complex-questions","title":"Should we use long context windows or RAG for complex questions?","text":"<p>Long context windows should be leveraged first when possible, as they generally produce better results than relying solely on chunk retrieval. The ideal approach is often to use document-level retrieval rather than chunk-level retrieval when working with long context models.</p> <p>When faced with specific tasks that require processing lengthy documents (like generating pricing emails based on policy documents), consider creating dedicated tools that use the full context window rather than breaking documents into chunks. This can be implemented as a function that uses a very long prompt containing all relevant policy documents.</p> <p>This approach simplifies the retrieval problem from needing good chunk retrieval to just needing good document retrieval, which can be accomplished with simpler techniques like full-text search. The decision becomes not about whether you have good chunk retrieval but rather if you have good document retrieval capabilities.</p> <p>As models' context windows continue to expand, this approach becomes increasingly viable for more use cases, potentially reducing the complexity of some RAG implementations.</p>"},{"location":"office-hours/cohort2/week6-summary/#how-important-is-human-labeled-data-for-rag-systems","title":"How important is human-labeled data for RAG systems?","text":"<p>Human-labeled data remains essential for building high-quality RAG systems, though many teams underestimate its importance. Teams that are reluctant to invest in data labeling often struggle to achieve meaningful performance improvements.</p> <p>From a consulting perspective, one effective approach is to demonstrate the impact of data quality through experimentation. Show how model performance improves with synthetic data, then demonstrate how it plateaus. This creates a data-driven argument that once synthetic data reaches diminishing returns, real human-labeled data becomes necessary for further improvement.</p> <p>For high-value applications, the investment in human labeling is justified. Companies like Vantage, which produces due diligence reports for investment decisions, dedicate staff to labeling and evaluating the quality of question-answer pairs. This reflects the understanding that without at least one human producing high-quality data, systems will struggle to achieve meaningful differentiation in output quality.</p> <p>The economic argument is compelling: if a model is helping make decisions that involve millions or billions of dollars (as in investment due diligence or hiring), the cost of high-quality human labeling is minimal compared to the value it creates.</p>"},{"location":"office-hours/cohort2/week6-summary/#how-do-you-handle-model-evaluation-when-generating-reports-rather-than-simple-answers","title":"How do you handle model evaluation when generating reports rather than simple answers?","text":"<p>Evaluating report generation presents different challenges than evaluating direct question answering. While individual components can be measured with standard metrics, evaluating complete reports often requires human judgment against a defined rubric.</p> <p>Language models can perform reasonably well as judges against a rubric, but they primarily assess whether all required elements are present rather than providing nuanced feedback on quality or analysis. Human evaluation remains important for assessing whether the analysis itself is valuable and meets business needs.</p> <p>This challenge mirrors broader evaluation difficulties in the generative AI space, where outputs become more complex and subjective. The solution often involves creating clear rubrics for what constitutes a good report in your specific domain, then combining automated checks with strategic human review.</p> <p>Teams should focus on defining what makes a report valuable to their specific users rather than pursuing generic quality metrics. This might involve understanding whether users need comprehensive information, specific recommendations, or particular formatting that helps with decision-making.</p>"},{"location":"office-hours/cohort2/week6-summary/#what-broader-trends-are-emerging-in-ai-consulting","title":"What broader trends are emerging in AI consulting?","text":"<p>The AI consulting landscape is evolving rapidly, with several key trends emerging:</p> <ol> <li> <p>Shift from implementation to experimentation: More consulting work now involves helping teams design and run effective experiments rather than just implementing specific techniques. This includes teaching scientific methods, hypothesis formation, and systematic testing.</p> </li> <li> <p>Focus on data quality over algorithms: Successful consultants emphasize improving data quality and data collection processes rather than just applying newer algorithms. Many organizations still lack basic data infrastructure for effective AI work.</p> </li> <li> <p>Organizational change management: A significant portion of AI consulting now involves helping teams adapt to new workflows and develop the right skills. This includes teaching software engineers to approach problems more like data scientists.</p> </li> <li> <p>Economic value alignment: The most successful AI implementations focus on creating decision-making value rather than just time savings. Products that help customers make better decisions (like hiring recommendations or investment analysis) can command higher prices than those that merely save time.</p> </li> </ol> <p>The role of consultants remains valuable even as AI tools become more accessible because they bring expertise in experiment design, data quality improvement, and aligning AI capabilities with business value.</p>"},{"location":"office-hours/cohort2/week6-summary/#how-will-ai-impact-the-consulting-industry-itself","title":"How will AI impact the consulting industry itself?","text":"<p>The consulting industry will continue to evolve alongside AI advancements, but consultants who adapt will remain valuable. The core value of consulting is increasingly about bringing expertise in scientific methods, data analysis, and business process transformation rather than simply implementing technology.</p> <p>Several shifts are occurring in the consulting space:</p> <ol> <li> <p>Distribution becomes more important: Consultants who can effectively share their insights through content creation (blogs, videos, courses) will have advantages in attracting clients.</p> </li> <li> <p>Process expertise over pure technical knowledge: As technical implementation becomes easier with AI tools, consultants who understand how to change organizational processes and workflows become more valuable.</p> </li> <li> <p>Organization and workflow design: Consultants who can help structure workflows and processes that leverage AI effectively will remain in demand, even as some technical implementation work becomes automated.</p> </li> <li> <p>Connection to economic value: Consultants who can clearly connect AI capabilities to business value and ROI will continue to thrive, focusing less on technology and more on business outcomes.</p> </li> </ol> <p>While AI will automate some aspects of consulting work, it simultaneously creates new opportunities for consultants who can help organizations navigate the complex landscape of AI implementation and business transformation.</p>"},{"location":"office-hours/cohort2/week6-summary/#how-should-we-handle-training-data-contamination-from-ai-generated-content","title":"How should we handle training data contamination from AI-generated content?","text":"<p>As more content on the internet becomes AI-generated, concerns about training data contamination and potential \"model collapse\" are valid but may be overstated for several reasons:</p> <ol> <li> <p>Unexplored modalities: Even if text data becomes saturated with AI-generated content, there are many other modalities (video, computer interaction data, etc.) that remain largely untapped for training.</p> </li> <li> <p>Mode covering vs. mode collapse: Advanced research at organizations like OpenAI focuses on developing models that can identify multiple solution modes rather than collapsing to the lowest-resistance path. Models that are \"mode covering\" can maintain diversity in their outputs even when trained on some low-quality data.</p> </li> <li> <p>Real-world data sources: For many specialized applications, the most valuable data isn't from the public internet but from proprietary sources or human interaction with systems. This data remains largely uncontaminated.</p> </li> <li> <p>Post-training refinement: Much of the current improvement in AI models comes from post-training techniques like RLHF rather than pre-training alone. This allows models to improve based on high-quality human feedback even if pre-training data becomes noisier.</p> </li> </ol> <p>OpenAI researchers reportedly maintain confidence that there's still significant high-quality data available, suggesting that concerns about running out of training data may be premature.</p>"},{"location":"office-hours/cohort2/week6-summary/#what-are-emerging-trends-in-ai-tool-development","title":"What are emerging trends in AI tool development?","text":"<p>Several noteworthy trends are emerging in AI tool development:</p> <ol> <li> <p>Advanced agents like Manus: New tools like Manus are providing powerful capabilities by combining foundation models (like Claude Sonnet) with extensive tooling. While details are limited, these systems represent a new generation of AI assistants with enhanced capabilities.</p> </li> <li> <p>Cloud Code improvements: Cloud Code has shown impressive performance for specific tasks, sometimes outperforming tools like Cursor for certain types of development work. However, success often depends on the user's expertise in the domain they're working in - users still need significant knowledge to effectively guide AI tools.</p> </li> <li> <p>Context management evolution: Newer AI tools are improving how they manage context over time, creating better continuity between sessions and maintaining understanding of project requirements.</p> </li> <li> <p>Focus on expert augmentation: The most successful AI tools are those that augment human expertise rather than trying to replace it entirely. Tools work best when users have clear goals and domain knowledge, with the AI handling implementation details.</p> </li> </ol> <p>Despite significant advances in AI capabilities, domain expertise remains crucial for effective use of these tools. The relationship between user expertise and AI capabilities creates a complex dynamic where both need to evolve together for optimal results.</p>"},{"location":"office-hours/cohort2/week6-summary/#how-will-data-collection-evolve-for-ai-applications","title":"How will data collection evolve for AI applications?","text":"<p>Data collection for AI is shifting in several important ways:</p> <ol> <li> <p>Purposeful logging: Companies are moving beyond debugging-focused logging to capturing data specifically designed for model training. This requires engineers to think about what signals might be useful for future models rather than just for troubleshooting.</p> </li> <li> <p>Structured feedback collection: More companies are implementing systematic ways to collect user feedback and interactions, recognizing these signals as valuable training data rather than just product metrics.</p> </li> <li> <p>Data quality over quantity: There's growing recognition that having smaller amounts of high-quality, well-labeled data is often more valuable than vast amounts of noisy data.</p> </li> <li> <p>Economic value alignment: Organizations are increasingly evaluating what data to collect based on economic value rather than technical feasibility alone. This means focusing data collection efforts on areas where improved model performance translates directly to business outcomes.</p> </li> </ol> <p>Many companies still struggle with basic data collection infrastructure, often lacking the systems needed to capture useful signals from user interactions. Building these foundations remains a critical first step before more advanced AI applications can be developed.</p>"},{"location":"office-hours/cohort2/week6-summary/#how-should-we-think-about-distribution-and-economic-viability-in-ai-products","title":"How should we think about distribution and economic viability in AI products?","text":"<p>The most successful AI applications focus on creating decision-making value rather than just time savings. This fundamental shift in value proposition affects pricing, distribution, and product design:</p> <ol> <li> <p>Value-based pricing: Products that help customers make better decisions (like hiring recommendations or investment analysis) can command higher prices than those that merely save time. For example, recruiters charge 25% of a hire's salary not because they save time but because they help make better hiring decisions.</p> </li> <li> <p>Structured outputs: There's increasing value in AI systems that produce standardized, structured outputs (like reports) rather than just answering ad-hoc questions. This creates more consistent value and makes the outputs more directly usable in business processes.</p> </li> <li> <p>Domain specialization: Applications focused on specific domains with clear economic value (financial analysis, legal research, specialized technical fields) can support higher pricing than general-purpose AI tools.</p> </li> <li> <p>Content as marketing: For many AI consultants and product builders, content creation (blog posts, courses, etc.) derived from their expertise serves as efficient marketing. This \"sawdust\" from their core work helps attract clients and build credibility.</p> </li> </ol> <p>The most economically viable AI products are those that align directly with high-value business decisions rather than just providing generalized capabilities or incremental efficiency improvements.</p>"},{"location":"office-hours/cohort2/week6-summary/#what-recommendations-do-you-have-for-structuring-the-course-and-its-content","title":"What recommendations do you have for structuring the course and its content?","text":"<p>Several suggestions emerged for improving the course structure and content:</p> <ol> <li> <p>Better content organization: Ensure core videos and tutorials are prominently featured in the main menu rather than buried under multiple links. This would improve discoverability and help students stay on track.</p> </li> <li> <p>Standardized office hours format: Implement a consistent format for office hours, with the first 10-20 minutes dedicated to setting context about the week's material before moving to questions. This helps orient participants who may be joining different sessions.</p> </li> <li> <p>Email reminders with direct links: Send regular emails with direct links to the week's core videos and tutorials to ensure students know exactly what to watch and when.</p> </li> <li> <p>Calendar integration: Consider adding placeholder calendar events for self-study time to help students schedule time to watch asynchronous content.</p> </li> <li> <p>Expanded coverage of enterprise tools: While OpenAI tools were featured prominently for practical reasons, more coverage of enterprise platforms (Azure, AWS, Google Vertex) would be valuable for many students working in corporate environments.</p> </li> <li> <p>Open-source alternatives: Include more examples using open-source tools alongside commercial offerings, especially for cases where data residency requirements make cloud services challenging.</p> </li> </ol> <p>The feedback emphasized that while the course content was valuable, improvements to structure and discoverability would help students manage the significant amount of material more effectively.</p>"},{"location":"office-hours/cohort2/week6-summary/#how-can-we-use-slack-effectively-after-the-course-ends","title":"How can we use Slack effectively after the course ends?","text":"<p>The Slack channel will remain available as an ongoing resource for students after the course concludes. Several recommendations for effective use include:</p> <ol> <li> <p>Specific questions get better answers: When asking questions in Slack, provide specific details about your use case, what you've already tried, and exactly what you're trying to accomplish. This allows for more targeted and helpful responses.</p> </li> <li> <p>Share real-world applications: Sharing how you're applying concepts from the course to real projects provides valuable context for others and creates learning opportunities for everyone.</p> </li> <li> <p>Ongoing community learning: The Slack channel offers an opportunity to continue learning from peers who are implementing RAG systems across different industries and use cases.</p> </li> <li> <p>Access to course materials: All course materials will remain accessible through Maven, and the Slack community provides a way to discuss those materials as you continue to review them.</p> </li> </ol> <p>The instructor emphasized that students will get as much value from the community as they put in through specific, thoughtful questions and sharing their own experiences.</p>"},{"location":"office-hours/cohort2/week6-summary/#what-future-trends-do-you-anticipate-in-ai-development","title":"What future trends do you anticipate in AI development?","text":"<p>Several key trends are likely to shape AI development in the near future:</p> <ol> <li> <p>Structured output generation: The ability to generate consistent, structured reports and analyses will become increasingly valuable, particularly for business applications where standardized formats are essential.</p> </li> <li> <p>Report generation workflows: Building on the structured output trend, more sophisticated workflows for generating comprehensive reports from multiple data sources will become mainstream.</p> </li> <li> <p>Scientific approach to AI development: Organizations that adopt rigorous experimentation, hypothesis testing, and data analysis will pull ahead of those that simply implement the latest techniques without careful evaluation.</p> </li> <li> <p>Economic alignment: AI applications that directly support high-value decision making will see stronger adoption and commercial success compared to those that merely provide incremental efficiency improvements.</p> </li> <li> <p>Integration of multiple modalities: While still evolving, the ability to reason across text, images, video, and interactive data will create new application possibilities, though many practical applications will still focus on extracting structured information from these inputs rather than general understanding.</p> </li> </ol> <p>The most successful organizations will be those that develop systematic processes for continuous improvement of their AI systems rather than chasing the latest models or techniques without a clear evaluation framework.</p>"},{"location":"office-hours/cohort2/week6-summary/#how-do-you-balance-providing-generic-ai-solutions-versus-domain-specific-implementations","title":"How do you balance providing generic AI solutions versus domain-specific implementations?","text":"<p>The balance between generic AI solutions and domain-specific implementations depends on both economic factors and technical feasibility:</p> <ol> <li> <p>Start with domain specificity: Focusing on specific domains allows for more valuable outputs, better evaluation, and clearer value propositions. This approach makes it easier to create systems that provide significant value.</p> </li> <li> <p>Specialize by intent rather than content: Even within a domain, segmenting by user intent (what they're trying to accomplish) rather than just content type creates more focused and effective solutions.</p> </li> <li> <p>Economic viability: Domain-specific solutions can often command higher prices because they solve specific high-value problems rather than providing general capabilities. This makes them more economically viable despite smaller potential market size.</p> </li> <li> <p>Technical feasibility: Creating effective general-purpose AI systems remains technically challenging, while domain-specific implementations can achieve high performance by narrowing the scope of what they need to handle.</p> </li> </ol> <p>For most organizations building AI applications, starting with a specific domain and set of well-defined use cases is likely to produce better results than attempting to build general-purpose systems. This focus allows for better data collection, more effective evaluation, and clearer alignment with business value.</p>"},{"location":"office-hours/cohort2/week6-summary/#key-takeaways-and-additional-resources","title":"Key Takeaways and Additional Resources","text":""},{"location":"office-hours/cohort2/week6-summary/#key-takeaways","title":"Key Takeaways:","text":"<ul> <li>Deep Research can be understood as RAG with high-quality data sources and strong reasoning capabilities</li> <li>Structured reports often provide more business value than ad-hoc question answering</li> <li>Long context windows should be leveraged first when possible before falling back to chunking</li> <li>Human-labeled data remains essential for high-quality RAG systems, especially as systems reach the limits of improvement from synthetic data</li> <li>Evaluating report generation often requires human judgment against defined rubrics</li> <li>AI consulting is shifting toward experimental design and process transformation rather than just implementation</li> <li>Data collection is evolving to focus more on purposeful logging and structured feedback collection</li> <li>The most economically viable AI products align with high-value business decisions rather than just providing efficiency improvements</li> <li>Content organization and standardized formats for course materials can significantly improve the learning experience</li> <li>Domain-specific AI implementations typically provide better economic and technical outcomes than general-purpose solutions</li> </ul>"},{"location":"office-hours/cohort2/week6-summary/#additional-resources","title":"Additional Resources:","text":"<ul> <li>The Future of RAG - Jason Liu's blog post on where RAG is heading</li> <li>Deep Research - OpenAI's introduction to Deep Research</li> <li>Vantage - Company mentioned as an example of advanced report generation</li> <li>Claude 3.7 Sonnet - Latest model referenced in discussions</li> <li>Cloud Code - AI coding tool discussed in the sessions</li> <li>Manus - Emerging AI agent mentioned in the discussions</li> </ul> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"office-hours/cohort3/week-1-1/","title":"Week 1 - Office Hour 1 (May 20)","text":"<p>Study Notes:</p> <p>I hosted an office hours session focused on RAG implementation challenges, precision-recall tradeoffs, and practical business applications of AI systems. Here are my insights on evaluating RAG systems, understanding model sensitivity to context quality, and identifying high-value business opportunities through data analysis rather than just technical improvements.</p>"},{"location":"office-hours/cohort3/week-1-1/#how-should-we-understand-precision-in-the-context-of-llms-and-rag","title":"How should we understand precision in the context of LLMs and RAG?","text":"<p>Precision has become increasingly important as most people are comfortable adding substantial information to the context window. The key question is how sensitive these models are to ambiguous or irrelevant information.</p> <p>Most current models are very good at recall because they're optimized for \"needle in the haystack\" tests. Due to the attention mechanism with sliding windows, they can typically catch important information even when it's buried among other content. However, the sensitivity to irrelevant information is less well-studied.</p> <p>As we increase the amount of data in the context, we need to understand what precision looks like and how it correlates with factuality or answer quality. Earlier models like GPT-3.5 were quite sensitive to long context with low precision - if you gave them too much information, they would \"overthink\" because of the volume of content to process.</p> <p>This is why it's valuable to experiment with different retrieval settings: \"For the top 10 documents, this is my precision and recall; for my top 25 documents, this is my precision and recall. If my recall is not increasing as a function of K (the number of documents), that's where I decide to set a threshold.\"</p> <p>Some people set thresholds based on re-ranker scores, but that can be dangerous since these scores aren't true probabilities. You can't just set 0.5 as a universal threshold - you need to understand the precision-recall tradeoffs for your specific application.</p> <p>Key Takeaway: Models can be sensitive to low-precision context, where irrelevant information causes them to incorporate red herrings into answers. Testing different document count thresholds is more reliable than using arbitrary re-ranker score cutoffs.</p>"},{"location":"office-hours/cohort3/week-1-1/#what-role-can-small-language-models-play-in-a-rag-architecture","title":"What role can small language models play in a RAG architecture?","text":"<p>Small language models can serve several valuable functions within a RAG system:</p> <p>First, they can rewrite queries quickly and efficiently. For example, if a user asks \"What is happening this week?\", a small model could convert this into a structured query like a JSON object specifying \"search all documents where the datetime is between today and today minus 7 days.\" This type of entity resolution and query parsing doesn't require the full knowledge of a large model but benefits from the lower latency of smaller models.</p> <p>Second, small models can build better embedding spaces. Most current embedding models are relatively simple, but a small language model fine-tuned on your specific task could significantly improve embedding quality or re-ranking performance.</p> <p>In this context, I think of \"small\" as meaning lower latency with less world knowledge - models that can perform specific tasks efficiently without needing to understand everything about the world.</p> <p>Key Takeaway: Small language models can enhance RAG systems through query rewriting and improved embeddings, offering lower latency for specific tasks that don't require comprehensive world knowledge.</p>"},{"location":"office-hours/cohort3/week-1-1/#how-can-we-measure-quality-in-multi-turn-conversations","title":"How can we measure quality in multi-turn conversations?","text":"<p>When evaluating multi-turn exchanges like simulated customer interactions or teacher training scenarios, there are several approaches to consider.</p> <p>One approach is to model the interaction as a state machine or \"LangDraft\" type model where there are defined states that can be traversed. For example, in a customer support scenario, you might have an intake state, followed by various question states, triage states, and resolution states.</p> <p>We've used this with \"LLM therapists\" where the system might say, \"I can tell you're angry. Let me transition you to this sub-draft that deals with negative emotions.\" The user remains in that state until they've acknowledged something, then returns to the main flow.</p> <p>Another approach is to use rubrics and data mining. We extract examples from historical transactions that match specific rubric criteria, then have experts score these examples. These scored examples become few-shot examples for future evaluations.</p> <p>For instance, with venture capital funding requests, we might extract 200 examples of founders discussing how well they know each other, then have experts grade these as good, bad, or great. This creates a training set for evaluating future conversations.</p> <p>The model we build with these rubrics typically extracts scores for 30+ criteria, with LLMs giving scores from 0-4, which then feed into a logistic regression model. This makes the evaluation somewhat explainable - if a candidate gets prioritized unexpectedly, we can see which features drove that decision.</p> <p>Key Takeaway: For evaluating multi-turn conversations, combine state machines to enforce guardrails with human-labeled examples to create scoring rubrics. Using a simple model like logistic regression on top of LLM-generated feature scores maintains interpretability.</p>"},{"location":"office-hours/cohort3/week-1-1/#how-do-you-approach-data-analysis-to-find-business-value-in-ai-applications","title":"How do you approach data analysis to find business value in AI applications?","text":"<p>My favorite content in the course is actually weeks 4 and 5, where I cover my process for data analysis and uncovering new capabilities needed in AI systems.</p> <p>When analyzing data from AI applications, I look for two main types of issues:</p> <ol> <li> <p>Inventory issues: These occur when the system lacks the necessary data to fulfill user requests. For example, if users search for content that doesn't exist in your database, the solution isn't to improve the AI - it's to add the missing content. Many companies don't realize their inventory might be the problem rather than their AI.</p> </li> <li> <p>Capabilities issues: These involve functionality gaps where the system can't perform certain types of queries or filters. For instance, you might need to add metadata filters or specialized search capabilities to handle specific user needs.</p> </li> </ol> <p>I've found tremendous business value by identifying these issues through data analysis. In one case with a restaurant voice AI system, we discovered that when the AI attempted upselling, it generated 20% more revenue 50% of the time - a 10% overall increase. However, the agent only tried upselling in 9% of calls.</p> <p>The solution wasn't to improve the AI's core capabilities but to add a simple check ensuring the agent always asks if the customer wants anything else before ending the call. This small change could generate an additional $2 million in revenue by increasing upselling attempts from 9% to 40%.</p> <p>For me, the most enjoyable work is identifying these business opportunities that don't necessarily require complex AI improvements. Software engineers often aren't trained to think this way, but my background in data science makes this approach natural.</p> <p>Key Takeaway: The biggest business value often comes from analyzing usage patterns to identify inventory gaps or missing capabilities, rather than improving core AI performance. Simple changes like adding missing data or implementing basic business rules can deliver millions in value.</p>"},{"location":"office-hours/cohort3/week-1-1/#how-do-you-balance-technical-implementation-with-business-outcomes","title":"How do you balance technical implementation with business outcomes?","text":"<p>I've worked with many companies where they think they want me to make their AI better, but my actual job is to make their business better. There's often substantial money to be captured by focusing on business outcomes rather than technical improvements.</p> <p>For example, with a construction project, I spoke with contractors to understand their actual pain points. While they initially thought they needed better document search, the real issue was tracking delays and identifying who was causing them. This led us to implement contact search with metadata filters - a solution that addressed a $100,000/month problem.</p> <p>Similarly, with Netflix, if users search for \"Oscar-nominated\" movies but get results about Oscar Wilde or actors named Oscar, the solution might not be more sophisticated AI. It could be as simple as paying IMDB for better awards metadata.</p> <p>I'm constantly looking for these opportunities where a relatively simple technical solution can unlock significant business value. This approach has been much more impactful than pursuing technical sophistication for its own sake.</p> <p>Key Takeaway: Focus on business outcomes over technical sophistication. Often the highest-value solutions involve simple changes that address real user needs rather than complex AI improvements.</p>"},{"location":"office-hours/cohort3/week-1-1/#what-are-your-thoughts-on-the-latest-ai-developments-like-claude-3","title":"What are your thoughts on the latest AI developments like Claude 3?","text":"<p>I'm currently reviewing the entire Instructor codebase to adapt it for Claude 3. The model is making about 15 pull requests for me, so we'll see how that goes.</p> <p>Regarding the guest speakers we've had, I found the Chroma presentation particularly valuable for its hands-on, practical approach. While the Exa presentation was more high-level and story-focused, both offered valuable perspectives.</p> <p>I try to balance technical depth with accessibility in these sessions. When Nils gave his talk, it quickly became very technical with neural network diagrams and mathematical equations, and I could see people leaving the call. It's challenging to find the right balance between technical content and storytelling.</p> <p>Key Takeaway: Balancing technical depth with accessibility is crucial when presenting AI concepts. Different audiences require different approaches to effectively communicate complex ideas.</p>"},{"location":"office-hours/cohort3/week-1-1/#how-should-we-approach-building-rag-applications-for-course-materials","title":"How should we approach building RAG applications for course materials?","text":"<p>If someone wanted to build a RAG application over all the course transcripts and office hours, I'd love to see that. However, this would quickly reveal the limitations of simple chunking approaches.</p> <p>You'd discover that people have different capability requests - like wanting to know who asked specific questions or what was discussed in a particular week. This would require metadata filters for cohort numbers, transcript types (lectures vs. office hours vs. lightning lessons), and speaker identification.</p> <p>You might also need to handle requests for information about guest speakers, like their LinkedIn profiles. All of these are inventory issues that could be solved by ensuring you have the right metadata alongside your content.</p> <p>For a dataset as small as course transcripts, long-context models like Claude 3 might work well without complex RAG. It's really the enterprise use cases with massive document collections that need sophisticated RAG approaches.</p> <p>Key Takeaway: Even simple datasets like course transcripts reveal the importance of metadata and structured information for effective retrieval. Many issues are inventory problems rather than AI problems.</p>"},{"location":"office-hours/cohort3/week-1-1/#how-do-you-handle-uiux-development-for-ai-applications","title":"How do you handle UI/UX development for AI applications?","text":"<p>I try to write most things as command line tools - I'm a \"filthy machine learning Python engineer\" who finds any UI to be too much work. Even Streamlit feels excessive to me when a command line interface would suffice.</p> <p>That said, Claude has demonstrated how well you can do with thoughtful UX patterns. In Week 3, I'll talk about UX patterns that make applications feel responsive - like how Claude shows progress counters as it's uploading and downloading tokens, ensuring something on the page is always moving to indicate work is happening.</p> <p>For those who need to build UIs but lack JavaScript skills, LLMs are remarkably good at writing JavaScript. I've built many bespoke data labeling applications in just 5 hours by prompting models to convert JSON structures to PostgreSQL databases and build the corresponding UIs.</p> <p>The software can be ephemeral enough that I don't worry about long-term maintenance. For more polished applications, I recommend checking out Lovable.dev - I've built about 20 apps with them that work quite well.</p> <p>Key Takeaway: Focus on learning the concepts rather than specific implementation details. Modern LLMs can generate high-quality UI code, making it easier than ever to build functional applications without deep frontend expertise.</p>"},{"location":"office-hours/cohort3/week-1-1/#whats-been-your-most-rewarding-project-in-the-ai-space","title":"What's been your most rewarding project in the AI space?","text":"<p>My background is in physics - I initially thought the universe would generate the most interesting datasets. Then I went to Facebook because I believed people-to-people interactions would be the most fascinating data. Now I'm focused on people-to-AI interactions, and in the future, it will likely be AI-to-AI interactions. I'm essentially chasing the most interesting datasets I can analyze.</p> <p>The most rewarding projects have been those where data analysis revealed clear business opportunities. For instance, with the restaurant voice AI system, identifying the upselling opportunity was straightforward but incredibly valuable.</p> <p>I enjoy working with teams that have access to subject matter experts who can help interpret the data. For the construction project, I spoke with contractors wearing hard hats on Zoom to understand why certain questions were valuable and what problems they were trying to solve.</p> <p>This approach of combining data analysis with domain expertise has consistently led to high-impact solutions that address real business needs rather than just technical challenges.</p> <p>Key Takeaway: The most rewarding AI projects combine data analysis with domain expertise to identify high-impact business opportunities rather than pursuing technical sophistication for its own sake.</p>"},{"location":"office-hours/cohort3/week-1-1/#final-thoughts-on-balancing-course-workload","title":"Final thoughts on balancing course workload","text":"<p>I recognize that the course material can be overwhelming, especially for those balancing it with full-time jobs. We'll have no notebooks in Week 3, which should provide a buffer, and you'll always have access to the Slack channel even after the 6 weeks are over.</p> <p>For those feeling overwhelmed, remember that many people take multiple cohorts to fully absorb the material. The flexible structure is intentional - unlike more prescriptive courses, this approach allows you to focus on what's most relevant to your specific needs.</p> <p>As one participant noted, they've found at least one \"golden nugget\" from each session so far, including the introduction where I presented the \"sandwich view\" of RAG systems. These conceptual frameworks can provide clarity when you're deep in implementation details.</p> <p>Remember that the AI field is moving incredibly quickly, and none of us can absorb everything. The goal isn't to become an expert on everything but to get really good at leveraging AI to stay ahead of everyone else.</p> <p>Key Takeaway: Learning complex technical skills requires finding the right balance between depth of content and time for absorption. Focus on what's most relevant to your needs and remember that continuous learning is more important than perfect comprehension.</p> <p>FAQs</p>"},{"location":"office-hours/cohort3/week-1-1/#how-can-i-balance-this-course-with-my-day-job","title":"How can I balance this course with my day job?","text":"<p>Managing this course alongside your regular work can be challenging. Many students find success by aligning the course with existing work projects, allowing them to apply what they're learning directly to their professional tasks. If you don't have a relevant project, the course notebooks provide boilerplate code you can use as a starting point. Remember that Week 3 has no notebooks, which gives you a buffer to catch up if needed. The course is designed with some flexibility, so you can prioritize the most relevant content for your needs.</p>"},{"location":"office-hours/cohort3/week-1-1/#what-should-i-do-if-i-dont-have-a-specific-project-to-apply-the-course-material-to","title":"What should I do if I don't have a specific project to apply the course material to?","text":"<p>You can start with the boilerplate code provided in the notebooks. These are designed to demonstrate key concepts even without a specific application in mind. Additionally, consider looking for datasets from colleagues or within your organization that might benefit from the techniques taught in the course. Many people have conversation data or other information they're not sure how to leverage effectively. The course materials are structured to help you experiment with these techniques regardless of whether you have a specific project.</p>"},{"location":"office-hours/cohort3/week-1-1/#how-are-the-course-materials-structured","title":"How are the course materials structured?","text":"<p>The course includes lecture videos, notebooks with code examples, office hours, and summary notes. Each set of notebooks focuses on a specific theme or concept, such as synthetic data generation or evaluation metrics. The notebooks are designed to be practical and applicable to real-world scenarios. Week 3 has no notebooks, providing a buffer period. Weeks 4-5 focus on data analysis processes and building specific tools based on identified needs. The course also includes guest lectures from industry experts to provide different perspectives.</p>"},{"location":"office-hours/cohort3/week-1-1/#where-can-i-find-the-summary-notes-and-faqs","title":"Where can I find the summary notes and FAQs?","text":"<p>Currently, summary notes are posted in Slack, but they will eventually be available in Notion or another website format. Many students find these notes helpful as they allow them to focus more on understanding the content rather than taking extensive notes during lectures.</p>"},{"location":"office-hours/cohort3/week-1-1/#whats-the-instructors-approach-to-evaluating-rag-applications","title":"What's the instructor's approach to evaluating RAG applications?","text":"<p>The instructor emphasizes a data-driven approach to evaluations rather than relying on subjective assessments. This includes measuring precision and recall for different numbers of retrieved documents, understanding how models respond to ambiguous information, and using metrics to make informed decisions about system design. The instructor discourages using adjectives to describe performance and instead encourages teams to use numbers, plots, and quantifiable metrics to evaluate their systems.</p>"},{"location":"office-hours/cohort3/week-1-1/#how-can-small-language-models-be-used-in-a-rag-architecture","title":"How can small language models be used in a RAG architecture?","text":"<p>Small language models can serve several purposes in a RAG architecture. They can be used to quickly rewrite queries, breaking them down into more structured formats. They can help build better embedding spaces or re-rankers that are fine-tuned for specific tasks. Small language models generally offer lower latency with less world knowledge, making them suitable for specific components of a RAG system where full context understanding isn't necessary.</p>"},{"location":"office-hours/cohort3/week-1-1/#what-are-the-most-valuable-insights-from-the-course-so-far","title":"What are the most valuable insights from the course so far?","text":"<p>Many students highlight the \"sandwich view\" of RAG systems (where RAG is presented as a recommendation system between LLM layers) as particularly insightful. The course provides practical \"golden nuggets\" in each session, including frameworks for thinking about RAG applications, evaluation techniques, and implementation strategies. The balance between technical details and storytelling across different guest lectures has been valuable for understanding both theoretical concepts and practical applications.</p>"},{"location":"office-hours/cohort3/week-1-1/#whats-the-instructors-perspective-on-building-uiux-for-llm-applications","title":"What's the instructor's perspective on building UI/UX for LLM applications?","text":"<p>The instructor suggests focusing on understanding concepts rather than specific UI technologies. Command-line tools can be highly effective for many applications, and modern LLMs are excellent at generating JavaScript and other frontend code when needed. Understanding server-sent events and streaming is particularly important for creating responsive LLM applications. The instructor emphasizes that streaming is essential for good user experience - applications without streaming capabilities are generally considered subpar in the current landscape.</p>"},{"location":"office-hours/cohort3/week-1-1/#how-does-the-instructor-approach-business-value-in-ai-projects","title":"How does the instructor approach business value in AI projects?","text":"<p>The instructor focuses on identifying business value through data analysis rather than just improving AI capabilities. This involves analyzing user interactions, identifying patterns, and determining whether issues are related to inventory (missing data) or capabilities (features the system can't perform). Often, the most valuable insights come from discovering simple business improvements that don't require complex AI solutions. The instructor recommends working closely with subject matter experts to understand the real business needs behind technical requirements.</p>"},{"location":"office-hours/cohort3/week-1-1/#will-there-be-opportunities-to-continue-learning-after-the-course-ends","title":"Will there be opportunities to continue learning after the course ends?","text":"<p>Yes, students will still have access to Slack after the 6-week course concludes, and the instructor encourages continued questions. Additionally, students can join future cohorts of the course if they need more time to absorb the material. Many students find they benefit from going through the content multiple times as the field evolves.</p>"},{"location":"office-hours/cohort3/week-1-2/","title":"Week 1 - Office Hour 2 (May 22)","text":"<p>Study Notes:</p> <p>I hosted an office hours session focused on RAG implementation challenges, precision-recall tradeoffs, and practical business applications of AI systems. Here are my insights on evaluating RAG systems, understanding model sensitivity to context quality, and identifying high-value business opportunities through data analysis rather than just technical improvements.</p>"},{"location":"office-hours/cohort3/week-1-2/#how-should-we-understand-precision-in-the-context-of-llms-and-rag","title":"How should we understand precision in the context of LLMs and RAG?","text":"<p>Precision has become increasingly important as most people are comfortable adding substantial information to the context window. The key question is how sensitive these models are to ambiguous or irrelevant information.</p> <p>Most current models are very good at recall because they're optimized for \"needle in the haystack\" tests. Due to the attention mechanism with sliding windows, they can typically catch important information even when it's buried among other content. However, the sensitivity to irrelevant information is less well-studied.</p> <p>As we increase the amount of data in the context, we need to understand what precision looks like and how it correlates with factuality or answer quality. Earlier models like GPT-3.5 were quite sensitive to long context with low precision - if you gave them too much information, they would \"overthink\" because of the volume of content to process.</p> <p>This is why it's valuable to experiment with different retrieval settings: \"For the top 10 documents, this is my precision and recall; for my top 25 documents, this is my precision and recall. If my recall is not increasing as a function of K (the number of documents), that's where I decide to set a threshold.\"</p> <p>Some people set thresholds based on re-ranker scores, but that can be dangerous since these scores aren't true probabilities. You can't just set 0.5 as a universal threshold - you need to understand the precision-recall tradeoffs for your specific application.</p> <p>Key Takeaway: Models can be sensitive to low-precision context, where irrelevant information causes them to incorporate red herrings into answers. Testing different document count thresholds is more reliable than using arbitrary re-ranker score cutoffs.</p>"},{"location":"office-hours/cohort3/week-1-2/#what-role-can-small-language-models-play-in-a-rag-architecture","title":"What role can small language models play in a RAG architecture?","text":"<p>Small language models can serve several valuable functions within a RAG system:</p> <p>First, they can rewrite queries quickly and efficiently. For example, if a user asks \"What is happening this week?\", a small model could convert this into a structured query like a JSON object specifying \"search all documents where the datetime is between today and today minus 7 days.\" This type of entity resolution and query parsing doesn't require the full knowledge of a large model but benefits from the lower latency of smaller models.</p> <p>Second, small models can build better embedding spaces. Most current embedding models are relatively simple, but a small language model fine-tuned on your specific task could significantly improve embedding quality or re-ranking performance.</p> <p>In this context, I think of \"small\" as meaning lower latency with less world knowledge - models that can perform specific tasks efficiently without needing to understand everything about the world.</p>"},{"location":"office-hours/cohort3/week-1-2/#how-can-we-measure-quality-in-multi-turn-conversations","title":"How can we measure quality in multi-turn conversations?","text":"<p>When evaluating multi-turn exchanges like simulated customer interactions or teacher training scenarios, there are several approaches to consider.</p> <p>One approach is to model the interaction as a state machine or \"LangDraft\" type model where there are defined states that can be traversed. For example, in a customer support scenario, you might have an intake state, followed by various question states, triage states, and resolution states.</p> <p>We've used this with \"LLM therapists\" where the system might say, \"I can tell you're angry. Let me transition you to this sub-draft that deals with negative emotions.\" The user remains in that state until they've acknowledged something, then returns to the main flow.</p> <p>Another approach is to use rubrics and data mining. We extract examples from historical transactions that match specific rubric criteria, then have experts score these examples. These scored examples become few-shot examples for future evaluations.</p> <p>For instance, with venture capital funding requests, we might extract 200 examples of founders discussing how well they know each other, then have experts grade these as good, bad, or great. This creates a training set for evaluating future conversations.</p> <p>The model we build with these rubrics typically extracts scores for 30+ criteria, with LLMs giving scores from 0-4, which then feed into a logistic regression model. This makes the evaluation somewhat explainable - if a candidate gets prioritized unexpectedly, we can see which features drove that decision.</p> <p>Key Takeaway: For evaluating multi-turn conversations, combine state machines to enforce guardrails with human-labeled examples to create scoring rubrics. Using a simple model like logistic regression on top of LLM-generated feature scores maintains interpretability.</p>"},{"location":"office-hours/cohort3/week-1-2/#how-do-you-approach-data-analysis-to-find-business-value-in-ai-applications","title":"How do you approach data analysis to find business value in AI applications?","text":"<p>My favorite content in the course is actually weeks 4 and 5, where I cover my process for data analysis and uncovering new capabilities needed in AI systems.</p> <p>When analyzing data from AI applications, I look for two main types of issues:</p> <ol> <li> <p>Inventory issues: These occur when the system lacks the necessary data to fulfill user requests. For example, if users search for content that doesn't exist in your database, the solution isn't to improve the AI - it's to add the missing content. Many companies don't realize their inventory might be the problem rather than their AI.</p> </li> <li> <p>Capabilities issues: These involve functionality gaps where the system can't perform certain types of queries or filters. For instance, you might need to add metadata filters or specialized search capabilities to handle specific user needs.</p> </li> </ol> <p>I've found tremendous business value by identifying these issues through data analysis. In one case with a restaurant voice AI system, we discovered that when the AI attempted upselling, it generated 20% more revenue 50% of the time - a 10% overall increase. However, the agent only tried upselling in 9% of calls.</p> <p>The solution wasn't to improve the AI's core capabilities but to add a simple check ensuring the agent always asks if the customer wants anything else before ending the call. This small change could generate an additional $2 million in revenue by increasing upselling attempts from 9% to 40%.</p> <p>For me, the most enjoyable work is identifying these business opportunities that don't necessarily require complex AI improvements. Software engineers often aren't trained to think this way, but my background in data science makes this approach natural.</p> <p>Key Takeaway: The biggest business value often comes from analyzing usage patterns to identify inventory gaps or missing capabilities, rather than improving core AI performance. Simple changes like adding missing data or implementing basic business rules can deliver millions in value.</p>"},{"location":"office-hours/cohort3/week-1-2/#how-do-you-balance-technical-implementation-with-business-outcomes","title":"How do you balance technical implementation with business outcomes?","text":"<p>I've worked with many companies where they think they want me to make their AI better, but my actual job is to make their business better. There's often substantial money to be captured by focusing on business outcomes rather than technical improvements.</p> <p>For example, with a construction project, I spoke with contractors to understand their actual pain points. While they initially thought they needed better document search, the real issue was tracking delays and identifying who was causing them. This led us to implement contact search with metadata filters - a solution that addressed a $100,000/month problem.</p> <p>Similarly, with Netflix, if users search for \"Oscar-nominated\" movies but get results about Oscar Wilde or actors named Oscar, the solution might not be more sophisticated AI. It could be as simple as paying IMDB for better awards metadata.</p> <p>I'm constantly looking for these opportunities where a relatively simple technical solution can unlock significant business value. This approach has been much more impactful than pursuing technical sophistication for its own sake.</p>"},{"location":"office-hours/cohort3/week-1-2/#what-are-your-thoughts-on-the-latest-ai-developments-like-claude-3","title":"What are your thoughts on the latest AI developments like Claude 3?","text":"<p>I'm currently reviewing the entire Instructor codebase to adapt it for Claude 3. The model is making about 15 pull requests for me, so we'll see how that goes.</p> <p>Regarding the guest speakers we've had, I found the Chroma presentation particularly valuable for its hands-on, practical approach. While the Exa presentation was more high-level and story-focused, both offered valuable perspectives.</p> <p>I try to balance technical depth with accessibility in these sessions. When Nils gave his talk, it quickly became very technical with neural network diagrams and mathematical equations, and I could see people leaving the call. It's challenging to find the right balance between technical content and storytelling.</p>"},{"location":"office-hours/cohort3/week-1-2/#how-should-we-approach-building-rag-applications-for-course-materials","title":"How should we approach building RAG applications for course materials?","text":"<p>If someone wanted to build a RAG application over all the course transcripts and office hours, I'd love to see that. However, this would quickly reveal the limitations of simple chunking approaches.</p> <p>You'd discover that people have different capability requests - like wanting to know who asked specific questions or what was discussed in a particular week. This would require metadata filters for cohort numbers, transcript types (lectures vs. office hours vs. lightning lessons), and speaker identification.</p> <p>You might also need to handle requests for information about guest speakers, like their LinkedIn profiles. All of these are inventory issues that could be solved by ensuring you have the right metadata alongside your content.</p> <p>For a dataset as small as course transcripts, long-context models like Claude 3 might work well without complex RAG. It's really the enterprise use cases with massive document collections that need sophisticated RAG approaches.</p>"},{"location":"office-hours/cohort3/week-1-2/#how-do-you-handle-uiux-development-for-ai-applications","title":"How do you handle UI/UX development for AI applications?","text":"<p>I try to write most things as command line tools - I'm a \"filthy machine learning Python engineer\" who finds any UI to be too much work. Even Streamlit feels excessive to me when a command line interface would suffice.</p> <p>That said, Claude has demonstrated how well you can do with thoughtful UX patterns. In Week 3, I'll talk about UX patterns that make applications feel responsive - like how Claude shows progress counters as it's uploading and downloading tokens, ensuring something on the page is always moving to indicate work is happening.</p> <p>For those who need to build UIs but lack JavaScript skills, LLMs are remarkably good at writing JavaScript. I've built many bespoke data labeling applications in just 5 hours by prompting models to convert JSON structures to PostgreSQL databases and build the corresponding UIs.</p> <p>The software can be ephemeral enough that I don't worry about long-term maintenance. For more polished applications, I recommend checking out Lovable.dev - I've built about 20 apps with them that work quite well.</p> <p>Key Takeaway: Focus on learning the concepts rather than specific implementation details. Modern LLMs can generate high-quality UI code, making it easier than ever to build functional applications without deep frontend expertise.</p>"},{"location":"office-hours/cohort3/week-1-2/#whats-been-your-most-rewarding-project-in-the-ai-space","title":"What's been your most rewarding project in the AI space?","text":"<p>My background is in physics - I initially thought the universe would generate the most interesting datasets. Then I went to Facebook because I believed people-to-people interactions would be the most fascinating data. Now I'm focused on people-to-AI interactions, and in the future, it will likely be AI-to-AI interactions. I'm essentially chasing the most interesting datasets I can analyze.</p> <p>The most rewarding projects have been those where data analysis revealed clear business opportunities. For instance, with the restaurant voice AI system, identifying the upselling opportunity was straightforward but incredibly valuable.</p> <p>I enjoy working with teams that have access to subject matter experts who can help interpret the data. For the construction project, I spoke with contractors wearing hard hats on Zoom to understand why certain questions were valuable and what problems they were trying to solve.</p> <p>This approach of combining data analysis with domain expertise has consistently led to high-impact solutions that address real business needs rather than just technical challenges.</p>"},{"location":"office-hours/cohort3/week-1-2/#final-thoughts-on-balancing-course-workload","title":"Final thoughts on balancing course workload","text":"<p>I recognize that the course material can be overwhelming, especially for those balancing it with full-time jobs. We'll have no notebooks in Week 3, which should provide a buffer, and you'll always have access to the Slack channel even after the 6 weeks are over.</p> <p>For those feeling overwhelmed, remember that many people take multiple cohorts to fully absorb the material. The flexible structure is intentional - unlike more prescriptive courses, this approach allows you to focus on what's most relevant to your specific needs.</p> <p>As one participant noted, they've found at least one \"golden nugget\" from each session so far, including the introduction where I presented the \"sandwich view\" of RAG systems. These conceptual frameworks can provide clarity when you're deep in implementation details.</p> <p>Remember that the AI field is moving incredibly quickly, and none of us can absorb everything. The goal isn't to become an expert on everything but to get really good at leveraging AI to stay ahead of everyone else.</p> <p>FAQs</p>"},{"location":"office-hours/cohort3/week-1-2/#how-can-i-balance-this-course-with-my-day-job","title":"How can I balance this course with my day job?","text":"<p>Managing this course alongside your regular work can be challenging. Many students find success by aligning the course with existing work projects, allowing them to apply what they're learning directly to their professional tasks. If you don't have a relevant project, the course notebooks provide boilerplate code you can use as a starting point. Remember that Week 3 has no notebooks, which gives you a buffer to catch up if needed. The course is designed with some flexibility, so you can prioritize the most relevant content for your needs.</p>"},{"location":"office-hours/cohort3/week-1-2/#what-should-i-do-if-i-dont-have-a-specific-project-to-apply-the-course-material-to","title":"What should I do if I don't have a specific project to apply the course material to?","text":"<p>You can start with the boilerplate code provided in the notebooks. These are designed to demonstrate key concepts even without a specific application in mind. Additionally, consider looking for datasets from colleagues or within your organization that might benefit from the techniques taught in the course. Many people have conversation data or other information they're not sure how to leverage effectively. The course materials are structured to help you experiment with these techniques regardless of whether you have a specific project.</p>"},{"location":"office-hours/cohort3/week-1-2/#how-are-the-course-materials-structured","title":"How are the course materials structured?","text":"<p>The course includes lecture videos, notebooks with code examples, office hours, and summary notes. Each set of notebooks focuses on a specific theme or concept, such as synthetic data generation or evaluation metrics. The notebooks are designed to be practical and applicable to real-world scenarios. Week 3 has no notebooks, providing a buffer period. Weeks 4-5 focus on data analysis processes and building specific tools based on identified needs. The course also includes guest lectures from industry experts to provide different perspectives.</p>"},{"location":"office-hours/cohort3/week-1-2/#where-can-i-find-the-summary-notes-and-faqs","title":"Where can I find the summary notes and FAQs?","text":"<p>Currently, summary notes are posted in Slack, but they will eventually be available in Notion or another website format. Many students find these notes helpful as they allow them to focus more on understanding the content rather than taking extensive notes during lectures.</p>"},{"location":"office-hours/cohort3/week-1-2/#whats-the-instructors-approach-to-evaluating-rag-applications","title":"What's the instructor's approach to evaluating RAG applications?","text":"<p>The instructor emphasizes a data-driven approach to evaluations rather than relying on subjective assessments. This includes measuring precision and recall for different numbers of retrieved documents, understanding how models respond to ambiguous information, and using metrics to make informed decisions about system design. The instructor discourages using adjectives to describe performance and instead encourages teams to use numbers, plots, and quantifiable metrics to evaluate their systems.</p>"},{"location":"office-hours/cohort3/week-1-2/#how-can-small-language-models-be-used-in-a-rag-architecture","title":"How can small language models be used in a RAG architecture?","text":"<p>Small language models can serve several purposes in a RAG architecture. They can be used to quickly rewrite queries, breaking them down into more structured formats. They can help build better embedding spaces or re-rankers that are fine-tuned for specific tasks. Small language models generally offer lower latency with less world knowledge, making them suitable for specific components of a RAG system where full context understanding isn't necessary.</p>"},{"location":"office-hours/cohort3/week-1-2/#what-are-the-most-valuable-insights-from-the-course-so-far","title":"What are the most valuable insights from the course so far?","text":"<p>Many students highlight the \"sandwich view\" of RAG systems (where RAG is presented as a recommendation system between LLM layers) as particularly insightful. The course provides practical \"golden nuggets\" in each session, including frameworks for thinking about RAG applications, evaluation techniques, and implementation strategies. The balance between technical details and storytelling across different guest lectures has been valuable for understanding both theoretical concepts and practical applications.</p>"},{"location":"office-hours/cohort3/week-1-2/#whats-the-instructors-perspective-on-building-uiux-for-llm-applications","title":"What's the instructor's perspective on building UI/UX for LLM applications?","text":"<p>The instructor suggests focusing on understanding concepts rather than specific UI technologies. Command-line tools can be highly effective for many applications, and modern LLMs are excellent at generating JavaScript and other frontend code when needed. Understanding server-sent events and streaming is particularly important for creating responsive LLM applications. The instructor emphasizes that streaming is essential for good user experience - applications without streaming capabilities are generally considered subpar in the current landscape.</p>"},{"location":"office-hours/cohort3/week-1-2/#how-does-the-instructor-approach-business-value-in-ai-projects","title":"How does the instructor approach business value in AI projects?","text":"<p>The instructor focuses on identifying business value through data analysis rather than just improving AI capabilities. This involves analyzing user interactions, identifying patterns, and determining whether issues are related to inventory (missing data) or capabilities (features the system can't perform). Often, the most valuable insights come from discovering simple business improvements that don't require complex AI solutions. The instructor recommends working closely with subject matter experts to understand the real business needs behind technical requirements.</p>"},{"location":"office-hours/cohort3/week-1-2/#will-there-be-opportunities-to-continue-learning-after-the-course-ends","title":"Will there be opportunities to continue learning after the course ends?","text":"<p>Yes, students will still have access to Slack after the 6-week course concludes, and the instructor encourages continued questions. Additionally, students can join future cohorts of the course if they need more time to absorb the material. Many students find they benefit from going through the content multiple times as the field evolves.</p>"},{"location":"office-hours/cohort3/week-2-1/","title":"Week 2 - Office Hour 1 (May 27)","text":"<p>Study Notes:</p> <p>I hosted an office hours session focused on advanced retrieval-augmented generation (RAG) strategies and implementation challenges. Here are my insights on building effective RAG systems for specialized domains, handling complex data structures, and designing economically valuable AI solutions that go beyond simple question-answering.</p>"},{"location":"office-hours/cohort3/week-2-1/#how-should-i-approach-medical-rag-systems-with-complex-queries","title":"How should I approach medical RAG systems with complex queries?","text":"<p>When dealing with specialized domains like medical records where users ask comprehensive questions (e.g., \"Give a complete medical history of patient X\"), the key is understanding that you can't just throw everything into a generic RAG system and expect good results.</p> <p>I've found that building separate indices for different document categories is essential. For example, with an oil and gas client I'm working with, we're processing millions of PDFs but categorizing them into about five different types: drill logs, specifications, geospatial diagrams, etc.</p> <p>Within each category, we extract specific data structures. For drill logs, we identify that some pages represent different days of logging, so we extract metadata like dates, drill IDs, and personnel information. This allows us to build specialized tools for querying each data type effectively.</p> <p>For medical history queries, you'd want to create structured data that can be queried directly - essentially turning it into a \"SELECT * FROM medical_history WHERE client_id = X\" type of operation rather than relying on semantic search across unstructured text.</p> <p>Key Takeaway: Don't try to build one universal RAG system. Instead, identify the categories of documents in your domain, extract relevant structured data from each category, and build specialized tools to query that structured data effectively.</p>"},{"location":"office-hours/cohort3/week-2-1/#whats-the-best-approach-to-handling-citations-in-rag-systems","title":"What's the best approach to handling citations in RAG systems?","text":"<p>When your LLM isn't reliable for generating citations and semantic/fuzzy similarity doesn't work well (particularly in domains with many abbreviations like medicine), you need a more structured approach.</p> <p>I recommend following Claude's citation approach, which uses XML tags to wrap citations. When you create statements, include XML that references the source ID. In your text chunks, you'll have chunk IDs and other metadata alongside the content.</p> <p>To make this more precise, especially with longer contexts, include the first three words and last three words of the cited span. For example, if citing \"similarity isn't reliable either for our use case,\" the citation would include both the chunk ID and \"start is similarity isn't reliable, end is for our use case.\"</p> <p>This approach works well with fine-tuning. We implemented something similar in Instructor, where an answer is structured as a list of facts, each with a substring quote, ensuring alignment between facts and quotes to minimize hallucinations.</p> <p>Key Takeaway: Structure your citations with explicit references to chunk IDs and text spans rather than relying on similarity matching. This approach can be implemented through fine-tuning and provides much more reliable attribution.</p>"},{"location":"office-hours/cohort3/week-2-1/#should-i-use-graph-based-rag-approaches","title":"Should I use graph-based RAG approaches?","text":"<p>I'm generally skeptical about graph-based RAG systems. In my experience with data analysis over many years, graph databases often fall away in favor of embeddings and SQL databases.</p> <p>The main challenge with graph RAG is that building out the taxonomy is often harder than you expect. You think you're avoiding the complexity of embedding models, but you're just substituting it with the problem of modeling out the taxonomy, which can be equally challenging.</p> <p>For most use cases where you might consider a graph, you can achieve similar results with a few SQL joins. Unless you need to do complex traversals (like LinkedIn finding connections-of-connections), the overhead of learning graph query languages and modeling data as graphs usually isn't worth it.</p> <p>Even Facebook, despite being fundamentally a social graph, uses a very large-scale MySQL instance rather than a dedicated graph database. If you only need one-way traversals, a standard relational database is typically sufficient.</p> <p>Key Takeaway: Unless your use case requires complex multi-step graph traversals, you're likely better off using embeddings with SQL databases rather than implementing a graph-based RAG system. The taxonomy development often becomes more complex than the embedding approach you were trying to avoid.</p>"},{"location":"office-hours/cohort3/week-2-1/#how-do-you-recommend-clustering-and-categorizing-user-queries","title":"How do you recommend clustering and categorizing user queries?","text":"<p>For understanding what users are asking about, we've developed a library called Cura (similar to Anthropic's Clio) that performs population-level analysis of conversation history.</p> <p>The process works like this:</p> <ol> <li>We summarize every conversation</li> <li>We extract key information: languages used, topics, tasks, requests, user complaints, and assistant errors</li> <li>We concatenate everything and create embeddings</li> <li>We perform clustering to identify patterns</li> <li>We use a language model to group and label clusters</li> </ol> <p>This approach gives you insights into what people are asking for, how big each cluster is, and metrics like error rates or user satisfaction for different types of queries. You can then identify which clusters are performing well and which need improvement, helping you decide where to invest in new tools or capabilities.</p> <p>We're releasing a new version of Cura soon with better ergonomics and UI for exploration. This will be covered in more detail in Week 4 of the course.</p> <p>Key Takeaway: Systematic analysis of user queries through summarization, extraction, embedding, and clustering helps identify patterns in how people use your system, allowing you to prioritize improvements where they'll have the most impact.</p>"},{"location":"office-hours/cohort3/week-2-1/#whats-your-recommendation-for-chunking-documentation","title":"What's your recommendation for chunking documentation?","text":"<p>When dealing with documentation PDFs containing tables, definitions, paragraphs, and figures, I challenge the conventional wisdom about chunking. For documentation, I believe the chunk should often be the size of the document page.</p> <p>The right question to ask is \"which page do I need to look on?\" rather than trying to break documents into arbitrary chunks. Modern models are large enough to handle page-sized chunks, and documentation typically uses consistent terminology (unlike cases where semantic search helps bridge vocabulary differences).</p> <p>By combining semantic and lexical search and focusing on page-level retrieval, you can often get better results than with smaller chunks. This approach also respects the semantic boundaries that document authors typically maintain - they rarely split headers from content across pages or break logical sections in awkward places.</p> <p>Key Takeaway: For documentation, consider using page-level chunking rather than arbitrary token-based chunking. This respects the document's inherent structure and works well when combined with both semantic and lexical search approaches.</p>"},{"location":"office-hours/cohort3/week-2-1/#what-are-the-trade-offs-between-different-vector-database-options","title":"What are the trade-offs between different vector database options?","text":"<p>I generally prefer using Postgres with pgvector because it allows me to join on different tables, which is extremely valuable. However, pgvector doesn't do exhaustive search by default, which can be a limitation with large datasets.</p> <p>If you're dealing with very large vector collections, consider Timescale's pgvector_scale, which has better streaming methods for exhaustive search. Another advantage of the Postgres approach is that you can install pg_search from PostgresML to get BM25 implementation, giving you both vector search and lexical search in the same database.</p> <p>This combination of vector search and lexical search in a single database that also supports filtering by metadata (like dates or access permissions) is powerful for real-world applications.</p> <p>Key Takeaway: Postgres with pgvector provides a good balance of functionality for most RAG applications, especially when combined with pg_search for lexical search. For very large datasets, consider specialized extensions like pgvector_scale.</p>"},{"location":"office-hours/cohort3/week-2-1/#how-do-you-approach-building-economically-valuable-ai-systems","title":"How do you approach building economically valuable AI systems?","text":"<p>When building AI systems, I focus on economic value rather than just time savings. Time savings is bounded - you can only save as much time as someone currently spends. But economic value can be much larger.</p> <p>For example, with construction blueprints, we realized that simply answering questions about window heights wasn't that valuable - it just saved a worker a few minutes. But by extracting structured data about room counts, building lines, and floor numbers, we could quickly locate the right blueprints when workers were on site, preventing costly delays.</p> <p>In another case, we built voice agents that call car owners to schedule maintenance appointments. Rather than charging by call duration, the system charges a percentage of what the mechanic makes. This aligns incentives - the AI provider is motivated to resolve phone numbers correctly, ensure calendar synchronization works, and get customers to actually show up.</p> <p>The most valuable systems help make better decisions, not just answer questions. If you're building a hiring assistant, don't just price based on tokens used - think about what a bad hire costs a company and how much value your system provides by helping them avoid that outcome.</p> <p>Key Takeaway: Focus on building systems that drive economic value through better decision-making rather than just answering questions or saving time. Structure your pricing to align with the value you create, such as taking a percentage of revenue generated or costs avoided.</p>"},{"location":"office-hours/cohort3/week-2-1/#how-did-you-handle-blueprint-analysis-for-construction-projects","title":"How did you handle blueprint analysis for construction projects?","text":"<p>For a construction project involving blueprints, we realized through user query analysis that workers needed to find specific documents based on their location in a building. They'd say things like \"I'm on the 40th floor in a room with two bedrooms and a bathroom on the north-facing side - find me the schemas for the windows.\"</p> <p>We built a system that extracted structured data from blueprints: which building, which floor, which \"line\" (position in the building), room counts, and directional orientation. This required a combination of bounding box models and LLMs to identify and extract this information.</p> <p>The challenge was proving that we needed to invest in these specialized models. By analyzing the types of questions being asked, we could justify building tools that could count rooms, identify directions, and extract other key metadata that made retrieval much more effective.</p> <p>For specialized domains like blueprints, it's crucial to understand the specific queries users have and build structured data models that directly address those needs rather than relying on generic text embeddings.</p> <p>Key Takeaway: For specialized visual content like blueprints, invest in extracting structured data that matches the way users think about and query the information. This often requires specialized models beyond general-purpose LLMs, but the investment pays off in much more effective retrieval.</p>"},{"location":"office-hours/cohort3/week-2-1/#final-thoughts-on-building-effective-rag-systems","title":"Final thoughts on building effective RAG systems","text":"<p>The most successful RAG implementations I've seen share a few common characteristics:</p> <ol> <li>They don't try to build one universal system but instead create specialized tools for different document categories and query types</li> <li>They extract structured data that matches the way users think about and query information</li> <li>They combine multiple search approaches - semantic, lexical, and metadata filtering</li> <li>They focus on delivering economic value, not just answering questions</li> <li>They evolve based on systematic analysis of user queries and pain points</li> </ol> <p>As we continue to develop these systems, I expect to see more specialized, domain-specific implementations that go beyond generic question-answering to provide decision support and drive measurable business outcomes. The future of these agents will be selling work and outcomes, not just time and tokens.</p> <p>FAQs</p>"},{"location":"office-hours/cohort3/week-2-1/#what-approach-should-i-take-for-medical-rag-with-complex-queries","title":"What approach should I take for medical RAG with complex queries?","text":"<p>For complex medical queries like \"give a complete medical history of patient,\" a generic chunking approach isn't sufficient. Instead, build separate indices for different categories of documents and create specific data structures for each type. For example, with medical records, you might create distinct structures for doctor's visits, referral letters, and prescriptions. This allows you to develop targeted tools that can directly query these structures rather than relying on general semantic search across all documents.</p>"},{"location":"office-hours/cohort3/week-2-1/#how-should-i-handle-citations-in-my-llm-responses","title":"How should I handle citations in my LLM responses?","text":"<p>When implementing citations in LLM responses, consider using an XML-based approach similar to Claude's citation system. This involves wrapping citations with XML tags that reference the source chunk ID along with the first and last few words of the cited span. For fine-tuned models, you can train the model to output citations in this format, which provides more precise references than simple chunk IDs. This approach works well even when the model rephrases information from abbreviation-heavy medical texts.</p>"},{"location":"office-hours/cohort3/week-2-1/#what-are-your-thoughts-on-graph-based-rag-versus-traditional-approaches","title":"What are your thoughts on graph-based RAG versus traditional approaches?","text":"<p>While graph-based RAG sounds promising, it often substitutes one complex problem (embedding models) with another (taxonomy modeling). For most use cases, a well-structured SQL database with appropriate joins is more practical than implementing a graph database. Graph databases require learning new query languages and modeling approaches, which adds significant overhead. Unless you need complex multi-step traversals (like LinkedIn's connection finder), the benefits rarely outweigh the costs. Most \"graph-like\" relationships can be effectively modeled with standard SQL joins.</p>"},{"location":"office-hours/cohort3/week-2-1/#how-should-i-approach-chunking-for-documentation-based-rag","title":"How should I approach chunking for documentation-based RAG?","text":"<p>For documentation, consider using page-level chunking rather than arbitrary token-based chunks. This aligns with how documentation is naturally structured and how authors organize information. Combine semantic search with lexical search for better results, as documentation typically uses consistent terminology. Test this approach with evaluations to verify its effectiveness for your specific use case. Remember that document creators are usually aware of page-level semantics and rarely split important concepts across pages.</p>"},{"location":"office-hours/cohort3/week-2-1/#how-can-i-understand-what-my-users-are-asking-about","title":"How can I understand what my users are asking about?","text":"<p>To analyze user queries effectively, use a conversation analysis tool like Cura. This approach involves:</p> <ol> <li>Summarizing each conversation</li> <li>Extracting key information (language used, topics, tasks, requests, complaints)</li> <li>Embedding this data</li> <li>Clustering similar conversations</li> <li>Using an LLM to label and group these clusters</li> </ol> <p>This gives you insights into what users are asking, which features are performing well, and which need improvement. You can then develop targeted tools to address the most common or high-value query types.</p>"},{"location":"office-hours/cohort3/week-2-1/#whats-your-experience-with-extracting-data-from-construction-blueprints","title":"What's your experience with extracting data from construction blueprints?","text":"<p>When working with construction blueprints, focus on extracting structured data that answers specific questions users ask. For example, in a condominium project, we extracted data like floor numbers, room counts, directional orientation, and unit identifiers. This required developing specialized bounding box models to identify key elements in the blueprints. The approach was driven by analyzing actual user queries, which revealed they needed to quickly locate specific information like window dimensions or material specifications for particular rooms or floors.</p>"},{"location":"office-hours/cohort3/week-2-1/#should-i-use-postgres-with-pgvector-for-my-rag-implementation","title":"Should I use Postgres with pgvector for my RAG implementation?","text":"<p>Postgres with pgvector is a good choice for RAG implementations because it allows you to combine vector search with traditional SQL queries, enabling pre-filtering by metadata like dates or access permissions. For better performance, consider pgvector-scale, which provides more efficient exhaustive search capabilities for larger datasets. Adding pg_search from PostgreSQL gives you BM25 implementation, allowing you to combine vector search with lexical search in the same database. This approach gives you flexibility to switch between semantic and lexical search while maintaining the ability to join with other data tables.</p>"},{"location":"office-hours/cohort3/week-2-1/#how-do-you-determine-which-user-questions-are-worth-optimizing-for","title":"How do you determine which user questions are worth optimizing for?","text":"<p>Focus on identifying which questions deliver the most economic value, not just which are most common. For example, in a construction project, helping workers quickly locate specific blueprint details might save a few minutes, but identifying unsigned contracts that could cause project delays delivers much higher value. Analyze your user conversations to identify these high-value query patterns, then build specialized tools to address them. The goal isn't just to answer questions faster but to help users make better decisions that impact their bottom line.</p>"},{"location":"office-hours/cohort3/week-2-1/#whats-your-recommendation-on-model-selection-for-rag-applications","title":"What's your recommendation on model selection for RAG applications?","text":"<p>There's no one-size-fits-all model recommendation for RAG. Start by getting your retrieval right, as reasoning over data you can't find is a bigger issue than reasoning capabilities. Then run evaluations to test different models against your specific use cases. Consider your budget constraints across three dimensions: cost, latency, and performance. Your choice will depend on the economic value of the application - a financial analysis tool might justify using GPT-4 at $4 per report if it's still cheaper than human analysis, while a nutritionist website chatbot might need a more cost-effective model.</p>"},{"location":"office-hours/cohort3/week-2-2/","title":"Week 2, Office Hour 2 (May 29)","text":"<p>Study Notes:</p> <p>I hosted an office hours session focused on retrieval-augmented generation (RAG) implementation challenges and strategies. Participants shared their experiences with the course materials and discussed specific technical issues they're facing in their RAG projects, from processing technical documentation to handling irrelevant data in vector databases.</p>"},{"location":"office-hours/cohort3/week-2-2/#how-can-i-apply-course-concepts-to-my-actual-project-while-balancing-time-constraints","title":"How can I apply course concepts to my actual project while balancing time constraints?","text":"<p>Several participants expressed the challenge of finding time to apply course concepts to their real-world projects while managing full-time jobs. One participant noted, \"I have a day job with a packed schedule. I already have to make room for lectures and these conversations, which leaves very little time to apply this to my project.\"</p> <p>This is a common challenge when learning new technical skills alongside existing responsibilities. For those in this situation, I recommend focusing on completing the course first and then applying the knowledge afterward. The community will remain active even after the course ends, with the Slack channel staying open and all videos remaining available.</p> <p>For those who need more immediate application, consider reaching out about a consulting engagement after the course. The reality is that deep implementation often requires dedicated time that's difficult to carve out while maintaining other responsibilities.</p> <p>Key Takeaway: Learning and implementation often need to be sequenced rather than parallel when you have limited time. Focus on absorbing the knowledge first, then plan dedicated time for application afterward.</p>"},{"location":"office-hours/cohort3/week-2-2/#what-happens-to-the-community-after-the-course-ends","title":"What happens to the community after the course ends?","text":"<p>While we won't have structured bi-weekly meetings after the course concludes, the Slack channel will remain active, and I'll check it regularly to share resources and interesting developments. All course materials, including videos and the Maven pages, will remain accessible.</p> <p>The community's activity level will largely depend on participant engagement - \"it's basically going to be like however much you put in is what you're going to get out.\" We don't have a community manager pushing conversations, as my goal isn't to maximize message volume.</p> <p>Many valuable interactions happen through direct messages rather than in public channels. For example, one participant is about to launch their own company, and we're jumping on calls to discuss their ideas and make introductions.</p> <p>Key Takeaway: The community will continue beyond the formal course structure, but its value will depend on your active participation and willingness to engage with others.</p>"},{"location":"office-hours/cohort3/week-2-2/#how-should-i-handle-irrelevant-data-being-pushed-into-my-vector-database","title":"How should I handle irrelevant data being pushed into my vector database?","text":"<p>One participant working on an application with high-performance RAG requirements asked about the impact of irrelevant data in their vector database: \"How much do I need to worry if there's irrelevant data being pushed into our vector database? Is it not that big of a deal because we have metadata filtering and good retrieval, or is it a big deal?\"</p> <p>This concern tends to be model-specific. Foundation model companies have been optimizing for recall after discovering the \"needle in a haystack\" problem, where models struggled to find specific information buried in large contexts. While this improved recall, it made models more sensitive to precision issues.</p> <p>The real risk now is that low precision might hurt your language model's ability to reason correctly. When dealing with irrelevant data, consider whether it's \"adversarially irrelevant\" - is the data actually conflicting rather than just unnecessary?</p> <p>For example, in construction documentation, you might have an email saying a wall is yellow, an architect's note saying it's blue, and a text message claiming it's purple. In these cases, you need to establish authority hierarchies or time-based weighting to resolve conflicts.</p> <p>Key Takeaway: The impact of irrelevant data depends on whether it's merely unnecessary or actively conflicting. Modern models are optimized for high recall but can be sensitive to precision issues, so conflicting information can be particularly problematic.</p>"},{"location":"office-hours/cohort3/week-2-2/#what-metrics-should-i-monitor-for-retrieval-quality-in-production","title":"What metrics should I monitor for retrieval quality in production?","text":"<p>When asked about vector databases providing retrieval quality measurements, I recommended focusing on metrics you can monitor yourself rather than trusting vendor-provided metrics.</p> <p>Consider tracking the average cosine distance of your queries over time. If this metric suddenly changes, it could indicate a shift in your data or user behavior. For example, in a previous recommendation system I built, we monitored cosine distance between products and noticed a sudden drop. After investigating by segmenting the data by signup date, gender, and life stage, we discovered we had onboarded many young users through a Super Bowl ad campaign who couldn't afford our $300 clothing items.</p> <p>You might also monitor average re-ranker scores and look for changes over time or across different user segments. These metrics are more valuable than arbitrary tests created by vector database providers.</p> <p>Key Takeaway: Focus on monitoring changes in metrics like average cosine distance rather than absolute values, and segment your analysis by relevant variables to identify the root causes of any shifts.</p>"},{"location":"office-hours/cohort3/week-2-2/#whats-the-best-approach-for-processing-complex-technical-documentation","title":"What's the best approach for processing complex technical documentation?","text":"<p>A participant working on processing technical manuals for question answering described their current approach: \"We're leveraging the internal structure of the document, taking sections, splitting them, but including the hierarchy of titles - section, chapter, and manual title. But it feels naive to me.\"</p> <p>This challenge is common when dealing with structured technical content. One approach is to use traversal rather than pure semantic search - similar to how code-based agents navigate repositories. Instead of embedding everything, the system can navigate the document structure to find relevant information.</p> <p>For example, when working with Brazilian tax codes (400-page PDFs), we implemented a system that traversed the documents using a combination of semantic search, full-text search, and grep-like tools. The system could navigate from main sections to specific appendices to find relevant information.</p> <p>The key insight is that traversal is still a form of retrieval. As you collect traversal data, you can use it to improve your embedding models, potentially reducing the need for complex traversal in the future.</p> <p>Key Takeaway: For complex technical documentation, consider combining semantic search with structural traversal. Use the document's inherent organization to guide your retrieval process, and collect this data to improve your embedding models over time.</p>"},{"location":"office-hours/cohort3/week-2-2/#should-i-build-complex-hierarchical-structures-for-document-retrieval","title":"Should I build complex hierarchical structures for document retrieval?","text":"<p>When discussing whether to build sophisticated graph structures for document retrieval, I emphasized the importance of getting to usable data quickly: \"My metric is: whatever I build should be the thing that gets me to 10,000 rows in a CSV file.\"</p> <p>Rather than spending extensive time modeling tax laws as a graph or building complex hierarchical indexes upfront, I recommend chunking everything, getting a working system, understanding the problems, and creating examples. This data-driven approach allows you to identify patterns that can inform more sophisticated solutions later.</p> <p>The better lesson in AI development is that segmenting and solving individual problems can help you make progress now, while preparing unified datasets that will allow you to combine approaches when technology improves. This mirrors the evolution of speech-to-text systems, which initially required separate stages for waveforms, phonemes, and words before end-to-end solutions became viable.</p> <p>Key Takeaway: Focus on collecting data and building working solutions rather than perfect architectures. The insights gained from real usage will guide your more sophisticated implementations later.</p>"},{"location":"office-hours/cohort3/week-2-2/#how-should-we-think-about-the-relationship-between-rag-and-agents","title":"How should we think about the relationship between RAG and agents?","text":"<p>An interesting perspective emerged during our discussion: \"RAG is like the superpower for AI right now.\" We explored how the boundaries between RAG and other AI capabilities are blurring, with one participant noting \"grep is RAG\" - highlighting that any method of retrieving context for an AI system shares fundamental similarities with RAG.</p> <p>I've been considering whether we should rename the course to focus on \"RAG applications\" since modern AI systems are essentially exposing a portfolio of tools to agents. Whether you're using semantic search or a grep-like function to pull in relevant code, you're still finding information to enhance the context available to the model.</p> <p>The core principle remains the same: \"It has to be put into the context at the right time so that you can get the response correct.\" This perspective frames RAG not just as a specific technique but as a fundamental paradigm for augmenting AI capabilities with relevant information.</p> <p>Key Takeaway: The distinction between RAG and other AI augmentation approaches is increasingly blurred. The fundamental goal is getting the right information into the context at the right time, regardless of the specific retrieval mechanism.</p>"},{"location":"office-hours/cohort3/week-2-2/#whats-the-value-of-the-office-hours-format-for-learning","title":"What's the value of the office hours format for learning?","text":"<p>Several participants expressed surprise at how valuable they found the office hours sessions. One noted, \"I thought they wouldn't be useful, but I'm surprised with the quality of the questions being asked.\"</p> <p>These interactive sessions provide an opportunity to hear how others are applying the course concepts and to discuss specific challenges that might not be covered in the structured lectures. The questions often reveal practical implementation issues that many participants are facing but might not have articulated themselves.</p> <p>The conversations also help connect theoretical concepts to real-world applications, making the material more concrete and actionable. For example, our discussion about monitoring cosine distances in production systems provided a practical perspective on evaluation that complements the more structured content on evaluation frameworks.</p> <p>Key Takeaway: Interactive learning formats like office hours provide valuable perspectives that complement structured course content, particularly for understanding how concepts apply to diverse real-world scenarios.</p>"},{"location":"office-hours/cohort3/week-2-2/#how-should-we-pace-the-course-to-maximize-learning","title":"How should we pace the course to maximize learning?","text":"<p>When asked about the pacing of the course, I acknowledged that many participants are finding it challenging to keep up with all the material. One suggestion was to include a week in the middle with no new material to allow people to catch up, which received positive feedback.</p> <p>I noted that Week 3 is intentionally lighter, with only a 40-minute video and no notebooks, designed as a catch-up week. However, I recognized that I should make this more explicit to help participants plan their time.</p> <p>The six-week format provides more depth than a one-week intensive course would allow, but it requires consistent engagement to get the full benefit. Finding the right balance between comprehensive coverage and manageable pacing remains a challenge.</p> <p>Key Takeaway: Learning complex technical skills requires finding the right balance between depth of content and time for absorption and practice. Building explicit catch-up periods into courses can help participants manage their learning journey more effectively.</p>"},{"location":"office-hours/cohort3/week-2-2/#what-can-we-learn-from-leaked-system-prompts-like-anthropics-claude","title":"What can we learn from leaked system prompts like Anthropic's Claude?","text":"<p>One participant asked about the recently leaked Anthropic Claude prompt, which was reportedly around 30,000 tokens: \"Where does it leave room for actual content to be processed? Is it even realistic or just hype?\"</p> <p>I wasn't surprised by the size of this prompt, explaining that it makes sense for the Claude web app experience, which includes tools for fetching information. The API version likely has a smaller prompt, but it's still substantial if web search capabilities are included.</p> <p>This reveals how much can be done through prompting without changing model weights. It's remarkable that models can now process 30,000 token system messages when just two years ago, the entire context was limited to 32K tokens.</p> <p>The existence of such extensive system prompts raises questions about where certain capabilities should reside - in the prompt or in the model weights. For example, if a fetch tool were baked into the model weights, what would happen if you named your custom tool \"web_search\" and the model tried to call a hardcoded \"fetch\" function?</p> <p>Key Takeaway: Large system prompts demonstrate how much functionality can be implemented through instructions rather than model training. This creates flexibility but also raises important questions about the boundary between prompt engineering and model architecture.</p> <p>FAQs</p>"},{"location":"office-hours/cohort3/week-2-2/#how-can-i-balance-the-course-with-my-regular-work-schedule","title":"How can I balance the course with my regular work schedule?","text":"<p>Many participants find balancing the course with their day job challenging. The course requires time for watching lectures, completing exercises, and participating in discussions. Consider setting aside specific time slots in your schedule for course activities and prioritize what aspects are most valuable to you. Remember that you can always revisit materials after the course ends if you're unable to complete everything during the active weeks.</p>"},{"location":"office-hours/cohort3/week-2-2/#will-course-materials-remain-available-after-the-course-ends","title":"Will course materials remain available after the course ends?","text":"<p>Yes, all course materials including videos, notebooks, and exercises will remain accessible after the course concludes. The Slack channel will also stay active, allowing you to continue asking questions and collaborating with other participants. While structured bi-weekly meetings won't continue, you'll still have access to all resources and can work through them at your own pace.</p>"},{"location":"office-hours/cohort3/week-2-2/#how-active-will-the-community-be-after-the-course-ends","title":"How active will the community be after the course ends?","text":"<p>The community's activity level will largely depend on participant engagement. While there won't be formal scheduled sessions after the course, the instructors will check the Slack channel regularly and share relevant resources. The value you get from the community will correlate with how much you contribute to it. Many valuable connections happen through direct messages rather than in public channels.</p>"},{"location":"office-hours/cohort3/week-2-2/#is-there-a-recommended-approach-to-catching-up-if-im-behind","title":"Is there a recommended approach to catching up if I'm behind?","text":"<p>Week 3 is intentionally lighter with only an hour-long video and no notebooks, providing an opportunity to catch up on previous materials. The course team is considering adding a \"break week\" in future cohorts to give participants more time to process information and complete exercises. Don't worry if you can't complete everything during the course timeframe\u2014the materials will remain available afterward.</p>"},{"location":"office-hours/cohort3/week-2-2/#how-can-i-apply-what-im-learning-to-my-actual-projects","title":"How can I apply what I'm learning to my actual projects?","text":"<p>The most effective way to apply course concepts to your work is to start with the exercises to build foundational understanding, then gradually incorporate techniques into your projects. Some participants find it helpful to wait until after the course to fully implement what they've learned, as this allows them to focus on understanding the concepts first. For more personalized guidance, reaching out about consulting engagements after the course can be beneficial.</p>"},{"location":"office-hours/cohort3/week-2-2/#whats-the-best-approach-to-rag-retrieval-augmented-generation-for-technical-documentation","title":"What's the best approach to RAG (Retrieval-Augmented Generation) for technical documentation?","text":"<p>When working with technical documentation, consider these approaches:</p> <ol> <li> <p>Start by focusing on getting retrieval right before worrying about other aspects</p> </li> <li> <p>Use document structure (sections, chapters, titles) to improve chunking</p> </li> <li> <p>Consider a combination of semantic search, full-text search, and traversal approaches</p> </li> <li> <p>Monitor metrics like cosine distance to evaluate retrieval quality</p> </li> <li> <p>Begin with a simple implementation that works for most of your documents rather than trying to solve every edge case immediately</p> </li> </ol>"},{"location":"office-hours/cohort3/week-2-2/#how-should-i-handle-irrelevant-data-in-my-vector-database","title":"How should I handle irrelevant data in my vector database?","text":"<p>The impact of irrelevant data depends on your specific model and use case. Modern language models are optimized for high recall, which can make them sensitive to low precision issues. Consider whether irrelevant data is merely noise or actually conflicting/adversarial. For conflicting information, you may need to implement authority rules (like prioritizing certain document types) or time-based weighting. Rather than trying to perfect your data filtering upfront, start with a basic implementation, collect examples, and iterate based on actual performance.</p>"},{"location":"office-hours/cohort3/week-2-2/#are-vector-databases-providing-built-in-retrieval-quality-measurements","title":"Are vector databases providing built-in retrieval quality measurements?","text":"<p>While some vector databases may offer metrics, it's generally better to implement your own monitoring. Focus on tracking metrics like average cosine distance of your queries and monitor how these change over time or across different user segments. This approach allows you to detect shifts in data patterns or user behavior that might affect retrieval quality. Looking at changes in these metrics is often more valuable than the absolute values themselves.</p>"},{"location":"office-hours/cohort3/week-3-1/","title":"Week 3, Office Hour (June 5)","text":"<p>Study Notes:</p> <p>I hosted a focused office hours session addressing questions about re-ranking models, embedding model fine-tuning, and data strategies for RAG systems. Here are my insights on selecting appropriate models, creating effective training datasets with hard negatives, and making strategic decisions about compute allocation in retrieval systems.</p>"},{"location":"office-hours/cohort3/week-3-1/#what-open-source-re-ranking-models-work-well-for-fine-tuning","title":"What open-source re-ranking models work well for fine-tuning?","text":"<p>When selecting re-ranking models for fine-tuning, I believe the right approach depends on your specific constraints and data volume. For many scenarios, the BGE models from the Beijing Academy of Artificial Intelligence (BAAI) have proven quite stable and easy to fine-tune with datasets of around 100,000 examples.</p> <p>The model selection process isn't about finding a single \"best\" model, but rather systematically testing different options against your specific requirements. I've found that BAAI models tend to have more predictable loss curves during training compared to some alternatives where parameters might need more careful tuning.</p> <p>Your model selection should consider several factors:</p> <ul> <li>Latency requirements (5-10 seconds for total search and retrieval in your case)</li> <li>Hosting constraints (on-premises deployment for medical applications)</li> <li>The volume of training data available</li> <li>The trade-off between performance and computational cost</li> </ul> <p>For on-premises medical applications requiring self-hosting, I'd recommend starting with the BGE models and systematically testing different configurations. The process is inherently experimental - you'll likely need to train numerous models with different parameters and dataset preparations before finding the optimal combination.</p> <p>Key Takeaway: Don't get fixated on finding the \"perfect\" model architecture. Instead, create a systematic testing framework where you can evaluate multiple models against your specific constraints of latency, hosting requirements, and performance needs.</p>"},{"location":"office-hours/cohort3/week-3-1/#how-should-i-approach-creating-training-datasets-for-embedding-models-versus-re-rankers","title":"How should I approach creating training datasets for embedding models versus re-rankers?","text":"<p>The fundamental difference between training embedding models and re-rankers lies in how they handle similarity scores. Embedding models typically work with triplets (this is similar to that, different from something else) with binary scores of 1 or -1. Re-rankers, however, can be more nuanced with scores like 0.8 or 0.9, allowing for finer distinctions between results.</p> <p>I've found that focusing first on building a strong dataset for your embedding model is usually the most efficient approach. If you're currently only training on positive examples, incorporating negative examples will dramatically improve performance - we're talking about a 30% improvement rather than just 6%.</p> <p>For creating effective negative examples, I recommend being strategic rather than random:</p> <p>In a financial context I worked with recently, we were distinguishing between \"fuel\" (for employee vehicle reimbursements) and \"equipment fuel\" (for company vehicles like tractors). Simply using random negative examples wouldn't help the model learn this subtle distinction. Instead, we created hard negatives by:</p> <ol> <li> <p>Taking a transaction from one category</p> </li> <li> <p>Finding another transaction in the same category as a positive example</p> </li> <li> <p>Using embedding search to find the most similar transaction from a different category as the negative example</p> </li> </ol> <p>This approach forces the model to learn the meaningful boundaries between similar but distinct concepts. For your medical data with abbreviations that have different meanings in different contexts, you could apply a similar strategy - finding examples where the same abbreviation appears in different contexts to create hard negatives.</p> <p>Key Takeaway: Including well-crafted negative examples in your training data is crucial for model performance. Focus on creating \"hard negatives\" that challenge the model to learn subtle distinctions rather than obvious differences.</p>"},{"location":"office-hours/cohort3/week-3-1/#what-are-effective-sources-of-negative-examples-for-training-data","title":"What are effective sources of negative examples for training data?","text":"<p>Some of the most valuable negative examples come from user interactions that indicate a mismatch between what the system thought was relevant and what the user actually found useful. I've implemented several approaches across different domains:</p> <p>For citation systems:</p> <p>When experts review citations and delete ones they find irrelevant, saying \"regenerate without this document because it's misleading\" - that's a perfect negative example. The question and the deleted chunk form a negative pair for training.</p> <p>For recommendation systems:</p> <ul> <li>In content recommendation, when a salesperson deletes a suggested blog post from an automated email, that's a negative example</li> <li>In music services like Spotify, skipping a song is a weaker negative signal than deleting it from a playlist</li> <li>In e-commerce, items that are purchased together but later returned indicate a false positive that can be used as a negative example</li> </ul> <p>For your medical context, you might consider:</p> <ul> <li>Tracking when users reject or ignore certain retrieved chunks</li> <li>Using expert feedback to identify misleading or irrelevant retrievals</li> <li>Creating synthetic data with language models to generate examples where abbreviations are used in different contexts</li> </ul> <p>The key insight is that these high-signal negative examples often come from cases where the system initially thought it was right but was ultimately wrong - these boundary cases are extremely valuable for training.</p> <p>Key Takeaway: The most valuable negative examples often come from user interactions that indicate a mismatch between system predictions and actual relevance. Design your system to capture these signals and incorporate them into your training data.</p>"},{"location":"office-hours/cohort3/week-3-1/#how-should-i-think-about-compute-allocation-in-retrieval-systems","title":"How should I think about compute allocation in retrieval systems?","text":"<p>When designing retrieval systems, especially for complex documents like legal texts or medical records, I think about it as a fundamental trade-off: where do I want to allocate my compute resources? This is essentially a decision between investing compute at \"write time\" (indexing) versus \"read time\" (retrieval).</p> <p>There are two main approaches to consider:</p> <p>Contextual retrieval (compute-heavy at write time):</p> <ul> <li>Rewrite text chunks during indexing to include all necessary context</li> <li>For example, converting \"He is unhappy with her\" to \"Jason the doctor is unhappy with Patient X\"</li> <li>This makes retrieval simpler but requires more upfront processing</li> <li>Anthropic has published about this approach for their Claude assistant</li> </ul> <p>Tool use and traversal (compute-heavy at read time):</p> <ul> <li>Store minimal context in each chunk</li> <li>Use additional compute during retrieval to navigate between related chunks</li> <li>Similar to how Cursor IDE navigates code by finding functions and then examining surrounding context</li> <li>This approach is more flexible but can feel slower to users</li> </ul> <p>For your medical application where the data is self-contained (not requiring external information), and where you want to minimize user wait time, I'd lean toward investing more compute at indexing time. This is especially true since you can run indexing jobs overnight without affecting user experience.</p> <p>The decision also relates to data normalization - do you want to denormalize data by including related information in each chunk (like adding phone numbers whenever a person is mentioned), or keep information separate and join it at retrieval time? The answer depends on your specific use case and resource constraints.</p> <p>Key Takeaway: Frame your retrieval system design as a strategic decision about compute allocation. For medical applications with self-contained data and latency constraints, investing more compute at indexing time to create context-rich chunks will likely provide a better user experience.</p>"},{"location":"office-hours/cohort3/week-3-1/#what-determines-the-complexity-of-the-architecture-i-should-use","title":"What determines the complexity of the architecture I should use?","text":"<p>I believe the volume and quality of your training data should be the primary factor determining architectural complexity. This is a principle I emphasize repeatedly: your dataset size dictates what approaches make sense.</p> <p>As a general guideline:</p> <ul> <li>With ~100 examples: Use few-shot prompting</li> <li>With thousands of examples: Fine-tune embedding models</li> <li>With millions of examples: Fine-tune language models</li> </ul> <p>The data volume determines what's feasible. If you told me you had a million examples, I'd probably just train a language model directly and worry about everything else later. With limited data, you need to be more strategic about targeting specific challenges like medical abbreviations with ambiguous meanings.</p> <p>This is why I'm skeptical when I see engineers celebrating 98% accuracy on their first model - it usually means they've created a test set that's too easy. As your model improves, you should be making your test data harder by finding more challenging examples. If your retrieval dashboard is showing 95% accuracy, that's a sign you need to create harder test cases.</p> <p>Key Takeaway: Let your data volume guide your architectural decisions. With limited data, focus on targeted improvements to specific challenges rather than complex architectures. As your model improves, continuously create harder test cases to drive further improvement.</p>"},{"location":"office-hours/cohort3/week-3-1/#how-can-i-improve-my-system-when-i-dont-yet-have-real-user-feedback","title":"How can I improve my system when I don't yet have real user feedback?","text":"<p>Without real user feedback, you can still make significant progress through synthetic data generation and expert knowledge. For your medical abbreviation challenge, you could:</p> <ol> <li> <p>Identify known ambiguous abbreviations in medical contexts</p> </li> <li> <p>Use language models like GPT-4 to generate synthetic examples showing these abbreviations in different contexts</p> </li> <li> <p>Have medical experts validate these examples or create additional ones</p> </li> <li> <p>Build a curated dataset of hard negatives focusing on these ambiguities</p> </li> </ol> <p>This approach lets you systematically address known challenges before deployment. Once you have real users, you can implement feedback mechanisms to capture when they reject or modify system outputs, creating a virtuous cycle of improvement.</p> <p>Remember that as your system improves, you need to continuously create harder test cases. If you're scoring 95% accuracy, it's not because your AI is exceptional - it's because your test data isn't challenging enough. The goal is to build a dataset that pushes the boundaries of what your system can handle.</p> <p>Key Takeaway: Before having real users, leverage synthetic data generation and expert knowledge to create challenging test cases. Design your system to capture user feedback from the start, as this will become your most valuable source of training data once deployed.</p> <p>FAQs</p>"},{"location":"office-hours/cohort3/week-3-1/#what-open-source-re-ranking-models-are-recommended-for-fine-tuning","title":"What open-source re-ranking models are recommended for fine-tuning?","text":"<p>The Beijing Academy of Artificial Intelligence (BAAI) models, such as BGE re-ranker v3, are often good choices for fine-tuning. These models have proven to be stable during the training process and work well with the sentence transformers library. When selecting a model, consider your specific data volume and performance requirements. Testing multiple models with your dataset is ultimately the best approach to finding the optimal solution for your specific use case.</p>"},{"location":"office-hours/cohort3/week-3-1/#how-should-i-approach-model-selection-for-my-re-ranking-needs","title":"How should I approach model selection for my re-ranking needs?","text":"<p>Start by exploring models that perform well on MTAB benchmarks on Hugging Face. Consider your constraints around latency, hosting requirements, and data volume. With the right dataset, the process becomes more about searching the model space to find what works best for your specific scenario. For medical or specialized domains, you'll want to test various models against your specific data to determine which one provides the best performance-to-cost ratio.</p>"},{"location":"office-hours/cohort3/week-3-1/#what-are-the-different-types-of-re-ranking-models-available","title":"What are the different types of re-ranking models available?","text":"<p>Re-rankers can be embedding models, cross-encoder models, or even large language models (LLMs). Each has different characteristics: embedding models typically classify results as relevant or not relevant, cross-encoders can provide more nuanced scoring (like 0.8 vs 0.9 relevance), and LLM-based re-rankers (like those used by Exa) can provide sophisticated re-ranking but may have higher latency. Your choice depends on your specific requirements and constraints.</p>"},{"location":"office-hours/cohort3/week-3-1/#training-data-for-embedding-and-re-ranking-models","title":"Training Data for Embedding and Re-Ranking Models","text":"<p>How important are negative examples when training embedding models?</p> <p>Negative examples are extremely valuable when training embedding models. Including hard negatives (examples that are similar but should be classified differently) can improve performance by 30% or more compared to training with only positive examples. These hard negatives help the model learn subtle distinctions that are crucial for accurate retrieval, especially in specialized domains like medicine or legal text.</p>"},{"location":"office-hours/cohort3/week-3-1/#whats-an-effective-way-to-generate-hard-negative-examples","title":"What's an effective way to generate hard negative examples?","text":"<p>One effective approach is to find examples that are semantically similar but belong to different categories. For instance, if you're working with medical abbreviations that have different meanings in different contexts, you could create examples showing the same abbreviation used in different medical specialties. Another method is to use embedding search to find the most similar items that should be classified differently, rather than just using random negative examples.</p>"},{"location":"office-hours/cohort3/week-3-1/#how-can-user-feedback-be-leveraged-to-improve-retrieval-models","title":"How can user feedback be leveraged to improve retrieval models?","text":"<p>User interactions provide valuable signals for creating training data. For example, if users delete a citation or regenerate an answer without a specific document, that's a strong signal that the document was irrelevant (a negative example). Similarly, if users consistently skip or remove certain recommendations, those can be used as negative examples in your training data. These real-world signals often create the most valuable training examples.</p>"},{"location":"office-hours/cohort3/week-3-1/#what-factors-should-i-consider-when-designing-a-retrieval-system-architecture","title":"What factors should I consider when designing a retrieval system architecture?","text":"<p>Consider where to allocate your compute resources: at indexing/write time or at query/read time. If you invest more in preprocessing your data (like contextual retrieval where you rewrite chunks to include necessary context), your query-time processing can be simpler and faster. Alternatively, you can keep preprocessing minimal and implement more sophisticated query-time processing like document traversal. Your decision should balance user experience requirements, cost constraints, and the specific characteristics of your data.</p>"},{"location":"office-hours/cohort3/week-3-1/#how-do-i-handle-context-in-long-documents-where-paragraphs-depend-on-previous-content","title":"How do I handle context in long documents where paragraphs depend on previous content?","text":"<p>There are two main approaches: (1) Contextual retrieval, where you rewrite text chunks at indexing time to include all necessary context, making each chunk self-contained; or (2) Document traversal, where your system can navigate through the document at query time to gather needed context. The first approach frontloads the processing cost but enables faster query responses, while the second requires more complex query-time processing but minimizes preprocessing.</p>"},{"location":"office-hours/cohort3/week-3-1/#what-hosting-considerations-should-i-keep-in-mind-for-medical-applications","title":"What hosting considerations should I keep in mind for medical applications?","text":"<p>For medical applications, especially in European contexts like the Netherlands, you'll likely need to host models on your own hardware or on-premises at hospitals. This requires selecting models that can run efficiently on your available hardware while meeting your latency requirements. Consider models that can be fully self-hosted without external API dependencies, and ensure your architecture complies with relevant healthcare data regulations.</p>"},{"location":"office-hours/cohort3/week-4-1/","title":"Week 4, Office Hour 1 (June 10)","text":"<p>Study Notes:</p> <p>I hosted an office hours session focused on practical RAG implementation challenges, data management strategies, and the business aspects of AI consulting. Here are my insights on handling evaluation data, choosing the right tools, and thinking strategically about AI business models and value capture.</p>"},{"location":"office-hours/cohort3/week-4-1/#how-should-we-approach-evaluation-data-collection-for-rag-systems","title":"How should we approach evaluation data collection for RAG systems?","text":"<p>One participant was struggling with exporting conversation data from Langsmith for evaluation purposes. This highlights a common challenge with many tracing tools - they're often better at collecting data than exporting it in useful formats.</p> <p>When Langsmith or similar tools create export difficulties, I recommend two alternative approaches:</p> <ol> <li>Direct database storage: Instead of relying on tracing software, consider saving queries directly to your database as the application runs.    \"This is something we do all the time - we just write the question, answer pairs, or chunks to Postgres. That way, we can build UI on top of that database rather than trying to export data out of tools like Langsmith.\"</li> <li>Create a simple, wide database table that includes:</li> <li>Session ID</li> <li>User ID</li> <li>Query text</li> <li>Retrieved chunks</li> <li>Generated answer</li> </ol> <p>This approach gives you direct access to your data without depending on third-party export functionality, which can be unreliable. It's like building your own analytics system rather than trying to export from something like Data Dog for analysis.</p> <p>Key Takeaway: While tracing tools like Langsmith and Log Fire are valuable for telemetry, consider implementing your own database storage for evaluation data to avoid export headaches and gain more control over your analysis process.</p>"},{"location":"office-hours/cohort3/week-4-1/#which-models-should-we-use-for-different-rag-applications","title":"Which models should we use for different RAG applications?","text":"<p>When choosing between models like GPT-4, GPT-4 Turbo, or GPT-3.5, I've observed different selection patterns based on the task's importance and time constraints:</p> <p>For high-value applications where accuracy is critical (like financial due diligence with 44 data rooms generating reports for clients paying $200,000 annually), companies often default to GPT-4 because \"if it is just 2% better, it'll be worth it.\"</p> <p>For applications requiring speed, GPT-3.5 or GPT-4 are common choices.</p> <p>Many developers are now using Gemini for RAG applications because its large context window allows for less precision in retrieval: \"You can just be really frivolous with how much context you use.\"</p> <p>The decision often comes down to the stakes involved rather than technical benchmarks. For example, when helping sales teams craft follow-up emails containing offers, we use GPT-4 because the potential revenue impact justifies the additional cost.</p> <p>Key Takeaway: Model selection should be driven by business value rather than technical specifications alone. For high-stakes applications where even small improvements matter, use the most capable model available. For less critical applications, prioritize speed and cost-efficiency.</p>"},{"location":"office-hours/cohort3/week-4-1/#how-can-we-enhance-report-generation-with-visual-elements","title":"How can we enhance report generation with visual elements?","text":"<p>One exciting development in RAG applications is the integration of visual elements into generated reports. I'm currently working with a company on two key improvements:</p> <ol> <li>Supporting mermaid diagrams in reports to visualize relationships and processes</li> <li>Intelligently adding relevant images to reports</li> </ol> <p>For example, in a construction permitting application, this could mean automatically including screenshots of potential errors in blueprints with accompanying explanations: \"If in a report of predicted potential errors that you should pay attention to on your project, it would actually take a screenshot of the error in the PDF of the blueprint, and then have a narrative around it.\"</p> <p>This approach dramatically increases the value of generated reports by combining visual and textual information, making complex issues immediately understandable to users.</p> <p>Key Takeaway: The next frontier in RAG applications involves intelligently incorporating visual elements like diagrams and contextual images to enhance understanding and provide more comprehensive analysis.</p>"},{"location":"office-hours/cohort3/week-4-1/#how-should-we-manage-expectations-around-ai-capabilities","title":"How should we manage expectations around AI capabilities?","text":"<p>Managing expectations is one of the biggest challenges when implementing AI systems, especially with clients who have either unrealistic expectations or excessive skepticism.</p> <p>For construction applications, one participant described their approach: \"We try to explain to people that ultimately in our field, you're an architect or a structural engineer. It's your stamp on the docs. You're making the call. We're just here to provide suggestions and things to look out for.\"</p> <p>This aligns with my experience working with large enterprises, where much of my consulting work involves \"dealing with the personality of the CEO\" who might want AI to be a major theme at the next sales conference without understanding the practical limitations.</p> <p>The most effective approach is focusing on how AI can augment human decision-making rather than replace it. For example, having the LLM run simulations and help humans interpret the results is more realistic than promising fully autonomous systems.</p> <p>Key Takeaway: Set clear boundaries around AI capabilities by positioning your system as a decision support tool rather than an autonomous decision-maker. Be explicit about where human judgment remains essential, especially in high-stakes domains like construction or finance.</p>"},{"location":"office-hours/cohort3/week-4-1/#whats-your-vision-for-building-open-source-ai-tools","title":"What's your vision for building open-source AI tools?","text":"<p>When asked about my vision for building AI tools, I explained that my approach differs from the typical venture-backed startup model:</p> <p>\"Before it was okay, consulting can drive revenue that allows us to do open source work. The open source projects don't need to raise venture capital or figure out how to monetize, which changes the nature of the code.\"</p> <p>This model allows me to create a portfolio of small, useful tools without worrying about monetization. The course serves as a way to connect with practitioners across different industries and identify common challenges:</p> <p>\"The most I ever did was like 7 clients in a month, and that was kind of a hazy period of my life where I have no memory of what happened. Whereas with the course, I can do these office hours - 10 people show up, great. I can understand how this permitting thing goes, maybe some architectural things, some construction things, some supply chain stuff.\"</p> <p>This broader exposure helps me identify patterns across industries, like the common need for better report generation or specialized table parsing, which informs both my consulting work and open-source development.</p> <p>Key Takeaway: By funding open-source development through consulting and courses rather than venture capital, I can focus on building genuinely useful tools without the pressure to monetize every component, leading to more sustainable and practical solutions.</p>"},{"location":"office-hours/cohort3/week-4-1/#how-should-we-think-about-pricing-and-value-capture-for-ai-systems","title":"How should we think about pricing and value capture for AI systems?","text":"<p>One of the most exciting developments I see in AI is the evolution of pricing models away from usage-based metrics toward outcome-based pricing:</p> <p>\"I'm personally curious about pricing the work that LLMs do. A lot of systems right now are being priced on usage. I'm really excited about what it would mean to have a system that has so much accountability that you can price on the outcome it delivers.\"</p> <p>I shared an example of a company that uses voice AI to make calls to car owners on behalf of dealerships. Under a usage-based model, the calls that make the most money are often those that waste time with confusion and errors. But with an outcome-based model, the incentives change dramatically:</p> <p>\"If you change the model to say, 'We want to take 3% of the mechanic's cost,' then it becomes, 'What if we had systems that are intelligently doing upsells? What if we intelligently figure out the right time and try to load balance the mechanic?'\"</p> <p>This shift changes the fundamental question from \"How much am I willing to pay to process one PDF file?\" (maybe 30 cents) to \"Under what circumstances would I be willing to pay $20 to process a PDF?\" The answer depends on the business value created.</p> <p>Key Takeaway: The future of AI pricing will likely move from usage-based models (tokens, API calls) to outcome-based models where vendors are compensated based on the business value they create. This will drive investment in higher-quality systems that optimize for results rather than minimizing usage.</p>"},{"location":"office-hours/cohort3/week-4-1/#will-ai-capabilities-eventually-be-built-into-platforms-or-remain-in-applications","title":"Will AI capabilities eventually be built into platforms or remain in applications?","text":"<p>When asked whether AI capabilities will eventually be absorbed into platforms rather than remaining in applications, I suggested it depends on the time horizon:</p> <p>\"On any reasonable time horizon, it will probably just be the applications. The limiting factor is that for any specific application, we don't actually have the training data to bake this back into the model.\"</p> <p>I referenced the \"bitter lesson\" in AI, which shows that when you have enough data and compute, general approaches tend to outperform specialized ones. However, we first need applications to generate the necessary data:</p> <p>\"We still have to build these applications as sort of sensors to create this data. And then, once we do, we can kind of sidestep the next innovation.\"</p> <p>This is similar to how speech recognition evolved from complex phoneme-based systems to end-to-end models, but only after platforms like YouTube created enough data to make this possible.</p> <p>\"We had to build YouTube to produce enough data to get to a world where now we can train the GPT-4 model. So we still have to build these applications as sensors to create this data.\"</p> <p>Key Takeaway: While AI capabilities will eventually be absorbed into platforms, we first need to build applications that generate the necessary training data. This creates a cycle where applications serve as data collection mechanisms that eventually enable more general-purpose AI systems.</p>"},{"location":"office-hours/cohort3/week-4-1/#how-might-ai-transform-business-models-and-value-chains","title":"How might AI transform business models and value chains?","text":"<p>I believe AI will fundamentally change how businesses capture value, potentially shifting from software-as-a-service models to more integrated approaches:</p> <p>\"Everything stops becoming SaaS budget and it's all headcount budget. If you absorb this entire 'dealership calls car owner to get them in the mechanic' thing, at some point you're just a sales guy.\"</p> <p>This could lead to companies expanding vertically to capture more of the value chain:</p> <p>\"Why don't you just own the entire value chain? Because then you can really price on the outcome that you're trying to deliver rather than just tokens.\"</p> <p>While this approach means taking on additional complexity (like owning car mechanics with \"all the pros and cons\"), it allows for capturing more of the value created. This is similar to how I view the difference between writers who charge by word versus those who are paid based on qualified leads that convert.</p> <p>\"If there was an agent that's like, 'We'll just take all your phone calls and turn them into blog posts, and we only get charged a commission of course sales,' I would probably be really happy with that.\"</p> <p>Key Takeaway: AI may drive a shift from software companies selling tools to companies that own entire value chains and are compensated based on business outcomes. This will require building systems that connect previously separate data streams to create end-to-end accountability.</p>"},{"location":"office-hours/cohort3/week-4-1/#whats-the-most-valuable-data-for-future-ai-development","title":"What's the most valuable data for future AI development?","text":"<p>The most valuable data for AI development has evolved over time:</p> <p>\"When I started, it was physics. And then it's like, 'Well, we're running out of sensors, but the next sensor is going to cost us 10 billion dollars.' So I went to Facebook - what's every post and comment and Facebook group and the social graph?\"</p> <p>Now, I believe the most valuable data will be how humans interact with AI:</p> <p>\"How humans use AI will be the most interesting dataset. And then in 10 years, it'll be how AI talks to AI. Most of the data produced will just be AI talking to AI.\"</p> <p>This is why I'm particularly interested in working with companies that have large proprietary datasets in specialized domains:</p> <p>\"Someone was like, 'Oh, we have the last 40 years of investment decisions.' I was like, 'What?' Now I'm willing to pay so much to process this. Let's actually think about what the schemas look like and how to design this system.\"</p> <p>These unique datasets offer opportunities to create specialized tools that can extract insights that general models can't access without the proper context and structure.</p> <p>Key Takeaway: The most valuable data is shifting from general internet content to human-AI interactions and eventually AI-to-AI interactions. Companies with large proprietary datasets in specialized domains are particularly well-positioned to create value with AI systems tailored to their unique information.</p> <p>FAQs</p>"},{"location":"office-hours/cohort3/week-4-1/#what-tools-are-recommended-for-tracing-and-evaluations-in-ai-applications","title":"What tools are recommended for tracing and evaluations in AI applications?","text":"<p>While Langsmith is commonly used, some users experience technical issues with data exports. Alternative options include Brain Trust for evaluations and Log Fire for tracing. The choice often depends on your specific needs and existing partnerships. For simpler implementations, consider storing question-answer pairs directly in a database rather than relying on third-party tracing software, which gives you more control and easier access to your data.</p>"},{"location":"office-hours/cohort3/week-4-1/#how-should-i-approach-data-collection-for-evaluating-my-ai-application","title":"How should I approach data collection for evaluating my AI application?","text":"<p>Start by creating an evaluation dataset from real user interactions. This can be done by exporting traces from tools like Langsmith or by directly storing question-answer pairs in your database. Once you have real data, you can generate synthetic questions to expand your test set. Focus on collecting both the user queries and your system's responses, along with any relevant context like retrieved document chunks, to enable comprehensive evaluation.</p>"},{"location":"office-hours/cohort3/week-4-1/#which-language-models-are-best-for-rag-retrieval-augmented-generation-applications","title":"Which language models are best for RAG (Retrieval-Augmented Generation) applications?","text":"<p>The choice depends on your specific requirements. GPT-4 is commonly used for standard implementations, while GPT-3.5 may be sufficient for applications where speed is critical. Gemini is popular for RAG applications due to its large context window, allowing you to include more retrieved content without worrying about token limits. For high-stakes applications where accuracy is paramount, GPT-3.5 is sometimes preferred despite being older, as it can be more reliable for certain use cases.</p>"},{"location":"office-hours/cohort3/week-4-1/#how-should-i-approach-improving-my-ai-applications-performance","title":"How should I approach improving my AI application's performance?","text":"<p>Focus on systematic evaluation before making changes. Create a representative dataset of real user queries, then establish metrics that align with your business goals. Prioritize experiments based on potential impact and resource constraints\u2014you can only run a limited number of experiments in a given timeframe. Remember that improving AI performance is an iterative process requiring continuous testing and refinement rather than a one-time fix.</p>"},{"location":"office-hours/cohort3/week-4-1/#what-are-effective-ways-to-manage-expectations-when-implementing-ai-solutions","title":"What are effective ways to manage expectations when implementing AI solutions?","text":"<p>Be transparent about both capabilities and limitations. Help stakeholders understand that AI implementation is an iterative process requiring ongoing refinement rather than a one-time deployment. Clearly define the role of AI as a tool to assist humans rather than replace them completely. For specialized fields like architecture or engineering, emphasize that professionals still need to make the final decisions, with AI serving as a support system that provides suggestions and identifies potential issues.</p>"},{"location":"office-hours/cohort3/week-4-1/#how-can-i-integrate-visuals-and-diagrams-into-ai-generated-reports","title":"How can I integrate visuals and diagrams into AI-generated reports?","text":"<p>This is an emerging area with promising developments. Consider implementing systems that can intelligently select and incorporate relevant images from your existing resources. For technical applications like construction or engineering, the ability to include screenshots of blueprints with annotations highlighting specific areas of concern can significantly enhance the value of AI-generated reports. Libraries like Mermaid for diagram generation are becoming more widely supported and can be integrated into AI workflows.</p>"},{"location":"office-hours/cohort3/week-4-1/#how-should-ai-applications-be-priced-to-capture-appropriate-value","title":"How should AI applications be priced to capture appropriate value?","text":"<p>Consider moving beyond usage-based pricing (like per-token or per-user) toward outcome-based models that align with the actual business value delivered. For example, charging per resolved customer support ticket rather than per API call creates better alignment between your pricing and the value customers receive. This shift requires building systems with sufficient accountability and measurement capabilities to track outcomes reliably. The most innovative pricing approaches treat AI capabilities as replacements for headcount rather than as traditional software tools.</p>"},{"location":"office-hours/cohort3/week-4-1/#whats-the-relationship-between-data-collection-and-future-ai-capabilities","title":"What's the relationship between data collection and future AI capabilities?","text":"<p>Every AI application serves as a sensor that generates valuable data. The applications built today create the datasets that will enable more advanced AI capabilities tomorrow. Proprietary datasets from specialized industries (like investment decisions, supply chain operations, or construction projects) are particularly valuable for building domain-specific AI capabilities. The most interesting future developments will likely come from analyzing how humans interact with AI systems, creating a feedback loop of continuous improvement.</p>"},{"location":"office-hours/cohort3/week-4-2/","title":"Week 4 - Office Hour 2 (June 12)","text":"<p>Study Notes:</p> <p>I hosted an office hours session focused on advanced report generation with dynamic visualizations, professional styling challenges, and effective approaches to analyzing unstructured customer feedback data. Here are my insights on integrating visual elements into AI-generated reports, managing professional styling requirements, and building effective feedback analysis systems.</p>"},{"location":"office-hours/cohort3/week-4-2/#how-should-i-approach-dynamic-data-visualization-in-ai-generated-reports","title":"How should I approach dynamic data visualization in AI-generated reports?","text":"<p>When creating AI-generated reports with dynamic visualizations, there are several approaches to consider depending on your specific needs.</p> <p>For deep research-style reports (like those from Gemini, Claude, or OpenAI), the LLM typically decides on a set of subtasks and executes them sequentially. These reports often don't include charts or visualizations by default, though OpenAI's deep research does incorporate images.</p> <p>For more structured reports with visualizations, I see three main approaches:</p> <ol> <li>Post-hoc image addition: You can have the LLM identify places where supplementary images would enhance the text, then add them afterward.</li> <li>Image citations during research: Treat images as another citation source that the LLM can reference while generating text. For example, with a client yesterday, the LLM decided to include an org chart in a leadership report because it had access to an org chart JPEG file during generation.</li> <li>Mermaid diagrams: These are particularly useful for creating dynamic visualizations directly in Markdown. The key challenge is validation - if Claude generates an incorrect Mermaid diagram, it simply fails to render. You need a validation loop or external server to check the diagram code, report errors, and iterate to fix them.</li> </ol> <p>For standard data visualizations, most companies use JavaScript libraries like Recharts, which allow you to pass data as props and generate visualizations.</p> <p>The approach depends on whether your report format is flexible or fixed. If fixed, each header might have its own RAG workflow - for example, every competitor analysis might need a leadership team section, which triggers a subtask to find the leadership team of the target company.</p>"},{"location":"office-hours/cohort3/week-4-2/#how-can-we-handle-styling-challenges-in-professional-reports","title":"How can we handle styling challenges in professional reports?","text":"<p>One of the biggest challenges in AI report generation is matching the exact styling expectations of professional reports. I work with companies that sell to consultants like McKinsey, and the hardest part isn't generating the content - it's making the slides and plots look exactly like McKinsey-branded material.</p> <p>While it's easy to plug in matplotlib or Recharts, it's extremely difficult to match the precise styling requirements of professional consulting firms. Some clients are literally saying, \"We're not going to pay you any of that $80,000 unless you can make it look like we actually made this.\"</p> <p>These firms often use specialized software from the early 2000s for plot generation, with very specific requirements about legend shapes, marker styles (X's versus T's), and other formatting details. The styling is so challenging that we're considering using computer vision to train systems to use PowerPoint and implement styling changes based on feedback comments.</p> <p>I believe there's a significant market opportunity here - you could easily sell software that generates McKinsey-style plots for $100,000 to an analyst team. The last 5% of styling is what makes the difference between something that looks AI-generated versus professionally produced.</p> <p>Key Takeaway: The styling challenge represents a major opportunity for AI tools that can match the exact visual requirements of professional consulting firms. The technical content generation is often easier than matching the precise styling expectations that make reports look professionally produced.</p>"},{"location":"office-hours/cohort3/week-4-2/#how-should-i-approach-analyzing-unstructured-customer-feedback-data","title":"How should I approach analyzing unstructured customer feedback data?","text":"<p>For a project like Netflix's customer feedback analysis, where you're collecting unstructured data through a \"report a problem\" feature, I recommend a hybrid approach combining semantic search with structured analysis.</p> <p>First, consider doing hierarchical clustering to build a taxonomy of error categories. This gives you a structured way to analyze the data beyond just semantic search. By tagging all feedback with these hierarchical categories, you can provide accurate counts and faceted navigation.</p> <p>When a user asks \"What are members saying about Seinfeld's aspect ratio?\", you might return 10-20 semantically relevant results, but also show facets like \"200 comments in this category, 80 in that category\" to help them understand the distribution of issues.</p> <p>This approach allows users to traverse the data in interesting ways - they might start with audio issues, discover that 20% of complaints are about Seinfeld, then dig into which season has the most problems. The goal is giving users a portfolio of tools to explore the hierarchy rather than just semantic search alone.</p> <p>For quantitative questions like \"How many audio sync issues were reported in Brazil last month?\", you need structured data. The LLM will hallucinate counts if you rely solely on semantic search. By building lightweight classifiers for common issues, you can provide accurate counts while still allowing semantic exploration of the unstructured text.</p> <p>I worked with a company called Interpret that built something similar - a chatbot that could talk to customer feedback and give realistic counts by combining semantic understanding with structured analysis.</p> <p>Key Takeaway: The most effective approach combines semantic search with structured analysis through hierarchical clustering and classification. This gives users both the flexibility to explore feedback semantically and the accuracy of structured data for quantitative questions.</p>"},{"location":"office-hours/cohort3/week-4-2/#whats-the-best-way-to-build-fast-classifiers-for-unstructured-data","title":"What's the best way to build fast classifiers for unstructured data?","text":"<p>When you need to quickly classify unstructured data, there are several approaches depending on your requirements.</p> <p>One approach is using embedding-based classification. As Jan mentioned, OpenAI's documentation describes a simple technique where you embed category descriptions and then classify items by finding the closest category embedding. This works well for straightforward classification tasks and is extremely fast to implement.</p> <p>In my previous work, we used a matrix-based approach where we'd embed all products in a matrix, then learn another matrix to multiply by the product embeddings whenever we needed to build a classifier. This allowed us to label about 1,000 examples, learn the weights, and then multiply the entire product space by that vector to get predictions for every product. It was very fast but typically achieved around 85% accuracy.</p> <p>For Netflix's feedback analysis, you might want to combine pre-defined categories from domain experts with data-driven clusters discovered through analysis. There will be common issues like rendering problems or audio sync issues that domain experts can define, plus a longer tail of soft clusters that emerge from the data.</p> <p>The key is building a system that can quickly create and apply these classifiers as new issues emerge. When a new feature launches, you want to detect feedback about it immediately, even if it wasn't in your training data.</p> <p>Key Takeaway: Fast classifier development is essential for responsive feedback analysis. Combining embedding-based approaches with domain expertise allows you to quickly identify both known issues and emerging patterns in user feedback.</p>"},{"location":"office-hours/cohort3/week-4-2/#how-should-we-think-about-tool-based-approaches-versus-semantic-search","title":"How should we think about tool-based approaches versus semantic search?","text":"<p>I believe we're moving toward a world where many RAG applications will use tool-based approaches rather than pure semantic search, especially for structured data.</p> <p>In the coming weeks, we'll have talks from teams building coding agents that use a portfolio of tools rather than semantic search. Their thesis is that for structured data, the right way to prepare context isn't one semantic search request, but an agent using multiple tools to build context incrementally.</p> <p>Think about how you debug an error message - you see the error came from a specific file, so you load that file, then you find the function causing the issue, load that file, and traverse the file tree building context before solving the problem. Coding agents are implementing this approach rather than embedding all code.</p> <p>You can implement this with simple tools like \"ls\" (list files), \"read_file\", and \"grep\". The agent uses these tools to navigate the data, building context as it goes. This approach might cost more at query time but requires less preprocessing of data.</p> <p>I'm curious if this approach would work for traversing complex documents like 1,000-page PDFs. Instead of embedding everything, you could provide tools like \"list_table_of_contents\", \"grep\", \"show_page\", and \"show_page_as_image\". The agent could navigate the document naturally, finding references and following them just as a human would.</p> <p>Key Takeaway: Semantic search is most valuable when the producer and consumer of data don't share vocabulary. For structured data or documents with clear organization, a tool-based approach that mimics human navigation may be more effective and require less preprocessing.</p>"},{"location":"office-hours/cohort3/week-4-2/#what-are-you-working-on-with-your-cura-project","title":"What are you working on with your Cura project?","text":"<p>We're making progress on building out Cura, which is an open-source project (not quite a product yet) focused on analyzing conversation data. In the next few days, we'll be benchmarking it on about a thousand conversations to see what patterns we discover.</p> <p>The core of the project involves hierarchical clustering, explaining clusters, and generating names for these clusters. We're planning to download every open-source chat conversation dataset and run our analysis on it to see what we find.</p> <p>My philosophy with any product I build is that it should function as a sensor that generates data. I want to \"trick\" users into labeling data for me. If I don't know which chart type works best, I'll generate three options and ask users to delete the ones they don't want. My messaging is that it's important for them to review the data, but I'm actually collecting valuable feedback on what visualizations work best.</p> <p>We apply the same approach to citations in paragraphs - users can mouse over citations to see the source data and delete or regenerate citations they don't trust. This creates a feedback loop that continuously improves the system.</p> <p>Key Takeaway: Building products that function as data collection sensors is a powerful approach. By giving users options and tracking their choices, you can gather valuable feedback that improves your system while providing a better user experience.</p>"},{"location":"office-hours/cohort3/week-4-2/#what-upcoming-content-are-you-excited-about-in-the-course","title":"What upcoming content are you excited about in the course?","text":"<p>I'm particularly excited about the second half of the course where we'll dive deeper into data analysis and explore the portfolio of tools approach.</p> <p>In the coming weeks, we'll have talks from Reducto, one of the best PDF parsing libraries available right now. They have contracts with companies like Vanta and government agencies and have achieved impressive results.</p> <p>We'll also be inviting teams building coding agents, including the Augment team and the Klein team. These companies are focusing less on RAG with semantic search and more on RAG using a portfolio of tools. Their thesis is that for structured data like code, the right approach isn't one semantic search request but an agent using multiple tools to build context.</p> <p>Beyond the course, I'm organizing a speaker series with guests from OpenAI's memory team and possibly Claude Code. My goal is to bring in the most interesting speakers in the field to share their insights.</p> <p>Key Takeaway: The future of RAG systems, especially for structured data like code, may involve less semantic search and more tool-based approaches where agents navigate information using a portfolio of tools to build context incrementally.</p> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"office-hours/cohort3/week-5-1/","title":"Week 5, Office Hour 1 (June 17)","text":"<p>I hosted an office hours session focused on fine-tuning models for citation accuracy and designing effective tool portfolios for RAG systems. Here are my insights on improving citation capabilities through fine-tuning, structuring data for temporal reasoning, and creating effective tool portfolios for specialized retrieval tasks.</p>"},{"location":"office-hours/cohort3/week-5-1/#how-effective-is-fine-tuning-for-improving-citation-accuracy","title":"How effective is fine-tuning for improving citation accuracy?","text":"<p>When working with citation requirements, fine-tuning can dramatically reduce error rates. In one project, we used OpenAI's fine-tuning API with about 1,000 examples to improve our marketing content generation system.</p> <p>The results were impressive - our error rates dropped from around 4% to essentially 0% on our test set of 200 examples. We didn't need complex frameworks like Fluoro since we weren't hosting local models, just using OpenAI's API directly.</p> <p>The key was having evaluators validate our offline data, filtering out incorrectly formatted examples before using them in the fine-tuning process. This approach worked particularly well because we weren't trying to change the model's knowledge - just its formatting behavior.</p> <p>When determining how much data you need, I recommend experimenting with different sample sizes:</p> <p>\"What I would often do is try to use a subset of my data for fine-tuning, then increase the sample size and figure out what that curve looks like. It's going to be performance versus volume.\"</p> <p>Different models will have different learning curves - a 1.3 billion parameter model might flatten out at 10,000 data points, while larger models might show different patterns. Adjusting learning rates can also affect these curves.</p> <p>Key Takeaway: Fine-tuning can be remarkably effective for formatting tasks like citation, often requiring less data than you might expect. Start with small batches, measure performance, and increase data volume until you reach your desired accuracy level.</p>"},{"location":"office-hours/cohort3/week-5-1/#should-we-shuffle-citation-sources-during-fine-tuning","title":"Should we shuffle citation sources during fine-tuning?","text":"<p>When fine-tuning models to cite sources correctly, shuffling the order of retrieved sources can be beneficial to prevent position bias. This approach makes the model invariant to the order of sources, which is particularly important if you're not sorting by relevance.</p> <p>However, if you are sorting by relevance, maintaining the original order might actually be preferable: \"Maybe it is important for the model to know that the first text chunk is the most relevant text chunk.\"</p> <p>The need for shuffling may also depend on the context length of your model. With older, smaller context models (like 4K token models), position bias was more pronounced due to the \"lost in the middle\" effect. Newer models with better attention mechanisms have improved recall across their context window.</p> <p>\"If you look at the newer models, they just have way better lost-in-the-middle sensitivity in general, and I would expect that when you fine-tune these things, they also preserve some of that ability to attend over long contexts.\"</p> <p>The real challenge that remains is reasoning over multiple \"needles\" of information scattered throughout a document - connecting facts from different sections remains difficult for most models.</p> <p>Key Takeaway: Consider shuffling citation sources during fine-tuning if you want position-invariant citations, but if you're sorting by relevance, maintaining order may be beneficial. Newer models have better attention across their context window, reducing the need for this technique.</p>"},{"location":"office-hours/cohort3/week-5-1/#how-should-we-approach-tool-design-for-specialized-retrieval-tasks","title":"How should we approach tool design for specialized retrieval tasks?","text":"<p>When designing tools for retrieval systems, focus on creating a portfolio of specialized tools rather than just distinguishing between semantic and structured data. The key question isn't \"Am I searching semantic or structured data?\" but rather \"What is the portfolio of tools I want to expose to my system?\"</p> <p>For example, in a construction use case, we implemented several specialized tools:</p> <ul> <li>Generic document search that searches everything</li> <li>Contact search for finding people</li> <li>RFI (Request for Information) search that takes specific RFI codes</li> <li>Contract search that returns not just text chunks but also responsible parties</li> </ul> <p>The implementation details (whether it's semantic search or structured data) matter less than how you present these tools to the language model. Your focus should be on making sure the model understands what tool to use and when.</p> <p>For evaluating tool selection, I recommend having the model \"write a plan of all the tools it might want to use\" for a given query, then evaluating that plan first. You can even present this plan to users for approval before execution, which creates valuable training data based on acceptance rates.</p> <p>\"That gets you a pretty good dataset in terms of customer plan acceptance rates, and then you can look at the ones that are not accepted and figure out what you need to do afterwards.\"</p> <p>The naming of tools significantly impacts how models use them. In coding agents, for example, providing a specific \"grep\" tool versus just mentioning grep in the command line instructions can change execution patterns by 2% in evaluations.</p> <p>Key Takeaway: Design a portfolio of specialized tools based on specific use cases rather than general data types. Focus on clear tool descriptions and evaluate how well the model selects the appropriate tools for different queries.</p>"},{"location":"office-hours/cohort3/week-5-1/#how-can-we-handle-temporal-reasoning-in-medical-data","title":"How can we handle temporal reasoning in medical data?","text":"<p>One of the most challenging aspects of working with medical data is reasoning about information across a timeline. When retrieving documents about medications, for example, you might get 20 documents all describing medications, but understanding what changed over time requires special handling.</p> <p>\"You might want to know what changed over time, or you have to always see it in the context of time. And you also need to find relationships like 'there's this medication and the patient became worse' or 'this medication went up' - that all needs to be conceived in the system.\"</p> <p>For presenting temporal data effectively to models, I recommend structuring it as a markdown table whenever possible. In our testing, markdown tables performed 12% better than CSV, JSON, or YAML formats for complex lookup tasks across large datasets.</p> <p>\"We've done tests where I put like 6,000 rows, 50 columns as CSV, as markdown, as JSON, as YAML - and markdown tables is like 12% better in terms of identifying on row X where the value is Y, find me the row that's one above and one to the left.\"</p> <p>The ordering of temporal data also matters significantly. You might get different results if you order events in ascending versus descending time. This affects how the model scans and reasons about cause and effect relationships.</p> <p>For building better temporal reasoning capabilities, consider:</p> <ol> <li> <p>Ordering retrieved documents chronologically</p> </li> <li> <p>Presenting data in markdown table format with clear timestamps</p> </li> <li> <p>Having the model first extract and reorganize relevant information before reasoning about it</p> </li> <li> <p>Mining reasoning chains from expert users to create training data</p> </li> </ol> <p>Key Takeaway: For temporal reasoning, structure data chronologically in markdown tables and implement a two-stage approach where the model first extracts and organizes relevant timeline information before reasoning about it.</p>"},{"location":"office-hours/cohort3/week-5-1/#whats-the-difference-between-multi-agent-and-single-agent-approaches","title":"What's the difference between multi-agent and single-agent approaches?","text":"<p>The debate between multi-agent and single-agent systems often comes down to context coordination challenges. For coding tasks, Devin (from Cognition) chose a single-threaded approach because coordinating between agents modifying different parts of a codebase is extremely difficult.</p> <p>\"If one agent modifies one directory and another agent modifies another directory, that communication channel is sort of not well defined yet.\"</p> <p>In contrast, Claude's Deep Research uses multiple agents, but they're all read-only - they don't need to coordinate changes because they're just retrieving information that will later be combined:</p> <p>\"In that multi-agent system, the agents are all read-only, so they don't need to manage that communication overhead because they're all going to be reduced. If I search about who I am, one agent searches childhood, one agent searches career, and once they bring all the information back, they can be reduced.\"</p> <p>The primary benefit of multi-agent systems appears to be token efficiency - you can use more tokens across multiple agents than with a single agent. \"The performance just increases with the amount of tokens each sub-agent is able to consume. If you have 10 sub-agents, you can use more tokens, and your research quality is better.\"</p> <p>For medical data applications that are primarily read-only, a multi-agent approach might work, but the challenge remains in ensuring no context is missed when combining information from different agents.</p> <p>Key Takeaway: Choose multi-agent approaches for read-only tasks where you need to process more tokens than a single context window allows. For tasks requiring coordination of changes, single-agent approaches remain more practical until better coordination mechanisms are developed.</p>"},{"location":"office-hours/cohort3/week-5-1/#how-can-we-use-document-summarization-to-improve-retrieval","title":"How can we use document summarization to improve retrieval?","text":"<p>Generating summaries during document ingestion can be a cost-effective approach to improving retrieval. Summaries function as a form of compression and can be particularly valuable when working with smaller context window models.</p> <p>\"In general, this is a good idea because that's almost in some ways just a more cost-effective way of doing this contextual retrieval stuff. Summary is just compression.\"</p> <p>The key is designing your summarization prompt based on the specific tasks your system needs to perform. For example, with architectural blueprints, we knew users would ask about room counts and dimensions, so we created summaries that explicitly counted and listed these elements:</p> <p>\"Because we know that our tasks involve things like extracting the names of rooms and counting things, if our language model can have a summary that counts everything, then it becomes much easier to think about 'the place with 4 bedrooms and 2 bathrooms.'\"</p> <p>We implemented this as a separate document search tool that only hits the summaries. Through iteration and evaluation, we improved our summary generation from 16% recall to 85% recall in just a few days.</p> <p>For implementation, you can:</p> <ol> <li> <p>Create a separate \"search summaries\" tool</p> </li> <li> <p>Design summary prompts that extract the specific types of information users will query</p> </li> <li> <p>Evaluate and iterate on summary quality using test queries</p> </li> <li> <p>Use summaries as synthetic text chunks that supplement your existing text chunks</p> </li> </ol> <p>This approach works particularly well for documents like financial reports, where structured information can be extracted, or for multimedia content where describing images or videos in text makes them searchable.</p> <p>Key Takeaway: Document summarization during ingestion creates valuable synthetic text chunks that can dramatically improve retrieval performance. Design summary prompts based on the specific information needs of your application and iterate based on evaluation results.</p>"},{"location":"office-hours/cohort3/week-5-1/#how-can-we-implement-price-quote-generation-using-rag","title":"How can we implement price quote generation using RAG?","text":"<p>One practical application we've built is an automated price quote system for sales teams. After multiple calls with a prospect, the system generates personalized pricing options and potential upsells.</p> <p>The process works like this:</p> <ol> <li> <p>We have 16 pages of pricing information (per-seat pricing, volume discounts, prepayment options)</p> </li> <li> <p>We have transcripts from 6 phone calls with the prospect</p> </li> <li> <p>We ask the language model to:</p> </li> <li> <p>Read the transcripts and list all relevant variables</p> </li> <li>Extract the values of those variables</li> <li>Reason about the pricing document</li> <li>Propose options and upsells</li> <li>Write an email to the prospect</li> </ol> <p>\"The email's like 'Great talking to you, Tim. It sounds like for a company your size, you can probably commit to 15 seats. This will get you a 20% discount. If you don't use it, we'll move it to next year, and if you pay upfront, we can give you another 20-25% discount because I know that's something your CTO really values.'\"</p> <p>Our evaluation method is simple but effective - we have salespeople review the generated emails before sending them, and we track whether they make edits. When edits are needed, we analyze what went wrong in the reasoning step.</p> <p>This approach of extracting variables, reasoning about them, and then generating output could be applied to medical data as well. For example, if a patient shows drowsiness, the system could first extract all timeline information about drowsiness, then reason about potential causes.</p> <p>Key Takeaway: For complex reasoning tasks, implement a multi-step process where the model first extracts and organizes relevant information, then reasons about it, and finally generates output. This structured approach makes the reasoning more transparent and easier to evaluate.</p>"},{"location":"office-hours/cohort3/week-5-1/#whats-the-best-way-to-format-data-for-language-models","title":"What's the best way to format data for language models?","text":"<p>When presenting structured data to language models, markdown tables consistently outperform other formats like CSV, JSON, or YAML. In our testing, markdown tables were 12% more effective for complex lookup tasks.</p> <p>\"We've done tests where I put like 6,000 rows, 50 columns as CSV, as markdown, as JSON, as YAML - and markdown tables is like 12% better in terms of identifying on row X where the value is Y, find me the row that's one above and one to the left.\"</p> <p>The formatting details matter significantly. For example, having spaces between tokens in markdown tables (like \"| data |\" instead of \"|data|\") affects how the model processes the information.</p> <p>\"If I search for the word Jason, the token is 'space Jason'. But if it's Jason in JSON, it's actually 'quote Jason' - those are different tokens. And so those things end up affecting the lookup a little bit.\"</p> <p>These seemingly minor formatting choices can have meaningful impacts on model performance, especially for tasks requiring precise information retrieval or table navigation.</p> <p>For temporal data specifically, presenting information in chronological order (either ascending or descending) can significantly affect how models reason about cause and effect. Testing both approaches is worthwhile, as one may work better than the other depending on your specific use case.</p> <p>Markdown tables consistently outperform other data formats for structured information. Pay attention to spacing and formatting details, as they affect tokenization and retrieval performance. For temporal data, experiment with both chronological and reverse-chronological ordering.</p>"},{"location":"office-hours/cohort3/week-5-1/#how-should-we-approach-end-to-end-evaluation-of-complex-rag-systems","title":"How should we approach end-to-end evaluation of complex RAG systems?","text":"<p>End-to-end evaluation of complex retrieval systems remains challenging, especially when there isn't a single correct answer or when the system needs to perform multi-step reasoning.</p> <p>\"The end-to-end evaluation of these kinds of things are still pretty challenging, unless it really is the case that there are just certain text chunks that we're trying to achieve or certain answers we already know ahead of time.\"</p> <p>For tool selection, one effective approach is evaluating the system's planning capabilities:</p> <ol> <li> <p>Ask the model to write a plan of which tools it would use for a query</p> </li> <li> <p>Evaluate the plan before executing it</p> </li> <li> <p>Allow users to approve or reject the plan</p> </li> <li> <p>Track plan acceptance rates and analyze rejected plans</p> </li> </ol> <p>For reasoning tasks, breaking evaluation into steps can be helpful:</p> <ol> <li> <p>Evaluate information extraction (did the system find the relevant information?)</p> </li> <li> <p>Evaluate reasoning (given the correct information, did it reach valid conclusions?)</p> </li> <li> <p>Evaluate output generation (was the final response clear and actionable?)</p> </li> </ol> <p>In some domains like coding, the evaluation metrics are clearer - does the code pass tests? In other domains like medical reasoning, evaluation may require expert review or comparison to known outcomes.</p> <p>For systems like our price quote generator, we use a practical metric - do salespeople edit the generated emails before sending them? This real-world usage metric helps us identify where the system's reasoning falls short.</p> <p>Key Takeaway: Break evaluation into component parts rather than relying solely on end-to-end metrics. Incorporate user feedback into your evaluation process, and track how often outputs require human editing or intervention.</p>"},{"location":"office-hours/cohort3/week-5-1/#how-does-fine-tuning-improve-citation-accuracy-in-llms","title":"How does fine-tuning improve citation accuracy in LLMs?","text":"<p>Fine-tuning can dramatically reduce error rates when teaching models to properly cite sources. In one example, fine-tuning reduced citation errors from 4% to nearly 0% for marketing content generation. The process involves collecting properly formatted examples, validating them, filtering out incorrect formats, and using them in the fine-tuning process.</p>"},{"location":"office-hours/cohort3/week-5-1/#how-many-examples-are-typically-needed-for-effective-fine-tuning","title":"How many examples are typically needed for effective fine-tuning?","text":"<p>Around 1,000 high-quality examples can be sufficient for format-related fine-tuning tasks. However, the exact number depends on your specific use case. It's recommended to experiment with increasing sample sizes to determine the optimal amount for your needs. Start with a smaller subset and gradually increase to identify where performance improvements begin to plateau.</p>"},{"location":"office-hours/cohort3/week-5-1/#should-i-shuffle-the-order-of-retrieved-sources-in-my-fine-tuning-dataset","title":"Should I shuffle the order of retrieved sources in my fine-tuning dataset?","text":"<p>Shuffling retrieved sources can be beneficial to make your model invariant to the order of information. This approach helps prevent the model from developing biases toward information presented first. However, if your retrieval system sorts by relevance, maintaining that order might be important as the first chunk would genuinely contain the most relevant information.</p>"},{"location":"office-hours/cohort3/week-5-1/#how-should-i-approach-tool-selection-for-my-llm-application","title":"How should I approach tool selection for my LLM application?","text":"<p>Focus on developing a portfolio of specialized tools rather than simply categorizing between semantic and structured data searches. Consider what specific capabilities would benefit your use case, such as date-range filtering, categorical filters, or metadata tag filtering. The implementation details (whether semantic or structured) matter less than ensuring your model understands when to use each tool.</p>"},{"location":"office-hours/cohort3/week-5-1/#whats-an-effective-way-to-evaluate-tool-selection-by-the-model","title":"What's an effective way to evaluate tool selection by the model?","text":"<p>A practical approach is to have the model write a plan listing all tools it would use for a given query, then evaluate that plan before execution. You can present this plan to users for approval or rejection, which generates valuable feedback data. Analyzing rejected plans helps identify improvements needed in your tool selection and routing logic.</p>"},{"location":"office-hours/cohort3/week-5-1/#how-do-coding-agents-approach-tool-integration","title":"How do coding agents approach tool integration?","text":"<p>Coding agents have made significant progress with tool integration. One key insight is that providing named tools for specific functions (rather than general capabilities) significantly changes how frequently these functions are used. For example, providing a dedicated \"grep\" tool versus expecting the model to remember to use grep through a general command line interface can improve performance by several percentage points.</p>"},{"location":"office-hours/cohort3/week-5-1/#how-should-i-organize-timeline-based-data-for-llm-processing","title":"How should I organize timeline-based data for LLM processing?","text":"<p>For timeline data, consider presenting information in a markdown table format, which models tend to process effectively. Order the data chronologically (either ascending or descending) and include clear date markers. This organization helps the model understand temporal relationships and reason about cause and effect. Testing both ascending and descending time orders may yield different results depending on your use case.</p>"},{"location":"office-hours/cohort3/week-5-1/#why-are-markdown-tables-particularly-effective-for-structured-data","title":"Why are markdown tables particularly effective for structured data?","text":"<p>Markdown tables have shown superior performance (approximately 12% better) compared to other formats like CSV, JSON, or YAML when models need to perform lookup tasks or understand relationships between data points. The spacing between tokens in markdown tables appears to be particularly well-suited to how models process information.</p>"},{"location":"office-hours/cohort3/week-5-1/#how-can-i-help-models-reason-across-complex-information","title":"How can I help models reason across complex information?","text":"<p>For complex reasoning tasks, consider implementing a two-step approach: first have the model extract and reorganize all relevant information from different sources, then reason about this reorganized information. This approach works well for tasks requiring synthesis across multiple data points, such as analyzing medical timelines or generating pricing quotes based on multiple conversations.</p>"},{"location":"office-hours/cohort3/week-5-1/#is-it-beneficial-to-generate-summaries-during-data-ingestion","title":"Is it beneficial to generate summaries during data ingestion?","text":"<p>Creating summaries during data ingestion can be very effective, especially for longer documents. Summaries act as compressed versions of your data that can be more efficiently processed. For specific use cases like blueprints or financial documents, you can design summarization prompts that extract the most relevant information (like room counts or key financial figures) to make subsequent queries more efficient.</p>"},{"location":"office-hours/cohort3/week-5-1/#how-can-i-handle-reasoning-across-multiple-documents","title":"How can I handle reasoning across multiple documents?","text":"<p>For reasoning across multiple documents, consider having the model first extract all relevant information related to the query, reorganize it (possibly chronologically or thematically), and then reason about the reorganized information. This approach helps manage context limitations and focuses the model's attention on the most pertinent details.</p>"},{"location":"office-hours/cohort3/week-5-1/#whats-the-best-way-to-handle-long-context-windows","title":"What's the best way to handle long context windows?","text":"<p>Newer models with improved attention mechanisms handle long contexts better than older models. However, for complex reasoning tasks involving multiple \"needles\" of information spread throughout a document, consider using tools that first organize the relevant information before reasoning about it. This approach remains effective even with models that have long context windows.</p> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"office-hours/cohort3/week-5-2/","title":"Week 5, Office Hour 2 (June 19)","text":"<p>Study Notes:</p> <p>In this office hours session, I addressed questions about specialized indices, data engineering for AI applications, and strategies for blending traditional ML with LLMs. The discussion covered practical approaches to metadata extraction, cost considerations for processing large datasets, and techniques for improving recommendation systems with AI.</p>"},{"location":"office-hours/cohort3/week-5-2/#how-should-i-approach-dynamically-generating-and-handling-metadata-for-documents","title":"How should I approach dynamically generating and handling metadata for documents?","text":"<p>When dealing with the need to extract new metadata from existing documents, the architectural approach depends largely on your current infrastructure. Most companies I work with already have some existing setup, so we're rarely building from scratch.</p> <p>In essence, this is just like any ETL (Extract, Transform, Load) job where a process creates a new database artifact. The key question is: what makes backfilling this data challenging in your specific context? Is it the cost of reprocessing millions of documents? Is it the unpredictability of expenses?</p> <p>For cost estimation, I recommend calculating the token volume of your data. We had a task to summarize a million conversations, and we made sure to calculate the expected input and output tokens. This allowed us to make informed decisions about model selection - for instance, we discovered that using open source models was only 8 times cheaper than using OpenAI's API.</p> <p>\"I was really disappointed to realize that the open source models are only 8 times cheaper. We're putting all this effort to save $60. And that was for a million conversations - it cost $60 to summarize a million conversations. These models are just so cheap now.\"</p> <p>For specialized extraction tasks, consider using smaller, purpose-built models. At Stitch Fix, we built a suite of small models doing specific extractions. For example, we realized we were selling belts with pants that had no belt loops, so we created a simple computer vision model to detect belt loops. This approach was efficient and solved a specific business problem worth millions of dollars.</p> <p>Key Takeaway: Calculate token volumes and costs before deciding on your extraction approach. Sometimes the cost difference between APIs and self-hosted models is smaller than expected, making the engineering effort to switch questionable. For specialized extractions, consider purpose-built models that solve specific business problems rather than trying to do everything with one large model.</p>"},{"location":"office-hours/cohort3/week-5-2/#what-are-the-challenges-with-extracting-multiple-attributes-in-a-single-api-call","title":"What are the challenges with extracting multiple attributes in a single API call?","text":"<p>When extracting multiple attributes from documents, be aware that prompts for some attributes can affect the extraction of other attributes. We found this when processing transcripts - when we asked for shorter action items, the summaries would also get shorter.</p> <p>To address this, we split our extraction into separate jobs: one for action items and another for summary and memo generation. This separation gave us better control over each component. We made this approach cost-effective by leveraging prompt caching - the transcript only needed to be processed once, with multiple outputs generated from that single input.</p> <p>Key Takeaway: Be cautious about extracting too many attributes in a single API call, as they can influence each other in unexpected ways. Consider splitting extractions into separate jobs with specific focuses, and use techniques like prompt caching to maintain cost efficiency.</p>"},{"location":"office-hours/cohort3/week-5-2/#how-should-i-approach-recommendation-systems-with-llms","title":"How should I approach recommendation systems with LLMs?","text":"<p>For recommendation systems like predicting product purchases, I wouldn't use an LLM directly in the recommendation system. Companies like Stitch Fix and YouTube use LLMs primarily to create better embeddings, not for the core recommendation logic.</p> <p>The approach I'd recommend is building item embeddings using historical data, where the inputs might include product images, descriptions, user comments, and checkout rates. Similarly, user embeddings would incorporate their feedback, fit comments, and other behavioral signals.</p> <p>One valuable application of LLMs is creating synthetic users to run simulations, particularly for addressing cold-start problems. When a new item appears, there's no transaction or impression data to train on. An LLM can simulate transaction data and returns, helping predict success rates for the first orders.</p> <p>\"At Stitch Fix we needed about 400 shipments of a single SKU before we had a good embedding for it. So our only job was: how do we get to a world where we either can simulate the SKUs or need less data?\"</p> <p>We addressed this by building a \"Tinder for clothes\" where users could swipe left or right on clothing items. This generated 6,000 labels much faster than waiting for 400 actual shipments, as users would label 30 items a day versus receiving only 5 items a month.</p> <p>Key Takeaway: Rather than using LLMs directly for recommendations, use them to generate better embeddings and synthetic data to address cold-start problems. Consider creative ways to gather user preferences at scale, as the velocity of data collection is often the limiting factor in recommendation quality.</p>"},{"location":"office-hours/cohort3/week-5-2/#how-can-i-blend-traditional-ml-with-unstructured-data-from-llms","title":"How can I blend traditional ML with unstructured data from LLMs?","text":"<p>The most promising approach I've seen is using LLMs for synthetic data generation and feature engineering. The challenge with many recommendation systems is the low velocity of data - unlike Spotify or Netflix where users consume content quickly, physical product recommendations might take weeks to validate through purchases and returns.</p> <p>Our focus at Stitch Fix was making each sample more efficient. Instead of building general-purpose computer vision models, we created specialized models for specific attributes (like detecting belt loops). These targeted models were more data-efficient and could directly drive business decisions (like upselling belts with pants that have belt loops).</p> <p>The workflow we found effective was:</p> <ol> <li>Use smaller, data-efficient models for specific extractions</li> <li>Use these models to generate simulations and synthetic data</li> <li>Feed this expanded dataset into larger, more powerful models</li> </ol> <p>\"Can we use LLMs for feature engineering and then use traditional models because they're gonna absorb the data faster? And then, once those cap out, how can we use the traditional models to create more data for the larger models to take in more capacity?\"</p> <p>This approach recognizes that different models have different data efficiency profiles, and leveraging their strengths in combination yields better results than trying to solve everything with a single approach.</p> <p>Key Takeaway: Blend traditional ML with LLMs by using LLMs for feature engineering and synthetic data generation. Build specialized, data-efficient models for specific attributes, then use these to feed larger models. This creates a virtuous cycle where each type of model enhances the capabilities of the others.</p>"},{"location":"office-hours/cohort3/week-5-2/#are-there-good-tools-for-data-engineering-in-the-llm-ecosystem","title":"Are there good tools for data engineering in the LLM ecosystem?","text":"<p>The data engineering landscape for LLMs is still developing, with most early-stage companies using relatively simple approaches like \"data to JSON\" pipelines. One company worth looking at is Tensor Lake, which provides sophisticated data processing for tensors.</p> <p>A critical area that's often overlooked is managing evaluation datasets. Many companies have inconsistent approaches where individual team members export data in ad-hoc ways:</p> <p>\"Almost every company I work with has datasets for evals, but they're all kind of like one guy wrote a SQL query to export things, saved it as a CSV file on their laptop and started working with it. And then they wrote this to Brain Trust, and that's what they're working on. But the other guy on a different team is using a different dataset.\"</p> <p>This creates problems when metrics improve - does anyone trust the results? Was the test data recent or old? Did it cover multiple organizations or just one customer? Proper data engineering for evaluation is a substantial undertaking that requires careful planning and coordination across teams.</p> <p>At Facebook, defining a new table for newsfeed views would involve a data engineer interviewing 20 teams, designing columns to support various query patterns, and ensuring everyone could write consistent SQL queries against the database. This level of rigor is often missing in LLM evaluation setups.</p> <p>Key Takeaway: The data engineering ecosystem for LLMs is still maturing. Pay special attention to how you organize evaluation datasets, as inconsistent approaches lead to unreliable metrics. Consider investing in proper data engineering for your evaluation pipeline, similar to how established companies handle critical data infrastructure.</p>"},{"location":"office-hours/cohort3/week-5-2/#whats-your-approach-to-topic-modeling-and-specialized-indices","title":"What's your approach to topic modeling and specialized indices?","text":"<p>For topic modeling and specialized indices, we've been developing tools like Kora, which helps with topic extraction from documents. This approach is becoming increasingly valuable as managing knowledge bases becomes more complex.</p> <p>The fundamental issue is that embeddings alone aren't sufficient for many complex queries. If someone asks \"Who is the best basketball player under 25 years old from Europe?\", embeddings might not find a direct answer unless that exact information exists in a paragraph somewhere.</p> <p>This is why we need to build a portfolio of tools rather than relying solely on embeddings. For the basketball player example, you might need:</p> <ol> <li>A structured player database with extracted attributes</li> <li>Specialized extractors that pull out statements about people</li> <li>Tools that can perform semantic search combined with structured filtering</li> </ol> <p>\"It's not that the tools are one-to-one with the retriever. It's actually gonna be the case that we probably have multiple tools hitting the same index.\"</p> <p>This is similar to how command-line tools interact with a file system - you have commands like \"list directories\" and \"view files,\" but also more specialized commands like \"list files sorted by last modified\" or \"list files by editor.\" A smart model can learn to use these various tools rather than trying to build one mega-search tool that works for all cases.</p> <p>Key Takeaway: Don't rely solely on embeddings for complex information retrieval. Build a portfolio of specialized tools that can work with your data in different ways. This approach is gaining traction in code generation and will likely become standard across other domains as well.</p>"},{"location":"office-hours/cohort3/week-5-2/#will-reasoning-models-eliminate-the-need-for-specialized-indices","title":"Will reasoning models eliminate the need for specialized indices?","text":"<p>Even with advanced reasoning models that can perform multi-step thinking, I don't believe they'll eliminate the need for specialized indices and tools. Instead, the focus should be on exposing a wide range of tools that these models can leverage.</p> <p>The key insight is that tools aren't necessarily one-to-one with retrievers. You might have multiple tools hitting the same index, similar to how command-line tools interact with a file system. For example, you might have tools for listing directories, viewing files, sorting by modification date, or filtering by editor.</p> <p>\"A smart enough model might just be able to reason about how to use all five tools rather than trying to build a mega search tool that will work in all cases.\"</p> <p>This is the direction that code generation tools are taking - they're finding that embedding your codebase isn't the right approach. Instead, they're building portfolios of tools, and I believe this pattern will spread to other domains as well.</p> <p>Key Takeaway: Even with advanced reasoning capabilities, models benefit from having access to specialized tools rather than trying to do everything through a single approach. The future lies in building portfolios of tools that models can intelligently select and combine, not in creating a single universal solution.</p>"},{"location":"office-hours/cohort3/week-5-2/#how-do-you-approach-cost-calculations-for-ai-processing","title":"How do you approach cost calculations for AI processing?","text":"<p>When calculating costs for AI processing, focus on understanding your token volumes. For any extraction or processing task, calculate the expected input and output tokens to make informed decisions about model selection.</p> <p>We had a surprising discovery when comparing OpenAI's API to open source models for summarizing a million conversations. The open source approach was only 8 times cheaper, saving just $60 total. While it was 26 times faster, the absolute cost was so low that it wasn't worth the engineering effort to switch.</p> <p>\"I was gonna write a blog post on how to use open source models to do the data extraction. I was like, 'Oh, it's not worth writing the blog post because 8 times cheaper for $60? Well, unless I'm doing this a hundred times, I don't need to save $50.'\"</p> <p>These calculations help you make rational decisions about where to invest your engineering time. Sometimes the cost difference between approaches is so small that it's not worth optimizing further, especially when the absolute costs are already low.</p> <p>Key Takeaway: Calculate token volumes and costs before investing in optimization. Modern AI models are often surprisingly affordable at scale, making some optimizations unnecessary. Focus your engineering efforts where they'll have meaningful impact rather than chasing small percentage improvements.</p>"},{"location":"office-hours/cohort3/week-5-2/#how-should-i-approach-dynamically-generating-and-handling-metadata-for-documents_1","title":"How should I approach dynamically generating and handling metadata for documents?","text":"<p>When building metadata extraction systems that need to evolve over time, consider treating each extraction as a separate ETL (Extract, Transform, Load) job. This approach allows you to add new extraction tasks without redoing everything. Before implementing, calculate the token volume to estimate costs - you might find that even with millions of records, the cost is surprisingly manageable (often just tens of dollars). For specialized extractions, consider using smaller, focused models rather than trying to extract everything in a single pass, as this can provide better control over individual attributes.</p>"},{"location":"office-hours/cohort3/week-5-2/#is-it-worth-using-open-source-models-for-data-extraction-tasks","title":"Is it worth using open source models for data extraction tasks?","text":"<p>It depends on your specific needs. In many cases, the cost difference between using open source models versus API models like GPT-4 may be smaller than expected - sometimes only 8x cheaper. For a job that costs $60 with an API model, saving $50 might not justify the engineering effort required to implement an open source solution. Always calculate the token volume and expected costs before making this decision, and consider factors beyond cost such as latency and maintenance requirements.</p>"},{"location":"office-hours/cohort3/week-5-2/#how-can-i-estimate-the-cost-of-running-extraction-jobs-on-large-datasets","title":"How can I estimate the cost of running extraction jobs on large datasets?","text":"<p>Create a table that tracks input token counts for your documents and calculate the expected costs based on current API pricing. This simple exercise can provide valuable insights that inform your architecture decisions. For many extraction tasks, you might find that using models like GPT-4 Mini or similar smaller models is cost-effective enough, especially for straightforward extraction tasks.</p>"},{"location":"office-hours/cohort3/week-5-2/#should-i-extract-multiple-attributes-in-a-single-api-call-or-separate-them","title":"Should I extract multiple attributes in a single API call or separate them?","text":"<p>It's often better to separate extraction tasks into multiple focused API calls rather than trying to extract everything at once. When multiple attributes are extracted in a single prompt, changes to one attribute's extraction can unintentionally affect others. For example, requesting shorter action items might inadvertently make summaries shorter as well. Breaking these into separate jobs gives you better control, and techniques like prompt caching can help manage costs by avoiding redundant processing of the same input text.</p>"},{"location":"office-hours/cohort3/week-5-2/#how-can-i-blend-traditional-ml-with-llms-for-recommendation-systems","title":"How can I blend traditional ML with LLMs for recommendation systems?","text":"<p>Rather than using LLMs directly in recommendation systems, consider using them to:</p> <ol> <li>Generate better embeddings for items and users</li> <li>Create synthetic data to help with cold-start problems</li> <li>Simulate user behavior for new items that lack transaction data</li> <li>Extract structured attributes that can feed into traditional recommendation models</li> </ol> <p>At companies like Stitch Fix, the approach has been to use a cascade of models (vision, text, feedback, factorization) that build different scores, then blend these scores into a final probability-of-sale model.</p>"},{"location":"office-hours/cohort3/week-5-2/#what-are-effective-strategies-for-specialized-indices-versus-general-embeddings","title":"What are effective strategies for specialized indices versus general embeddings?","text":"<p>For complex queries like \"Who is the best European basketball player under 25 years old?\", general embeddings often fall short. Instead, consider:</p> <ol> <li>Building structured data extractors that pull out specific attributes (age, nationality, sport)</li> <li>Creating a portfolio of specialized tools rather than relying on a single embedding approach</li> <li>Using different representations for different types of data</li> <li>Exposing multiple tools that might access the same index in different ways</li> </ol> <p>The trend is moving toward having multiple specialized tools rather than trying to build a single \"mega search tool\" that works for all cases.</p>"},{"location":"office-hours/cohort3/week-5-2/#how-are-companies-handling-data-engineering-for-llm-applications","title":"How are companies handling data engineering for LLM applications?","text":"<p>Data engineering remains a significant challenge. Many companies are still figuring out best practices for:</p> <ol> <li>Creating and maintaining evaluation datasets</li> <li>Building extraction pipelines that can be easily updated</li> <li>Managing backfills when new attributes need to be extracted</li> <li>Ensuring consistency across teams using the same data</li> </ol> <p>For companies exploring this space, tools like Tensor Lake might be worth investigating, as they're designed for tensor-based data processing at scale.</p>"},{"location":"office-hours/cohort3/week-5-2/#will-better-reasoning-models-eliminate-the-need-for-specialized-indices","title":"Will better reasoning models eliminate the need for specialized indices?","text":"<p>Not entirely. Even as models improve at reasoning, having a portfolio of specialized tools remains valuable. The approach is shifting toward giving models access to multiple tools that can retrieve and process data in different ways, rather than expecting a single model to handle everything. For example, instead of one mega-search tool, you might have tools for listing directories, viewing files, filtering by metadata, semantic search, and full-text search - all potentially accessing the same underlying data but in different ways.</p> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"talks/","title":"Talks and Presentations","text":"<p>This section contains talks and presentations from the Systematically Improving RAG Applications series, featuring insights from industry experts and practitioners. Each talk provides specific learning outcomes, actionable techniques, and often surprising insights that challenge conventional RAG wisdom.</p>"},{"location":"talks/#talks-by-chapter","title":"Talks by Chapter","text":""},{"location":"talks/#chapter-1-foundation-and-evaluation","title":"Chapter 1: Foundation and Evaluation","text":"<p>Establishing evaluation frameworks and building feedback systems.</p> <p>Building Feedback Systems for AI Products - Vitor (Zapier) Simple UX changes increased feedback collection from 10 to 40+ submissions per day (4x improvement). Game-changing insight: specific feedback questions like \"Did this run do what you expected?\" dramatically outperform generic \"How did we do?\" prompts. The team discovered they were missing positive feedback entirely due to poor collection mechanisms.</p> <p>Text Chunking Strategies - Anton (ChromaDB) Why chunking remains critical even with infinite context windows due to embedding model limitations and retrieval performance. Surprising discovery: default chunking strategies in popular libraries often produce terrible results for specific datasets. Essential practice: always manually examine your chunks.</p> <p>Understanding Embedding Performance through Generative Evals - Kelly Hong Generative benchmarking for creating custom evaluation sets from your own data. Surprising finding: model rankings on custom benchmarks often contradict MTEB rankings, showing that public benchmark performance doesn't guarantee real-world success. Method: filter document chunks for relevance \u2192 generate realistic queries with context and examples \u2192 evaluate retrieval performance.</p>"},{"location":"talks/#chapter-2-training-and-fine-tuning","title":"Chapter 2: Training and Fine-Tuning","text":"<p>Creating custom embedding models and fine-tuning for specific domains.</p> <p>Enterprise Search and Fine-tuning Embedding Models - Manav (Glean) Custom embedding models for each customer achieve 20% performance improvements over 6 months through continuous learning. Counter-intuitive insight: smaller, fine-tuned models often outperform larger general-purpose models for company-specific terminology. Each customer gets their own model that learns from user feedback.</p> <p>Fine-tuning Re-rankers and Embedding Models for Better RAG Performance - Ayush (LanceDB) Re-rankers provide 12-20% retrieval improvement with minimal latency penalty, making them \"low-hanging fruit\" for RAG optimization. Even small 6M parameter models show significant improvements. ColBERT architecture offers effective middle ground between bi-encoders and cross-encoders.</p>"},{"location":"talks/#chapter-3-production-and-monitoring","title":"Chapter 3: Production and Monitoring","text":"<p>Deployment strategies and production monitoring for RAG systems.</p> <p>Online Evals and Production Monitoring - Ben &amp; Sidhant Trellis framework for managing AI systems with millions of users. Critical discovery: traditional error monitoring (like Sentry) doesn't work for AI since there's no exception when models produce bad outputs. Their approach: discretize infinite outputs \u2192 prioritize by impact \u2192 recursively refine. Key insight: \"vibe checks\" often beat complex automated evaluation.</p> <p>RAG Anti-patterns in the Wild - Skylar Payne 90% of teams adding complexity to RAG systems see worse performance when properly evaluated. Major discovery: silent failures in document processing can eliminate 20%+ of corpus without detection. Golden rule: teams who iterate fastest on data examination consistently outperform those focused on algorithmic sophistication.</p>"},{"location":"talks/#chapter-4-query-analysis-and-data-organization","title":"Chapter 4: Query Analysis and Data Organization","text":"<p>Understanding user queries and routing them effectively.</p> <p>Query Routing for RAG Systems - Anton (ChromaDB) Why the \"big pile of records\" approach reduces recall due to approximate nearest neighbor algorithms. When filtering large indexes, compute budget is wasted on irrelevant nodes. Solution: separate indexes per user/data source often outperform filtered large indexes because filtering inherently reduces recall.</p>"},{"location":"talks/#chapter-5-specialized-retrieval-systems","title":"Chapter 5: Specialized Retrieval Systems","text":"<p>Building specialized capabilities for different content types and use cases.</p> <p>Agentic RAG - Colin Flaherty Surprising findings from top SWE-Bench performance: simple tools like grep and find outperformed sophisticated embedding models due to agent persistence and course-correction capabilities. Key recommendation: expose existing retrieval systems as tools to agents rather than replacing them.</p> <p>Better RAG Through Better Data - Adit (Reducto) Hybrid computer vision + VLM pipelines outperform pure approaches for document parsing. Critical finding: even 1-2 degree document skews can dramatically impact extraction quality. Essential insight: invest heavily in domain-specific evaluation rather than generic benchmarks.</p> <p>Encoder Stacking and Multi-Modal Retrieval - Daniel (Superlinked) LLMs as \"pilots that see the world as strings\" fundamentally can't understand numerical relationships. Solution: mixture of specialized encoders for different data types (text, numerical, location, graph) rather than forcing everything through text embeddings. This approach eliminates over-reliance on re-ranking.</p> <p>Lexical Search in RAG Applications - John Berryman Why semantic search struggles with exact matching, product IDs, and specialized terminology. Lexical search provides efficient simultaneous filtering and rich metadata that helps LLMs make better decisions. Recommended approach: use lexical search for filtering, semantic search for understanding meaning.</p>"},{"location":"talks/#chapter-6-advanced-topics-and-innovation","title":"Chapter 6: Advanced Topics and Innovation","text":"<p>Cutting-edge approaches and innovative techniques.</p> <p>Semantic Search Over the Web with Exa - Will Bryk (Exa) Why AI systems need fundamentally different search engines than humans. Vision for \"perfect search\" includes test-time compute where complex queries may take hours or days. Prediction: search market will fragment into specialized providers rather than one-size-fits-all solutions.</p> <p>RAG Without APIs: Browser-Based Retrieval - Michael (OpenBB) Browser-as-data-layer for secure financial data access without traditional API redistribution. Innovation: stateless agent protocol enables remote function execution in browser, solving compliance and security issues. Philosophy: anything humans can do, AI must be able to do.  </p>"},{"location":"talks/#overarching-themes","title":"Overarching Themes","text":"<p>Most Critical Learning: Data quality examination beats algorithmic sophistication - teams that iterate fastest on understanding their data consistently build better RAG systems</p> <p>Most Underutilized Technique: Fine-tuning embeddings and re-rankers - both are more accessible and impactful than most teams realize</p> <p>Biggest Gap: Most teams focus on model selection and prompting but underinvest in document processing, evaluation frameworks, and understanding their specific data distribution</p> <p>The series reveals that successful RAG systems require a portfolio of techniques rather than silver bullets, with data understanding and systematic evaluation being the foundational capabilities that enable everything else.</p> <p>For more information about the broader curriculum, see the main index.</p>"},{"location":"talks/chromadb-anton-chunking/","title":"Text Chunking Strategies for RAG Applications [Anton Troynikov]","text":"<p>Study Notes:</p> <p>I hosted a special session with Anton from ChromaDB to discuss their latest technical research on text chunking for RAG applications. This session covers the fundamentals of chunking strategies, evaluation methods, and practical tips for improving retrieval performance in your AI systems.</p> <p>What is chunking and why is it important for RAG systems? Chunking is the process of splitting documents into smaller components to enable effective retrieval of relevant information. Despite what many believe, chunking remains critical even as LLM context windows grow larger.</p> <p>The fundamental purpose of chunking is to find the relevant text for a given query among all the divisions we've created from our documents. This becomes especially important when the information needed to answer a query spans multiple documents.</p> <p>There are several compelling reasons why chunking matters regardless of context window size:</p> <ol> <li>Embedding model limitations - While LLM context windows are growing, embedding models typically have fixed input sizes and will silently truncate oversized inputs</li> <li>Inference efficiency - You're paying per token, so retrieving only relevant information reduces costs</li> <li>Information accuracy - Effective chunking eliminates distractors that could confuse the model</li> <li>Retrieval performance - Proper chunking significantly improves your system's ability to find all relevant information</li> </ol> <p>Key Takeaway: Chunking will remain important regardless of how large context windows become because it addresses fundamental challenges in retrieval efficiency, accuracy, and cost management.</p> <p>What approaches exist for text chunking? There are two broad categories of chunking approaches that are currently being used:</p> <p>Heuristic approaches rely on separator characters (like newlines, question marks, periods) to divide documents based on their existing structure. The most widely used implementation is the recursive character text splitter, which uses a hierarchy of splitting characters to subdivide documents into pieces not exceeding a specified maximum length.</p> <p>These methods generally produce good results with clean documents but become brittle when dealing with unusual formatting or special characters. They require significant preprocessing and cleaning.</p> <p>Semantic approaches are more experimental but promising. These use embedding or language models to identify semantic boundaries in documents - points where the topic changes. This approach avoids the brittleness of heuristics by focusing on meaning rather than characters.</p> <p>What's particularly interesting is that you can use the same embedding model for both chunking and retrieval, potentially finding an embedding-optimal chunking strategy. Since embeddings are relatively cheap, this approach is becoming more viable.</p> <p>Key Takeaway: While heuristic approaches like recursive character text splitters are most common today, semantic chunking methods that identify natural topic boundaries show promise for more robust performance across diverse document types.</p> <p>Does chunking strategy actually matter for performance? According to Anton's research, chunking strategy matters tremendously. Their technical report demonstrates significant performance variations based solely on chunking approach, even when using the same embedding model and retrieval system.</p> <p>They discovered two fundamental rules of thumb that exist in tension with each other:</p> <ol> <li>Fill the embedding model's context window as much as possible - Strategies that produce very short chunks tend to yield noisy retrieval results</li> <li>Don't group unrelated information together - Embedding models struggle to summarize chunks containing contradictory or differing information</li> </ol> <p>The most important insight, however, is that you must always examine your data. Many default chunking strategies produce chunks that are far too short because the delimiter characters are in the wrong order or are the wrong characters entirely.</p> <p>By looking at your actual chunks, you can develop intuition about how your chunking strategy is working for your specific use case. This is critical because there's likely no universal \"best\" chunking strategy - the optimal approach depends on your data and task.</p> <p>Key Takeaway: There's no one-size-fits-all chunking strategy. The best approach depends on your specific data and task, which is why examining your actual chunks is essential for diagnosing retrieval problems.</p> <p>How should we evaluate chunking strategies? When evaluating chunking strategies, focus on the retriever itself rather than the generative output. This differs from traditional information retrieval benchmarks in several important ways:</p> <p>Recall is the single most important metric. Modern models are increasingly good at ignoring irrelevant information, but they cannot complete a task if you haven't retrieved all the relevant information in the first place.</p> <p>You should measure recall at the passage level rather than the document level. Traditional IR benchmarks focus on whole document retrieval, but in RAG applications, we care about retrieving specific relevant passages, not just any part of a relevant document.</p> <p>Ranking metrics like NDCG (which consider the order of retrieved documents) are less relevant for RAG applications. As long as the information is available somewhere in the context window, the model can usually extract what it needs regardless of position.</p> <p>The ChromaDB team has released code for their generative benchmark, which can help evaluate chunking strategies against your specific data.</p> <p>Key Takeaway: Focus on passage-level recall rather than document-level metrics or ranking-sensitive measures. The model can handle irrelevant information, but it can't work with information that wasn't retrieved.</p> <p>What practical advice can improve our chunking implementation? The most emphatic advice from Anton was: \"Always, always, always look at your data.\" This point was stressed repeatedly throughout the presentation.</p> <p>Many retrieval problems stem from poor chunking that isn't apparent until you actually examine the chunks being produced. Default settings in popular libraries often produce surprisingly poor results for specific datasets.</p> <p>Retrieval is not a general system - it's dependent on your specific task and data. This means your evaluation needs to be based on the types of queries you expect to see in your application, not generic benchmarks.</p> <p>While better tooling is being developed to help with this process, in the meantime, the best approach is to:</p> <ol> <li>Generate chunks using your chosen strategy</li> <li>Manually examine those chunks to ensure they make semantic sense</li> <li>Test with queries representative of your actual use case</li> <li>Measure passage-level recall to evaluate performance</li> </ol> <p>This approach acknowledges that we're in an interesting era of software development where AI application builders are being forced to learn machine learning best practices that have evolved over decades.</p> <p>Key Takeaway: No amount of sophisticated algorithms can compensate for not understanding your data. Examining your chunks and evaluating them against representative queries is the most reliable path to improving retrieval performance.</p> <p>Final thoughts on chunking for RAG applications The fundamental tension in chunking is between maximizing the use of the embedding model's context window and avoiding the grouping of unrelated information. Finding the right balance requires understanding your specific data and use case.</p> <p>While semantic chunking approaches show promise, even the most basic heuristic methods can perform well if properly configured for your data. The defaults, however, are rarely optimal.</p> <p>As Anton emphasized, retrieval is not a general system but a task-specific one. Your evaluation should reflect the queries your application will actually encounter rather than generic benchmarks.</p> <p>The ChromaDB team is developing better tooling to help with this process, but in the meantime, the most reliable approach is to manually examine your chunks and measure passage-level recall against representative queries.</p>","tags":["text chunking","ChromaDB","retrieval performance","semantic chunking","heuristic chunking","evaluation"]},{"location":"talks/chromadb-anton-chunking/#by-focusing-on-these-fundamentals-rather-than-blindly-applying-frameworks-or-following-defaults-you-can-significantly-improve-the-performance-of-your-rag-applications-and-deliver-better-results-to-your-users","title":"By focusing on these fundamentals rather than blindly applying frameworks or following defaults, you can significantly improve the performance of your RAG applications and deliver better results to your users.","text":"<p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>","tags":["text chunking","ChromaDB","retrieval performance","semantic chunking","heuristic chunking","evaluation"]},{"location":"talks/colin-rag-agents/","title":"Agentic RAG","text":"<p>I hosted Colin Flaherty, previously a founding engineer at Augment and co-author of Meta's Cicero AI, to discuss autonomous coding agents and retrieval systems. This session explores how agentic approaches are transforming traditional RAG systems, what we can learn from state-of-the-art coding agents, and how these insights might apply to other domains.</p> <p>Do agents make traditional RAG obsolete? Colin shared his experience building an agent for SWE-Bench Verified, a canonical AI coding evaluation where agents implement code changes based on problem descriptions. His team's agent reached the top of the leaderboard with a surprising discovery: embedding-based retrieval wasn't the bottleneck they expected.</p> <p>\"We explored adding various embedding-based retrieval tools, but found that for SweeBench tasks this was not the bottleneck - grep and find were sufficient,\" Colin explained. This initially surprised him, as he expected embedding models to be significantly more powerful.</p> <p>The evolution of code retrieval complexity has followed a clear progression:</p> <ul> <li>2023 (Completions): Simple retrieval with low latency requirements</li> <li>2024 (Chatbots): More complex retrieval across multiple files</li> <li>2025 (Agents): Highly complex retrieval across many parts of codebases</li> </ul> <p>When examining how the agent solved problems, Colin observed it would use simple tools like grep and find persistently, trying different approaches until it found what it needed. The agent's persistence effectively compensated for less sophisticated tools.</p> <p>Key Takeaway: Agents don't necessarily make traditional RAG obsolete, but they change how we should think about retrieval systems. The persistence and course-correction capabilities of agents can sometimes overcome limitations in the underlying retrieval tools.</p> <p>Benefits of agentic retrieval with simple tools Agentic retrieval with grep and find offers several advantages:</p> <ol> <li>Iterative retrieval becomes much simpler - instead of complex multi-step embedding processes, agents can simply run multiple searches and refine as they go</li> <li>Token budget management is straightforward - when the agent hits token limits, it can simply truncate old tool calls and rerun them if needed</li> <li>Implementation is low-effort - no need to maintain vector databases, syncing mechanisms, or other infrastructure</li> <li>Course correction happens naturally - if one search approach fails, the agent tries another</li> </ol> <p>However, these simple approaches have clear limitations:</p> <ul> <li>They don't scale well to large codebases</li> <li>They struggle with unstructured natural language content</li> <li>They're relatively slow and expensive compared to optimized embedding lookups</li> </ul> <p>The best approach might be combining both worlds - an agentic loop with access to high-quality embedding models as tools.</p> <p>How to architect retrieval systems for different needs When deciding between traditional RAG, agent+grep/find, or agent+embeddings, Colin recommends considering several factors:</p> <ul> <li>Quality: How good is the final output?</li> <li>Latency: How quickly does the system respond?</li> <li>Cost: What are the computational expenses?</li> <li>Reliability: Does the system correct itself when it fails?</li> <li>Scalability: How well does it handle large indices?</li> <li>Maintenance effort: How much engineering work is required?</li> </ul> <p>Traditional RAG offers decent quality with excellent speed, low cost, and high scalability, but lacks course correction. Agent+grep provides excellent quality and reliability but struggles with speed, cost, and scale. Agent+embeddings combines the best of both but remains slow and expensive.</p> <p>Key Takeaway: Don't throw away your existing retrieval systems - instead, expose them as tools to agents. This gives you the benefits of both approaches while allowing you to optimize based on your specific constraints.</p> <p>Evaluating agentic retrieval systems Colin emphasized a \"vibe-first\" approach to evaluation:</p> <p>\"Start with 5-10 examples and do end-to-end vibe checks before moving to quantitative evaluation. With natural language systems, you can learn so much just from looking at a few examples.\"</p> <p>He noted that improving embedding models doesn't necessarily improve end-to-end performance because agents are persistent - they'll eventually find what they need even with suboptimal tools. This makes traditional embedding evaluation metrics less useful for agentic systems.</p> <p>For those starting from scratch, Colin recommends:</p> <ol> <li>Build the simplest possible retrieval tool</li> <li>Put an agent loop on top</li> <li>Iterate based on what causes the most pain for users</li> <li>Only move to quantitative evaluation once you've addressed obvious issues</li> </ol> <p>I found it refreshing that Colin focused on specific examples of queries rather than abstract discussions of model architectures. As he put it, \"Being a researcher is actually very similar to being a product person - you're working backwards from use cases and examples.\"</p> <p>When to use embedding models vs. simple search tools While grep and find worked well for SWE-Bench's relatively small codebases, Colin identified several scenarios where embedding models become essential:</p> <ol> <li>Searching large codebases</li> <li>Retrieving from unstructured content like Slack messages or documentation</li> <li>Searching across third-party code that models haven't memorized</li> <li>Retrieving from non-text media like video recordings of user sessions</li> </ol> <p>\"If I was a human working on this use case, and I was a really persistent human that never got tired, would having this other search tool help me? If the answer is yes, then it's probably going to be useful for the agent.\"</p> <p>Colin noted that SWE-Bench is somewhat artificial - its repositories are smaller than real-world codebases, and 90% of its problems take less than an hour for a good engineer to solve. In more complex environments, embedding models become increasingly valuable.</p> <p>Improving agentic retrieval systems To enhance agentic retrieval, Colin recommends:</p> <ol> <li>Adding re-rankers to embedding tools to improve precision and reduce token usage</li> <li>Training specialized embedding models for different tasks (e.g., one for code, another for Slack messages)</li> <li>Prompt-tuning tool schemas to guide agents toward efficient usage patterns</li> <li>Creating hierarchical retrieval systems that summarize files and directories</li> <li>Leveraging language server protocols as additional tools</li> </ol> <p>One particularly effective technique is asynchronous pre-processing: \"I've taken songs and used an LLM to create a dossier about each one. This simple pre-processing step took a totally non-working search system and turned it into something that works really well.\"</p> <p>Why aren't more people training great embedding models? When asked what question people aren't asking enough, Colin highlighted the lack of expertise in training embedding models: \"Very few people understand how to build and train good retrieval systems. It just confuses me why no one knows how to fine-tune really good embedding models.\"</p> <p>He attributed this partly to the specialized nature of the skill and partly to data availability. For code, there's abundant data on GitHub, but most domains lack comparable resources. Additionally, the most talented engineers often prefer working on LLMs rather than embedding models.</p> <p>Key Takeaway: As agents become more capable, the quality of their tools becomes increasingly important. Even though agents can compensate for suboptimal tools through persistence, providing them with better retrieval mechanisms significantly improves their efficiency and capabilities.</p> <p>Final thoughts on the future of retrieval Colin believes we're entering an era where the boundaries between traditional RAG and agentic systems are blurring. The ideal approach combines the strengths of both: the speed and efficiency of well-tuned embedding models with the persistence and course-correction of agents.</p> <p>As these systems evolve, we'll likely see more specialized tools emerging for different retrieval contexts, along with more sophisticated pre-processing techniques that make retrieval more effective. The key is focusing on the specific problems you're trying to solve rather than getting caught up in architectural debates.</p> <p>\"Agents are getting radically smarter, but even Einstein preferred writing on paper instead of a stone tablet,\" Colin noted. \"Yes, these agents are persistent, but you should give them whatever you can to improve the odds that they find what they're looking for.\"</p> <p>FAQs</p> <p>What is agentic retrieval and how does it differ from traditional RAG?</p> <p>Agentic retrieval is an approach where AI agents use tools like grep, find, or embedding models to search through code and other content. Unlike traditional RAG (Retrieval-Augmented Generation), which typically uses embedding databases and vector searches, agentic retrieval gives the agent direct control over the search process. This allows the agent to be persistent, try multiple search strategies, and course-correct when initial attempts fail. Traditional RAG is more rigid but can be faster and more efficient for certain use cases.</p> <p>Do agents make traditional RAG obsolete?</p> <p>No, agents don't make traditional RAG obsolete\u2014they complement it. The best approach is often to build agentic retrieval on top of your existing retrieval system by exposing your embedding models and search capabilities as tools that an agent can use. This combines the strengths of both approaches: the persistence and flexibility of agents with the efficiency and scalability of well-tuned embedding models.</p> <p>What are the benefits of using grep and find tools with agents?</p> <p>Using simple tools like grep and find with agents offers several advantages:</p> <ul> <li>Iterative retrieval becomes much easier as agents can refine searches based on previous results</li> <li>Token budget management is simpler since old tool calls can be truncated and rerun if needed</li> <li>The system is easier to build and maintain without complex vector database dependencies</li> <li>Agents can course-correct when searches don't yield useful results by trying different approaches</li> </ul> <p>What are the limitations of using grep and find for retrieval?</p> <p>While grep and find work well for certain scenarios, they have significant limitations:</p> <ul> <li>They don't scale well to very large codebases (millions of files)</li> <li>They're ineffective for searching through unstructured natural language content</li> <li>They work best with highly structured content like code that contains distinctive keywords</li> <li>They can be slower than optimized embedding-based searches for large datasets</li> </ul> <p>What's the ideal approach to retrieval for coding agents?</p> <p>The best approach is often a hybrid system that combines:</p> <ol> <li>An agentic loop that gives the agent control over the search process</li> <li>Access to multiple search tools including grep, find, and embedding-based search</li> <li>The ability to choose the most appropriate tool based on the specific search task</li> <li>Course correction capabilities when initial searches don't yield useful results</li> </ol> <p>How should I evaluate agentic retrieval systems?</p> <p>Start with a qualitative \"vibe check\" using 5-10 examples to understand how the system performs. Observe the agent's behavior, identify patterns in successes and failures, and develop an intuition for where improvements are needed. Only after this initial assessment should you move to quantitative end-to-end evaluations or specific evaluations of individual components like embedding tools. Remember that improving a single component (like an embedding model) may not necessarily improve the end-to-end performance if the agent is already persistent enough to overcome limitations.</p> <p>I already built a retrieval system with custom-trained embedding models. Should I replace it with agentic retrieval?</p> <p>No, don't replace it\u2014enhance it. Build agentic retrieval on top of your existing system by exposing your embedding models and search capabilities as tools that an agent can use. This gives you the best of both worlds: the quality and efficiency of your custom embeddings plus the persistence and flexibility of an agent.</p> <p>How can I improve my agentic retrieval system?</p> <p>Focus on building better tools for your agent:</p> <ul> <li>Add re-rankers to your embedding tools to improve precision and reduce token usage</li> <li>Train different embedding models for different specific tasks</li> <li>Prompt-tune your tool schemas to help the agent use them effectively</li> <li>Consider hierarchical retrieval approaches like creating summaries of files or directories</li> <li>Add specialized tools for specific retrieval tasks (like searching commit history)</li> </ul> <p>How do memories work with agentic retrieval systems?</p> <p>Memories in agentic systems can be implemented by adding tools that save and read memories. These memories can serve as a semantic cache that speeds up future searches by storing information about the codebase structure, relevant interfaces, or other insights gained during previous searches. This can significantly improve performance on similar tasks in the future.</p> <p>Why did embedding models not improve performance on SWE-Bench?</p> <p>For the SWE-Bench coding evaluation, embedding models didn't significantly improve performance because:</p> <ol> <li>The repositories were relatively small, making grep and find sufficient</li> <li>The code was highly structured with distinctive keywords that made text-based search effective</li> <li>The agent's persistence compensated for less sophisticated search tools</li> <li>The tasks were relatively simple, typically solvable by a good engineer in under an hour</li> </ol>"},{"location":"talks/colin-rag-agents/#this-doesnt-mean-embedding-models-arent-valuablethey-become-essential-for-larger-codebases-less-structured-content-or-more-complex-retrieval-tasks","title":"This doesn't mean embedding models aren't valuable\u2014they become essential for larger codebases, less structured content, or more complex retrieval tasks.","text":"<p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"talks/embedding-performance-generative-evals-kelly-hong/","title":"Understanding Embedding Performance through Generative Evals [Kelly Hong]","text":"<p>I hosted a session with Kelly Hong from Chroma, who presented her research on generative benchmarking for retrieval systems. She explained how to create custom evaluation sets from your own data to better test embedding models and retrieval pipelines, addressing the limitations of standard benchmarks like MTEB.</p> <p>Why public benchmarks like MTEB don't reflect real-world performance</p> <p>When evaluating embedding models for retrieval applications, many teams rely on public benchmarks like MTEB (Massive Text Embedding Benchmark). While convenient for comparing models, these benchmarks have significant limitations:</p> <p>The data is too generic and doesn't match specific domain needs. Even domain-specific MTEB datasets like legal documents aren't as specific as your actual production data.</p> <p>The query-document pairs are artificially clean - questions are perfectly formulated and documents are perfect matches. In real applications, user queries are ambiguous, incomplete, and often phrased as statements rather than questions.</p> <p>There's significant risk of data contamination since embedding models may have seen these public datasets during training, making it unclear if you're testing retrieval capability or just memorization.</p> <p>As Kelly put it: \"If you have really good performance on a public benchmark for a given embedding model, that doesn't necessarily guarantee that you'll also get that good performance for your specific production pipeline.\"</p> <p>Key Takeaway: Good performance on public benchmarks doesn't translate to good performance in your specific application. You need to evaluate embedding models on your own data to get meaningful results.</p> <p>The generative benchmarking approach</p> <p>Generative benchmarking creates custom evaluation sets directly from your data through a two-step process:</p> <ol> <li> <p>Chunk filtering: Using an LLM judge to identify document chunks that users would realistically query about</p> </li> <li> <p>Query generation: Creating realistic queries from those filtered chunks</p> </li> </ol> <p>This approach addresses the limitations of public benchmarks because:</p> <ul> <li>It's specific to your data</li> <li>It generates more realistic queries that reflect actual user behavior</li> <li>It tests retrieval on private data that embedding models haven't seen during training</li> </ul> <p>Kelly shared a case study with Weights &amp; Biases where they started with 13,000 document chunks and about 2,000 real user queries from production. After filtering, they generated queries that matched the style and content of real user questions.</p> <p>The results were revealing - the embedding model Weights &amp; Biases had been using (text-embedding-3-small) actually performed worse than alternatives they tested. Even more interesting, the rankings of embedding models on their custom benchmark contradicted the MTEB rankings.</p> <p>Creating realistic queries that match user behavior</p> <p>One of the most important aspects of generative benchmarking is generating queries that actually reflect how users search. Kelly demonstrated that naively generated queries (without context or examples) tend to be perfectly formed questions that make retrieval too easy.</p> <p>For example, a naive query generation might produce \"What is the purpose of artifact versioning in Weights &amp; Biases?\" But real users are more likely to search with something like \"artifact versioning not working.\"</p> <p>To generate realistic queries, Kelly recommended providing:</p> <ol> <li> <p>Context about your application (e.g., \"This is a technical support bot for Weights &amp; Biases\")</p> </li> <li> <p>Example queries from real users (5-7 examples is usually sufficient)</p> </li> </ol> <p>When I work with companies building vertical AI applications, I approach this by:</p> <ul> <li>Interviewing subject matter experts about what they actually search for</li> <li>Having LLMs generate candidate queries through role-playing exercises</li> <li>Getting experts to review and validate the generated queries</li> </ul> <p>I've recently been experimenting with using OpenAI's Deep Research for this - I'll have it research a domain like \"search systems for lawyers dealing with employment claims\" and use that research as part of my prompt. This approach has been surprisingly effective for generating realistic queries.</p> <p>The importance of human involvement in the evaluation process</p> <p>Kelly emphasized that generative benchmarking isn't a fully automated \"press one button and everything's done\" approach. Human involvement remains critical:</p> <p>\"Human involvement is very critical. If you want really good evals, I think it applies to basically any case where you're working with AI as well. I think it's very rare that your system is going to work well with absolutely no human in the loop.\"</p> <p>For the chunk filtering step, the Chroma team:</p> <ol> <li> <p>Started with a small set of about 300 document chunks</p> </li> <li> <p>Manually labeled them as relevant or irrelevant</p> </li> <li> <p>Iterated on their LLM judge criteria 4-5 times to align with human judgment</p> </li> <li> <p>Only then scaled to the full document corpus</p> </li> </ol> <p>I've seen too many teams try to fully automate their evaluation process without ever looking at the actual data. I often tell clients, \"You can't delegate all the thinking to these LLMs.\" The reluctance to actually look at examples is a common problem.</p> <p>I find that many teams lean on LLM evals which give you this feeling of control because you're always twiddling with the generation prompt. But if your recall is very low - if you just can't find the document because of some phrasing in the text - then everything downstream is really bad. People feel empowered to adjust prompts but don't focus enough on making search better.</p> <p>Practical applications of generative benchmarking</p> <p>Kelly outlined several practical applications of generative benchmarking beyond just selecting the best embedding model:</p> <ul> <li>Identifying irrelevant content in your document corpus</li> <li>Iterating on specific components of your retrieval pipeline (reranking, chunk rewriting)</li> <li>Aligning your evaluation set with production query distributions</li> <li>Identifying knowledge gaps in your document corpus</li> </ul> <p>\"I think it becomes even more powerful once you actually have production traffic to work with,\" Kelly noted. \"You can cluster your production queries by topic, and you can see the distribution of topics that your users are asking about, and then you can use that distribution and align your eval set to that.\"</p> <p>Balancing cost and performance in retrieval optimizations</p> <p>When discussing contextual chunk rewriting (where you provide document context for each chunk), Kelly acknowledged its benefits but cautioned about the costs:</p> <p>\"Contextual rewriting is useful, but also very expensive. So I would be more intentional about how you use it.\"</p> <p>She suggested being selective about which chunks need context rather than rewriting everything, potentially using the chunk filtering process to identify which documents would benefit most from context.</p> <p>I emphasized the value of having quantifiable metrics for these decisions: \"This is the benefit of having an actual eval harness. The real conversation isn't 'should we use retrieval, or should we use contextual retrieval,' it is 'if we use contextual retrieval, our metrics go up 2%. Is that worth it? Probably not. But if you did contextual retrieval, and it went up 20%, then the question is, okay, well, is spending 20 cents per document worth the 20% improvement in retrieval?' That is a quantified decision that you can make.\"</p> <p>I've seen similar patterns with hybrid search. In one case, I worked with companies processing transcripts where lexical search performed nearly as well as semantic search but was 10 times faster. I told them, \"If I want to find meeting notes, I want that to be as fast as possible.\"</p> <p>Setting up efficient evaluation infrastructure</p> <p>When asked about hyperparameter tuning and experimentation time, Kelly shared that their process was relatively quick because they worked with small data samples first:</p> <p>\"We did around 4 or 5 iterations, but we did it across a small subset of data. So we were only working with about 300 labeled document chunks. So it was very quick iteration. We weren't doing it over our entire document corpus. So I think that probably took less than a day for me to do.\"</p> <p>I emphasized that how much time you spend on evaluation really depends on what kind of existing infrastructure you have: \"I've joined companies where their embedding model is like sequential call to OpenAI. And I was like, 'well, this is why it's taking you 6 days to do these experiments.'\"</p> <p>I recommend time-boxing experimentation: \"I mostly try to time box how much research we're going to do, and then figure out how many experiments we can run. And usually what's going to happen is as you iterate on your prompts and on your models, the marginal improvement in performance will slowly decrease.\"</p> <p>The goal isn't perfection - it's making the most of your experimental time. I personally use Modal for this - I can reembed all of Wikipedia with any arbitrary model in about 10 minutes, which makes hyperparameter sweeping much more feasible.</p> <p>Handling metadata in retrieval evaluations</p> <p>When asked about incorporating metadata into the evaluation process, Kelly suggested several approaches:</p> <p>\"I think maybe metadata could be useful when you're performing the retrieval task... if you have metadata like, let's say you have a query and retrieve a category of documents that have a certain metadata tag, maybe that can count as a partial retrieval success.\"</p> <p>She also noted that metadata can be valuable for pre-filtering documents before applying an LLM judge: \"If all this data was just scraped from the website, maybe there was a metadata tag that says 'this is from our news section.' We'd want to filter those out, because that's not really relevant to our users.\"</p> <p>I added that metadata filters can be evaluated separately from retrieval quality: \"If you have really simple ones like 'show me all the news articles from last week,' we can individually test our ability to select date ranges.\" This allows you to verify both retrieval accuracy and structured extraction tasks independently.</p> <p>Final thoughts</p> <p>Kelly concluded by emphasizing that generative benchmarking requires human involvement to be effective: \"When we talk about generative benchmarking, that also doesn't mean that this entire process is 100% automated. You do need some human in the loop if you really want a good eval.\"</p> <p>The main takeaway from the session was that good performance on public benchmarks doesn't guarantee good performance for your specific use case.</p> <p>As Kelly put it: \"I think the main takeaway from this entire talk is that if you have good performance on public benchmarks like MTEP, that doesn't necessarily guarantee that same performance for your specific use case, and generative benchmarking is a pretty good solution to that.\"</p> <p>For those interested in trying generative benchmarking, Kelly mentioned that Chroma has made their tools available, and I noted they're offering $1,000 in API credits for participants in the cohort.</p> <p>FAQs</p> <p>What is generative benchmarking?</p> <p>Generative benchmarking is a method to create custom evaluation sets from your own data to test AI retrieval systems. It involves generating realistic queries from your document corpus and using these query-document pairs to evaluate how well different embedding models and retrieval systems perform with your specific data. Unlike public benchmarks, this approach gives you insights directly relevant to your use case.</p> <p>Why are custom benchmarks better than public benchmarks like MTEB?</p> <p>Custom benchmarks address several limitations of public benchmarks like MTEB (Massive Text Embedding Benchmark). While MTEB is widely used for comparing embedding models, it uses generic data that may not reflect your specific domain, contains artificially clean query-document pairs, and may have been seen by models during training. Good performance on MTEB doesn't guarantee good performance on your specific data and use case.</p> <p>How does the generative benchmarking process work?</p> <p>The process involves two main steps. First, chunk filtering identifies document chunks that users would realistically query about, filtering out irrelevant content. Second, query generation creates realistic user queries from these filtered chunks. The resulting query-document pairs form your evaluation set, which you can use to test different embedding models and retrieval components.</p> <p>What's involved in the chunk filtering step?</p> <p>Chunk filtering uses an aligned LLM judge to identify document chunks that contain information users would actually query. This involves creating criteria for relevance, providing a small set of human-labeled examples, and iterating on the LLM judge to improve alignment with human judgment. This step helps filter out irrelevant content like news articles or marketing material that wouldn't be useful in a support context.</p> <p>How do you generate realistic queries?</p> <p>Query generation uses an LLM with specific context about your application and example queries. Providing this context helps the LLM focus on topics users would ask about, while example queries guide the style of generated queries. This approach creates more realistic, sometimes ambiguous queries that better reflect how users actually search, rather than perfectly formed questions that match document content exactly.</p> <p>How do you evaluate retrieval performance with the generated benchmark?</p> <p>Once you have your evaluation set with query-document pairs, you can test different embedding models by embedding each document chunk, storing them in a vector database, and then embedding each query to retrieve the top K document chunks. If the matching document is in the top K results, that counts as a success. This gives you metrics like recall@K and NDCG that you can compare across different models and configurations.</p> <p>What insights can generative benchmarking provide?</p> <p>Generative benchmarking can help you select the best embedding model for your specific data, identify irrelevant content in your document corpus, and evaluate changes to your retrieval pipeline like adding re-ranking or chunk rewriting. It can also reveal when public benchmark rankings don't align with performance on your data, as demonstrated in a case study where model rankings differed from MTEB rankings.</p> <p>Do I need production data to use generative benchmarking?</p> <p>No, you can use generative benchmarking even if you don't have production data yet. All you need is a document corpus to generate an evaluation set. However, if you do have production queries, you can use them to further align your generated queries to real user behavior, identify knowledge gaps in your document corpus, and make your evaluation set even more representative.</p> <p>Is generative benchmarking fully automated?</p> <p>No, generative benchmarking isn't 100% automated. It requires human involvement to get good results. You'll need to align your LLM judge, provide context and example queries to steer query generation, and manually review data throughout the process. The human-in-the-loop aspect is critical for creating evaluation sets that truly reflect your use case.</p> <p>How can I try generative benchmarking on my own data?</p> <p>You can try generative benchmarking on your own data by using Chroma's open-source tools. The full technical report is available at research.trychroma.com, and you can run the process with just a few lines of code. Chroma Cloud is also available if you want to use their hosted vector database solution.</p> <p>How does contextual chunk rewriting fit into retrieval evaluation?</p> <p>Contextual chunk rewriting involves adding context to document chunks to improve retrieval. While it can be effective, especially for content like tables or technical information that lacks context, it's also expensive since it requires running an LLM on every chunk. A more efficient approach might be to only rewrite chunks that need additional context, which you can identify during the filtering process. The value of this approach can be quantified through your evaluation metrics.</p>","tags":["embeddings","evaluation","benchmarking","generative evals","Chroma"]},{"location":"talks/fine-tuning-rerankers-embeddings-ayush-lancedb/","title":"Fine-tuning Re-rankers and Embedding Models for Better RAG Performance","text":"<p>I hosted a session with Ayush, an ML Engineer at LanceDB, to explore how fine-tuning re-rankers and embedding models can significantly improve retrieval performance in RAG systems. We discussed practical approaches to enhancing retrieval quality, the trade-offs involved, and when these techniques make the most business sense.</p>"},{"location":"talks/fine-tuning-rerankers-embeddings-ayush-lancedb/#what-are-re-rankers-and-why-should-we-use-them-in-rag-systems","title":"What are re-rankers and why should we use them in RAG systems?","text":"<p>Re-rankers fit into the RAG pipeline after retrieval and before the context is provided to the LLM. When you retrieve documents from a vector database, the embedding model gets these documents based on semantic search. Re-rankers then rearrange them so that the most relevant documents to the query rank at the top.</p> <p>The power of re-rankers comes from their architecture. While embedding models (bi-encoders) calculate embeddings independently for documents and queries, cross-encoders process both together, allowing them to attend to each other throughout the pipeline. This cross-attention enables much better similarity assessment than independent embeddings.</p> <p>You might wonder why we don't just use cross-encoders for retrieval directly. The answer is computational intensity - comparing a query with each document in your database isn't feasible at scale. However, once you've narrowed down to your top 5-10 results, re-ranking becomes practical and only adds milliseconds to your pipeline.</p> <p>Key Takeaway: Re-rankers are a \"low-hanging fruit\" for improving RAG systems because they don't disrupt your pipeline - you don't need to re-ingest data into your embedding database. They simply plug in after retrieval and before sending context to the LLM.</p>"},{"location":"talks/fine-tuning-rerankers-embeddings-ayush-lancedb/#how-do-you-train-a-re-ranker-from-scratch","title":"How do you train a re-ranker from scratch?","text":"<p>Training a re-ranker requires query-context pairs, similar to training embedding models. In Ayush's benchmark, he used Google's QA dataset with 3 million query-context pairs, using 2 million for training and 5,000 for evaluation.</p> <p>The training process involves providing an anchor (the query) along with positive examples (correct answers) and negative examples (incorrect answers). The most effective approach is mining \"hard negatives\" - wrong answers that are very close to the query in embedding space. These challenge the model to learn subtle distinctions.</p> <p>For the base architecture, Ayush experimented with two models: MiniLM - A small 6 million parameter model that's easy to train and test Modern BERT - A 150 million parameter model developed by AnswerAI that performs better than some billion-parameter models</p> <p>On top of these base models, he attached re-ranking heads (either cross-encoder or ColBERT) and trained them to function as re-rankers. The results showed significant improvements: Vector search improved by 12% at top-5 and 6% at top-10 Full-text search saw improvements up to 20% in some cases Even the small MiniLM model showed improvements over baseline performance</p> <p>Key Takeaway: Even a small re-ranker model can provide significant retrieval improvements. If you're unsure whether re-ranking will help your specific data, try training a small model in 30-60 minutes to get a signal before investing in more sophisticated approaches.</p>"},{"location":"talks/fine-tuning-rerankers-embeddings-ayush-lancedb/#what-is-the-colbert-architecture-and-how-does-it-compare-to-cross-encoders","title":"What is the ColBERT architecture and how does it compare to cross-encoders?","text":"<p>ColBERT is a late interaction model that offers a middle ground between bi-encoders and cross-encoders. It independently calculates document embeddings offline (which can be stored in a vector database), but at query time, it compares token-level embeddings with the query.</p> <p>This approach allows for more nuanced matching than standard bi-encoders while being more computationally efficient than cross-encoders. In Ayush's benchmarks, ColBERT architectures performed well, particularly when built on top of the Modern BERT base model.</p> <p>The key advantage of ColBERT is that it still attends to token-level features but does so in two stages - the part that can be calculated offline is saved in a vector database, and at query time, it performs a maximum operation on token-level similarities to find the best match.</p>"},{"location":"talks/fine-tuning-rerankers-embeddings-ayush-lancedb/#should-i-fine-tune-an-existing-re-ranker-or-train-one-from-scratch","title":"Should I fine-tune an existing re-ranker or train one from scratch?","text":"<p>When deciding whether to train a re-ranker from scratch or fine-tune an existing one, consider these factors:</p> <p>If there are no re-rankers available for your base model of choice, you'll need to train from scratch. If your dataset has become specialized enough (drifted from the original data the model was trained on), you should fine-tune to align with your distribution.</p> <p>In Ayush's experiments, using an off-the-shelf re-ranker (AnswerAI's ColBERT Small) immediately improved performance by about 10% over the baseline. Fine-tuning that model further improved results by an additional 2-3%.</p> <p>Fine-tuning has practical advantages - it converges much faster since the weights aren't random and already have context. However, be careful about catastrophic forgetting - if you fine-tune for too long on low-quality data, the model's performance can degrade from its baseline.</p> <p>Key Takeaway: Start with an existing re-ranker if available for your use case, and fine-tune it on your domain-specific data. This approach is faster and often produces better results than training from scratch.</p>"},{"location":"talks/fine-tuning-rerankers-embeddings-ayush-lancedb/#what-are-the-trade-offs-when-implementing-re-rankers","title":"What are the trade-offs when implementing re-rankers?","text":"<p>The main trade-off with re-rankers is added latency. Ayush presented latency comparisons on an L4 GPU (similar to a T4 with larger memory): Without re-ranking: Baseline latency With ColBERT on Modern BERT: ~30ms additional latency With smaller models like MiniLM: Negligible additional latency on GPU</p> <p>When fetching larger result sets (overfetching documents to then re-rank them), the slowdown factor ranges from 2-3x. On CPU, the penalty is larger - up to 4-5x depending on the architecture.</p> <p>Given these trade-offs, who should use re-rankers? Ayush suggests that rather than asking who should use them, we should ask who shouldn't: If latency is absolutely critical (even 10ms matters), you might want to skip re-ranking In all other cases where a few hundred milliseconds doesn't matter (which is most use cases), re-rankers are worth implementing</p> <p>Key Takeaway: Re-rankers add some latency to your pipeline, but for most applications, the performance improvement outweighs this cost. Only the most latency-sensitive applications should avoid them.</p>"},{"location":"talks/fine-tuning-rerankers-embeddings-ayush-lancedb/#how-effective-is-fine-tuning-embedding-models","title":"How effective is fine-tuning embedding models?","text":"<p>Embedding models fit into your retriever when you ingest data - they create vector representations of your documents that get stored in your database. Fine-tuning these models can also improve retrieval performance.</p> <p>In Ayush's experiments with the MiniLM model: Baseline performance at top-5: 48% After fine-tuning: 58% (a 10% improvement) Similar improvements at top-10</p> <p>However, not all embedding models should be fine-tuned. The ideal scenario for fine-tuning is when: Your domain-specific data has drifted from the original training distribution You have a sufficiently large dataset (tens of thousands of examples)</p> <p>If your data is just a subset of what the model was already trained on, fine-tuning might lead to overfitting and catastrophic forgetting. This was demonstrated when fine-tuning on the Stanford Question Answering Dataset (SQuAD) - only 3 of the fine-tuned models performed better than baseline because most embedding models are already trained on this common dataset.</p> <p>Key Takeaway: Fine-tune embedding models when you have domain-specific data that differs from general web content, and when you have enough examples to properly train the model.</p>"},{"location":"talks/fine-tuning-rerankers-embeddings-ayush-lancedb/#can-we-combine-re-ranking-and-embedding-model-fine-tuning","title":"Can we combine re-ranking and embedding model fine-tuning?","text":"<p>Yes, and this approach yields the best results. In Ayush's experiments: Baseline: 48% (top-5) and 60% (top-10) Best embedding-tuned model: 62% (top-5) and 69% (top-10) Combined with best re-ranking model: 64% (top-5) and 71% (top-10)</p> <p>This gives you flexibility in addressing trade-offs. If latency is a concern, you might focus more on embedding model improvements. If re-ingesting data is problematic (e.g., with billions of entries), you might emphasize re-ranking instead.</p>"},{"location":"talks/fine-tuning-rerankers-embeddings-ayush-lancedb/#how-do-you-find-and-use-hard-negatives-for-training","title":"How do you find and use hard negatives for training?","text":"<p>Hard negatives are incorrect answers that are very similar to the query in embedding space. To mine them: Take your corpus and use an independent embedding model (not the one in your RAG pipeline) Find wrong answers that are closest to your query according to the embedding model Use these as negative examples during training</p> <p>In real-world applications, there are other ways to find negatives: If a user deletes a citation that an LLM provided, that's a strong negative signal You can use an LLM to propose potential negatives and verify them</p> <p>These approaches help create training data that challenges the model to learn subtle distinctions between relevant and irrelevant content.</p>"},{"location":"talks/fine-tuning-rerankers-embeddings-ayush-lancedb/#what-are-real-world-examples-where-fine-tuning-makes-sense","title":"What are real-world examples where fine-tuning makes sense?","text":"<p>Fine-tuning is particularly valuable in specialized domains like legal or healthcare, where general training data from the internet doesn't contain all the necessary information or where much of the information is proprietary.</p> <p>Another scenario is when you understand the specific types of queries users are asking. For example, if you know users frequently ask about timing-related information, but the relevant text doesn't contain explicit time words, fine-tuning can help bridge that semantic gap.</p> <p>Project-specific terminology is another case - if your company uses internal codenames or jargon (like Pokemon names for projects), fine-tuning helps the model understand these connections.</p> <p>Key Takeaway: Fine-tuning is most valuable when dealing with domain-specific knowledge, proprietary information, or company-specific terminology that general models wouldn't have encountered during training.</p>"},{"location":"talks/fine-tuning-rerankers-embeddings-ayush-lancedb/#should-i-use-data-augmentation-or-synthetic-data-generation","title":"Should I use data augmentation or synthetic data generation?","text":"<p>Ayush cautions against confusing these two approaches: Data augmentation helps generalize an already good dataset with more examples to prevent overfitting Synthetic data generation creates new examples from an existing dataset</p> <p>The problem with synthetic data generation is that LLMs (even powerful ones like GPT-4) can hallucinate up to 70% of the time when creating synthetic query-context pairs. This means you might be adding low-quality data to your training set.</p> <p>In Ayush's experiments with SQuAD, synthetic data generation consistently produced worse results than baseline. Be very careful with this approach and verify the quality of synthetic data before using it.</p>"},{"location":"talks/fine-tuning-rerankers-embeddings-ayush-lancedb/#whats-the-business-benefit-of-all-this-fine-tuning-work","title":"What's the business benefit of all this fine-tuning work?","text":"<p>As I explained during the session, if you're not fine-tuning your embedding models or re-rankers, you're probably closer to Blockbuster than Netflix in your approach. At some point, these performance improvements directly correlate with business outcomes.</p> <p>For example: In product recommendations, better re-ranking might mean fewer returns or higher sales volume For content platforms like Netflix, better embeddings ensure users find relevant content without scrolling For search experiences, the difference between the 1st and 10th result is enormous for user experience</p> <p>Whenever you're building for humans (rather than AI), ranking quality matters tremendously. Even small improvements in retrieval performance can translate to significant business value.</p>"},{"location":"talks/fine-tuning-rerankers-embeddings-ayush-lancedb/#what-should-we-be-thinking-about-next-in-retrieval-systems","title":"What should we be thinking about next in retrieval systems?","text":"<p>Ayush highlighted multimodality as the next frontier. While we've made significant progress with text retrieval, humans interact with technology in multimodal ways (text, images, audio, video).</p> <p>The default approach for multimodal embeddings has been CLIP, with many architectures built on top of it. However, we've come a long way from the original CLIP model, and there hasn't been enough content written about these advances.</p> <p>Building better benchmarks for multimodal data retrieval and establishing stronger baselines for multimodal RAG systems represents a significant opportunity. While companies like Cohere now offer RAG over PDF screenshots, it's unclear how well these approaches extend to scenes in movies, audio clips, or podcasts.</p> <p>Key Takeaway: Multimodal retrieval is likely the next wave of innovation in RAG systems, but we still lack established benchmarks and best practices in this area.</p>"},{"location":"talks/fine-tuning-rerankers-embeddings-ayush-lancedb/#how-should-i-approach-model-selection-for-my-rag-system","title":"How should I approach model selection for my RAG system?","text":"<p>When selecting models for your RAG pipeline, it's never just about which model has the best performance. Instead, you need to consider a portfolio of factors: Maintenance costs Inference complexity Latency requirements Performance improvements</p> <p>You might love using a powerful model like GPT-3 as a re-ranker, but if it takes 2 minutes per API call, that's probably not practical for most applications. The best model is the one that balances these considerations for your specific use case.</p> <p>This is why having a range of options - from lightweight models like MiniLM to more powerful ones like Modern BERT - gives you flexibility in designing your system. Start with simpler approaches, measure their impact, and scale up complexity only when necessary.</p> <p>Key Takeaway: Model selection should be driven by your specific constraints and requirements, not just raw performance numbers. Consider the entire system when making these decisions.</p>"},{"location":"talks/fine-tuning-rerankers-embeddings-ayush-lancedb/#faqs","title":"FAQs:","text":"<p>What are re-rankers and why should I use them? Re-rankers are models that improve retrieval quality by reordering documents after they've been retrieved from a database. They fit into your pipeline after retrieval and before the context is provided to an LLM, helping to ensure the most relevant documents appear at the top. Re-rankers are particularly valuable because they don't disrupt your existing pipeline\u2014you don't need to re-ingest your entire dataset, making them a low-hanging fruit for improving retrieval performance.</p> <p>How do re-rankers work compared to embedding models? While embedding models (bi-encoders) calculate embeddings for documents and queries independently, re-rankers (cross-encoders) process document-query pairs together, allowing them to calculate cross-attention between both inputs. This enables re-rankers to better understand the relevance between a query and document, resulting in more accurate rankings. However, this power comes with higher computational costs, which is why re-rankers are typically used only on a small subset of already retrieved documents.</p> <p>What performance improvements can I expect from re-rankers? Based on extensive benchmarking, re-rankers typically improve retrieval performance by 10-20% depending on the algorithm used. In the experiments presented, vector search results improved by 12% for top-5 retrieval and 6% for top-10 retrieval. Full-text search saw even more dramatic improvements, with some models showing up to 20% better performance.</p> <p>What are the trade-offs when using re-rankers? The main trade-off is latency. Re-rankers add processing time after retrieval, typically in the range of tens of milliseconds when using a GPU. On CPUs, the latency penalty is higher, potentially 3-4x the baseline retrieval time. For most applications, this additional latency is acceptable, but if your use case is extremely latency-sensitive (where even 10ms matters), you might want to consider other approaches.</p> <p>What re-ranker architectures are available? There are two main re-ranker architectures: Cross-encoders: These process the query and document together, allowing for maximum interaction but requiring more computation. ColBERT: This \"late interaction\" architecture calculates document embeddings offline and compares token-level embeddings with the query at retrieval time, offering a balance between performance and speed.</p> <p>When should I train a re-ranker from scratch versus fine-tuning an existing one? Train a re-ranker from scratch when: There are no re-rankers available for your preferred base model Your dataset has become highly specialized or has drifted significantly from general data Fine-tune an existing re-ranker when: You want faster convergence during training You have a specialized dataset but don't want to risk catastrophic forgetting Remember that fine-tuning typically converges much faster since the weights aren't random, but training for too long on low-quality data can lead to performance degradation.</p> <p>How do I train a re-ranker? To train a re-ranker, you need query-context pairs with positive (relevant) and negative (irrelevant) examples. The most effective approach is to mine \"hard negatives\"\u2014documents that are semantically similar to the query but aren't actually relevant answers. This challenges the model to learn nuanced distinctions. Tools like sentence-transformers provide frameworks for training re-rankers with appropriate loss functions.</p> <p>Should I fine-tune my embedding models as well? Fine-tuning embedding models can also improve retrieval performance by 8-10%, but it's more disruptive to your pipeline since it requires re-embedding your entire dataset. Consider fine-tuning your embedding model when: Your data has a different distribution than what the model was originally trained on You have a sufficiently large dataset (tens of thousands of examples) Your data is domain-specific (like legal or medical content)</p> <p>Can I combine re-ranking with fine-tuned embedding models? Yes, and this approach can yield even better results. In the experiments presented, combining the best fine-tuned embedding model with the best re-ranker improved performance from a baseline of 48% to 64% for top-5 retrieval, and from 60% to 71% for top-10 retrieval. This gives you flexibility to choose the approach that best fits your latency and performance requirements.</p> <p>What about data augmentation and synthetic data generation? Be cautious with synthetic data generation. While it might seem like a solution for limited data, LLMs can hallucinate up to 70% of the time when generating synthetic query-context pairs. Data augmentation works best when you already have a good dataset and want to prevent overfitting, not as a solution for poor-quality data.</p> <p>How do I evaluate if re-ranking will help my specific use case? Start with a small experiment using a lightweight model that you can train quickly (like MiniLM). If you see improvements with this simple approach, it's a strong signal that investing in more sophisticated re-ranking will yield even better results. This allows you to validate the approach before committing significant resources.</p> <p>What's the future of retrieval improvement beyond re-ranking? Multimodal retrieval is likely the next frontier. While much work has been done on text retrieval, there's still significant room for improvement in retrieving and ranking content across different modalities like images, audio, and video. Building better benchmarks and baselines for multimodal RAG systems represents an important area for future development.</p>"},{"location":"talks/glean-manav/","title":"Enterprise Search and Fine-tuning Embedding Models with Glean","text":"<p>Study Notes</p> <p>I recently hosted Manav from Glean for an insightful guest lecture in my \"Systematically Improving RAG Applications\" course. This session focused on enterprise search and fine-tuning embedding models - a surprisingly underutilized approach that can dramatically improve RAG system performance. Here's what we learned about Glean's approach to optimizing AI and enterprise search through custom embedding models.</p> <p>What is Glean and why does their approach to enterprise search matter? Glean has built a comprehensive Work AI platform that unifies enterprise data across various applications (Google Drive, GitHub, Jira, Confluence) into a single system. Their flagship product, the Glean Assistant, leverages this unified data model to generate relevant answers to user questions and automate workflows.</p> <p>The foundation of their system is their semantic search capability, which Manav emphasized is absolutely critical for enterprise AI success. As he put it, \"Search quality matters - you can't have a good RAG system, you can't have a good overall enterprise AI product unless you have good search.\" This makes intuitive sense - without retrieving the right context from your enterprise data, even the best LLMs will produce hallucinations and incorrect information.</p> <p>What makes enterprise data uniquely challenging? Unlike internet data, which has a significant \"head problem\" where most searches target popular websites or common information sources, enterprise data is far more heterogeneous and doesn't fit neatly into a single mold. Manav explained:</p> <p>\"Enterprise data is very different than internet data... You have your basic document data sources like Google Drive, Google Docs, Confluence, Notion... But you're also working with a bunch of different types of applications, like Slack, which is a messaging platform. You have meetings, which doesn't really meet the standard concept of what a document is. You have GitHub and GitLab... They all behave in slightly different ways.\"</p> <p>This diversity requires a robust, generalized unified data model that can handle the nuances of different data types while maintaining security and privacy. Additionally, company-specific language (project names, initiatives, internal terminology) creates another layer of complexity that generic models struggle with.</p> <p>Key Takeaway: Enterprise search is fundamentally different from web search because of data heterogeneity and company-specific language. A unified data model that can handle diverse data types while preserving security is essential for effective enterprise AI.</p> <p>Why fine-tune embedding models for each customer? One of the most fascinating aspects of Glean's approach is that they build custom embedding models for each customer. While many companies focus on using large, general-purpose embedding models, Glean has found that smaller, fine-tuned models often perform better for specific enterprise contexts.</p> <p>Manav explained their process:</p> <ol> <li>Start with a high-performance base model (typically BERT-based)</li> <li>Perform continued pre-training on company data using masked language modeling</li> <li>Convert the language model into an embedding model through various training techniques</li> <li>Continuously update the model as the company evolves</li> </ol> <p>The results are impressive - after six months, they typically see a 20% improvement in search performance just from learning from user feedback and adapting to company changes.</p> <p>I found it particularly interesting that they prioritize smaller models when appropriate: \"When you're thinking about building really performant enterprise AI... you want to also think about using smaller embedding models when you can, because small embedding models when fine-tuned to the domain and the specific task you have in hand can give you a lot better performance compared to just using large LLMs.\"</p> <p>How do they generate high-quality training data? Creating effective training data for fine-tuning embedding models is challenging, especially with enterprise privacy constraints. Glean uses several creative approaches:</p> <ol> <li>Title-body pairs: Mapping document titles to passages from the document body</li> <li>Anchor data: Using documents that reference other documents to create relevance pairs</li> <li>Co-access data: Identifying documents accessed together by users in short time periods</li> <li>Public datasets: Incorporating high-quality public datasets like MS MARCO</li> </ol> <p>They also leverage synthetic data generation using LLMs to create question-answer pairs for documents, which is particularly valuable for smaller corpuses with limited user activity.</p> <p>What I found most impressive was their attention to application-specific nuances. For example, with Slack data, they don't just treat each message as a document. Instead, they create \"conversation documents\" from threads or messages within a short timespan, then use the first message as a title and the rest as the body. This understanding of how different applications work leads to much higher quality training data.</p> <p>Key Takeaway: Generating high-quality training data requires understanding the nuances of different enterprise applications. Creative approaches like title-body pairs, anchor data, co-access signals, and synthetic data generation can provide valuable training signals even with privacy constraints.</p> <p>How do they learn from user feedback? Once users start interacting with their products, Glean incorporates those signals to further improve their models:</p> <p>For their search product, they use query-click pairs as direct signals of relevance.</p> <p>For RAG-only settings (like their Assistant product), where users don't explicitly click on documents, they face a more challenging problem. They implement various approaches:</p> <ul> <li>Upvote/downvote systems (though these tend to get sparse usage)</li> <li>Tracking when users click on citations to read more about a topic</li> <li>Monitoring various interaction patterns to infer relevance</li> </ul> <p>I've encountered similar challenges in my consulting work - getting explicit feedback signals for generative AI products is notoriously difficult. Manav's candid acknowledgment that \"this is like a pretty hard open question\" resonated with me. Their approach of combining multiple weak signals seems pragmatic.</p> <p>How do they evaluate embedding model quality? Evaluating embedding models in enterprise settings is particularly challenging because:</p> <ol> <li>You can't access customer data directly due to privacy concerns</li> <li>Each customer has a unique model</li> <li>End-to-end RAG evaluation involves many moving parts</li> </ol> <p>Glean's solution is to build \"unit tests\" for their models - targeted evaluations for specific behaviors they want their models to exhibit. For example, they test how well models understand paraphrases of the same query.</p> <p>This approach allows them to:</p> <ul> <li>Set performance targets for each customer's model</li> <li>Identify underperforming models before customers experience issues</li> <li>Focus optimization efforts on specific areas</li> </ul> <p>I particularly liked Manav's emphasis on isolating and improving individual components: \"If you want to really make good tangible progress day by day, isolating and optimizing individual components is always going to be much more scalable than trying to improve everything all together all at once.\"</p> <p>What role does traditional search play alongside embeddings? Despite all the focus on embedding models, Manav emphasized that traditional search techniques remain crucial:</p> <p>\"You don't want to over-index on semanticness or LLM-based scoring as the only thing that your search system should use... you can get a lot more bang for your buck by not using any semanticness at all to answer most questions.\"</p> <p>He estimated that for 60-70% of enterprise search queries, basic lexical search with recency signals works perfectly well. Semantic search becomes more important for complex queries, particularly in agent-based systems.</p> <p>This aligns with my experience - I often tell clients that getting 80% of the way there with full-text search and then adding semantic search as the cherry on top is a practical approach.</p> <p>Key Takeaway: Don't abandon traditional search techniques in pursuit of embedding-based approaches. A hybrid system that leverages both lexical and semantic search, along with signals like recency and authority, will deliver the best results for enterprise search.</p> <p>How do they handle document relevance over time? One interesting question addressed how Glean handles outdated documents that have been superseded by newer information. Their approach centers around a concept they call \"authoritativeness,\" which incorporates:</p> <ol> <li>Recency: Newer documents are generally more relevant</li> <li>Reference patterns: Documents that continue to be linked to or accessed remain authoritative</li> <li>User satisfaction signals: Documents that consistently satisfy user queries maintain relevance</li> </ol> <p>For example, a document containing WiFi password information might be old but still highly relevant if people continue to reference it when answering related questions.</p> <p>This multi-faceted approach to document authority seems more sophisticated than simply prioritizing recent content, which would miss important evergreen documents.</p> <p>Final thoughts on building enterprise search systems Manav concluded with several key insights that resonated with me:</p> <ol> <li>A unified data model is critical for handling heterogeneous enterprise data</li> <li>Company-specific language matters tremendously for search quality</li> <li>Fine-tuned smaller models often outperform generic large models for specific tasks</li> <li>Learning from user feedback, though challenging, provides invaluable signals</li> <li>Evaluating models through targeted \"unit tests\" enables scalable quality assessment</li> <li>Traditional search techniques remain powerful and shouldn't be discarded</li> </ol> <p>As someone who's helped many early-stage companies build RAG systems, I found Glean's approach refreshingly pragmatic. They've clearly learned that the path to high-quality enterprise search isn't just about using the latest, largest models, but about understanding the unique characteristics of enterprise data and building systems that address those specific challenges.</p>","tags":["enterprise search","embedding models","fine-tuning","RAG optimization","Glean"]},{"location":"talks/glean-manav/#the-emphasis-on-company-specific-language-models-particularly-stood-out-to-me-this-is-an-area-where-ive-seen-many-companies-struggle-when-they-try-to-apply-generic-embedding-models-to-their-unique-terminology-and-document-structures-gleans-success-with-this-approach-suggests-that-more-companies-should-consider-fine-tuning-strategies-rather-than-relying-solely-on-off-the-shelf-embedding-models","title":"The emphasis on company-specific language models particularly stood out to me - this is an area where I've seen many companies struggle when they try to apply generic embedding models to their unique terminology and document structures. Glean's success with this approach suggests that more companies should consider fine-tuning strategies rather than relying solely on off-the-shelf embedding models.","text":"<p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>","tags":["enterprise search","embedding models","fine-tuning","RAG optimization","Glean"]},{"location":"talks/john-lexical-search/","title":"Lexical Search in RAG Applications [John Berryman]","text":"<p>Study Notes:</p> <p>I hosted a session featuring John Berryman, who shared his expertise on lexical search and its application in RAG systems. John, who previously worked at GitHub and co-authored books on prompt engineering and information retrieval, provided valuable insights on how traditional search techniques can complement modern vector-based approaches for more effective retrieval augmented generation.</p> <p>Why is lexical search still relevant in the age of semantic search?</p> <p>While semantic search has become the default approach for RAG systems, John highlighted several limitations that make it problematic in certain scenarios. The primary issues include:</p> <ul> <li>Semantic search struggles with exact matches for product IDs, people's names, and specific phrases</li> <li>It performs poorly with niche jargon not present in the embedding training data</li> <li>Relevance is opaque, making debugging difficult when results aren't as expected</li> <li>Filtering is particularly clunky in vector-based systems</li> </ul> <p>The filtering challenge is especially significant. With semantic search, you're forced into suboptimal approaches: either search first and then filter (risking empty results if filters are strict), or filter first and then semantically re-rank (which can be computationally expensive for large datasets).</p> <p>Key Takeaway: Semantic search excels at understanding meaning but struggles with exact matching, specialized terminology, and efficient filtering - all areas where lexical search has established strengths.</p> <p>How does lexical search actually work?</p> <p>John provided a concise explanation of lexical search fundamentals, breaking it down into three components:</p> <ol> <li>Indexing: Documents are processed through analysis pipelines that include character filtering (like lowercasing), tokenization (splitting text into words), stopword removal (filtering common words), and stemming (normalizing word forms). This creates an inverted index where each token points to documents containing it.</li> <li>Searching: The inverted index makes retrieval extremely efficient. For a search like \"brown AND fox,\" the system simply retrieves the document IDs for each term and finds their intersection. This approach handles Boolean logic naturally and can incorporate relevance scoring through methods like TF-IDF (term frequency-inverse document frequency).</li> <li>Results: Beyond just returning matching documents, lexical search can provide rich metadata like aggregations (facet counts showing distribution of values), snippets (relevant text excerpts), and highlights (showing where terms matched).</li> </ol> <p>The key advantage is that lexical search can process filtering and relevance scoring simultaneously, unlike the sequential approach required in semantic search.</p> <p>Key Takeaway: Lexical search's inverted index structure enables efficient simultaneous filtering and relevance scoring, with established relevance algorithms like TF-IDF and BM25 that have been refined over decades.</p> <p>How can lexical search be applied in RAG applications?</p> <p>John demonstrated a practical application using the Wayfair Annotation Dataset (WANDs), containing about 43,000 e-commerce products. He showed how to:</p> <ol> <li>Structure the index with appropriate field types (product name, description, class, rating count)</li> <li>Build queries that combine must-match conditions with should-match boosting factors</li> <li>Apply filters for availability, product class, and minimum ratings</li> <li>Return not just results but also aggregations that provide insight into the matching set</li> </ol> <p>When integrated into a RAG application, this approach allows the LLM to:</p> <ul> <li>Search with pre-applied filters (like showing only products available in the user's state)</li> <li>Explore results through multiple searches to narrow options</li> <li>Leverage aggregation data to understand the distribution of matching products</li> <li>Make more informed recommendations based on both the results and metadata</li> </ul> <p>In the demonstration, a user complaint about back pain led the assistant to search for both ergonomic chairs and standing desks, then refine based on the facet data to focus on adjustable standing desks specifically.</p> <p>Key Takeaway: Lexical search provides RAG systems with powerful filtering capabilities and metadata that allows LLMs to make more informed decisions about how to refine searches and present options to users.</p> <p>What are the limitations of lexical search?</p> <p>Despite its strengths, John acknowledged several significant limitations:</p> <ul> <li>Lexical search struggles with word order and context (e.g., confusing \"dress shoe\" with a shoe that is a dress)</li> <li>It uses a bag-of-words approach that loses semantic meaning</li> <li>It can't recognize synonyms or different words with the same meaning</li> <li>It doesn't understand negation (searching for \"not something\" still matches \"something\")</li> <li>It misses contextual clues that embedding models naturally capture</li> </ul> <p>These are precisely the areas where semantic search excels, suggesting that neither approach is sufficient on its own.</p> <p>Key Takeaway: Lexical search's limitations around understanding meaning, context, and synonyms are the exact strengths of semantic search, pointing toward hybrid approaches as the optimal solution.</p> <p>What hybrid search approaches show promise?</p> <p>John explored several approaches to combining the strengths of lexical and semantic search:</p> <ol> <li>Lexical search plus re-ranking: Use lexical search to filter and provide an initial ranking, then apply semantic re-ranking to the top results. This is established but complex.</li> <li>SPLADE (Sparse Lexical and Dense Expansion): Use language models to identify synthetic synonyms that should have been in the text but weren't, then add these to the lexical index. This expands recall but still has bag-of-words limitations.</li> <li>Acorn: A promising approach that traverses vector structures while simultaneously checking filter matches.</li> <li>Superlinked: Uses different embeddings for different data types, concatenating them into unified vectors that can be searched with nearest neighbor techniques while maintaining filtering capabilities.</li> </ol> <p>John admitted that the ideal hybrid solution remains elusive, but the industry is actively working on approaches that combine the filtering power of lexical search with the semantic understanding of vector-based methods.</p> <p>Key Takeaway: The future likely belongs to hybrid approaches that combine lexical search's filtering capabilities with semantic search's understanding of meaning, though the ideal implementation is still evolving.</p> <p>How do you optimize lexical search for specific domains?</p> <p>During the Q&amp;A, John shared insights on improving search for domain-specific applications:</p> <ol> <li>Start by collecting data on what should match versus what actually matches</li> <li>For e-commerce, analyze click logs to understand user behavior patterns</li> <li>Focus on high-impact queries - often the top 10 queries account for 50% of traffic</li> <li>Use human curators familiar with the domain to create judgment lists</li> <li>Consider learn-to-rank algorithms that can automatically tune parameters based on this data</li> </ol> <p>For hybrid approaches, he suggested getting the top 100-1000 results from lexical search, then using a feature store to retrieve additional information (including embeddings) for re-ranking with models like LambdaMart or XGBoost.</p> <p>Key Takeaway: Optimizing search requires a combination of data analysis, human expertise, and appropriate algorithms. The most effective approach often involves using lexical search for initial filtering and retrieval, then applying more sophisticated ranking methods.</p> <p>Final thoughts on the lexical versus semantic debate</p> <p>While John humorously admitted his bias toward lexical search (\"lexical search is my hammer and the world is my nail\"), he clearly recognized that both approaches have complementary strengths. Lexical search excels at filtering, exact matching, and providing rich metadata, while semantic search better understands meaning, context, and synonyms.</p> <p>The most promising direction appears to be hybrid approaches that leverage the strengths of both methods, though implementing these effectively remains challenging. As LLMs become more capable, they may also play a role in dynamically adjusting search parameters based on user needs, further blurring the line between lexical and semantic approaches.</p>","tags":["lexical search","traditional search","hybrid search","information retrieval","TF-IDF","filtering"]},{"location":"talks/john-lexical-search/#key-takeaway-the-debate-isnt-really-about-lexical-versus-semantic-search-but-rather-how-to-effectively-combine-them-to-create-retrieval-systems-that-are-both-precise-and-understanding-offering-both-the-filtering-power-of-traditional-search-and-the-semantic-comprehension-of-modern-embedding-based-approaches","title":"Key Takeaway: The debate isn't really about lexical versus semantic search, but rather how to effectively combine them to create retrieval systems that are both precise and understanding - offering both the filtering power of traditional search and the semantic comprehension of modern embedding-based approaches.","text":"<p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>","tags":["lexical search","traditional search","hybrid search","information retrieval","TF-IDF","filtering"]},{"location":"talks/online-evals-production-monitoring-ben-sidhant/","title":"Online Evals and Production Monitoring [Ben Hylak &amp; Sidhant Bendre]","text":"<p>I hosted a lightning lesson featuring Ben from Raindrop and Sid from Oleve to discuss AI monitoring, production testing, and data analysis frameworks. This session explored how to effectively identify issues in AI systems, implement structured monitoring, and develop frameworks for improving AI products based on real user data.</p> <p>What are the fundamentals of AI monitoring and why are traditional approaches insufficient?</p> <p>The foundation of AI monitoring begins with evals, which function similarly to unit tests in traditional software development. An eval consists of an input (either a single message or conversation) and an expected output from the model.</p> <p>While evals are useful for offline testing, they have significant limitations when applied to production environments:</p> <p>\"The naive solution that people reach for is to run evals on some small percentage of production traffic,\" Ben explained. \"But this can be extremely expensive, especially if you're using larger models as judges.\"</p> <p>Beyond cost concerns, there are deeper issues with relying solely on LLM judges for evaluation:</p> <ul> <li>They're difficult to set up accurately and require detailed definitions of what constitutes \"good\" or \"bad\" performance</li> <li>They only evaluate what you already know to look for, missing novel failure modes</li> <li>They struggle to identify emerging problem patterns</li> </ul> <p>I've seen this challenge firsthand with clients who implement sophisticated eval systems but still miss critical issues that only emerge in production. The fundamental problem is that in AI applications, unlike traditional software, there's often no exception being thrown when something goes wrong - the model simply produces an inadequate response.</p> <p>Key Takeaway: Traditional error monitoring tools like Sentry don't work for AI products because there's no explicit error message when an AI system fails. Instead, we need specialized approaches that can identify problematic patterns in model outputs and user interactions.</p> <p>How do we effectively identify issues in AI systems?</p> <p>Ben introduced what he calls \"the anatomy of an AI issue,\" which consists of two main components: signals and intents.</p> <p>Signals come in two varieties:</p> <ol> <li> <p>Implicit signals - Signs from the data itself that something is wrong:</p> </li> <li> <p>User frustration (\"Wait, no, you should be able to do that\")</p> </li> <li>Task failures (when the model says it can't do something)</li> <li>NSFW content (users trying to hack the system)</li> <li>Laziness (model not completing requested tasks)</li> <li> <p>Forgetting (model losing context of previous interactions)</p> </li> <li> <p>Explicit signals - Trackable user actions that indicate satisfaction or dissatisfaction:</p> </li> <li> <p>Thumbs up/down ratings</p> </li> <li>Regeneration requests (suggesting the first response was inadequate)</li> <li>Search abandonment</li> <li>Code errors (especially valuable for coding assistants)</li> <li>Content copying or sharing (positive signals)</li> </ol> <p>\"You really need this sort of constant IV of your app's data,\" Ben emphasized. \"There's nothing right now where you can just hit 'go' and the thing is going to constantly improve itself for your customers. That tool doesn't exist yet.\"</p> <p>For smaller applications with fewer than 500 daily events, Ben recommends piping every user interaction into a Slack channel where you can manually review them. This helps you discover not just where the model is wrong, but what's confusing about your product and what features users expect but don't yet exist.</p> <p>Key Takeaway: Effective AI monitoring requires tracking both implicit signals (patterns in user and model language that suggest problems) and explicit signals (user actions that indicate satisfaction or dissatisfaction), then exploring these signals to identify recurring issues.</p> <p>What framework can help organize and prioritize AI improvements?</p> <p>Sid introduced the Trellis framework (Targeted Refinement of Emergent LLM Intelligence through Structured Segmentation), which his team at Oleve uses to manage AI products that reach millions of users within weeks of launch.</p> <p>The framework has three core axioms:</p> <ol> <li> <p>Discretization - Converting the infinite plane of possible AI outputs into specific, mutually exclusive buckets (like \"math homework help\" or \"history assignment assistance\")</p> </li> <li> <p>Prioritization - Scoring mechanisms to rank which buckets matter most based on metrics like sentiment, conversion, retention, and strategic priorities</p> </li> <li> <p>Recursive refinement - Continuously organizing within buckets to find more structure within the chaos of outputs</p> </li> </ol> <p>\"The idea in general with Trellis is to break down your infinite output space into mutually exclusive buckets, figure out what buckets matter to you, and keep recurring down until you've solved your entire space of what matters for your users,\" Sid explained.</p> <p>The implementation follows six steps:</p> <ol> <li> <p>Initialize your output space by launching a minimal but generally capable MVP</p> </li> <li> <p>Cluster user interactions by specific intents</p> </li> <li> <p>Convert clusters into semi-deterministic workflows</p> </li> <li> <p>Prioritize workflows based on company KPIs</p> </li> <li> <p>Analyze workflows to discover sub-intents or misclassified intents</p> </li> <li> <p>Recursively apply the process to refine each workflow</p> </li> </ol> <p>For prioritization, Sid recommends going beyond simple volume metrics: \"A very naive approach is pretty much volume only... This could generally be useful, but it can be misleading if you're getting a lot of traffic on something you're already good at.\"</p> <p>Instead, he suggests a formula: Volume \u00d7 Negative Sentiment \u00d7 Achievable Delta \u00d7 Strategic Relevance. This helps identify areas where improvements will have the greatest impact with reasonable effort.</p> <p>Key Takeaway: The Trellis framework provides a structured approach to taming the chaos of AI outputs by categorizing user intents, creating specialized workflows for each intent, and prioritizing improvements based on a combination of volume, sentiment, achievability, and strategic importance.</p> <p>How can we fix issues once we've identified them?</p> <p>Once you've identified and categorized issues, Ben outlined several approaches to fixing them:</p> <ol> <li> <p>Prompt changes - Often the first and simplest solution</p> </li> <li> <p>Offloading to tools - Routing problematic intents to specialized tools or more capable models</p> </li> <li> <p>RAG pipeline adjustments - Modifying storage, memory descriptions, or retrieval methods</p> </li> <li> <p>Fine-tuning - Using identified issues as training data for model improvements</p> </li> </ol> <p>Sid shared a real example from Oleve's product Unstuck, where they noticed recurring alerts from Raindrop about summary quality issues. Because they had already organized their product around the Trellis framework, they knew exactly which workflow needed improvement.</p> <p>\"We decided to prioritize that. The good thing is, we had summaries already aligned to our summarize workflow, so we knew it was just one workflow to fix instead of a bunch of others,\" Sid explained.</p> <p>After implementing changes, they saw an immediate decrease in alerts and received direct user feedback confirming the improvement: \"One of my co-founders got a text from one of our users who said that whatever I pushed the night before had suddenly helped him get better summaries for his Spanish class.\"</p> <p>This case study demonstrates the value of having \"self-contained, blameable pieces of your infrastructure\" that allow you to identify, isolate, and fix specific issues without affecting the entire system.</p> <p>Key Takeaway: Fixing AI issues requires a portfolio of approaches from simple prompt changes to sophisticated fine-tuning. The key is having a structured system that allows you to attribute problems to specific workflows and measure the impact of your improvements.</p> <p>What are some notable examples of AI failures in production?</p> <p>Ben shared several examples of high-profile AI failures that traditional testing might have missed:</p> <ul> <li>Virgin Money's chatbot threatening users because they kept using the word \"Virgin\" (the company's name)</li> <li>Grok responding to unrelated questions with statements about \"white genocide in South Africa\"</li> <li>Google Gemini Cloud Console misinterpreting basic questions about account credits</li> <li>OpenAI's model encouraging harmful user behaviors after being optimized too heavily on user preferences</li> </ul> <p>What's particularly telling is OpenAI's admission that \"our evals didn't catch it\" and their statement that \"evals won't catch everything. Real world use helps us spot problems and understand what matters most to users.\"</p> <p>These examples highlight why production monitoring is essential - the real world introduces edge cases and user behaviors that even the most comprehensive testing regimes will miss.</p> <p>Key Takeaway: Even the largest AI companies with sophisticated testing infrastructure experience unexpected failures in production. This underscores the importance of robust monitoring systems that can detect novel issues as they emerge in real-world usage.</p> <p>How long does it take to implement effective AI monitoring?</p> <p>When I asked Sid about the timeline for implementing the Trellis framework, he described a gradual process that began before public launch:</p> <p>\"We launched it to a private beta of people like a month and a half before our public launch. This was about 10 to 15 students out in NYU.\"</p> <p>They integrated Raindrop on day one of their public launch, which then tracked their growth from a few thousand users in the first week to 500,000 by the first month, and then a million the month after.</p> <p>The process of refining their workflows and monitoring was continuous: \"For those first few months it was a lot of me just looking at the data, understanding what made sense, understanding if we're routing to the right workflows, understanding if we had the right clusters, and then tuning those workflows.\"</p> <p>Sid noted that they only reached stability about four months after launch: \"We only really started hitting stability towards December. But it was a continuous loop in terms of looking at data, trying things out, looking at data, trying things out.\"</p> <p>Key Takeaway: Implementing effective AI monitoring is not a one-time setup but an iterative process that begins before launch and continues throughout a product's lifecycle. Even with sophisticated tools, the human element of analyzing data and refining systems remains essential.</p> <p>What unexpected insights can emerge from AI monitoring?</p> <p>One of the most valuable aspects of comprehensive monitoring is discovering unexpected user behaviors that wouldn't be apparent otherwise.</p> <p>Sid shared an interesting example from Unstuck: \"Even though we have a side pane with a transcript available to you when you upload a lecture, for some reason people still wanted the transcript in chat.\"</p> <p>This insight revealed that users wanted to engage with transcripts differently than the team had anticipated, despite having built what they thought was an intuitive interface for accessing this information.</p> <p>I've had similar experiences with clients whose products suddenly went viral in unexpected regions: \"We'll launch a product and all of a sudden our evals start struggling, and then we come back and say, 'Oh, we just went viral in Turkey, and a lot of our prompts are in English.'\"</p> <p>These types of insights are nearly impossible to anticipate through traditional testing but become immediately apparent with proper monitoring systems.</p> <p>Key Takeaway: Comprehensive AI monitoring often reveals unexpected user behaviors and preferences that wouldn't be discovered through traditional testing. These insights can drive product improvements that better align with how users actually interact with your system.</p> <p>How do we ultimately make AI products better?</p> <p>When I asked what question we should have covered, Ben highlighted the fundamental challenge: \"How do you actually make things better? Is it just changing a word in a prompt? Is it actually fine-tuning something? What is the actual tool that's going to make your product better?\"</p> <p>The answer varies significantly depending on the specific product and issue. As I explained, it's about having a portfolio of tools at your disposal:</p> <p>\"People are asking, 'Do we build agents? Should we use RAG?' But really it's about having a portfolio of tools at your disposal. Are there tools that are underutilized? Are there tools that are not performant? Are there tools that need to be expanded? Or are they just tools that don't exist that we need to invest in?\"</p> <p>Effective monitoring and analysis frameworks like those presented by Ben and Sid allow teams to inspect this portfolio and make better decisions about resource allocation and technical investments.</p> <p>Key Takeaway: There's no one-size-fits-all solution for improving AI products. Success requires a diverse toolkit of approaches, from prompt engineering to fine-tuning, combined with monitoring systems that help you determine which tools will have the greatest impact on your specific challenges.</p> <p>FAQs</p> <p>What are evals in AI monitoring?</p> <p>Evals are similar to unit tests in traditional software engineering. They consist of an input (either a single message or an entire conversation) and an expected output from the model. Evals can provide a binary pass/fail result or a score, and they're primarily used for offline testing to iterate on prompts and ensure your AI system performs as expected.</p> <p>How do offline evals differ from production monitoring?</p> <p>Offline evals are run locally or in CI/CD pipelines to test specific scenarios and prevent regressions. They're useful for iterating on prompts and ensuring changes don't break existing functionality. Production monitoring, however, involves analyzing real user interactions to identify issues that may not have been anticipated during development, providing insights into how your AI system performs in the real world.</p> <p>What are LLM judges and why should I be cautious about them?</p> <p>LLM judges are language models used to evaluate outputs from other models. While they can be useful for assessing subjective qualities (like whether a joke is funny), they can be misleading if not set up properly. The main concerns are that they're expensive to run at scale, difficult to configure accurately, and may not detect novel problems outside their evaluation criteria. It's best to use LLM judges sparingly and primarily for binary decisions with well-defined conditions.</p> <p>What signals should I look for to identify AI issues in production?</p> <p>There are two types of signals to monitor: implicit and explicit. Implicit signals come from the data itself, such as user frustration expressions, task failures, or NSFW content. Explicit signals are actions users take that indicate satisfaction or dissatisfaction, like thumbs up/down, regenerating responses, abandoning searches, or copying/sharing content. Both types of signals help identify patterns of issues in your AI system.</p> <p>How can I effectively explore and categorize AI issues?</p> <p>Start by breaking down issues by metadata (like browser type, model used, or user plan) to identify patterns. Analyze keywords associated with problematic interactions and examine the intersection of user intents and issue types. Use tools like semantic search to find similar issues and cluster them. This exploration helps you understand the scope and impact of different problems.</p> <p>Why is it important to maintain a constant flow of production data?</p> <p>Without continuous monitoring of production data, you'll miss emerging issues and user frustration patterns. For high-volume applications, use tools that summarize patterns and notify you of significant issues. For lower-volume applications (less than 500 events daily), consider reviewing every user interaction to understand what's confusing about your product and what features users expect but don't yet exist.</p> <p>What is the Trellis framework?</p> <p>Trellis (Targeted Refinement of Emergent LLM Intelligence through Structured Segmentation) is an operating framework for designing reliable AI experiences. It helps organize the \"infinite chaos\" of AI outputs into controllable, structured segments so you can prioritize engineering efforts on what matters most. The framework has three core axioms: discretization, prioritization, and recursive refinement.</p> <p>How do I implement the Trellis framework?</p> <p>Start by launching a minimal viable product to gather real user interactions. Cluster these interactions by intent, then convert the clusters into semi-deterministic workflows with an intent router that directs user requests to the appropriate workflow. Prioritize workflows based on metrics relevant to your business goals, then recursively analyze each workflow to identify sub-intents or misclassified intents that could become new workflows.</p> <p>How should I prioritize which AI issues to fix first?</p> <p>While volume (how many users experience an issue) is important, it shouldn't be your only consideration. A more effective approach is to multiply volume by negative sentiment score and then by an estimated achievable delta (how much you can realistically improve the experience). This helps you focus on issues that affect many users, cause significant frustration, and can be fixed relatively easily.</p> <p>What are the main approaches to fixing issues in AI systems?</p> <p>There are several approaches to improving AI performance: prompt changes (usually the first and simplest solution), offloading problematic intents to more capable models or specialized tools, improving your RAG (Retrieval-Augmented Generation) pipeline for memory-related issues, and fine-tuning models using supervised or reinforcement learning techniques based on the ground truth signals you've collected.</p> <p>Why is it important to make AI improvements attributable and testable?</p> <p>When building AI systems, you want your improvements to be engineered, repeatable, testable, and attributable\u2014not accidental. By organizing your system into discrete workflows, you can identify exactly which component is causing an issue and fix it without affecting other parts of the system. This makes your improvements more reliable and your system easier to maintain.</p> <p>How can I validate that my AI improvements are working?</p> <p>Monitor your system before and after making changes to see if the frequency of related issues decreases. Look for positive user feedback that specifically mentions the improved experience. The most reliable validation comes from seeing a measurable reduction in the issues you were targeting, combined with positive user sentiment about the specific improvements you made.</p>","tags":["monitoring","evaluation","production","AI systems","debugging"]},{"location":"talks/query-routing-anton/","title":"Data Organization and Query Routing for RAG Systems [Anton Troynikov]","text":"<p>Study Notes:</p> <p>I recently hosted a session featuring Anton Troynikov from ChromaDB who shared critical insights about organizing data for retrieval systems. This often-overlooked aspect of RAG implementation can significantly impact retrieval accuracy and overall system performance. Here's a breakdown of the key concepts and best practices for structuring your data to optimize query routing.</p> <p>Why is data organization so critical for RAG systems? Anton emphasized that before doing anything else with your retrieval system, you need to look at two critical elements: your data and your queries. This fundamental step is often skipped, leading to suboptimal performance.</p> <p>\"You need to look at your data. You need to look at what exactly is going into your vector database or your database. You need to actually see what's in there before you do literally anything else,\" Anton explained.</p> <p>Beyond just examining your data, you should also analyze what your users are actually trying to access. There's often a misalignment between the data you've stored and what users are attempting to retrieve, which can lead to unexpected results.</p> <p>This initial examination forms the foundation for how you'll organize your data for effective retrieval. Without this understanding, you're essentially building on unstable ground.</p> <p>Key Takeaway: Always examine both your data and user queries before setting up your retrieval pipeline. This fundamental step will inform your data organization strategy and significantly impact your system's performance.</p> <p>What factors should you consider when organizing data for retrieval? When organizing data for retrieval, Anton highlighted three main considerations: user access patterns, data source characteristics, and query scope.</p> <p>For user access patterns, ask yourself:</p> <ul> <li>Do users have access to their own data or shared data?</li> <li>Is access limited by team, role, or other factors?</li> <li>Does access change over time (adding/removing users, changing permissions)?</li> </ul> <p>For data sources, consider:</p> <ul> <li>Are you dealing with different types of data (financial reports, scientific papers, etc.)?</li> <li>Is your data multimodal (text, images, audio)?</li> <li>How frequently is each data source updated?</li> <li>Do all users have access to all data sources?</li> </ul> <p>For query scope, think about:</p> <ul> <li>Do queries need to be filtered by date range, keywords, or other metadata?</li> <li>Are queries uniform across users, or do different users query the same data differently?</li> </ul> <p>Understanding these factors will help you determine the most effective way to organize your data for retrieval. This isn't about finding a one-size-fits-all solution, but rather developing a conceptual framework for making informed decisions.</p> <p>The \"big pile of records\" approach and its limitations Anton described what he calls the \"big pile of records\" approach - using one giant index for all data in your application. This is the default assumption for many vector database vendors, primarily because they were originally built as large-scale search indexes rather than application databases.</p> <p>\"This is what most vendors try to push you into doing,\" Anton noted. \"It's not strictly illegal. You're probably not going to get arrested for doing this yet.\"</p> <p>However, this approach has several significant drawbacks:</p> <ol> <li>Security and compliance issues: User data is commingled with no logical separation except through filters, which may not provide the same guarantees as proper access controls.</li> <li>Reduced recall: This is a technical but crucial point. When you filter a large index, you're more likely to miss relevant results due to how vector search works under the hood.</li> </ol> <p>Anton explained: \"The way that vector search works under the hood for most implementations today is you have your vectors stored as a graph data structure, and you have a compute budget for how many edges of that graph you're going to traverse.\"</p> <p>When filtering, you're essentially wasting part of your compute budget on irrelevant nodes, which reduces your ability to find all relevant results. The more specific your filter (like filtering to just one user out of many), the worse this problem becomes.</p> <ol> <li>Performance issues: Writing to an already giant index is more expensive than writing to smaller, dedicated indexes.</li> <li>Commingled data types: Different types of data (like code vs. documentation) may require different embedding models for optimal performance, but a single index forces you to use a general embedding model.</li> </ol> <p>Key Takeaway: The \"big pile of records\" approach is simple but comes with significant drawbacks in security, performance, and recall. It's suitable for simple use cases like searching Wikipedia, but problematic for complex applications with multiple users and data sources.</p> <p>The denormalized approach: One index per user per data source As an alternative to the \"big pile\" approach, Anton proposed what he calls the \"fully denormalized\" approach: creating one index per user per data source.</p> <p>While this sounds complicated and potentially expensive, it offers several significant advantages:</p> <ol> <li>Complete separation of user data, eliminating commingling concerns</li> <li>Improved recall because you're no longer filtering indexes</li> <li>Independent indexing, allowing data sources to be updated individually</li> <li>More efficient reads, as you're working with smaller indexes</li> </ol> <p>\"It basically overcomes all the problems that I discussed so far,\" Anton explained.</p> <p>The main challenges with this approach include:</p> <ul> <li>Managing more indexes (though Anton noted that ChromaDB was designed to make this efficient)</li> <li>Determining where to route queries</li> <li>Handling duplicate updates and deletes</li> </ul> <p>For mapping users to indexes, Anton suggested simple solutions like keeping a relational table of which data source goes to which user. This approach also moves access control out of the vector database itself, which may not have robust controls for compliance.</p> <p>Key Takeaway: The denormalized approach of one index per user per data source offers better security, performance, and recall than the \"big pile\" approach, though it requires more sophisticated management of indexes and query routing.</p> <p>How do you route queries to the right data sources? With multiple indexes, routing queries becomes a critical challenge. Anton outlined two main approaches:</p> <ol> <li>Full multiplexing: Send the query to all applicable data sources and combine the results. This raises the question of how many results to return from each source. With longer context windows and lower per-token costs, you could simply send all results to the model and let it figure out what's relevant.</li> </ol> <p>Re-ranking models can be particularly effective here: \"I think re-ranking models are actually underrated, mostly because people try to use them in the one giant index context instead of using them as an augmentation to query routing.\"</p> <ol> <li>LLM routing: Have an LLM determine which sources are relevant for a given query. This works by telling the model which sources are available and having it decide where to route the query.</li> </ol> <p>\"This is a classification task that they're really good at,\" Anton noted. You don't need the latest and greatest model for this - even smaller, faster models can handle this type of classification effectively.</p> <p>When using an LLM as a judge for routing, Anton emphasized the importance of calibration: \"Go back and look at your data, go and see what the LLM is actually really doing and see what you as a human would do.\"</p> <p>Key Takeaway: Query routing can be handled through full multiplexing (sending queries to all relevant sources) or LLM routing (having a model decide which sources are relevant). Both approaches have merit, with re-ranking models being particularly valuable for combining results from multiple sources.</p> <p>Why does filtering reduce recall in vector search? One of the most technical but important points Anton made was explaining why filtering reduces recall in vector search. This isn't immediately obvious to many practitioners.</p> <p>Vector search at scale uses approximate nearest neighbor (ANN) search rather than exact matching. As Anton explained:</p> <p>\"In order to make vector search efficient, we cannot use exact matching. Beyond a certain scale, it becomes infeasible to store all of your vectors in memory, and then to perform a nearest neighbor computation against all of those vectors.\"</p> <p>Instead, vectors are stored in graph data structures that allow for logarithmic lookup times. This is a fundamental trade-off: \"We trade recall for speed, and we trade recall for space.\"</p> <p>When filtering is applied in this context, there are two approaches:</p> <ol> <li>Pre-filtering: Traversing the graph and only considering nodes that match the filter. If your filter has low selectivity (matches a small percentage of data), most nodes you encounter won't contribute relevant results, wasting your compute budget.</li> <li>Post-filtering: Grabbing many more results than needed (say 1,000 instead of 10) and then filtering that list. This reduces recall even further.</li> </ol> <p>Anton provided a geometric intuition: \"Imagine you have red and green points in space, and you're looking for green points. If there's roughly 50-50 red and green points and you draw a circle around them, you're going to have a good chance of getting the number of green points that you're looking for. However, if there are 10 times more red points than green points, and you're selecting 100 points, you're only going to get one-tenth of all the ones that you're looking for.\"</p> <p>Key Takeaway: Filtering in vector search inherently reduces recall due to the approximate nature of vector search algorithms. The more specific your filter, the worse this problem becomes, which is why organizing data into separate indexes can significantly improve performance.</p> <p>Should you fine-tune embedding models for specific indexes? On the topic of fine-tuning embedding models, Anton was enthusiastic: \"It's cheap, and you should do it.\"</p> <p>He noted that many people get intimidated by fine-tuning because it seems like \"big brain AI PhD land,\" but the reality is that \"the hardest part of fine-tuning is creating a dataset and then benchmarking the results.\"</p> <p>With separate indexes for different users or data sources, fine-tuning becomes even more powerful. Anton gave the example of an engineering firm with a parts catalog: \"The people who are creating the products have a different use for the data in that parts catalog to what the sales people have. So it's the same data, but two groups of users are accessing it completely differently.\"</p> <p>Using techniques like embedding adapters, you can fine-tune for different user groups without even needing to re-embed your data, making it a very cost-effective approach.</p> <p>Key Takeaway: Fine-tuning embedding models for specific indexes or user groups is more accessible than many realize and can significantly improve retrieval performance. With separate indexes, this becomes even more practical and powerful.</p> <p>How should you handle dynamic metadata filtering? For implementing dynamic metadata filtering, Anton shared examples from financial research and legal domains:</p> <p>In financial research, you might filter documents based on stock tickers. If a user asks about Uber, it's more effective to either filter against the Uber ticker or have a dedicated index for Uber-related documents.</p> <p>In the legal domain, you might have different collections for court decisions, briefs, and evidence. When a user asks about a specific court's determination on a particular case, you can route the query specifically to the court decisions index rather than searching across all legal documents.</p> <p>The key is understanding the natural categories in your domain and creating either separate indexes or robust metadata filtering based on those categories.</p> <p>Key Takeaway: Dynamic metadata filtering should be based on natural categories in your domain and user query patterns. This can be implemented through dedicated indexes or metadata filters, with the choice depending on your specific requirements for recall, performance, and security.</p> <p>Final thoughts on data organization for RAG Anton emphasized that many of the best practices for organizing data in retrieval systems are still evolving. The field of AI application development is relatively young, and many current approaches still carry conceptual baggage from traditional search technologies.</p> <p>\"A lot of what we do in retrieval still has this conceptual overhang from when these were primarily search technologies,\" Anton noted. The difference is that search indexes rarely remove data, while application databases need to accurately represent a constantly changing world.</p> <p>As we continue to develop best practices for RAG and retrieval systems, we need to carefully consider which concepts from search technology apply to retrieval and which need to be reconsidered.</p> <p>The key message throughout Anton's presentation was that data organization isn't just a technical implementation detail - it's a fundamental design decision that impacts security, performance, and most importantly, the quality of results your system can deliver.</p>","tags":["data organization","query routing","vector search","ChromaDB","retrieval optimization","filtering"]},{"location":"talks/query-routing-anton/#key-takeaway-data-organization-for-rag-systems-requires-thinking-beyond-traditional-search-paradigms-by-carefully-considering-user-access-patterns-data-source-characteristics-and-query-requirements-you-can-design-a-system-that-delivers-better-results-while-maintaining-security-and-performance","title":"Key Takeaway: Data organization for RAG systems requires thinking beyond traditional search paradigms. By carefully considering user access patterns, data source characteristics, and query requirements, you can design a system that delivers better results while maintaining security and performance.","text":"<p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>","tags":["data organization","query routing","vector search","ChromaDB","retrieval optimization","filtering"]},{"location":"talks/rag-antipatterns-skylar-payne/","title":"RAG Anti-patterns in the Wild, and How to Fix Them [Skylar Payne]","text":"<p>I hosted a Lightning Lesson with Skylar Payne, an experienced AI practitioner who's worked at companies like Google and LinkedIn over the past decade. Skylar shared valuable insights on common RAG (Retrieval-Augmented Generation) anti-patterns he's observed across multiple client engagements, providing practical advice for improving AI systems through better data handling, retrieval, and evaluation practices.</p> <p>What are the most common RAG anti-patterns across different industries? Skylar has worked with various use cases including customer support knowledge bases, medical advice chatbots, financial news summarization, academic research assistants, and e-commerce product comparison tools. Across these diverse applications, he's consistently observed similar problems that prevent RAG systems from performing optimally.</p> <p>The fundamental issue that keeps appearing is insufficient attention to data quality and evaluation. As Skylar emphasized, \"Look at your data. You really need to start from your user, understand what they want, work backwards. And then you need to look at your data at every step.\" This approach of continuous data inspection throughout the pipeline is critical for success.</p> <p>When breaking down the RAG pipeline into its component parts, problems can emerge at each stage:</p> <ol> <li>Data Collection and Curation</li> <li>Extraction and Enrichment</li> <li>Indexing and Storage</li> <li>Retrieval</li> <li>Re-ranking</li> <li>Generation</li> </ol> <p>Each of these stages presents unique challenges that can significantly impact the overall performance of your RAG system.</p> <p>Key Takeaway: The teams who can iterate quickly through the cycle of examining data, getting ideas, and adapting their system are invariably the ones who succeed. This process of continuous improvement based on data insights is the foundation of effective RAG implementation.</p> <p>What problems occur during data collection and curation? Two major issues frequently arise during the initial data collection phase:</p> <p>Documents with varied encodings or formats often cause silent failures. In one medical chatbot project, 21% of the document corpus was silently dropped because the system assumed all documents were UTF-8 when many were actually Latin-1 encoded. This kind of silent failure is particularly dangerous because your index shrinks without your knowledge, leading to degraded answers and lost user trust.</p> <p>Irrelevant document sets create \"ticking time bombs\" in your index. When documents that aren't relevant to any potential user query are included, they're just waiting to be incorrectly retrieved. In a financial news summarization project, including general macroeconomic trend articles when users only cared about specific industry updates led to strange, unhelpful summaries.</p> <p>To address these issues, Skylar recommends:</p> <ul> <li>Understanding what formats and encodings your documents use</li> <li>Using robust parsers and off-the-shelf libraries when possible</li> <li>Monitoring failures rather than silently dropping documents</li> <li>Tracking document counts at each pipeline stage</li> <li>Curating document sets to include only relevant content</li> <li>Using metadata tagging to filter documents for specific query types</li> <li>Analyzing query logs to refine filters over time</li> </ul> <p>Key Takeaway: Silent failures in data processing can dramatically reduce the quality of your RAG system without any obvious errors. Implement robust monitoring and be intentional about which documents you include in your index.</p> <p>What challenges exist in extraction and enrichment? Information extraction from complex documents presents significant challenges, particularly with PDFs and tables. For an academic research assistant project, extracting tables from research papers was critical but difficult with standard tools.</p> <p>Many off-the-shelf PDF extraction tools perform poorly on table extraction, leading to missing or malformed data. This affects the quality of information that can be retrieved and presented to users.</p> <p>The solution approach includes:</p> <ul> <li>Using specialized tools designed specifically for the artifacts you need to extract</li> <li>Validating extracted chunks to ensure they make sense and meet accuracy requirements</li> <li>Looking at your data throughout the process to catch extraction errors</li> </ul> <p>Another common issue is chunking documents into pieces that are too small. Many implementations default to tiny chunks (around 200 characters) because they're copying tutorials or using AI coding assistants that reference outdated approaches designed for models with limited context windows.</p> <p>In an e-commerce product comparison project, spec sheets were split into such small snippets that no single chunk contained complete information, causing the model to hallucinate answers for about 13% of queries. Modern models can often handle much larger chunks, sometimes eliminating the need for chunking entirely.</p> <p>Similarly, keeping low-value chunks like copyright footers or boilerplate text creates noise in your retrieval system. These chunks rarely contain useful information but can be retrieved if they happen to match a query, crowding out more relevant content.</p> <p>Key Takeaway: Don't blindly follow chunking strategies from tutorials - examine your specific use case to determine the optimal chunk size, and be sure to filter out low-value content that adds noise to your retrieval system.</p> <p>What issues arise in indexing and storage? Naive embedding usage is a common problem. Most embeddings are trained for semantic similarity (matching synonyms and similar meanings) but are often used to compare document chunks with questions, which typically have different forms and structures.</p> <p>Several techniques can help bridge this gap:</p> <ul> <li>Query expansion (modifying queries to look more like documents)</li> <li>Late chunking or contextual retrieval (modifying documents at indexing time)</li> <li>Fine-tuning embeddings for your specific use case</li> </ul> <p>Another critical issue is failing to check for index staleness. In the financial news summarization case, the index hadn't been refreshed for two weeks, causing the system to return outdated earnings data when users asked for the \"latest\" information. For time-sensitive applications, monitoring index freshness is essential, and you may want to filter out documents based on their age.</p> <p>Key Takeaway: The mismatch between query form and document form is a fundamental challenge in RAG systems. Address this through query expansion, document modification techniques, or embedding fine-tuning, and always monitor index freshness for time-sensitive applications.</p> <p>What problems occur during retrieval? Accepting vague queries like \"health tips\" forces RAG systems to retrieve broadly, making it difficult to provide focused, helpful answers. Similarly, accepting off-topic queries (like \"write a poem about unicorns\" in a product comparison tool) can lead to bizarre outputs that damage user trust.</p> <p>To address these issues:</p> <ul> <li>Detect low-information queries through heuristics or classifiers</li> <li>Prompt users for clarification when queries are too vague</li> <li>Route off-topic queries to fallback responses</li> <li>Use intent classification to detect and handle queries outside your domain</li> <li>Implement relevance thresholds to avoid returning poor matches</li> </ul> <p>Another common mistake is failing to break down complex tasks. For example, a customer support system was using the full RAG pipeline to answer \"What is my billing date?\" when a simple metadata lookup would be faster, cheaper, and more reliable. Identifying common query patterns and routing them to specialized handlers can significantly improve performance.</p> <p>Key Takeaway: Not every query needs the full RAG treatment. Implement intent classification to route simple queries to specialized handlers and reject off-topic requests that your system isn't designed to handle.</p> <p>How should we approach evaluation of RAG systems? Many teams evaluate only the documents they retrieve, missing critical insights about false negatives. This is like \"looking for your keys under the lamppost because that's where the light is\" - you're only examining a small portion of the potential solution space.</p> <p>For comprehensive evaluation:</p> <ul> <li>Look beyond your retrieval window when logging and evaluating</li> <li>Log or reproduce scores for your ranking/retrieval to analyze performance</li> <li>Evaluate both relevance (are retrieved documents relevant?) and sufficiency (do they contain enough information to answer the query?)</li> </ul> <p>Creating a quadrant analysis of correct/incorrect answers versus sufficient/insufficient retrieval provides powerful insights into where to focus improvement efforts. Each quadrant suggests different approaches to enhancing system performance.</p> <p>Another common mistake is increasing system complexity without proper evaluation. Skylar has seen many clients implement sophisticated retrieval and re-ranking systems without first establishing whether they actually improve performance. In over 90% of these cases, the new system performed worse when properly evaluated.</p> <p>Key Takeaway: Always implement evaluations before increasing system complexity. It's easy to fool yourself into thinking you know what the problem is, but data-driven evaluation provides essential guardrails.</p> <p>What re-ranking challenges do RAG systems face? Overusing boosting rules can make systems difficult to understand and maintain. While boosting (adjusting ranking scores based on specific criteria) can be a useful hack to improve relevance, adding too many rules creates complexity. In the financial news example, boosting semiconductor-related content, recent articles, and earnings-related terms created a system that was hard to debug and understand.</p> <p>Another issue is allowing \"facepalm results\" - outputs that are so obviously wrong that users lose trust in the system. These often occur because lower layers of retrieval optimize for high recall (getting anything potentially relevant), and the re-ranking layer fails to filter out inappropriate content.</p> <p>To prevent these issues:</p> <ul> <li>Minimize manual boosting rules</li> <li>Consider training a custom cross-encoder re-ranker for better performance</li> <li>Apply metadata filters to exclude known irrelevant document types</li> <li>Blacklist domains or content patterns that typically produce poor results</li> <li>Remove low-value chunks from your index</li> <li>Monitor your system with test queries that have previously produced bad results</li> </ul> <p>Key Takeaway: Re-ranking is your last line of defense against irrelevant content. Invest in robust filtering and monitoring to prevent embarrassing outputs that damage user trust.</p> <p>What generation-phase problems affect RAG systems? Simple RAG systems often struggle with reasoning-based queries that require connecting information from multiple sources. For example, an academic research assistant needed to understand relationships between papers, which required multiple retrieval steps to connect the dots.</p> <p>For these complex use cases, consider:</p> <ul> <li>Adopting agentic RAG workflows that interleave retrieval and reasoning</li> <li>Pre-computing synthesis documents that connect related information</li> <li>Constructing knowledge graphs to help traverse relationships between documents</li> </ul> <p>Finally, hallucination remains a significant challenge, particularly in sensitive domains like healthcare. In the medical advice chatbot, the system hallucinated a drug side effect that wasn't present in the source material.</p> <p>The most effective approach to reducing hallucination in RAG systems is:</p> <ul> <li>Force the LLM to provide inline citations</li> <li>Validate that each citation exists in the retrieved documents</li> <li>Semantically validate that each citation actually supports the claimed content</li> </ul> <p>Key Takeaway: For complex reasoning tasks, simple RAG may not be sufficient. Consider more sophisticated approaches like agentic workflows or knowledge graphs, and always implement citation validation to prevent hallucination in sensitive domains.</p> <p>How important is metadata tagging in practice? According to Skylar, about 40% of clients have indexes so small that metadata tagging doesn't provide significant benefits. Many B2B companies have segregated data by customer, further reducing the need for complex tagging within each customer's dataset.</p> <p>The value of metadata tagging increases with:</p> <ol> <li>The diversity of queries you're answering</li> <li>The scale of data you're working with</li> </ol> <p>For specific use cases like legal documents, metadata about authorship, ownership, and modification history can be crucial. By embedding these tags into the text chunks themselves (rather than just using them as filter properties), you can answer questions about document history and changes.</p> <p>Key Takeaway: The value of metadata tagging depends on your data scale and query diversity. For smaller datasets or narrowly focused applications, extensive tagging may not be necessary, but for complex domains like legal documents, rich metadata can significantly enhance retrieval capabilities.</p> <p>What's the most important thing to remember when implementing RAG? The most critical principle is to continuously examine your data at every stage of the pipeline. This means:</p> <ol> <li>Starting with user needs and working backward</li> <li>Looking at your data inputs, intermediate results, and outputs</li> <li>Building robust evaluation systems before increasing complexity</li> <li>Creating fast feedback loops to iterate on improvements</li> </ol> <p>As Skylar emphasized throughout the session, \"The teams who can make that loop go as fast as possible are the ones who win, and that is pretty invariable.\"</p> <p>By focusing on data quality, implementing proper evaluation, and iterating quickly based on real insights rather than assumptions, you can avoid the common anti-patterns that plague RAG implementations and build systems that truly deliver value to users.</p> <p>FAQs</p>","tags":["RAG","antipatterns","data quality","evaluation","best practices"]},{"location":"talks/rag-antipatterns-skylar-payne/#what-are-the-most-common-mistakes-in-rag-implementations","title":"What are the most common mistakes in RAG implementations?","text":"<p>The most common mistake is increasing system complexity without proper evaluation. About 90% of the time, teams implement complex retrieval paths and re-ranking systems without evaluating if they actually improve performance. Always establish evaluation metrics before adding complexity to your RAG system to ensure you're making meaningful improvements rather than creating unnecessary maintenance burden.</p>","tags":["RAG","antipatterns","data quality","evaluation","best practices"]},{"location":"talks/rag-antipatterns-skylar-payne/#whats-the-single-most-important-principle-for-successful-rag-systems","title":"What's the single most important principle for successful RAG systems?","text":"<p>Look at your data at every step of the process. Start from understanding what your users want, work backwards, and examine your data throughout the entire pipeline. It's not enough to just check inputs and outputs when you have a complex system with multiple steps\u2014problems might be occurring somewhere in the middle. Teams who can quickly iterate through the cycle of examining data, getting ideas, and adapting their systems are invariably the ones who succeed.</p>","tags":["RAG","antipatterns","data quality","evaluation","best practices"]},{"location":"talks/rag-antipatterns-skylar-payne/#what-are-the-key-stages-of-a-rag-pipeline","title":"What are the key stages of a RAG pipeline?","text":"<p>A typical RAG pipeline consists of five main stages:</p> <ol> <li>Data collection and curation - Gathering the right documents to answer queries</li> <li>Extraction and enrichment - Representing documents well with proper metadata</li> <li>Indexing and storage - Setting up documents for easy retrieval</li> <li>Retrieval and re-ranking - Finding relevant documents with high recall, then filtering for precision</li> <li>Generation - Using an LLM to create answers based on retrieved information</li> </ol>","tags":["RAG","antipatterns","data quality","evaluation","best practices"]},{"location":"talks/rag-antipatterns-skylar-payne/#what-problems-commonly-occur-during-data-collection-and-curation","title":"What problems commonly occur during data collection and curation?","text":"<p>Two major issues arise during this phase:</p> <ol> <li>Documents with varied encodings or formats often silently fail to process. In one medical chatbot implementation, 21% of documents were silently dropped due to encoding issues. Always monitor document counts at each stage and implement robust error handling.</li> <li>Irrelevant document sets create a \"ticking time bomb\" waiting to be retrieved for some query. Carefully curate your document index to include only content relevant to the types of queries you intend to serve. Consider using metadata tagging to filter documents for specific query types.</li> </ol>","tags":["RAG","antipatterns","data quality","evaluation","best practices"]},{"location":"talks/rag-antipatterns-skylar-payne/#what-extraction-and-enrichment-issues-should-i-watch-for","title":"What extraction and enrichment issues should I watch for?","text":"<p>Information extraction failures are particularly common with PDFs and tables. Many off-the-shelf PDF extraction tools struggle with complex layouts like tables, multi-column text, and specialized formats. Use tools specifically designed for the artifacts you need to extract and always validate the extracted content to ensure it meets your accuracy requirements.</p>","tags":["RAG","antipatterns","data quality","evaluation","best practices"]},{"location":"talks/rag-antipatterns-skylar-payne/#what-are-the-common-chunking-mistakes-in-rag-systems","title":"What are the common chunking mistakes in RAG systems?","text":"<p>Two primary chunking issues to avoid:</p> <ol> <li>Chunking too small - Many implementations use tiny chunks (like 200 characters) because they follow outdated tutorials, but this dilutes context and meaning. In one e-commerce implementation, small chunks meant no single chunk contained complete information, leading to hallucinations in 13% of queries. Use longer context windows and chunk by semantic boundaries.</li> <li>Keeping bad chunks - Retaining low-value content like footers, copyright notices, and duplicative content creates noise in your system. Inspect your shortest chunks manually and implement deduplication through content hashing.</li> </ol>","tags":["RAG","antipatterns","data quality","evaluation","best practices"]},{"location":"talks/rag-antipatterns-skylar-payne/#what-indexing-and-storage-problems-should-i-be-aware-of","title":"What indexing and storage problems should I be aware of?","text":"<p>Two critical issues in this phase:</p> <ol> <li>Naive embedding usage - Most embeddings are trained for semantic similarity but questions often differ in form from document chunks. Consider techniques like query expansion, late chunking, contextual retrieval, or fine-tuning embeddings to bridge this gap.</li> <li>Index staleness - Without monitoring index freshness, you risk providing outdated information. In one financial news system, the index hadn't been refreshed for two weeks, resulting in outdated earnings reports. Track staleness metrics and consider adding timestamp filters to exclude outdated content.</li> </ol>","tags":["RAG","antipatterns","data quality","evaluation","best practices"]},{"location":"talks/rag-antipatterns-skylar-payne/#what-retrieval-issues-commonly-impact-rag-systems","title":"What retrieval issues commonly impact RAG systems?","text":"<p>Several retrieval problems can undermine your system:</p> <ol> <li>Accepting vague queries like \"health tips\" forces your system to retrieve anything remotely relevant. Detect low-information queries and prompt users for clarification.</li> <li>Accepting off-topic queries (like \"write a poem about unicorns\" in a product comparison tool) can produce inappropriate responses. Implement intent classification to detect and handle off-topic requests.</li> <li>Lack of task breakdown - Not recognizing patterns in user queries means missing opportunities to create more efficient workflows. For common, structured queries (like \"What is my billing date?\"), consider direct metadata lookups instead of using the full RAG pipeline.</li> </ol>","tags":["RAG","antipatterns","data quality","evaluation","best practices"]},{"location":"talks/rag-antipatterns-skylar-payne/#how-should-i-approach-evaluation-in-my-rag-system","title":"How should I approach evaluation in my RAG system?","text":"<p>Many teams evaluate only the relevance of retrieved documents but miss two critical evaluation dimensions:</p> <ol> <li>False negatives - Examine documents that weren't retrieved but should have been. Look beyond your retrieval window to find relevant documents that were missed.</li> <li>Retrieval sufficiency - Evaluate whether the retrieved documents contain enough information to fully answer the query, not just whether they're relevant. This helps identify whether problems stem from retrieval or from your document corpus itself.</li> </ol>","tags":["RAG","antipatterns","data quality","evaluation","best practices"]},{"location":"talks/rag-antipatterns-skylar-payne/#what-re-ranking-problems-should-i-watch-for","title":"What re-ranking problems should I watch for?","text":"<p>Two common re-ranking issues:</p> <ol> <li>Overusing boosting - Adding too many manual boosting rules (like boosting recent content or specific keywords) makes systems complex and unpredictable. Minimize manual boosting and consider training a custom re-ranker for better performance.</li> <li>Allowing \"facepalm results\" - Obviously bad results that make users question your system's competence. Apply metadata filters to exclude irrelevant document types and monitor your system for these embarrassing failures.</li> </ol>","tags":["RAG","antipatterns","data quality","evaluation","best practices"]},{"location":"talks/rag-antipatterns-skylar-payne/#what-generation-phase-issues-should-i-address","title":"What generation phase issues should I address?","text":"<p>Two key generation concerns:</p> <ol> <li>Using simple RAG for reasoning queries - Single-pass retrieval can't connect dots between concepts. For complex reasoning, consider agentic RAG workflows that interleave retrieval and reasoning, or pre-compute synthesis documents.</li> <li>Lack of output guardrails for hallucination - Especially critical in sensitive domains like healthcare. Force your LLM to provide inline citations, validate that each citation exists, and semantically validate that citations support the content.</li> </ol>","tags":["RAG","antipatterns","data quality","evaluation","best practices"]},{"location":"talks/rag-antipatterns-skylar-payne/#how-can-i-reduce-hallucinations-in-my-rag-system","title":"How can I reduce hallucinations in my RAG system?","text":"<p>The most effective technique is implementing a three-step verification process:</p> <ol> <li>Force your LLM to provide inline citations for claims</li> <li>Validate that each citation actually exists in your retrieved documents</li> <li>Semantically validate that each citation actually supports the content it's referencing</li> </ol> <p>This approach is particularly important for sensitive domains like healthcare where hallucinated information could have serious consequences.</p>","tags":["RAG","antipatterns","data quality","evaluation","best practices"]},{"location":"talks/rag-antipatterns-skylar-payne/#what-tools-are-recommended-for-rag-evaluation","title":"What tools are recommended for RAG evaluation?","text":"<p>While tool preferences vary by situation, Lily Pad (from Microscope) is highlighted as particularly useful for teams without AI engineering backgrounds because it enforces best practices around versioning. However, the best approach is often to meet teams where they are and use their existing tools rather than introducing new ones that require vendor approval.</p>","tags":["RAG","antipatterns","data quality","evaluation","best practices"]},{"location":"talks/rag-antipatterns-skylar-payne/#how-important-is-metadata-tagging-in-rag-systems","title":"How important is metadata tagging in RAG systems?","text":"<p>The value of metadata tagging depends on two factors:</p> <ol> <li>The scale of your data - About 40% of implementations have indexes so small that extensive tagging provides little benefit</li> <li>The diversity of queries you're answering</li> </ol> <p>Metadata becomes more valuable when you have both high data volume and diverse query types. For B2B applications where customer data is segregated, the benefit may be limited since each customer's data volume is relatively small.</p>","tags":["RAG","antipatterns","data quality","evaluation","best practices"]},{"location":"talks/rag-without-apis-browser-michael-struwig/","title":"RAG Without APIs: When Function Calling Talks to Your Browser [Michael Struwig]","text":"<p>I hosted a session with Michael Struwig, Head of AI at OpenBB, who shared a fascinating approach to RAG systems that leverages the browser as a data layer. This conversation explored how financial data platforms can connect agents to sensitive data without traditional APIs, creating more secure and flexible AI-powered analysis tools.</p> <p>Why is the browser the most natural system for connecting LLMs to data?</p> <p>Michael's core thesis is that in the age of web apps, the browser serves as the most natural intermediary for shuffling data between systems, particularly for LLMs. While this might seem counterintuitive at first, it elegantly solves several critical problems in the financial industry.</p> <p>The traditional approach to RAG involves building APIs that allow agents to pull data from external services. OpenBB takes a different path - they use the browser itself as the data layer, allowing their workspace application to connect directly to data sources without requiring data to flow through backend servers.</p> <p>This browser-first approach provides significant advantages:</p> <ul> <li>You don't have to distribute sensitive financial data through your servers</li> <li>It creates excellent security since you must be on the same network as the data</li> <li>You can serve data directly from localhost, which is fantastic for development</li> <li>It eliminates legal concerns around data distribution licenses</li> </ul> <p>As Michael explained, \"We don't want to distribute your data... there's a lot of legal fuzziness around what counts as distribution. If you've purchased a data license somewhere, are you allowed to distribute it in this way?\"</p> <p>Key Takeaway: Using the browser as a data layer creates a clean separation between data integration and agent functionality. Once data is integrated into the platform for human use, it's instantly available to AI agents without requiring separate API development or data redistribution.</p> <p>How does OpenBB's agent protocol work?</p> <p>OpenBB developed a protocol that allows agents to request data through the browser rather than directly accessing APIs. The workflow is elegantly simple:</p> <ol> <li> <p>The frontend application makes a query request to the agent, specifying available widgets containing data, their descriptions, and parameters</p> </li> <li> <p>If the agent needs specific information, it submits a function call back to the frontend</p> </li> <li> <p>A function calling handler running in the browser locally interprets this call, executes it, and fetches the data</p> </li> <li> <p>The handler responds with a follow-up query containing both the original messages and the results</p> </li> </ol> <p>This creates a powerful separation of concerns - you integrate data once, and your agent doesn't need to know anything about how to access it. The agent simply specifies which widget and input arguments it wants, using a unified interface.</p> <p>When designing this protocol, Michael emphasized several priorities:</p> <ul> <li>It must be easy to debug, test, and reason about</li> <li>It should use standards people are already familiar with</li> <li>It should avoid being overly clever or complex</li> </ul> <p>The resulting API is stateless (the agent doesn't handle state), uses REST (familiar to all web developers), and employs server-sent events (the standard for LLM communication). They've published the entire protocol on GitHub as the \"OpenBB custom agent SDK.\"</p> <p>I found it particularly interesting how they've created a system with minimal primitives that handle all essential functions - yielding message chunks, reasoning steps, widget data requests, citations, and charts - all through simple yield statements in the execution loop.</p> <p>Key Takeaway: By creating a simple, stateless protocol that runs in the browser, OpenBB allows agents to access data that might otherwise be inaccessible due to network constraints or security concerns, all while maintaining a clean separation between data integration and agent functionality.</p> <p>What is OpenBB Workspace and how does it leverage this approach?</p> <p>OpenBB Workspace is an AI-driven analysis tool primarily focused on financial data. It provides a dashboard interface where users can:</p> <ul> <li>View various data sources like news, watchlists, and company information</li> <li>Ask questions to an AI copilot that can access all visible data</li> <li>Create charts and visualizations based on analysis</li> <li>Bring in custom data backends and write custom widgets</li> </ul> <p>The platform treats the browser as a data layer, allowing users to connect to data sources running on localhost or available via their network. This creates a powerful environment where:</p> <ul> <li>Users can bring their own data without it flowing through OpenBB's servers</li> <li>The AI agent can access this data through the browser</li> <li>Everything the human can do, the agent can do as well</li> </ul> <p>Michael emphasized their philosophy that \"anything the human can do, the AI must be able to do, and vice versa.\" This creates a unified interface where humans and AI can collaborate effectively, with the AI leveraging the same tools and data access that humans have.</p> <p>For enterprise customers, this approach is particularly valuable because they already have their data and don't want data provisioning - they just want a way to analyze it effectively with AI assistance.</p> <p>Key Takeaway: OpenBB's approach creates a platform where data flows directly through the browser to both humans and AI agents, maintaining security while enabling powerful analysis capabilities without requiring data to be redistributed through third-party servers.</p> <p>How does OpenBB handle state management and function execution?</p> <p>One of the most interesting aspects of OpenBB's approach is how they handle function execution. Michael described two types of functions:</p> <ol> <li> <p>Locally executed functions - The traditional approach where functions run in the same environment where they're called</p> </li> <li> <p>Remotely executed functions - A novel approach where functions are executed in the browser through a mini-interpreter</p> </li> </ol> <p>This remote execution model allows the agent to request data from widgets in the browser without needing direct API access to the data sources. As Michael explained, \"Nobody said anything about where the function call needs to be executed.\"</p> <p>For state management, OpenBB takes a stateless approach where the workspace handles all state, not the agent. This makes debugging easier, creates reproducible behavior, and simplifies testing since you can hit an endpoint with a payload and get a predictable response without managing complex state.</p> <p>Their architecture resembles a state machine model with a main execution loop that can drop into sub-modules for specific tasks. For example, when analyzing a PDF, the main agent might recognize it's a PDF and call a sub-agent with its own execution loop specifically designed for PDF analysis. These sub-agents can yield events back up to the main loop, which then yields them to the client.</p> <p>Key Takeaway: By using remotely executed functions and a stateless architecture, OpenBB creates a system that's easy to reason about while enabling powerful capabilities like accessing data that would otherwise be inaccessible to remote agents.</p> <p>How does OpenBB handle error management and system improvement?</p> <p>When I asked about their approach to error management and system improvement, Michael's answer was refreshingly straightforward: they look at the logs.</p> <p>Rather than building complex systems to analyze errors, they:</p> <ol> <li> <p>Ensure they have good error messages that help the model try again</p> </li> <li> <p>Keep detailed logs of what goes wrong</p> </li> <li> <p>Look for patterns in those logs</p> </li> <li> <p>Fix the system to make errors less likely</p> </li> </ol> <p>As Michael put it, \"It's astounding how just looking at logs... we don't do any crazy, smart stuff. We're a small team. We want to move fast, and we want our systems to be reliable. No, we just go look.\"</p> <p>He shared an example where they noticed their agent was consistently generating input arguments for a particular data source even when instructed not to. By identifying this pattern, they realized they needed to make certain input arguments optional, fixing the system rather than trying to improve the prompting.</p> <p>This approach reminded me of Toyota sending executives to Namibia to watch what breaks on their vehicles in harsh conditions. As Michael noted, \"That is surprisingly effective. You don't need workflows. You don't need crazy tooling. Sometimes, just putting your boots on the ground and looking at what's going wrong - that's the highest information signal you can get.\"</p> <p>Key Takeaway: Sometimes the simplest approaches are most effective. Rather than building complex systems to analyze errors, directly examining logs and identifying patterns can provide the highest-quality information for improving your AI systems.</p> <p>How does OpenBB manage latency expectations?</p> <p>For managing user expectations around latency, OpenBB employs two key strategies:</p> <ol> <li> <p>Providing continuous status updates - Similar to how ChatGPT shows \"searching the web\" indicators, OpenBB shows step-by-step reasoning as the agent works. This includes information about which widgets are being accessed, what queries are being made, and what the agent is thinking. As Michael noted, \"One of the things you don't want is to have a user sit there wondering what's happening.\"</p> </li> <li> <p>Parallelizing independent tasks - When querying multiple data sources that don't need to interact, they split the work into parallel processes. For example, when summarizing 10 PDFs, they'll summarize them simultaneously and combine the results rather than processing them sequentially.</p> </li> </ol> <p>These approaches keep users engaged and informed while minimizing perceived latency. As Michael observed, \"Users don't seem to complain. You can only read so fast, anyway.\"</p> <p>I found his comment about anthropomorphizing LLMs particularly insightful: \"Sometimes I think anthropomorphizing LLMs more is a good thing, not a bad thing.\" Good error messages help both humans and LLMs understand what went wrong, as models are trained on human data and respond well to the same clear communication that helps people.</p> <p>What's the future of browser-based AI?</p> <p>Michael posed a fascinating question that he believes more people should be asking: \"When will locally running language models be integrated into browsers, and when will a web standard be published that allows web apps and websites to interact with those language models in a native way?\"</p> <p>He predicts that browsers will eventually have built-in LLMs, with standardized interfaces for web applications to leverage these models. This makes sense because:</p> <ul> <li>The browser is the most important application on most computers</li> <li>The web is effectively \"the operating system of the world\" now</li> <li>Most users won't install specialized AI software, but they already use browsers</li> <li>Edge computing for AI is becoming increasingly feasible</li> </ul> <p>When I asked whether this would happen first on desktop or mobile, Michael was confident: \"Mobile, almost certainly mobile.\" The challenge remains monetization - how do big tech companies justify the investment in building these capabilities if they can't generate revenue from them?</p> <p>Key Takeaway: The future may involve browsers with native LLM capabilities and standardized interfaces for web applications to leverage these models, creating a more accessible AI ecosystem that doesn't require users to install specialized software.</p> <p>How does OpenBB approach testing and evaluation?</p> <p>Evaluating AI systems in financial analysis presents unique challenges. Unlike code generation where you can run unit tests to verify correctness, financial analysis often involves subjective judgments without clear right or wrong answers.</p> <p>OpenBB takes a multi-faceted approach to evaluation:</p> <ol> <li> <p>User feedback through thumbs up/down ratings (though only about 1% of users provide this)</p> </li> <li> <p>\"Radical observability\" - providing full citation traces for every answer</p> </li> <li> <p>Direct observation of user interactions and \"vibe checks\"</p> </li> <li> <p>Detailed logging and pattern recognition</p> </li> </ol> <p>Michael emphasized that \"vibe checks\" provide surprisingly high-quality information. By examining what the agent saw and the answer it provided, they can identify and fix issues more effectively than through automated systems.</p> <p>For monitoring and tracing, they use two main tools:</p> <ul> <li>Magnetic (by Jack Collins) - A minimal LLM framework that provides the right abstraction level</li> <li>Logfire - For fast tracing with a good UI and OpenTelemetry compatibility</li> </ul> <p>Key Takeaway: In domains where evaluation is subjective, combining user feedback with radical observability and direct examination of system behavior can be more effective than complex automated evaluation systems.</p> <p>What are the limitations of frameworks like LangChain?</p> <p>When I asked about his \"God forbid\" comment regarding LangChain, Michael clarified that while LangChain popularized the important concept of chaining LLM calls together, it has significant limitations:</p> <p>\"It's heavily abstracted. It makes a lot of strong assumptions about how your business logic should work, and it ends up solving the average of a lot of problems. And the issue with solving the average of a lot of problems is... you solve no one's problem.\"</p> <p>He explained that LangChain works well for \"Hello World\" examples, but as soon as you need custom functionality, you find yourself fighting the framework. This has been his experience and that of many others who have tried to build production systems with it.</p> <p>Instead, OpenBB prefers more minimal abstractions that give them greater control over execution flow. They've found that a state machine model with execution loops works better for their needs, allowing them to drop into specialized sub-modules for specific tasks while maintaining a clean overall architecture.</p> <p>Key Takeaway: While frameworks like LangChain can help with initial development, they often become limiting when building production systems that require custom functionality. More minimal abstractions that give you direct control over execution flow can be more effective for complex applications.</p> <p>Final thoughts on browser-based RAG</p> <p>The browser-as-data-layer approach that OpenBB has developed offers a compelling alternative to traditional API-based RAG systems, particularly for sensitive data in regulated industries. By leveraging the browser's existing capabilities and security model, they've created a system that:</p> <ol> <li> <p>Keeps data secure and compliant with licensing requirements</p> </li> <li> <p>Simplifies development by eliminating the need for separate API integration</p> </li> <li> <p>Creates a unified interface for both humans and AI</p> </li> <li> <p>Enables access to data that might otherwise be inaccessible to remote agents</p> </li> </ol> <p>This approach represents a pragmatic solution to real-world constraints around data access and security, demonstrating that sometimes the most elegant solutions come from rethinking fundamental assumptions rather than adding more complexity.</p> <p>As AI becomes more integrated into our daily tools, approaches like this that leverage existing infrastructure rather than building entirely new systems may prove to be the most sustainable path forward.</p> <p>FAQs:</p> <p>What is the main concept behind OpenBB's approach to data integration?</p> <p>The browser is the most natural system for shuffling data between different systems, particularly when working with Large Language Models (LLMs). Rather than using traditional APIs where an agent hits an endpoint to pull data from external services, OpenBB leverages the browser itself as a data layer. This approach allows you to connect data running on localhost or your local network to an agent running elsewhere, creating a more seamless integration experience.</p> <p>What is OpenBB Workspace?</p> <p>OpenBB Workspace is an AI-driven analysis tool primarily focused on financial data. It features a series of dashboards with various data sources including news, watchlists, company information, and price targets. The platform includes OpenBB Copilot (an AI assistant) on the right side of the application that can answer questions, produce charts, and provide full citations for its responses. Users can create custom widgets, bring in their own data backends, and perform comprehensive analysis.</p> <p>Why did OpenBB choose a browser-based approach instead of traditional APIs?</p> <p>There are several key reasons:</p> <ul> <li>It eliminates the need to distribute user data through OpenBB's systems, which is resource-intensive and costly</li> <li>It avoids legal complications around data distribution, especially with licensed financial data</li> <li>It removes the requirement for data to be reachable over a public network</li> <li>It provides excellent security since you must be on the same network as the data</li> <li>It allows users to serve data directly from localhost, which is ideal for development</li> </ul> <p>How does the browser-based data integration work technically?</p> <p>The system uses what OpenBB calls the \"agent protocol.\" When a user makes a query, the front-end application sends a request to the agent with messages and information about available widgets containing data. If the agent needs specific information to answer the query, it submits a function call back to the front-end with parameters specifying which widget to access and which input arguments to use. A function-calling handler running in the browser locally interprets this call, executes it, fetches the data, and responds with the results back to the agent.</p> <p>What are the benefits of this approach for developers?</p> <p>This approach offers several advantages:</p> <ul> <li>You only need to integrate your data once into OpenBB Workspace</li> <li>The agent doesn't need to know how to access the data directly</li> <li>There's a clear separation of concerns between data integration and agent functionality</li> <li>The system works with any AI system (OpenAI, Anthropic, Google, etc.)</li> <li>The protocol is stateless, making it easy to debug, test, and reason about</li> <li>It uses familiar REST APIs and server-sent events, which are standard for web development</li> </ul> <p>Is the agent protocol publicly available?</p> <p>Yes, OpenBB has published the entire protocol on GitHub. It's called the \"OpenBB Custom Agent SDK\" and is pip-installable. This is the same protocol used for OpenBB's own copilot, and they encourage users to build their own agents with it.</p> <p>What primitives does the agent protocol provide?</p> <p>The protocol includes several simple primitives:</p> <ul> <li>Yielding message chunks to stream text back to the front-end</li> <li>Yielding reasoning steps to provide status updates about what the agent is doing</li> <li>Fetching widget data from OpenBB Workspace</li> <li>Producing citations for information sources</li> <li>Returning tables and various chart types (pie, line, bar, scatter)</li> </ul> <p>How do you handle latency in complex workflows?</p> <p>OpenBB manages user expectations around latency in two main ways:</p> <ol> <li> <p>Providing continuous feedback through status updates that show what the model is doing (querying widgets, writing SQL, analyzing data)</p> </li> <li> <p>Parallelizing tasks when possible (e.g., summarizing multiple PDFs simultaneously rather than sequentially)</p> </li> </ol> <p>How does OpenBB approach error handling and system improvement?</p> <p>Rather than building complex self-improving systems, OpenBB focuses on:</p> <ul> <li>Creating clear, informative error messages that help the model try again</li> <li>Maintaining detailed logs of errors and identifying common patterns</li> <li>Having team members regularly review these logs to spot trends</li> <li>Modifying the system to make errors less likely, rather than just changing prompts</li> </ul> <p>What does OpenBB believe about the future of AI integration?</p> <p>OpenBB believes that human plus AI will be better than either human or AI alone. Their vision focuses on collaboration rather than replacement, with AI making humans better or offloading certain tasks. They maintain that anything the human can do, the AI must be able to do, and vice versa, creating a unified interface for both. Looking further ahead, they predict that locally-running language models will eventually be integrated directly into browsers as a web standard, particularly on mobile devices.</p>","tags":["RAG","browser","function calling","data security","OpenBB"]},{"location":"talks/reducto-docs-adit/","title":"Document Ingestion and Parsing (Reducto)","text":"<p>Study Notes:</p> <p>I hosted a session with Adit, CEO of Reducto, to explore the challenges and solutions in document ingestion for AI systems. This conversation covered parsing complex documents, handling tables and forms, optimizing data representation for language models, and addressing the long tail of edge cases that make production-ready AI systems difficult to build.</p> <p>Why is accurate document parsing so critical for AI applications? The fundamental challenge with document parsing isn't just extracting text - it's about providing high-quality inputs that enable language models to reason effectively. As Adit explained, \"Models today, especially reasoning models, are incredible with reasoning on good data. What really ends up causing accuracy drifts is the long tail of cases.\"</p> <p>When you pass documents directly to vision language models like GPT-4, they can fail in surprising ways - misreading tables, hallucinating content, or dropping information. These aren't necessarily model problems but input quality issues. The same models that struggle with raw documents can perform exceptionally well when given properly structured representations.</p> <p>I've seen this pattern repeatedly in my consulting work - clients focus heavily on model selection and prompt engineering but underinvest in the quality of their document processing pipeline. This creates a ceiling on performance that no amount of prompt tuning can overcome.</p> <p>Key Takeaway: Even the most advanced language models will produce unreliable outputs when given poor-quality inputs. Investing in robust document parsing is essential for production AI systems, especially in domains where accuracy is critical.</p> <p>What are the most challenging document elements to parse correctly? Through their work with financial, legal, healthcare, and enterprise clients, Reducto has identified several consistently problematic document elements:</p> <p>Hard tables - Especially those with merged cells, complex layouts, or that span multiple pages Layout interpretation - Understanding indentation, list hierarchies, and associating related blocks of content Form regions - Pairing key-value relationships (like knowing \"Jason Liu\" maps to \"Name\") Reading order - Determining the correct sequence in multi-column layouts, slide decks, or documents with non-standard flows</p> <p>Tables are particularly challenging because they represent two-dimensional associations of data that can be formatted in countless ways. The failures are often subtle - a model might extract what appears to be a valid table but silently drop rows, columns, or individual values.</p> <p>What's particularly concerning is how these errors can be difficult to detect in production. Adit showed examples where vendor solutions using Gemini, Claude, and other models produced outputs that looked structurally correct but contained hallucinated values or missing data.</p> <p>Key Takeaway: The most challenging parsing problems involve understanding document structure rather than just text extraction. Tables, forms, and complex layouts require specialized approaches beyond what general-purpose vision models can reliably handle.</p> <p>When should you use vision language models versus traditional computer vision? I found Adit's perspective on this particularly interesting because it challenges the \"just throw it all into a VLM\" approach I've seen many teams adopt. Reducto has found that a hybrid approach works best:</p> <p>Traditional CV excels at:</p> <ul> <li>Clean, structured information</li> <li>Providing precise bounding boxes</li> <li>Generating confidence scores</li> <li>Token-efficient processing</li> </ul> <p>Vision language models excel at:</p> <ul> <li>Handwriting recognition</li> <li>Chart extraction</li> <li>Understanding figures and diagrams</li> <li>Handling visually complex layouts</li> </ul> <p>\"What we do see as a really effective place to use VLMs is things that traditional OCR has always been horrible at,\" Adit explained. For example, when Reducto first approached chart extraction, they tried using traditional CV to measure pixel values relative to axis labels. This approach failed because charts can be represented in so many different ways that VLMs simply perform better.</p> <p>Their most effective approach uses a multi-pass system: traditional CV for the initial extraction, followed by VLMs that grade the outputs and make corrections. This allows them to triage simple pages to lightweight models while deploying more powerful VLMs only for complex cases.</p> <p>Key Takeaway: Rather than choosing between traditional CV and VLMs, build a pipeline that leverages the strengths of each approach. Use traditional CV for structured content and VLMs for visually complex elements, with a multi-pass system to catch and correct errors.</p> <p>How should you evaluate document parsing performance? Evaluation emerged as a critical but often overlooked aspect of building document processing systems. Adit emphasized that the biggest mistake teams make is conducting \"back of the napkin\" tests with just a few hard documents before moving to production.</p> <p>This approach is dangerous because ingestion errors get magnified downstream in retrieval and inference steps. Without thorough evaluation of each component, it becomes impossible to isolate where problems originate.</p> <p>Reducto's approach includes:</p> <ul> <li>Creating benchmarks with diverse, hand-labeled examples (they've open-sourced RD Table Bench with 1,000 diverse cases)</li> <li>Developing scoring frameworks that evaluate both text accuracy and structural correctness</li> <li>Recommending that teams compile 100-200 hard cases from their specific document distribution</li> </ul> <p>What struck me is how this evaluation process isn't just about measuring performance - it's about understanding failure modes. The examples Adit showed revealed how models can fail in ways that look correct at first glance but contain subtle errors that would be catastrophic in sensitive domains.</p> <p>Key Takeaway: Invest in rigorous evaluation of your document processing pipeline using representative examples from your domain. Test each component separately to isolate errors, and focus on understanding failure modes rather than just overall accuracy.</p> <p>What's the best way to represent document data for language models? The ideal output structure depends heavily on the type of document you're processing. For simple content, Markdown provides a clean, token-efficient representation. But for complex structures like tables with merged cells, HTML often works better despite being more verbose.</p> <p>Reducto has developed specific heuristics based on document complexity:</p> <ul> <li>For tables with 3+ merged cells, they use HTML</li> <li>For simpler tables, they use Markdown for token efficiency</li> <li>For figures and visual elements, they sometimes preserve the original image region</li> </ul> <p>This approach recognizes that different document elements have different optimal representations. As Adit explained, \"If you have a table like this, if you try to encode this in Markdown, you're going to end up with something really cumbersome.\"</p> <p>I've seen similar issues in my consulting work - teams often standardize on a single representation format without considering how it handles different document types. This one-size-fits-all approach creates unnecessary challenges for the language model.</p> <p>Key Takeaway: Choose representation formats based on document complexity rather than standardizing on a single approach. Consider token efficiency, structural fidelity, and how well the format preserves the information needed for reasoning.</p> <p>How should you approach chunking documents for retrieval? While chunking has become less critical as context windows have expanded, it remains important for efficient retrieval across large document collections. Adit shared several principles that have proven effective:</p> <ol> <li>Never split individual blocks of information (keep paragraphs, tables, and other logical units intact)</li> <li>Consider both semantic information and structural signals like section headers and position</li> <li>Dynamically adjust chunk sizes rather than enforcing rigid token limits</li> </ol> <p>What I found particularly interesting was Adit's observation about position as a signal: \"Position is a surprisingly high value signal for chunking because humans inherently, when we create documents, encode things like grouping similar text together.\"</p> <p>This insight aligns with my experience - document authors naturally organize related information spatially, making position a strong proxy for semantic relatedness that's computationally cheap to extract.</p> <p>Key Takeaway: Develop a chunking strategy that preserves logical document units and leverages both semantic and structural signals. Position within the document often provides valuable information about content relationships.</p> <p>How do you optimize document data for retrieval? One of the most valuable insights from our conversation was recognizing that embedding models and LLMs have different limitations that your ingestion pipeline should account for.</p> <p>While LLMs can reason effectively with dense HTML structures, embedding models often struggle with them. As Adit explained, \"If you had a table that contains revenue over time, your end user is probably going to say something like, 'How did revenue change over time?' They're probably not going to phrase the question as 'In fiscal year 14, how did the number change from 210 to something else?'\"</p> <p>This creates a mismatch between user queries and document representations that naive cosine similarity can't bridge. Reducto addresses this by creating embedding-optimized representations for each chunk:</p> <ul> <li>Generating natural language summaries of table contents</li> <li>Adding context about document sections</li> <li>Creating modified representations specifically optimized for embedding similarity</li> </ul> <p>I've seen similar approaches work well in my consulting projects - creating \"retrieval-friendly\" versions of complex document elements that maintain the semantic meaning while being more aligned with how users phrase queries.</p> <p>Key Takeaway: Create separate representations optimized for embedding models and LLMs. For complex elements like tables, generate natural language summaries that describe the content in ways that align with likely user queries.</p> <p>What are the unique challenges of processing Excel files? While much of the discussion focused on PDFs, Adit shared fascinating insights about Excel processing that I hadn't considered before. The challenges are quite different from PDFs:</p> <ol> <li>Scale issues - Spreadsheets can contain hundreds of thousands of rows, easily exceeding model context windows</li> <li>Information clustering - A single spreadsheet might contain multiple unrelated tables with arbitrary spacing between them</li> <li>Header preservation - When chunking large tables, headers need to be repeated with each section</li> </ol> <p>Reducto's approach includes breaking large tables into smaller chunks with headers preserved, and using a combination of data density analysis and visual information to identify separate clusters of information within a sheet.</p> <p>What I found most interesting was their experimental work on using vision models specifically for spreadsheet clustering: \"We are training a vision model specifically for spreadsheet clustering. And we've just found that that works better.\"</p> <p>This highlights how specialized the solutions need to be for different document types - the techniques that work for PDFs often don't transfer directly to spreadsheets.</p> <p>Key Takeaway: Excel processing requires specialized approaches that address scale challenges and information clustering. Consider using visual representations to identify logical groupings within spreadsheets rather than relying solely on cell-based analysis.</p> <p>What surprising edge cases have emerged in document processing? Some of the most interesting insights came from discussing unexpected challenges that only became apparent through extensive production experience:</p> <ul> <li>Minor skews (1-2 degrees) can dramatically impact extraction quality, even with VLMs</li> <li>Reading order determination is far more complex than anticipated, especially with multi-column layouts</li> <li>Watermarks can confuse models and corrupt extracted text</li> <li>Model refusals occur when content appears to violate safety guidelines (like medical prescriptions)</li> <li>Checkboxes are surprisingly difficult for VLMs to interpret consistently</li> </ul> <p>The checkbox issue is particularly problematic in healthcare: \"Checkboxes are one of the things that I see vision language models out of the box struggle with the most... They'll sort of arbitrarily decide whether or not the checkbox is filled or not. Those are like complete bipolar meanings - you can't say, was the patient vaccinated yes or no, and choose arbitrarily with 50-50 probability.\"</p> <p>These edge cases highlight the gap between demo-quality and production-quality systems. While models might handle 95% of cases well, the remaining 5% often require specialized solutions.</p> <p>Key Takeaway: Production document processing systems need to address a long tail of edge cases that aren't apparent in initial testing. Invest in preprocessing, confidence scoring, and fallback mechanisms to handle these challenging scenarios.</p> <p>How is document processing evolving with newer AI capabilities? Throughout our conversation, Adit shared perspectives on how document processing is likely to evolve:</p> <ul> <li>VLMs will continue improving but aren't yet reliable enough for sensitive use cases without additional safeguards</li> <li>Agentic approaches are emerging that can reason about what information to extract and how to structure it</li> <li>Multi-pass systems that combine traditional methods with AI verification show promise for high-accuracy applications</li> <li>Evaluation remains critical and will likely become more sophisticated as applications mature</li> </ul> <p>I was particularly interested in Adit's thoughts on agentic RAG, where models reason about what information to retrieve rather than relying solely on embedding similarity: \"The cases where it is really helpful is when a given answer needs to be composed of information from a lot of different sources.\"</p> <p>This aligns with my experience - as we move beyond simple question-answering to more complex reasoning tasks, the limitations of traditional retrieval become more apparent.</p> <p>Key Takeaway: Document processing is evolving toward more intelligent, multi-stage pipelines that combine traditional methods with AI reasoning. While VLMs will continue improving, hybrid approaches that leverage the strengths of different techniques will likely remain dominant for production systems.</p> <p>How should teams approach building document processing systems? Based on our conversation, I'd recommend several principles for teams building document processing systems:</p> <ol> <li>Use a hybrid approach combining traditional CV and VLMs based on document characteristics</li> <li>Invest heavily in evaluation using representative examples from your domain</li> <li>Choose representation formats based on document complexity rather than standardizing on one approach</li> <li>Create separate representations optimized for embedding models versus LLMs</li> <li>Build multi-pass systems that can verify and correct initial extraction results</li> <li>Develop specialized approaches for different document types (PDFs, Excel, forms, etc.)</li> <li>Focus on understanding and addressing the long tail of edge cases</li> </ol> <p>The most successful teams I've worked with treat document processing as a critical foundation rather than just a preprocessing step. They recognize that the quality of their document understanding directly impacts everything downstream.</p> <p>As Adit summarized: \"Accurate parsing is really critical. You should try to use the best tools that are available for you... The best way to do that is to use a combination of both traditional CV for the things that traditional CV is good at, and also VLMs and document metadata.\"</p> <p>Key Takeaway: Document processing requires a thoughtful, multi-faceted approach that combines different techniques based on document characteristics. Invest in this foundation to enable reliable, accurate AI applications, especially in domains where precision matters.</p> <p>FAQs</p> <p>What is document ingestion in the context of AI applications?</p> <p>Document ingestion refers to the process of extracting, processing, and structuring data from various document formats (like PDFs, Excel files, images) so they can be effectively used by AI models. This includes parsing text, understanding document structure, recognizing tables, and preparing the data in a format that language models can reason with accurately.</p> <p>Why is accurate document parsing so important for AI applications?</p> <p>Accurate parsing is critical because even the most advanced AI reasoning models can only provide quality outputs when given quality inputs. Inaccurate parsing leads to hallucinations, missing information, or incorrect interpretations. In sensitive industries like healthcare, finance, and legal, even minor errors in document parsing can have significant consequences, potentially impacting decisions worth millions of dollars.</p> <p>What are the main challenges in document parsing?</p> <p>The most common challenges include:</p> <ul> <li>Extracting complex tables with merged cells</li> <li>Interpreting document layout correctly (indentation, list hierarchies)</li> <li>Pairing form fields with their values</li> <li>Determining the correct reading order, especially in multi-column layouts</li> <li>Processing handwritten text</li> <li>Handling charts and figures</li> <li>Managing document skew and rotation</li> <li>Working with multilingual documents that have different reading directions</li> </ul> <p>How do Vision Language Models (VLMs) compare to traditional OCR for document parsing?</p> <p>VLMs excel at tasks that traditional OCR struggles with, such as handwriting recognition and chart extraction. However, they can sometimes hallucinate content or drop information in structured formats like tables. The most effective approach is often a hybrid one, using traditional computer vision models for clean structured content and VLMs for more complex visual elements like figures, handwriting, and charts.</p> <p>What's the best approach for handling tables in documents?</p> <p>Tables require special attention because they associate data in two dimensions. The most effective approach involves:</p> <ol> <li> <p>Properly detecting the table structure including merged cells</p> </li> <li> <p>Preserving the relationships between headers and data</p> </li> <li> <p>Using HTML format for complex tables with multiple merged cells</p> </li> <li> <p>Using Markdown for simpler tables to maintain token efficiency</p> </li> <li> <p>Creating natural language summaries of tables to improve retrieval</p> </li> </ol> <p>How should document data be represented for language models?</p> <p>The ideal representation depends on the content:</p> <ul> <li>Simple text works well in Markdown format</li> <li>Complex tables with merged cells are better represented in HTML</li> <li>Large tables may need to be broken down with headers repeated for each section</li> <li>Images and figures are sometimes best left as cropped images passed directly to the model</li> <li>For forms, it's important to pair keys with their values</li> </ul> <p>What are best practices for chunking documents?</p> <p>Effective chunking strategies include:</p> <ul> <li>Never splitting individual blocks of information (keeping paragraphs and tables intact)</li> <li>Considering section information and position when creating chunks</li> <li>Using embedding similarity to group related content</li> <li>Dynamically adjusting chunk size based on content type rather than using fixed sizes</li> <li>Preserving document structure and hierarchy</li> </ul> <p>How can document ingestion be optimized for retrieval?</p> <p>To improve retrieval performance:</p> <ul> <li>Create embedding-optimized representations for each chunk</li> <li>For tables, generate natural language summaries describing the content</li> <li>Add context about document sections</li> <li>Include metadata that helps tie document data to potential user queries</li> <li>Consider how users actually phrase their questions rather than just the literal content</li> </ul> <p>Why do embedding models and language models need different optimizations?</p> <p>Embedding models and language models have different limitations. While language models can reason through complex HTML structures, embedding models may struggle with matching user queries to content that contains mostly HTML tags and numbers. Creating natural language descriptions of structured content helps bridge this gap and improves retrieval performance.</p> <p>How should document parsing quality be evaluated?</p> <p>Evaluation should be thorough and context-dependent:</p> <ul> <li>Test on your own data rather than generic benchmarks</li> <li>Develop a comprehensive evaluation framework with at least 100-200 hard cases</li> <li>Evaluate both text accuracy and structural preservation</li> <li>Test each component of your pipeline separately to isolate issues</li> <li>Consider edge cases specific to your document types</li> </ul> <p>What are common failure modes in document parsing?</p> <p>Common failures include:</p> <ul> <li>Hallucinating content in tables</li> <li>Dropping rows or columns</li> <li>Misinterpreting checkboxes and form elements</li> <li>Struggling with document skew or rotation</li> <li>Incorrectly determining reading order in complex layouts</li> <li>Model refusals when content appears to violate safety guidelines</li> <li>Watermarks interfering with text extraction</li> </ul> <p>How should Excel files be handled differently from PDFs?</p> <p>Excel files present unique challenges:</p> <ul> <li>They can contain massive amounts of data exceeding model context windows</li> <li>A single spreadsheet may contain multiple unrelated tables or data clusters</li> <li>Data representation isn't always in a clean grid format</li> <li>Effective approaches include:</li> <li>Breaking large tables into smaller chunks with headers repeated</li> <li>Using visual information to detect natural clusters of data</li> <li>Analyzing data density to identify separate information regions</li> </ul> <p>How can handwritten documents be processed effectively?</p> <p>Handwritten documents are best processed using VLMs, which perform much better than traditional OCR for this task. For particularly difficult cases, a multi-pass approach works well:</p> <ol> <li> <p>Make an initial prediction</p> </li> <li> <p>Use a VLM specifically fine-tuned to grade those outputs</p> </li> <li> <p>Use another VLM to make corrections based on the reasoning trace</p> </li> </ol> <p>This approach significantly improves accuracy for handwritten content.</p> <p>How much data is needed to fine-tune models for document extraction?</p> <p>Fine-tuning VLMs for document extraction tasks can be effective with hundreds to perhaps a thousand examples. However, the quality of the training data is far more important than quantity. Even minor issues in labels can be amplified during fine-tuning, so it's critical to have genuinely perfect inputs rather than a large volume of imperfect data.</p> <p>What's the role of \"agentic RAG\" in document processing?</p>"},{"location":"talks/reducto-docs-adit/#agentic-rag-involves-adding-reasoning-capabilities-to-determine-what-information-to-pass-to-the-final-inference-step-this-approach-is-particularly-valuable-when-answers-need-to-be-composed-from-multiple-different-sources-rather-than-relying-solely-on-similarity-based-retrieval-an-agent-can-identify-which-specific-snippets-are-most-relevant-to-answering-a-particular-question","title":"Agentic RAG involves adding reasoning capabilities to determine what information to pass to the final inference step. This approach is particularly valuable when answers need to be composed from multiple different sources. Rather than relying solely on similarity-based retrieval, an agent can identify which specific snippets are most relevant to answering a particular question.","text":"<p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"talks/semantic-search-exa-will-bryk/","title":"Semantic search over the web with Exa [Will Bryk]","text":"<p>I hosted a session with Will Bryk from Exa who shared insights about the evolution of search technology, how AI is changing search requirements, and the technical challenges of building a semantic search engine. This session explores how traditional search engines like Google differ from next-generation semantic search systems designed for AI applications rather than human users.</p> <p>Why is traditional search inadequate for the AI era?</p> <p>The fundamental problem with traditional search engines like Google is that they were built for humans, not AI systems. This creates a significant mismatch in capabilities and requirements.</p> <p>Humans and AI systems interact with search in fundamentally different ways. Humans typically use simple keyword queries because they're \"lazy\" and can't type full sentences. They generally want just a few high-quality links since they have limited capacity to process information. This is why Google optimized for keyword-based algorithms that return a handful of results.</p> <p>AI systems, by contrast, can instantly generate complex, precise queries that specify exactly what they want. They can consume and analyze massive amounts of information without getting overwhelmed. When an AI asks for \"startups working on something huge like Bell Labs,\" it doesn't want Reddit discussions - it wants comprehensive, structured information about actual startups.</p> <p>As Will explained: \"It would kind of be insane if the same search engine that was optimal for humans would also be optimal for this very different creature.\" This insight is still not widely understood in the industry, even as everyone recognizes that combining search with LLMs is powerful.</p> <p>Key Takeaway: Traditional search engines were optimized for human behavior patterns (simple keywords, few results), while AI systems need search engines that can handle complex queries and return comprehensive, precisely targeted information.</p> <p>How do AI search needs differ from human search needs?</p> <p>AI systems have several distinct requirements that traditional search engines weren't designed to address:</p> <ol> <li> <p>Precise, controllable information: AIs need search engines that respect exactly what they ask for, rather than optimizing for what humans typically click on. If an AI is searching for \"startups working on something huge,\" it needs a list of actual startups, not Reddit discussions about the topic.</p> </li> <li> <p>Context-rich queries: AIs often have extensive context from user interactions that should inform their searches. Traditional search engines have keyword limits and can't handle paragraph-length queries, forcing the AI to convert rich context into a few keywords, which loses tremendous information.</p> </li> <li> <p>Comprehensiveness: Unlike humans who want a few links, AIs can process thousands of results. If an AI is asked to analyze \"every YC-funded AI startup,\" it needs a search engine that can deliver comprehensive results rather than just the top 10 most popular pages.</p> </li> </ol> <p>Will used a powerful visual metaphor, showing the space of possible search queries as a vast gray zone, with traditional search engines only covering a small blue bubble within it. As he explained: \"Google has trained us to only think about this keyword bubble... But there's actually many different types of queries that people might want to make that go way beyond what Google can handle.\"</p> <p>Key Takeaway: AI systems need search engines that can handle precise queries, incorporate rich context, and deliver comprehensive results - capabilities that fall outside the design parameters of traditional search engines built for humans.</p> <p>How does Exa's approach to search differ from traditional search engines?</p> <p>Exa takes a fundamentally different approach to search compared to traditional engines like Google. While Google's original PageRank algorithm from 1998 focused on analyzing the web's link structure and matching keywords, Exa uses an embedding-based neural approach that understands the semantic meaning of content.</p> <p>The core technical difference is that Exa processes documents into embedding vectors that capture their meaning, rather than just indexing keywords. When a query comes in, Exa embeds it in the same vector space and finds documents with similar meaning, regardless of whether they share the exact keywords.</p> <p>This approach allows Exa to handle queries that would be impossible with traditional search engines:</p> <ul> <li>Long, contextual queries (like pasting an entire research paper and asking for similar papers)</li> <li>Semantic queries (\"people in SF who know assembly language\")</li> <li>Complex analytical queries (\"find every article that argues X and not Y from author Z\")</li> </ul> <p>Will emphasized that Exa's philosophy is to give users complete control over their search experience: \"The philosophy here is we want to give users full control. Give the AI system full control to get what information they want.\" This includes the ability to filter by date ranges, domains, and content categories, and even specify what sources they consider authoritative.</p> <p>This approach reflects a deeper philosophical stance: \"If you give full control to the user to specify what they want, you are freeing them from whatever ideology that previous search engines had.\"</p> <p>Key Takeaway: Exa uses neural embedding technology rather than keywords to understand query meaning, enabling it to handle complex, contextual searches that traditional engines can't process. Their philosophy emphasizes giving users complete control over their search parameters.</p> <p>What technical challenges arise when building a semantic search engine?</p> <p>Building a web-scale semantic search engine involves numerous technical challenges, particularly around efficiency and scale. Will walked through several key components of Exa's system:</p> <ol> <li> <p>Crawling and storage: The process begins with gathering URLs to crawl, building distributed crawling systems that can handle different formats (like PDFs), and storing petabytes of data in formats that enable efficient batch processing.</p> </li> <li> <p>Embedding generation: Documents are processed through embedding models that convert text into vector representations capturing their meaning. This is computationally intensive at web scale.</p> </li> <li> <p>Vector database optimization: The most significant challenge is efficiently searching billions of embedding vectors. A naive approach would be prohibitively expensive and slow.</p> </li> </ol> <p>To make this practical, Exa employs several optimization techniques:</p> <ul> <li>Matryoshka embeddings: Training models to create embeddings where smaller subsets of the vector still represent the full meaning, allowing for dimension reduction</li> <li>Clustering: Grouping similar embeddings so searches only need to examine the most relevant clusters</li> <li>Binary compression: Converting floating-point embeddings to boolean values for faster processing</li> <li>Assembly-level optimizations: Using SIMD operations and low-level CPU optimizations</li> </ul> <p>As Will explained: \"When you're doing over billions of documents, billions of binary compressed embeddings using clustering, it's still too slow. A lot of the advances or speed-ups we got were from going into the actual assembly and the SIMD operations on the low-level stuff on the CPU.\"</p> <p>Even with these optimizations, some complex queries require what Will calls \"test-time compute\" - searches that might take minutes or hours rather than milliseconds: \"Perfect search requires test-time compute... your search could sometimes take half a second, but sometimes it could take a minute or 10 minutes, or an hour or a day for super complex things.\"</p> <p>Key Takeaway: Building semantic search at web scale requires sophisticated optimizations at every level, from embedding generation to vector database design. Some complex queries fundamentally require more computation time, creating a new paradigm of \"test-time compute\" search.</p> <p>How does Exa position itself in the AI ecosystem?</p> <p>Exa positions itself as search infrastructure rather than a consumer application. In the current AI ecosystem, there are search infrastructure providers (Google, Bing, Exa) that crawl and index the web, and AI applications that build on top of them (like Gemini using Google or Search GPT using Bing).</p> <p>Will explained: \"Exa is trying to be the search infrastructure, and we want to be able to handle all these possible queries. That's our goal.\" When an AI application needs information from the web, it forms a query, sends it to Exa, and Exa returns the most relevant information, which the application can then use in its processing.</p> <p>This infrastructure approach differs from consumer-facing products like Perplexity. While those products use search under the hood, Exa aims to be the underlying search engine that powers all types of AI applications, with an API optimized for AI rather than human users.</p> <p>The business model also creates different incentives compared to traditional search engines. While Google makes money from ads, Exa charges per query: \"Our incentive is to make people like the search as much as possible, get the highest quality information so that they use it more. Whereas Google's incentive is making them click on ads.\"</p> <p>This creates what Will calls \"pristine incentives\" where Exa's mission aligns with its financial interests: \"Because we have these very pristine incentives, our mission is very aligned with our financial incentives. So it's a very beautiful thing.\"</p> <p>Key Takeaway: Exa positions itself as search infrastructure optimized for AI applications rather than human users, with a business model based on query volume rather than advertising, creating incentives aligned with delivering high-quality search results.</p> <p>What does \"perfect search\" look like?</p> <p>Will's vision for \"perfect search\" extends far beyond current capabilities, enabling queries that most people don't even think to try because they know current tools can't handle them.</p> <p>Examples of perfect search capabilities include:</p> <ul> <li>Finding people with specific expertise: \"I'm creating a Discord of best prompting practices. Find me anyone who's ever thought deeply about this on the web.\"</li> <li>Location-specific expertise: \"Who in Berlin has written about new search algorithms?\"</li> <li>Multimodal search: \"Find images of shirts without stripes that have a cool mix of colors like a Jackson Pollock\"</li> <li>Curated media: \"Find me Hans Zimmer music with a fast beat similar to the song Supermarine\"</li> <li>Complex analysis: \"Show me analyses of this topic by liberal authors, then by conservative authors, then by conservative authors who used to be liberal\"</li> </ul> <p>Will believes that perfect search would fundamentally transform our relationship with information: \"People don't realize how much more the world's knowledge would actually be at your fingertips.\"</p> <p>However, he acknowledges that achieving this vision will take time: \"I would say like 2028 search will be amazing, like magical. And we'll look back on 2025, and we'll be like, 'Oh my God! They didn't know anything about the world.'\" Even then, he believes there will still be many types of searches that remain impossible.</p> <p>Like AI itself, search is a domain where there's always room for improvement: \"No matter how good your LLM is, you could always do better. I think similarly, no matter how good your search is, you could always do better.\"</p> <p>Key Takeaway: Perfect search would enable queries that most people don't even consider trying today, fundamentally transforming our access to information. While significant progress will be made in the coming years, search will remain an open-ended problem with continuous room for improvement.</p> <p>How can developers build effective search systems for specific domains?</p> <p>For developers building search systems over specific domains (like medical records or legal documents), Will offered several practical insights:</p> <p>The search problem becomes easier at smaller scales. When you're dealing with millions rather than billions of documents, you don't need the same sophisticated techniques required for web-scale search.</p> <p>For most domain-specific applications, a hybrid approach combining keyword methods and embedding space methods works well. The optimal approach depends on your query distribution - if users primarily make simple queries, simpler systems may suffice, but complex semantic queries require more sophisticated models.</p> <p>Query expansion (taking a query and expanding it into hundreds of related terms) can be effective for improving recall, though it won't match the capabilities of true semantic search for complex queries.</p> <p>When building these systems, developers should focus on understanding what their users actually need: \"You have to think about their query distribution - are your queries simple ones like 'GPT-3 paper,' or complex ones like 'a paper where people have this experiment setup that feels like this other setup'?\"</p> <p>For teams with limited resources (3-6 months to build something), Will suggested focusing on hybrid approaches that combine the strengths of keyword and embedding-based systems, as this provides a good balance of capabilities without requiring the development of custom models.</p> <p>Key Takeaway: Domain-specific search systems can use simpler approaches than web-scale engines, with hybrid keyword and embedding methods working well for most applications. Understanding your users' query patterns should guide your technical approach.</p> <p>How will search evolve in the coming years?</p> <p>Will believes we're at the beginning of a fundamental transformation in search technology. He used the metaphor of San Francisco during an earthquake to describe the current state of the industry: \"The whole software world is going through this tectonic shift. Because the way we get information seemed to be radically changing.\"</p> <p>Several trends are driving this transformation:</p> <ol> <li> <p>The limitations of LLMs: Despite their capabilities, LLMs haven't memorized the entire web. Will explained: \"The LLM simply cannot memorize the entire web... The web is gigantic. It's really, really big.\" This creates an inherent need for retrieval systems.</p> </li> <li> <p>The expansion of query types: As search tools improve, people will discover entirely new types of queries they never thought to try before. Will believes there are \"many, many types of queries that go beyond the ones listed here that we haven't even thought of, because our tools can't handle it.\"</p> </li> <li> <p>Test-time compute: Complex queries fundamentally require more computation time. Future search engines will need to handle queries that take minutes, hours, or even days to complete for particularly complex analyses.</p> </li> <li> <p>Multimodal search: Future search will extend beyond text to include sophisticated image, audio, and video search capabilities.</p> </li> </ol> <p>Will sees the search market fragmenting rather than being dominated by a single player: \"The space of possible searches is so broad that it's actually really hard for one company to be perfect at everything... It's bifurcating in all these ways. It's splintering.\"</p> <p>This creates opportunities for specialized search providers focused on particular capabilities or domains: \"I think the search space is big enough where there are lots of winners.\"</p> <p>Key Takeaway: Search is undergoing a fundamental transformation driven by AI capabilities, with the market likely to fragment into specialized providers rather than being dominated by a single approach. Future search will handle query types and modalities that we can't even imagine today.</p> <p>FAQs:</p> <p>What is Exa and how is it different from traditional search engines?</p> <p>Exa is a semantic search engine built specifically for AI systems rather than humans. Unlike traditional search engines like Google that use keyword-based algorithms optimized for human users, Exa uses neural embedding-based algorithms that understand the meaning and context of queries. This allows Exa to handle complex, lengthy queries and return precisely relevant results rather than just matching keywords.</p> <p>Why do AI systems need a different kind of search engine than humans?</p> <p>AI systems interact with search engines very differently than humans do. While humans typically use simple keyword queries and only want a few results, AI systems can formulate complex, detailed queries instantly and can process thousands of results at once. Traditional search engines were designed around human limitations and preferences, making them suboptimal for AI applications that need comprehensive, precise information retrieval.</p> <p>How does Exa's search technology work?</p> <p>Exa processes web documents through embedding models that convert text into vectors (lists of numbers) that capture the meaning of the content. When a query comes in, it's also converted to an embedding, and Exa finds the most semantically similar documents. To make this process efficient at scale, Exa uses several techniques including Matryoshka embeddings (nested representations of different sizes), clustering (grouping similar documents), binary compression, and low-level optimizations to handle billions of documents quickly.</p> <p>What kinds of queries can Exa handle that traditional search engines cannot?</p> <p>Exa can handle several types of queries that traditional search engines struggle with:</p> <ul> <li>Long, contextual queries (entire paragraphs or code snippets)</li> <li>Semantic queries (e.g., \"people in SF who know assembly language\")</li> <li>Complex, multi-condition queries (e.g., \"Find every article that argues X and not Y from author Z\")</li> <li>Queries with very specific information needs that require precise understanding of language</li> </ul> <p>How does Exa fit into the AI application ecosystem?</p> <p>Exa positions itself as search infrastructure for AI applications. When an AI application needs information from the web, it sends a query to Exa, which returns the most relevant information. This is similar to how AI applications like Google's Gemini or OpenAI's ChatGPT use Google and Bing respectively as their search infrastructure. Exa aims to provide a more powerful and flexible API specifically designed for AI needs.</p> <p>What is \"test time compute\" search and why is it important?</p> <p>Test time compute search refers to search operations that may take longer than traditional search (from seconds to minutes or even hours) because they involve more complex processing. This approach is necessary for truly complex search queries that require deeper analysis. Similar to how complex AI tasks might require more powerful models, complex search queries require more computational resources. Exa's approach acknowledges that some valuable search capabilities simply can't be delivered in milliseconds.</p> <p>What is \"perfect search\" and how is Exa working toward it?</p> <p>Perfect search is Exa's vision of a search system that can handle any type of query across any type of content. This includes finding people with specific expertise, searching for multimodal content with precise attributes, curating content based on complex criteria, and performing sophisticated analyses across different perspectives. Exa is working toward this by building a search infrastructure that understands language deeply and can be customized to handle increasingly complex information needs.</p> <p>What is Exa's business model?</p> <p>Unlike traditional search engines that make money from advertising, Exa charges on a usage basis (per query). This creates an incentive structure where Exa benefits from providing the highest quality search results possible so that customers will use the service more, rather than optimizing for ad clicks.</p> <p>Does Exa handle content behind paywalls?</p> <p>No, Exa only searches content that is publicly accessible and not behind paywalls.</p> <p>How does Exa handle SEO manipulation?</p> <p>Exa's neural embedding-based approach makes it inherently more resistant to traditional SEO manipulation techniques that work on keyword-based algorithms. Additionally, since Exa is focused on providing high-quality information rather than generating ad revenue, it has stronger incentives to filter out low-quality SEO-optimized content.</p> <p>How does Exa handle document size and chunking?</p> <p>Exa works with web documents which naturally have a structure that makes them manageable for embedding. While there are limits to document size, Exa can handle most web content effectively. The company is exploring chunking strategies for longer documents but has found that the natural structure of web content often provides reasonable document boundaries.</p>","tags":["semantic search","AI search","embeddings","web search","Exa"]},{"location":"talks/superlinked-encoder-stacking/","title":"Encoder Stacking and Multi-Modal Retrieval Systems [Daniel - Superlinked]","text":"<p>Study Notes:</p> <p>I hosted a special session with Daniel from Superlinked to explore how we can improve retrieval systems by applying lessons from recommender systems. This conversation revealed critical insights about the limitations of current search approaches and how to build more sophisticated retrieval architectures that handle diverse data types beyond just text.</p> <p>Why traditional retrieval systems fall short for complex queries</p> <p>When we look at complex queries like \"popular family-friendly hotels with good Wi-Fi near Manhattan midtown under $400,\" traditional retrieval systems struggle because they're trying to handle multiple types of data with tools designed primarily for text.</p> <p>Each component of this query represents either a bias (steering results toward certain attributes) or a filter (binary yes/no conditions):</p> <ul> <li>\"Popular\" is a numerical bias</li> <li>\"Family-friendly\" is a categorical bias</li> <li>\"Good Wi-Fi\" is a semantic bias found in reviews</li> <li>\"Near Manhattan midtown\" is a location bias</li> <li>\"Under $400\" is a price filter</li> </ul> <p>If your toolkit is just a text embedding model, it simply can't understand these different data types effectively. The current approach using text-to-SQL or similar query generation has significant limitations in latency, reliability, and handling non-textual inputs like user clicks or contextual parameters.</p> <p>Key Takeaway: Complex queries contain multiple data types that text embedding models alone can't properly understand. We need specialized approaches for handling numerical, categorical, location, and other non-textual data.</p> <p>Three fundamental problems with current search systems</p> <p>I've observed that even if you generate the perfect query, today's search systems have three fundamental flaws:</p> <ol> <li>Overuse of filters instead of biases When users say \"near Manhattan midtown,\" they don't mean \"exactly within X kilometers\" - they have a preference that gradually decreases with distance. But systems typically implement this as a hard Boolean filter (a step function) rather than a smooth bias (a sigmoid function), creating an artificial cutoff that excludes potentially excellent results.</li> <li>Reliance on re-ranking as a hack Re-ranking exists because our underlying retrieval is inadequate. If retrieval worked properly, you wouldn't need to shuffle results afterward. The problem is that re-ranking only applies to a tiny fraction of your database (typically less than 1%), so whatever you miss in candidate selection can't be fixed through re-ranking.</li> <li>Misuse of text embeddings for non-textual data People try to stringify everything (JSON objects, numbers, etc.) and feed it to text embedding models. This fundamentally doesn't work because these models understand numbers through co-occurrence in training data, not as actual numerical values. If you stringify integers and calculate similarities between their embeddings, there's no concept that \"49 is one less than 50\" in the latent space.</li> </ol> <p>Key Takeaway: Current systems rely too heavily on Boolean filters instead of smooth biases, use re-ranking to compensate for poor retrieval, and misapply text embeddings to non-textual data. These fundamental issues limit search quality regardless of prompt engineering.</p> <p>A better approach: Mixture of encoders</p> <p>Instead of forcing everything through text embeddings, we should use specialized encoders for different data types and then combine their outputs. This \"encoder stacking\" or \"mixture of encoders\" approach works like this:</p> <ol> <li>Break down the query into components using a DSPy-style optimized prompt</li> <li>Feed each component to the appropriate specialized encoder:<ul> <li>Text to a text encoder</li> <li>Numbers to a numerical encoder</li> <li>Locations to a location encoder</li> <li>User interactions to a graph encoder</li> </ul> </li> <li>Each encoder produces an embedding that natively understands its data type</li> <li>Combine these embeddings with appropriate weights to create a final query vector</li> <li>Add any necessary filter predicates for binary conditions</li> <li>Send this to a vector database for retrieval</li> </ol> <p>This approach compresses all biases into a single embedding while maintaining the ability to filter when absolutely necessary. The result is more accurate retrieval without needing extensive re-ranking or complex boosting logic.</p> <p>Key Takeaway: By using specialized encoders for different data types rather than forcing everything through text embeddings, we can create retrieval systems that better understand user intent and produce higher quality results.</p> <p>The pilot that sees the world as strings</p> <p>One metaphor that really captures the current limitations is that language models are like \"pilots that see the whole world as strings.\" This creates fundamental problems when dealing with non-textual data.</p> <p>Consider how LLMs handle numbers - they don't truly understand numerical relationships. That's why they might think 3.11 is larger than 3.9, or struggle with basic arithmetic. This isn't just a minor issue but a gateway to a much deeper problem: we can't effectively stringify the world and expect models to understand it properly.</p> <p>This limitation extends to graph-based approaches too. If you're using an LLM to navigate a graph where nodes have non-textual metadata, you're still forcing that data through a text-based understanding that fundamentally misrepresents its nature.</p> <p>Key Takeaway: Language models fundamentally see everything as text, which creates inherent limitations when dealing with numerical data, location data, or other non-textual information. This \"world as strings\" problem requires specialized solutions.</p> <p>Practical implementation considerations</p> <p>When implementing these systems in production, several practical considerations emerge:</p> <ol> <li>Refreshing embedding models With traditional approaches, distribution drift on any property forces you to retrain the entire model. With a mixture of encoders, you can selectively update individual components as needed, making maintenance more manageable.</li> <li>Handling data updates If a hotel's popularity changes but its description doesn't, you don't need to re-encode the entire entity - just update the specific embedding signal for popularity and recombine it with the rest.</li> <li>Sparse vs. dense representations The more efficient and dense your encoders, the worse they respond to aggregation. If you average several dense embeddings together, you often destroy information. More sparse encoders (closer to bag-of-words) handle aggregation better because they represent a union of features rather than a point in a compressed space.</li> <li>Keyword-heavy queries For exact matches, you can either treat these as part of the filtering system or incorporate sparse representations alongside dense ones, depending on your vector database capabilities.</li> </ol> <p>Key Takeaway: A modular approach with specialized encoders makes systems more maintainable, allowing selective updates to components affected by data drift rather than requiring complete retraining.</p> <p>What we're not asking about retrieval systems</p> <p>There are several critical issues that aren't getting enough attention in discussions about retrieval:</p> <ol> <li>We need to look at the world through lenses beyond text tokenization Text is just one data type among many, and forcing everything through text encoders fundamentally limits what we can achieve.</li> <li>Re-ranking is overused and often masks deeper problems Rather than fixing poor retrieval with re-ranking, we should improve the underlying retrieval to make re-ranking less necessary.</li> <li>Boolean filters are poor approximations of user preferences Most user preferences are gradual rather than binary, but we implement them as hard filters that exclude potentially valuable results.</li> </ol> <p>The future likely involves more sophisticated encoders that can handle diverse data types natively, but until then, we need to combine specialized encoders for different data types to get the maximum signal and control over our results.</p> <p>Key Takeaway: The field needs to move beyond text-centric approaches, reduce reliance on re-ranking, and replace hard Boolean filters with smooth biases that better represent user preferences. This requires fundamentally rethinking how we encode and retrieve information.</p> <p>FAQs</p> <p>What are the limitations of current search systems?</p> <p>Most search systems today have three fundamental limitations. First, they overuse filters to approximate user preferences instead of using smoother bias functions. Second, they rely heavily on re-ranking, which is essentially a workaround for poor initial retrieval. Third, they misuse text embeddings by trying to encode non-textual data (like numbers) through text models, which creates inaccurate representations.</p> <p>How do traditional search systems handle complex queries?</p> <p>Traditional systems typically use a text-to-SQL approach where they convert natural language queries into database queries. This process often struggles with non-textual inputs (like user clicks or contextual data), has latency issues, and can be unreliable when generating complex queries. The resulting search pipeline usually involves pre-filters, boosting, vector search, re-ranking, and post-filtering\u2014a complicated process with many potential failure points.</p> <p>What's wrong with using text embedding models for all search needs?</p> <p>Text embedding models understand the world as strings, which creates fundamental limitations. They process numbers by how they co-occur in training data rather than understanding their mathematical properties. For example, these models don't inherently understand that 49 is one less than 50 in their latent space. This makes them poorly suited for handling numerical data, location data, and other non-textual information that's critical in real-world search applications.</p> <p>What is encoder stacking (or mixture of encoders)?</p> <p>Encoder stacking is an approach that uses specialized encoders for different types of data instead of relying solely on text encoders. It involves breaking down a query into components and routing each component to the appropriate specialized encoder\u2014such as numerical encoders for popularity scores, location encoders for geographical data, or graph encoders for user interaction patterns. The outputs from these encoders are then aggregated into a final embedding that captures all relevant signals.</p> <p>How does encoder stacking handle a complex travel query?</p> <p>For a query like \"popular, family-friendly hotels with good Wi-Fi near Manhattan midtown under $400,\" encoder stacking would:</p> <ol> <li> <p>Route \"popular\" to a numerical encoder that creates a bias toward highly-rated properties</p> </li> <li> <p>Send \"family-friendly\" to a categorical encoder that biases toward that attribute</p> </li> <li> <p>Process \"good Wi-Fi\" through a semantic encoder that understands sentiment in reviews</p> </li> <li> <p>Direct \"near Manhattan midtown\" to a location encoder that understands geographical proximity</p> </li> <li> <p>Create a filter predicate for \"under $400\"</p> </li> <li> <p>Combine all these signals into a single embedding that captures all these preferences</p> </li> </ol> <p>What are the benefits of using specialized encoders?</p> <p>Specialized encoders provide more accurate representations of different data types than text-only models. They allow for smoother bias functions rather than rigid filters, eliminate the need for re-ranking by getting better initial results, and handle non-textual data properly. This approach also makes it easier to update individual components when data distributions change, rather than retraining an entire model.</p> <p>How does Superlinked implement encoder stacking?</p> <p>Superlinked provides an open-source framework that allows developers to create a system of specialized encoders. The framework includes components for query understanding, encoder routing, and result aggregation. It works with various vector databases (including Redis, MongoDB, and Quadrant) and provides APIs for building retrieval systems for RAG, recommendation systems, and search applications.</p> <p>How often do you need to refresh embedding models in this approach?</p> <p>One advantage of encoder stacking is that you can selectively update individual encoders when their specific data distributions change, rather than retraining the entire system. This modular approach makes maintenance more manageable. You can also implement caching at the individual encoder level, allowing you to update only the affected parts of an embedding when certain attributes change.</p> <p>How do you handle keyword-heavy queries in this framework?</p> <p>Keyword queries are typically handled through the filter component of the system. While sparse data structures (like traditional keyword search) could theoretically be integrated into the encoder framework, current vector databases are more optimized for handling these as filters. Superlinked uses the underlying sparse indexing capabilities (like BM25) of the database for keyword matching.</p> <p>How should encoders be designed and combined?</p> <p>Encoders should be designed to produce dense vector representations of their specific input types. These outputs are then aggregated\u2014typically by normalizing each encoder's output and concatenating them dimension-wise into a larger vector. This approach preserves the signal from each encoder in separate dimensions, which helps with explainability and allows for query-time weighting of different signals.</p> <p>Will text models eventually replace specialized encoders?</p> <p>While large companies with massive data might be able to train transformers that outperform specialized encoders, most organizations benefit from having a toolkit that allows them to build systems specific to their needs. Until we reach true AGI, specialized encoders will likely remain valuable for handling non-textual data types and providing more control over search results.</p> <p>What should we be focusing on to improve search systems?</p>","tags":["encoder stacking","multi-modal retrieval","specialized encoders","Superlinked","recommender systems","data types"]},{"location":"talks/superlinked-encoder-stacking/#we-should-focus-on-three-key-areas-1-looking-at-the-world-through-lenses-beyond-text-tokenization-2-reducing-our-reliance-on-re-ranking-by-improving-initial-retrieval-and-3-using-smooth-bias-functions-instead-of-rigid-boolean-filters-these-approaches-will-lead-to-more-accurate-efficient-and-controllable-search-systems","title":"We should focus on three key areas: 1) Looking at the world through lenses beyond text tokenization, 2) Reducing our reliance on re-ranking by improving initial retrieval, and 3) Using smooth bias functions instead of rigid Boolean filters. These approaches will lead to more accurate, efficient, and controllable search systems.","text":"<p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>","tags":["encoder stacking","multi-modal retrieval","specialized encoders","Superlinked","recommender systems","data types"]},{"location":"talks/zapier-vitor-evals/","title":"Building Feedback Systems for AI Products","text":"<p>Study Notes:</p> <p>I hosted a session with Vitor from Zapier to discuss how they dramatically improved their feedback collection systems for AI products. This conversation reveals practical strategies for gathering, analyzing, and implementing user feedback to create a continuous improvement cycle for RAG systems and AI applications.</p> <p>How did Zapier Central improve their feedback collection process? Zapier has been building numerous AI-powered features, with Zapier Central being their newest product - essentially an AI automation platform that connects with third-party apps and keeps data in sync. It functions as both a chat interface for your live data and an automation tool that can be controlled through natural language.</p> <p>Initially, they faced a common challenge with AI products: limited feedback. Despite having users actively engaging with the product, their feedback submission rates were \"abysmally low\" - around 10 submissions per day. Even worse, the feedback they did receive was almost exclusively negative, coming from frustrated users experiencing errors.</p> <p>The team realized they needed to be more aggressive and strategic about soliciting feedback. They made a seemingly small change that produced dramatic results - instead of using tiny, muted feedback buttons in the corner of the interface, they added a natural-looking chat message at the end of workflow tests that directly asked: \"Did this run do what you expected it to do?\"</p> <p>This simple change, combined with larger, more visible thumbs-up and thumbs-down buttons, increased their feedback submissions from 10 to approximately 40 per day - a 4x improvement. Even more surprising was that they started receiving substantial positive feedback, which had been almost non-existent before.</p> <p>Key Takeaway: The positioning, visibility, and wording of feedback requests dramatically impacts response rates. Being more direct and contextual with feedback requests can yield significantly more data, including the often-missing positive feedback that helps you understand what's working well.</p> <p>What strategies can you use to mine implicit feedback from user behavior? Beyond explicit feedback buttons, Vitor shared several clever approaches for mining implicit feedback from user interactions:</p> <ol> <li>Workflow activation signals: When a user tests a workflow and then activates it, that's a strong positive signal that the system is working as expected.</li> <li>Tool call validation errors: If a tool call returns a validation error, it likely indicates the LLM made a mistake, which can be treated as negative feedback.</li> <li>Follow-up message analysis: User follow-up messages often contain valuable signals - if they're rephrasing their question or expressing frustration, it suggests the previous response wasn't satisfactory.</li> <li>Hallucination detection: By identifying common patterns in hallucinations (like phrases with \"[you can replace]\" in square brackets), they could mine for these specific failure cases.</li> </ol> <p>The team implemented a nightly job that fetched runs from their database and analyzed them for these implicit feedback signals, providing additional data beyond what users explicitly submitted.</p> <p>Key Takeaway: Look beyond explicit feedback mechanisms and mine your application data for implicit signals. User behaviors like activating workflows, rephrasing questions, or abandoning sessions can provide valuable insights about system performance without requiring direct user input.</p> <p>How did Zapier organize and scale their feedback analysis process? With feedback volumes increasing dramatically, the team needed a systematic approach to process and learn from this data. They built an internal feedback triaging system where all submissions would land by default, providing staff with an optimized view of each run's details.</p> <p>To scale beyond what one person could handle, they implemented \"labeling parties\" - weekly team meetings where everyone would review and categorize feedback submissions together. While initially slower than having one person process everything, these sessions built team members' confidence in labeling independently and fostered a \"look at your data\" mentality throughout the organization.</p> <p>The team added extensive metadata to each submission, including tools used, context, and entry point (web app, Chrome extension, Slack). This allowed them to filter feedback by specific subsystems or contexts, making it easier for specialized teams to focus on relevant submissions.</p> <p>From the triaging interface, staff could label feedback with categories corresponding to product capabilities, add notes about findings, and easily create evaluation tests directly from user interactions. This streamlined the process of turning real-world feedback into reproducible tests.</p> <p>Key Takeaway: Creating structured processes for feedback analysis and involving the entire team builds a data-driven culture. Regular \"labeling parties\" not only distribute the workload but also ensure everyone understands user pain points and the value of examining raw data.</p> <p>How did feedback collection improve Zapier's product development process? The improved feedback system transformed Zapier's product development in several ways:</p> <ol> <li>Clearer prioritization: They could identify which capabilities were failing most frequently and cross-reference this with usage data to focus on high-traffic areas with low accuracy.</li> <li>Targeted improvements: They discovered that their \"AI Actions\" tool, which translates natural language into API parameters, was failing in specific ways - like sending Slack DMs to random people instead of asking for clarification about the recipient.</li> <li>Expanded evaluation coverage: They grew from 23 curated evaluations to 383 evaluations based on real user interactions, giving them much greater confidence when making system changes.</li> <li>Better model selection decisions: When GPT-4.0 was released, they had enough evaluation data to determine that it actually regressed performance on some of their use cases, allowing them to delay adoption until they could address these issues with prompt tuning.</li> </ol> <p>The team fed all their labeled data into their analytics platform (Databricks), allowing them to cross-reference feedback with product usage metrics and make more informed decisions about feature development.</p> <p>Key Takeaway: A robust feedback system creates a flywheel effect - more feedback leads to better understanding of user needs, which enables more targeted improvements, resulting in better user experiences and ultimately more positive feedback.</p> <p>What were the key outcomes of Zapier's feedback improvement initiative? The initiative produced several valuable outcomes beyond just the immediate product improvements:</p> <ol> <li>Cultural shift: Everyone on the team now feels empowered to examine raw data and understands its value. Team discussions became more focused on improving specific capabilities rather than making vague statements about general improvements.</li> <li>Improved decision-making: They could make more informed decisions about model upgrades, like waiting to adopt a new GPT-4.0 snapshot until they had addressed regressions in their evaluations.</li> <li>Business metrics improvement: Activation and retention metrics began trending upward as the product improved based on feedback insights.</li> <li>Positive feedback ratio: The ratio of positive to negative feedback submissions improved, indicating their changes were addressing user pain points.</li> </ol> <p>The team is now looking to further automate their labeling process, potentially using LLM-as-judge approaches to draft initial categorizations for human review, continuing to make their feedback flywheel more efficient.</p> <p>Key Takeaway: Investing in feedback systems pays dividends beyond immediate product improvements - it creates a data-driven culture, enables better decision-making, and ultimately drives business metrics like activation and retention.</p> <p>How important is the specific wording of feedback requests? Vitor emphasized that the wording of feedback requests significantly impacts the quality of responses. Initially, they considered using generic language like \"How did we do?\" but after consulting with Jason, they opted for more specific phrasing: \"Did this run do what you expected it to do?\"</p> <p>This specificity guided users toward providing feedback about the workflow's functionality rather than other aspects like speed or interface design. By focusing the question on their primary concern - whether the workflow performed the intended action - they received more actionable feedback.</p> <p>Different contexts might require different feedback questions. For a RAG system handling complex multi-step workflows, functional correctness might be the priority, while other systems might need to focus on response quality, hallucination rates, or other metrics.</p> <p>Key Takeaway: Craft feedback requests that specifically target the dimensions you care most about improving. Generic questions yield generic answers, while specific questions guide users toward providing the most valuable insights for your particular system.</p> <p>What's the value of turning feedback into formal evaluations? Zapier emphasized the importance of converting user feedback into formal evaluations that could be run repeatedly. This approach provided several benefits:</p> <ol> <li>Reproducibility: They could reliably test the same scenarios across different model versions or system changes.</li> <li>Regression prevention: By capturing both successful and unsuccessful interactions, they could ensure new changes didn't break previously working functionality.</li> <li>Targeted improvement: They could focus on specific failure modes and measure progress in addressing them.</li> <li>Confidence in changes: With hundreds of evaluations based on real user interactions, they could confidently make system changes knowing they weren't introducing new problems.</li> </ol> <p>The team built tooling to easily create evaluations from feedback submissions, including the ability to mock third-party API calls to simulate production conditions without making actual external requests.</p> <p>Key Takeaway: Converting real-world feedback into formal evaluations creates a safety net for future development. This approach transforms one-time user experiences into persistent quality checks that protect against regressions and guide improvements.</p> <p>How does feedback collection fit into the broader AI product development cycle? Vitor described a virtuous cycle for AI product development:</p> <ol> <li>Start with a prototype using a strong foundational model</li> <li>Create initial evaluations based on intuition or synthetic data</li> <li>Ship to users and collect real-world feedback</li> <li>Use that feedback to build more comprehensive evaluations</li> <li>Improve the system based on these evaluations</li> <li>Repeat the process, continuously refining both the product and evaluation suite</li> </ol> <p>This cycle becomes particularly valuable when evaluating new model versions. When GPT-4.0 was released, Zapier's evaluation suite revealed that while many capabilities improved, some actually regressed - like the model's ability to recover from tool calls with invalid parameters. Without these evaluations, they might have blindly upgraded and introduced new problems.</p>","tags":["feedback systems","evaluation","user feedback","AI products","Zapier"]},{"location":"talks/zapier-vitor-evals/#key-takeaway-feedback-collection-isnt-just-about-fixing-immediate-issues-its-a-critical-component-of-a-sustainable-ai-product-development-cycle-each-round-of-feedback-strengthens-your-evaluation-suite-which-in-turn-enables-more-confident-decision-making-about-system-changes-and-model-upgrades","title":"Key Takeaway: Feedback collection isn't just about fixing immediate issues - it's a critical component of a sustainable AI product development cycle. Each round of feedback strengthens your evaluation suite, which in turn enables more confident decision-making about system changes and model upgrades.","text":"<p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>","tags":["feedback systems","evaluation","user feedback","AI products","Zapier"]},{"location":"workshops/","title":"Workshops","text":"<p>This series of workshops guides you through the complete process of building, evaluating, and continuously improving RAG systems. Learn how to transform your RAG implementation from a static technical deployment into a continuously evolving product that systematically improves through user feedback and data-driven enhancement.</p>"},{"location":"workshops/#detailed-table-of-contents","title":"Detailed Table of Contents","text":""},{"location":"workshops/#introduction-beyond-implementation-to-improvement","title":"Introduction: Beyond Implementation to Improvement","text":"<p>The Product Mindset for RAG Systems</p> <ul> <li>Shifting from technical implementation to product-focused continuous improvement</li> <li>Understanding RAG as a recommendation engine wrapped around language models</li> <li>The improvement flywheel: transforming user interactions into system enhancements</li> <li>Moving from ad-hoc tweaking to systematic, data-driven improvement</li> <li>Case studies showing the difference between implementation and product mindsets</li> </ul>"},{"location":"workshops/#chapter-1-kickstarting-the-data-flywheel-with-synthetic-data","title":"Chapter 1: Kickstarting the Data Flywheel with Synthetic Data","text":"<p>Establishing Evaluation Frameworks and Overcoming the Cold Start Problem</p> <ul> <li>Common pitfalls in AI development: reasoning fallacy, vague metrics, generic solutions</li> <li>Leading vs. lagging metrics: focusing on controllable inputs like experiment velocity</li> <li>Understanding precision and recall for retrieval evaluation</li> <li>Synthetic data generation techniques using chain-of-thought and few-shot prompting</li> <li>Building evaluation pipelines that run continuously</li> <li>Case studies: improving recall from 50% to 90% through systematic evaluation</li> </ul>"},{"location":"workshops/#chapter-2-converting-evaluations-into-training-data-for-fine-tuning","title":"Chapter 2: Converting Evaluations into Training Data for Fine-Tuning","text":"<p>From Evaluation to Production Improvement</p> <ul> <li>Why generic embeddings fall short for specialized applications</li> <li>Converting evaluation examples into effective few-shot prompts</li> <li>Understanding contrastive learning and hard negative mining</li> <li>Fine-tuning embedding models vs. language models: cost, complexity, and benefits</li> <li>Building training datasets from user interactions and feedback</li> <li>Re-rankers and linear adapters as cost-effective enhancement strategies</li> </ul>"},{"location":"workshops/#chapter-3-user-experience-and-feedback-collection","title":"Chapter 3: User Experience and Feedback Collection","text":""},{"location":"workshops/#chapter-31-feedback-collection-building-your-improvement-flywheel","title":"Chapter 3.1: Feedback Collection - Building Your Improvement Flywheel","text":"<p>Designing Feedback Mechanisms That Users Actually Use</p> <ul> <li>Making feedback visible and engaging: increasing rates from &lt;1% to &gt;30%</li> <li>Proven copy patterns: \"Did we answer your question?\" vs. generic feedback</li> <li>Segmented feedback targeting specific pipeline components</li> <li>Mining implicit feedback: query refinements, engagement time, citation clicks</li> <li>Creative UI patterns for collecting hard negatives</li> <li>Enterprise feedback collection through Slack integrations</li> </ul>"},{"location":"workshops/#chapter-32-overcoming-latency-streaming-and-interstitials","title":"Chapter 3.2: Overcoming Latency - Streaming and Interstitials","text":"<p>Transforming Waiting Time into Engagement Opportunities</p> <ul> <li>Psychology of waiting: perceived vs. actual performance</li> <li>Implementing streaming responses for 30-40% higher feedback collection</li> <li>Skeleton screens and meaningful interstitials</li> <li>Platform-specific implementations: Slack bots, web interfaces</li> <li>Technical implementation: Server-Sent Events, structured data streaming</li> <li>Streaming function calls and reasoning processes</li> </ul>"},{"location":"workshops/#chapter-33-quality-of-life-improvements","title":"Chapter 3.3: Quality of Life Improvements","text":"<p>Citations, Chain of Thought, and Validation Patterns</p> <ul> <li>Interactive citations that build trust while collecting feedback</li> <li>Chain of thought reasoning for 8-15% accuracy improvements</li> <li>Monologues for better comprehension in complex contexts</li> <li>Validation patterns as safety nets: reducing errors by 80%</li> <li>Strategic rejection of work to set appropriate expectations</li> <li>Showcasing capabilities to guide users toward successful interactions</li> </ul>"},{"location":"workshops/#chapter-4-understanding-your-users-through-data-analysis","title":"Chapter 4: Understanding Your Users Through Data Analysis","text":""},{"location":"workshops/#chapter-41-topic-modeling-and-analysis","title":"Chapter 4.1: Topic Modeling and Analysis","text":"<p>Finding Patterns in User Feedback and Queries</p> <ul> <li>Moving from individual feedback to systematic pattern identification</li> <li>Topics vs. capabilities: understanding what users ask about vs. what they want the system to do</li> <li>Clustering and classification techniques for query segmentation</li> <li>Transforming \"make the AI better\" into specific, actionable priorities</li> <li>Resource allocation frameworks for maximum impact improvements</li> </ul>"},{"location":"workshops/#chapter-42-prioritization-and-roadmapping","title":"Chapter 4.2: Prioritization and Roadmapping","text":"<p>From Insights to Strategic Action Plans</p> <ul> <li>Impact/effort prioritization using 2x2 frameworks</li> <li>Failure mode analysis: identifying root causes vs. symptoms</li> <li>Building strategic roadmaps based on user behavior patterns</li> <li>Continuous improvement systems that scale with usage</li> <li>Case studies: how query analysis changes development priorities</li> </ul>"},{"location":"workshops/#chapter-5-building-specialized-retrieval-capabilities","title":"Chapter 5: Building Specialized Retrieval Capabilities","text":""},{"location":"workshops/#chapter-51-understanding-specialized-retrieval","title":"Chapter 5.1: Understanding Specialized Retrieval","text":"<p>Beyond Basic RAG: The Power of Specialization</p> <ul> <li>Why monolithic approaches reach limits with diverse query types</li> <li>Two complementary strategies: extracting metadata vs. creating synthetic text</li> <li>Mathematics of specialization: local models outperforming global approaches</li> <li>Organizational benefits: division of labor and incremental improvement</li> <li>Two-level measurement: router accuracy \u00d7 retriever performance</li> </ul>"},{"location":"workshops/#chapter-52-implementing-multimodal-search","title":"Chapter 5.2: Implementing Multimodal Search","text":"<p>Practical Techniques for Documents, Images, Tables, and SQL</p> <ul> <li>Advanced document retrieval: contextual chunks, page-level strategies, hybrid signals</li> <li>Image search challenges: bridging visual and textual understanding with rich descriptions</li> <li>Table search dual approach: tables as documents vs. queryable databases</li> <li>SQL generation using RAG playbook: inventory \u2192 exemplars \u2192 business context</li> <li>RAPTOR hierarchical summarization for complex documents</li> <li>Performance improvements: 40% better image retrieval, 85% table lookup accuracy</li> </ul>"},{"location":"workshops/#chapter-6-unified-architecture-and-intelligent-routing","title":"Chapter 6: Unified Architecture and Intelligent Routing","text":""},{"location":"workshops/#chapter-61-query-routing-foundations","title":"Chapter 6.1: Query Routing Foundations","text":"<p>Building Cohesive Systems from Specialized Components</p> <ul> <li>The API mindset: treating retrievers as services for language models</li> <li>Organizational structure: interface, implementation, router, and evaluation teams</li> <li>Evolution from monolithic to modular architecture</li> <li>Performance formula: P(success) = P(right tool) \u00d7 P(right document | right tool)</li> <li>Framework development perspective for distributed RAG systems</li> </ul>"},{"location":"workshops/#chapter-62-tool-interfaces-and-implementation","title":"Chapter 6.2: Tool Interfaces and Implementation","text":"<p>Implementing Routing Layers and Tool Selection</p> <ul> <li>Designing tool interfaces with Pydantic models and comprehensive documentation</li> <li>Router implementation using structured outputs and few-shot examples</li> <li>Dynamic example selection based on query similarity</li> <li>Multi-agent vs. single-agent architecture decisions</li> <li>Tool portfolio design: multiple access patterns for same data</li> <li>MCP (Model Context Protocol) as emerging standard</li> </ul>"},{"location":"workshops/#chapter-63-performance-measurement-and-improvement","title":"Chapter 6.3: Performance Measurement and Improvement","text":"<p>Building Learning Systems That Continuously Improve</p> <ul> <li>Measuring tool selection effectiveness: precision, recall, confusion matrices</li> <li>Dual-mode UI: chat interface + direct tool access</li> <li>User feedback as high-quality training data</li> <li>Diagnostic frameworks for identifying routing vs. retrieval problems</li> <li>Automated evaluation pipelines and continuous monitoring</li> <li>Creating improvement flywheel: interactions \u2192 data \u2192 better routing \u2192 higher satisfaction</li> </ul>"},{"location":"workshops/#workshop-structure","title":"Workshop Structure","text":"<p>Each workshop combines theoretical concepts with practical exercises that you can apply directly to your own RAG implementations. Workshops are designed to be completed sequentially, as each one builds on concepts from previous sessions.</p> <p>The workshops follow a complete methodology:</p> <ol> <li>Foundation (Introduction &amp; Chapter 1): Product mindset and evaluation frameworks</li> <li>Improvement Mechanics (Chapter 2): Converting evaluation into training data  </li> <li>User Experience (Chapter 3): Feedback collection, streaming, and quality improvements</li> <li>Analysis (Chapter 4): Understanding user patterns and prioritizing improvements</li> <li>Specialization (Chapter 5): Building specialized capabilities for different content types</li> <li>Unification (Chapter 6): Intelligent routing and unified architecture</li> </ol> <p>Prerequisites</p> <p>These workshops assume basic familiarity with RAG implementations and foundational AI concepts. If you're new to RAG, we recommend reviewing the Introduction before diving into the other chapters.</p> <p>What You'll Build</p> <p>By completing this workshop series, you'll have built a comprehensive RAG system that:</p> <ul> <li>Continuously improves through systematic feedback collection</li> <li>Routes queries intelligently to specialized retrieval components  </li> <li>Provides engaging user experiences with streaming and transparency</li> <li>Uses data-driven prioritization for enhancement decisions</li> <li>Implements validation patterns and quality safeguards</li> <li>Scales across teams and complexity levels</li> </ul>"},{"location":"workshops/#newsletters","title":"Newsletters","text":"<p>If you want to get the latest news and updates, you can subscribe to our newsletter.</p>"},{"location":"workshops/chapter0/","title":"Beyond Implementation to Improvement: A Product Mindset for RAG","text":"<p>Chapter Overview</p> <pre><code>In this opening chapter, I introduce you to a fundamental shift in how we approach RAG systems\u2014from static implementations to continuously improving products. You'll discover:\n\n- Why the most successful RAG systems are built as products, not just technical implementations\n- How to think about RAG as a recommendation engine wrapped around language models\n- The \"improvement flywheel\" that transforms user interactions into system enhancements\n- How to shift from ad-hoc tweaking to systematic, data-driven improvement\n- The mindset that separates successful AI products from those that stagnate after launch\n- How to dismantle guesswork in AI development with structured, measurable approaches\n</code></pre>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#the-product-mindset-why-most-rag-implementations-fail","title":"The Product Mindset: Why Most RAG Implementations Fail","text":"<p>When organizations implement RAG systems, they often approach it as a purely technical challenge. They focus on selecting the right embedding model, vector database, and LLM, then consider the project \"complete\" once these components are integrated and deployed.</p> <p>This approach inevitably leads to disappointment. The system works well for demo queries and simple use cases, but struggles with the complexity and diversity of real-world questions. As users encounter these limitations, they lose trust in the system and engagement drops. Without clear metrics or improvement processes, teams resort to ad-hoc tweaking based on anecdotal feedback.</p> <p>The fundamental issue? They've built a technical implementation, not a product.</p> <p>Throughout my career building AI systems\u2014from computer vision and computational mathematics at the University of Waterloo, to content policy and safety systems at Facebook, to recommendation systems at Stitch Fix that boosted revenue by $50 million\u2014I've observed a consistent pattern: the teams that succeed are those that treat their RAG implementations as products that continuously evolve rather than projects that eventually conclude.</p> <p>My background spans diverse applications of AI: managing large-scale data curation budgets, designing multimodal retrieval models using variational autoencoders and GANs, and processing hundreds of millions of recommendations weekly. This experience has directly informed my consulting work with companies like HubSpot, Zapier, and many others on query understanding, prompt optimization, embedding search, and fine-tuning.</p> <p>Consider these contrasting approaches:</p> Implementation Mindset Product Mindset \"We need to implement RAG\" \"We need to solve specific user problems\" Technical metrics (embedding dimensions, context window) User-centered metrics (answer relevance, task completion) Project with a defined endpoint Ongoing system that improves over time Success = working demo Success = sustained user value One-time architecture decisions Evolutionary architecture that adapts Focus on model selection Focus on feedback loops and data collection <p>The product mindset recognizes that launching your RAG system is just the beginning. The real work\u2014and the real value\u2014comes from how you systematically improve it based on user interactions.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#rag-as-a-recommendation-engine","title":"RAG as a Recommendation Engine","text":"<p>Mental Model</p> <p>The most effective way to think about RAG isn't as a pipeline of retrieval, augmentation, and generation steps\u2014it's as a recommendation engine wrapped around language models.</p> <p>This shift in perspective is transformative. When you view RAG as a recommendation system, you naturally focus on the aspects that truly determine performance: selecting the most relevant information to present to the language model.</p> <pre><code>flowchart TD\n    A[User Query] --&gt; B[Query Understanding]\n    B --&gt; C[Multiple Retrieval Paths]\n\n    C --&gt; D[Document Index]\n    C --&gt; E[Image Index]\n    C --&gt; F[Table Index]\n    C --&gt; G[Code Index]\n\n    D --&gt; H[Filtering]\n    E --&gt; H\n    F --&gt; H\n    G --&gt; H\n\n    H --&gt; I[Scoring/Ranking]\n    I --&gt; J[Context Assembly]\n    J --&gt; K[Prompt Construction]\n    K --&gt; L[Generation]\n    L --&gt; M[Response to User]\n\n    M --&gt;|Feedback| A</code></pre> <p>This recommendation engine perspective reveals important insights:</p> <ol> <li> <p>Generation quality is capped by retrieval quality. No amount of prompt engineering can overcome providing the wrong information to the LLM.</p> </li> <li> <p>Different queries need different retrieval strategies. Just as Amazon uses different recommendation algorithms for books versus electronics, your RAG system needs specialized approaches for different query types.</p> </li> <li> <p>User feedback is essential. Recommendation systems learn from interactions\u2014what users click on, purchase, or engage with. Your RAG system should similarly learn from how users interact with responses.</p> </li> <li> <p>Cold start is a significant challenge. Just as Netflix needs to learn your preferences, RAG systems need data to understand what makes a good response for your specific use case.</p> </li> <li> <p>The best recommendations are personalized. As your system evolves, it should adapt to specific user preferences and patterns.</p> </li> </ol> <p>This perspective also explains why many RAG implementations underperform\u2014they're built like simple search engines rather than sophisticated recommendation systems with feedback loops and personalization.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#the-improvement-flywheel-from-static-to-dynamic-systems","title":"The Improvement Flywheel: From Static to Dynamic Systems","text":"<p>At the core of the product mindset is what I call the \"improvement flywheel\"\u2014a systematic process that transforms user interactions into continuous enhancements.</p> <pre><code>graph TD\n    A[Build Basic RAG] --&gt; B[Create Synthetic Evaluation Data]\n    B --&gt; C[Define Metrics]\n    C --&gt; D[Test Hypotheses]\n    D --&gt; E[Deploy &amp; Collect Real User Feedback]\n    E --&gt; F[Categorize &amp; Analyze User Questions]\n    F --&gt; G[Make Targeted Improvements]\n    G --&gt; H[Implement Monitoring]\n    H --&gt; B\n\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style E fill:#bbf,stroke:#333,stroke-width:2px\n    style G fill:#dfd,stroke:#333,stroke-width:2px</code></pre> <p>This flywheel addresses the most common challenges in RAG development:</p> Phase Business Challenge Technical Challenge Flywheel Solution Cold Start No data to guide design decisions No examples to train or evaluate against Generate synthetic questions from contentEstablish baseline metricsCompare architectural approaches Initial Deployment Understanding what users actually need Learning what causes poor performance Instrument application for data collectionImplement feedback mechanismsCapture query patterns and failure modes Growth Prioritizing improvements with limited resources Addressing diverse query types effectively Use topic modeling to segment questionsIdentify highest-impact opportunitiesBuild specialized capabilities for key segments Optimization Maintaining quality as usage scales Combining multiple specialized components Create unified routing architectureImplement monitoring and alertsEstablish continuous improvement processes <p>The beauty of this approach is that each phase feeds into the next, creating momentum that accelerates improvement over time. As you collect more data, you gain clearer insights into what's working and what isn't, allowing you to make increasingly targeted enhancements.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#optimizing-feedback-collection","title":"Optimizing Feedback Collection","text":"<p>The effectiveness of your improvement flywheel depends heavily on how you collect feedback. During our office hours, we discovered that simply changing feedback prompt copy from \"How did we do?\" to \"Did we answer your question?\" increased feedback rates by 5x. This more specific framing focused users on the core value proposition rather than secondary concerns like latency or formatting.</p> <p>Proven Feedback Patterns</p> <p>When implementing feedback mechanisms, remember that every metric you track should lead to a specific follow-up action\u2014it's not just about knowing the number.</p> <p>Based on real-world implementations:</p> <p>Copy that Works: - \u2705 \"Did we answer your question?\" (5x higher response rate) - \u2705 \"Did we take the correct actions?\" (for action-oriented systems) - \u2705 \"Was this information helpful?\" (for knowledge bases) - \u274c \"How did we do?\" (too vague) - \u274c \"Rate your experience\" (focuses on UI, not content)</p> <p>Implementation Tips: - Binary feedback (thumbs up/down) gets 3x more responses than 5-star ratings - For enterprise: Post feedback to Slack channels for transparency - Add optional text field only after binary feedback is given - Track feedback rates by query type to identify problem areas</p> <p>When implementing feedback mechanisms, remember that every metric you track should lead to a specific follow-up action\u2014it's not just about knowing the number.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#the-system-vs-ad-hoc-approaches","title":"The System vs. Ad Hoc Approaches","text":"<p>A system is a structured approach to solving problems that guides how we think about and tackle challenges. For RAG applications, this includes:</p> <ul> <li>A framework for evaluating technologies</li> <li>A decision-making process for prioritizing development efforts</li> <li>A methodology for diagnosing and improving performance</li> <li>Standard metrics and benchmarks for measuring success</li> </ul> <p>The contrast between systematic and ad-hoc approaches is stark:</p> <pre><code>flowchart LR\n    A[Ad-hoc Approach] --&gt;|Leads to| B[Guesswork &amp; Anxiety]\n    C[Systematic Approach] --&gt;|Leads to| D[Confidence &amp; Progress]\n\n    subgraph \"Ad-hoc Results\"\n    B --&gt;|Results in| E[Inconsistent Outcomes]\n    B --&gt;|Results in| F[Resource Waste]\n    B --&gt;|Results in| G[Unclear Priorities]\n    end\n\n    subgraph \"Systematic Results\"\n    D --&gt;|Results in| H[Measurable Improvements]\n    D --&gt;|Results in| I[Efficient Resource Use]\n    D --&gt;|Results in| J[Clear Priorities]\n    end\n\n    style A fill:#f99,stroke:#333,stroke-width:2px\n    style C fill:#9f9,stroke:#333,stroke-width:2px</code></pre>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#the-cost-of-lacking-a-system","title":"The Cost of Lacking a System","text":"<p>Without a systematic approach, teams face significant challenges:</p> Common Challenge Without a System With a System \"Make the AI better\" Anxiety and guesswork Clear metrics and priority areas Allocating engineering resources Political decisions Data-driven prioritization Evaluating improvement ideas Subjective opinions Objective measurement Communicating progress Vague assertions Concrete metrics and examples Addressing user complaints Reactive firefighting Proactive improvement <p>Having a system frees up mental energy for innovation and problem-solving by creating clarity around what's working, what isn't, and what to focus on next.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#from-engineer-to-product-thinker-the-mindset-shift","title":"From Engineer to Product Thinker: The Mindset Shift","text":"<p>To fully embrace the product mindset for RAG, you need to expand your thinking beyond technical implementation details. Here's what this mindset shift looks like in practice:</p> Technical Implementation Focus Product Development Focus \"Which embedding model has the best performance?\" \"Which embedding approach best solves our users' problems?\" \"How do we implement vector search?\" \"How do we discover which search features matter most to users?\" \"What's the optimal chunk size?\" \"How do we measure whether our chunking approach is working for users?\" \"How do we reduce hallucinations?\" \"How do we build user trust through transparent, accurate responses?\" \"Which model has the best capabilities?\" \"Which capabilities deliver the most value for our use case?\" <p>This shift doesn't mean abandoning technical rigor\u2014quite the opposite. It means applying that rigor to problems that actually matter to your users, guided by data rather than assumptions.</p> <p>Real-World Case: The Restaurant Voice AI Revolution</p> <p>A restaurant chain implemented voice AI for taking orders, initially focusing on speech recognition accuracy. But when they shifted to a product mindset, they discovered something surprising:</p> <p>Instead of perfecting order-taking, they analyzed actual conversations and found that 30% of callers asked \"What's good here?\" By implementing a simple upselling feature that recommended popular items, they generated 9% more revenue\u2014without improving the core AI technology at all.</p> <p>The lesson? Sometimes the biggest wins come from understanding what users actually need, not from technical improvements.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#case-study-the-improvement-flywheel-in-action","title":"Case Study: The Improvement Flywheel in Action","text":"<p>To illustrate the power of this approach, let's look at a real example from my consulting work with a legal technology company:</p> <ol> <li> <p>Initial Implementation: The company built a RAG system to help lawyers search through case law and legal documents. The initial implementation used standard embeddings and chunking, with middling performance.</p> </li> <li> <p>Synthetic Evaluation: We created a dataset of 200 synthetic legal queries with gold-standard answers derived from their knowledge base, establishing baseline metrics showing only 63% retrieval accuracy.</p> </li> <li> <p>Hypothesis Testing: Testing different chunking strategies and embedding models revealed that legal terminology required specialized approaches, boosting performance to 72%.</p> </li> <li> <p>Deployment and Feedback: We implemented explicit feedback buttons and implicit tracking (time spent, copy actions), collecting data on 5,000+ real queries over two months.</p> </li> <li> <p>Pattern Analysis: Topic modeling revealed distinct query categories (case citation, legal definition, procedural question) with varying performance.</p> </li> <li> <p>Targeted Improvements: We built specialized retrievers for each category (citation parser, definition extractor, procedure classifier), pushing overall performance to 87%.</p> </li> <li> <p>Monitoring and Refinement: Continuous tracking showed which query types were growing in popularity and which still needed improvement, guiding ongoing development.</p> </li> </ol> <p>The result wasn't just better technical performance\u2014it was significantly higher user adoption, reduced time spent on research, and ultimately, better legal outcomes for clients.</p> <p>Key Insight: Inventory vs Capabilities</p> <p>When diagnosing RAG performance issues, always ask: \"Is this an inventory problem or a capabilities problem?\"</p> <p>Inventory Problems: - Missing documents or data - Incomplete knowledge base - Outdated information - Solution: Add more/better content</p> <p>Capabilities Problems: - Poor retrieval for existing content - Inability to understand query intent - Wrong type of search for the task - Solution: Improve retrieval/processing</p> <p>This framework helps you avoid wasting time improving retrieval when you simply don't have the right content, or adding more content when your retrieval is the bottleneck.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#who-this-book-is-for","title":"Who This Book Is For","text":"<p>This book is designed for a diverse range of practitioners involved in building AI applications. Based on my experiences teaching these concepts, I've found that the audience typically includes:</p> <ul> <li>Technical Leaders (30%): CTOs, founders, and technical directors who need to make strategic decisions about AI implementation and improvement</li> <li>Senior Engineers (20%): Experienced developers responsible for designing and implementing RAG systems</li> <li>Cross-Functional Teams (50%): A mix of software engineers, data scientists, product managers, solution engineers, and consultants who collaborate on AI products</li> </ul> <p>Organizations of all sizes benefit from this approach\u2014from startups to technology giants like Amazon, Google, Microsoft, OpenAI, and Anthropic. The systematic framework is particularly valuable when multiple stakeholders need to align on how to improve AI applications and allocate resources effectively.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#what-youll-learn-in-this-book","title":"What You'll Learn in This Book","text":"<p>Throughout this book, I'll guide you through implementing every aspect of the improvement flywheel, with practical examples and code you can adapt to your own projects. The goal is to replace guesswork with structured, measurable approaches that free up mental energy for innovation.</p> <p>Here's what we'll cover in the upcoming chapters:</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#chapter-1-starting-the-flywheel-with-data","title":"Chapter 1: Starting the Flywheel with Data","text":"<p>Learn how to overcome the cold-start problem through synthetic data generation, establish meaningful metrics that align with business goals, and create a foundation for data-driven improvement.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#chapter-2-from-evaluation-to-product-enhancement","title":"Chapter 2: From Evaluation to Product Enhancement","text":"<p>Discover how to transform evaluation insights into concrete product improvements through fine-tuning, re-ranking, and targeted capability development.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#chapter-3-the-user-experience-of-ai","title":"Chapter 3: The User Experience of AI","text":"<p>Explore how to design interfaces that both delight users and gather valuable feedback, creating the virtuous cycle at the heart of the improvement flywheel.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#chapter-4-understanding-your-users","title":"Chapter 4: Understanding Your Users","text":"<p>Learn techniques for segmenting users and queries to identify high-value opportunities and create prioritized improvement roadmaps.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#chapter-5-building-specialized-capabilities","title":"Chapter 5: Building Specialized Capabilities","text":"<p>Develop purpose-built solutions for different user needs, spanning documents, images, tables, and structured data.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#chapter-6-unified-product-architecture","title":"Chapter 6: Unified Product Architecture","text":"<p>Create a cohesive product experience that intelligently routes to specialized components while maintaining a seamless user experience.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#what-success-looks-like","title":"What Success Looks Like","text":"<p>When you successfully adopt the systematic approach outlined in this book, you'll experience several tangible benefits:</p> <ul> <li>Reduced Anxiety in Team Discussions: No more dread when stakeholders say \"just make the AI better\"\u2014you'll have clear paths forward</li> <li>Data-Informed Decision Making: Less guesswork and more confidence in prioritization decisions</li> <li>Resource Optimization: Ability to identify high-impact tasks and make informed tradeoffs about time investment</li> <li>Metrics That Matter: Selection of relevant measurements that correlate with business outcomes</li> <li>Demonstrated Business Value: Improved user satisfaction, retention, and increased system usage</li> </ul> <p>The difference between teams with and without a systematic approach is stark. Those without systems often spend meetings debating subjective opinions about what might work, while teams with systems focus discussions on objective data and strategic improvements.</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter0/#reflection-questions","title":"Reflection Questions","text":"<p>As you prepare for the next chapter, consider these questions about your current approach to RAG:</p> <ol> <li>Are you treating your RAG implementation as a completed project or an evolving product?</li> <li>What mechanisms do you have in place to learn from user interactions?</li> <li>How do you currently measure the success of your RAG application?</li> <li>What processes do you have for prioritizing improvements?</li> <li>How would your approach change if you viewed RAG as a recommendation engine rather than a pipeline?</li> <li>How much time does your team currently spend debating what might work versus testing hypotheses?</li> <li>Do you have a framework for allocating resources to different improvement opportunities?</li> </ol> <p>By shifting from an implementation mindset to a product mindset, you'll move from building RAG systems that work once to creating AI products that continuously improve and deliver increasing value over time.</p> <p>In the next chapter, we'll take the first concrete step in the improvement flywheel: creating synthetic evaluation data to establish a foundation for systematic enhancement.</p> <p>Author Note: This approach has been refined through work with organizations ranging from startups to Fortune 500 companies across diverse domains including legal, financial, healthcare, and e-commerce. While the technical details vary by domain, the fundamental principles of the improvement flywheel remain consistent\u2014focus on users, measure what matters, and systematically enhance based on data rather than assumptions.</p> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>","tags":["product thinking","systems","RAG","fundamentals","improvement"]},{"location":"workshops/chapter1/","title":"Kickstarting the Data Flywheel with Synthetic Data","text":"<p>Chapter Overview</p> <pre><code>This chapter focuses on starting the improvement flywheel by establishing proper evaluation frameworks:\n\n- Understanding common pitfalls in AI development\n- Distinguishing between different types of metrics\n- Creating synthetic data for evaluation\n- Building frameworks to drive systematic improvements\n- Measuring retrieval quality with precision and recall\n- Implementing practical evaluation pipelines\n</code></pre>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#introduction","title":"Introduction","text":"<p>Welcome to the first practical chapter on systematically improving RAG applications. This week is all about giving you the tools to kickstart the data flywheel\u2014focusing on evaluations, understanding common mistakes, and using synthetic data to address concerns before you even have users.</p> <p>The main challenge many teams face is knowing where to start when improving RAG applications. Without proper evaluation frameworks, teams often fall into a cycle of making random changes based on intuition rather than data. This chapter aims to break that cycle by establishing data-driven methods for improvement.</p> <p>Key Philosophy</p> <p>\"You can't improve what you don't measure. The goal is not to chase the latest AI techniques blindly, but to establish a flywheel of continuous improvement driven by clear metrics aligned with user outcomes.\"</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#common-pitfalls-in-ai-development","title":"Common Pitfalls in AI Development","text":"<p>When I work with companies building RAG applications, I consistently see the same vicious cycle play out. Let's explore these common pitfalls to ensure you don't fall into the same traps.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#the-reasoning-fallacy","title":"The Reasoning Fallacy","text":"<p>How often do we hear statements like \"we need more complex reasoning\" or \"the model isn't smart enough\"? In my experience, this is rarely the actual problem. Instead, it usually indicates a fundamental lack of user empathy and specificity in the tools we build.</p> <p>Common Mistake</p> <p>When you hear statements like \"we need more reasoning power,\" challenge yourself to answer:</p> <pre><code>- When was the last time we looked at data from customers?\n- When did we last read user feedback?\n- Are we actively asking for feedback?\n</code></pre> <p>The root issue is rarely that we need more complex reasoning\u2014it's that we don't understand what our users actually want. This leads to building generic tools that don't solve specific problems well.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#the-vague-metrics-problem","title":"The Vague Metrics Problem","text":"<p>Many developers unknowingly sabotage their applications by defining vague metrics. Thinking in terms like \"does it look better?\" or \"does it feel right?\" makes it impossible to measure progress objectively.</p> <p>You would be surprised how pervasive this problem is. I've worked with companies valued at $100 million that maintain fewer than 30 evaluation examples. When something changes in their system, they have no way of understanding what actually moved the needle.</p> <p>Without clear metrics, teams end up making random changes, seeing unclear results, becoming frustrated, and continuing the cycle of vagueness. Breaking this cycle requires establishing concrete, measurable objectives.</p> <pre><code>flowchart TD\n    A[Random Changes Based on Intuition] --&gt;|leads to| B[Unclear Results]\n    B --&gt;|creates| C[Frustration &amp; Uncertainty]\n    C --&gt;|triggers more| A\n    D[Concrete Metrics &amp; Evaluation Framework] --&gt;|breaks| A\n    style D fill:#90EE90,stroke:#006400,stroke-width:2px</code></pre>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#building-generic-solutions","title":"Building Generic Solutions","text":"<p>Another common mistake is building very generic solutions for broad problems instead of specific work that delivers economic value. This means focusing on features rather than outcomes.</p> <p>This typically happens because the mandate is too broad or the team has overpromised. The result? A generic tool with 30-40% churn rates that teams are too scared to fully launch because users might lose interest.</p> <p>The solution is clear: try to be world-class in just a few narrow domains and earn complexity as you discover what your customers truly care about. Start specific, then expand gradually based on validated user needs.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#leading-versus-lagging-metrics","title":"Leading versus Lagging Metrics","text":"<p>Understanding the distinction between leading and lagging metrics can transform your approach to improvement. This concept was profound to me when I worked at Facebook, and it applies perfectly to RAG applications. Shifting your focus from outcomes to inputs is often the most significant mindset change for successful RAG development.</p> <pre><code>flowchart TD\n    subgraph \"Leading Metrics&lt;br&gt;(Controllable Inputs)\"\n        A1[Number of Experiments Run]\n        A2[Retrieval Precision &amp; Recall]\n        A3[User Feedback Collection Rate]\n        A4[Evaluation Coverage]\n    end\n\n    subgraph \"Lagging Metrics&lt;br&gt;(Business Outcomes)\"\n        B1[User Satisfaction]\n        B2[Application Quality]\n        B3[Churn Rate]\n        B4[Revenue]\n    end\n\n    A1 --&gt;|\"predicts &amp; influences\"| B1\n    A1 --&gt;|\"predicts &amp; influences\"| B2\n    A2 --&gt;|\"predicts &amp; influences\"| B1\n    A2 --&gt;|\"predicts &amp; influences\"| B2\n    A3 --&gt;|\"predicts &amp; influences\"| B1\n    A3 --&gt;|\"predicts &amp; influences\"| B3\n    A4 --&gt;|\"predicts &amp; influences\"| B2\n    A4 --&gt;|\"predicts &amp; influences\"| B3\n\n    B1 --&gt;|\"eventually impacts\"| B3\n    B1 --&gt;|\"eventually impacts\"| B4\n    B2 --&gt;|\"eventually impacts\"| B1\n    B3 --&gt;|\"eventually impacts\"| B4\n\n    classDef leading fill:#D4F1F9,stroke:#05445E\n    classDef lagging fill:#FFD700,stroke:#B8860B\n\n    class A1,A2,A3,A4 leading\n    class B1,B2,B3,B4 lagging</code></pre>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#lagging-metrics","title":"Lagging Metrics","text":"<p>Lagging metrics are outcomes that are difficult to improve directly but easy to measure. They're measurements of past results, often unresponsive to immediate changes, and represent the outputs of your system.</p> <p>Examples of lagging metrics include:</p> <ul> <li>Application quality</li> <li>User satisfaction</li> <li>Churn rates</li> <li>Revenue</li> </ul> <p>Think of lagging metrics like your body weight or strength. You can easily measure how much you weigh or how much you can lift, but you can't directly change these factors in the short term.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#leading-metrics","title":"Leading Metrics","text":"<p>Leading metrics, by contrast, are factors you can easily change but might be harder to measure perfectly. They predict future performance and provide feedback on when and where to intervene.</p> <p>Examples of leading metrics include:</p> <ul> <li>Number of experiments run per week</li> <li>Evaluation coverage of different question types</li> <li>Retrieval precision and recall</li> <li>User feedback collection rate</li> </ul>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#the-calories-in-calories-out-analogy","title":"The Calories In, Calories Out Analogy","text":"<p>Let me offer a simple but powerful analogy from physical fitness that perfectly illustrates the relationship between leading and lagging metrics:</p> <p>If your goal is weight management (a lagging metric), the most reliable path is focusing on calories consumed and calories burned (leading metrics). At any moment, you can count how many calories you've eaten and how many you've expended through activity. If you want to gain weight, ensure you've eaten enough; if you want to lose weight, ensure you've created a caloric deficit.</p> <p>While this is admittedly reductive, the principle is profound: focus on the inputs you can control (calories), and the outputs you care about (weight) will follow predictably. The obsessive tracking of weight (the lagging metric) provides little actionable information compared to tracking calories (the leading metric).</p> <p>In physical fitness, leading metrics would be calories consumed, workout frequency, and sleep quality. These factors are within your direct control and predictive of future outcomes.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#the-1-leading-metric-experiment-velocity","title":"The #1 Leading Metric: Experiment Velocity","text":"<p>The most important leading metric for early-stage RAG applications is simply the number of experiments run. Just like counting calories is more effective for weight management than weighing yourself constantly, tracking experiment velocity is more useful than obsessing over end-user metrics that change slowly.</p> <p>My recommendation? Completely restructure how you think about team stand-ups. Instead of focusing primarily on outcomes, count and increase the number of experiments you can run. If your team feels lost or unsure what to do next, the answer is simple: run more experiments.</p> <p>During team meetings, shift focus from asking about the outcomes of experiments to increasing the number of experiments you can run. What infrastructure investments would improve that velocity? How can you brainstorm new ideas to design better experiments? This mindset shift makes all the difference.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#absence-blindness-and-intervention-bias","title":"Absence Blindness and Intervention Bias","text":"<p>Two cognitive biases frequently derail RAG improvement efforts: absence blindness and intervention bias.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#absence-blindness","title":"Absence Blindness","text":"<p>Absence blindness is simple: you don't fix what you can't see. In RAG applications, I see this daily with almost every client. Teams focus obsessively on generation quality and latency\u2014the visible parts of the system\u2014while neglecting to check whether retrieval is even working correctly.</p> <p>I've seen teams spend weeks fine-tuning prompts to improve the quality of generated text, only to discover that their retrieval system wasn't functioning properly at all. No amount of prompt engineering can compensate for a system that retrieves irrelevant documents or misses critical information.</p> <p>Key questions almost no one asks until I prompt them:</p> <ul> <li>Have you verified whether your retrieval is bringing back the right documents?</li> <li>Are your text chunks properly segmented?</li> <li>Is your data extraction pipeline working as expected?</li> <li>Do you have metrics for retrieval separate from generation?</li> </ul> <pre><code>graph LR\n    classDef visible fill:#FF9999,stroke:#CC0000\n    classDef invisible fill:#CCCCCC,stroke:#666666,stroke-dasharray: 5 5\n\n    subgraph \"RAG System Components\"\n        A[Data Extraction]:::invisible\n        B[Text Chunking]:::invisible\n        C[Vector Embedding]:::invisible\n        D[Retrieval]:::invisible\n        E[Generation]:::visible\n        F[UI/UX]:::visible\n        G[Response Time]:::visible\n\n        A --&gt; B --&gt; C --&gt; D --&gt; E --&gt; F\n        E -.-&gt; G\n    end\n\n    subgraph \"Team Focus\"\n        H[\"\u2705 High Attention&lt;br&gt;(Visible Components)\"]:::visible\n        I[\"\u274c Low Attention&lt;br&gt;(Invisible Components)\"]:::invisible\n    end</code></pre> <p>Have you verified whether your retrieval brings back the right documents? Are your text chunks properly segmented? Is your data extraction pipeline working as expected? These invisible components often cause the most significant issues but receive the least attention.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#intervention-bias","title":"Intervention Bias","text":"<p>Intervention bias is our tendency to take action just to feel in control, regardless of whether that action is effective. In RAG applications, this manifests as constantly switching between models, adding prompt tweaks, or implementing new features without measuring their impact.</p> <p>In my consulting practice, I frequently get emails asking: \"Should we use GPT-4 or Claude?\" or \"Will adding this prompt technique fix our issues?\" The honest answer is always the same: it depends on your data, your evaluations, and your benchmarks. There are no universal answers.</p> <p>Many teams make changes to feel like they're making progress rather than taking specific interventions against specific metrics and testing clear hypotheses. I see this pattern repeatedly:</p> <ol> <li>Team faces RAG performance issues</li> <li>Someone suggests a new model, chunking approach, or prompt technique</li> <li>Quick implementation without proper testing</li> <li>Subjective assessment: \"It feels better\"</li> <li>New problems emerge, cycle repeats</li> </ol> <p>While it feels good to be taking action, randomized changes create technical debt and confuse attribution of improvements. Each change should target a specific metric and test a clear hypothesis.</p> <p>The solution is to establish evaluations that enable methodical improvement\u2014evaluations that reflect and correlate with business outcomes you care about.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#the-rag-flywheel-and-retrieval-evaluations","title":"The RAG Flywheel and Retrieval Evaluations","text":"<p>The basic principle of improving RAG applications is that everything we've learned in search is incredibly relevant to retrieval. If you already have a basic RAG setup, the next step is to bring in synthetic questions that test your system's retrieval capabilities.</p> <pre><code>flowchart TD\n    A[\"1. Evaluate Retrieval&lt;br&gt;with Synthetic Data\"] --&gt;|\"identify gaps\"| B[\"2. Implement&lt;br&gt;Targeted Improvements\"]\n    B --&gt;|\"measure impact\"| C[\"3. Observe&lt;br&gt;Performance Changes\"]\n    C --&gt;|\"collect real data\"| D[\"4. Gather User&lt;br&gt;Behavior &amp; Feedback\"]\n    D --&gt;|\"refine evaluation set\"| A\n\n    style A fill:#FFD700,stroke:#B8860B,stroke-width:2px\n    style B fill:#98FB98,stroke:#006400,stroke-width:2px\n    style C fill:#ADD8E6,stroke:#0000A0,stroke-width:2px\n    style D fill:#FFA07A,stroke:#A52A2A,stroke-width:2px\n\n    %% Arrow styling to create flywheel effect\n    linkStyle 0,1,2,3 stroke-width:2px</code></pre>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#why-prioritize-retrieval-evaluations","title":"Why Prioritize Retrieval Evaluations","text":"<p>Many teams without machine learning backgrounds focus on subjective generation evaluations too early. While language models will continue improving at synthesis, it's our responsibility to improve search and retrieval.</p> <p>Retrieval evaluations offer several advantages over generation evaluations:</p> <ol> <li>Speed: Retrieval tests take milliseconds versus seconds or minutes for generation tests</li> <li>Cost: They're significantly cheaper to run</li> <li>Objectivity: Results are clear and unambiguous</li> <li>Scalability: You can run thousands of tests quickly</li> </ol> <p>When you focus on generation evaluations prematurely, factuality becomes subjective and confusing. As you adopt evaluation tools, you get flooded with information without clear actions. By contrast, improving precision and recall directly allows you to test whether lexical search, semantic search, or re-rankers improve your retrieval in a measurable way.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#understanding-precision-and-recall","title":"Understanding Precision and Recall","text":"<p>Before diving into implementation, let's clarify what precision and recall actually mean in the context of RAG:</p> <p>Practical Guidance from Production Systems</p> <p>Testing Different K Values: - Start with K=10 for initial testing - Test K=3, 5, 10, 20 to understand your precision/recall tradeoffs - Higher K improves recall but may hurt precision with simpler models - Advanced models (GPT-4, Claude) handle irrelevant docs better, so favor higher K</p> <p>Why Embedding or Re-ranker Score Thresholds Are Dangerous: - Score distributions vary dramatically by query type - A threshold that works for one category fails for others, for example average ada-002 score is .7 and .5 for ada-003 - Better approach: Always return top K results, let the LLM filter - If you must filter, use percentile-based thresholds, not absolute scores</p> <pre><code>graph TD\n    subgraph \"Document Universe\"\n        subgraph \"All Relevant Documents\"\n            A[\"Relevant &amp;&lt;br&gt;Retrieved&lt;br&gt;(True Positives)\"]\n            B[\"Relevant but&lt;br&gt;Not Retrieved&lt;br&gt;(False Negatives)\"]\n        end\n\n        subgraph \"All Retrieved Documents\"\n            A\n            C[\"Retrieved but&lt;br&gt;Not Relevant&lt;br&gt;(False Positives)\"]\n        end\n\n        D[\"Not Relevant &amp;&lt;br&gt;Not Retrieved&lt;br&gt;(True Negatives)\"]\n    end\n\n    P[\"Precision = &lt;br&gt;Relevant &amp; Retrieved&lt;br&gt;All Retrieved\"]\n    R[\"Recall = &lt;br&gt;Relevant &amp; Retrieved&lt;br&gt;All Relevant\"]\n\n    A ~~~ P\n    A ~~~ R\n\n    classDef relevant fill:#90EE90,stroke:#006400\n    classDef retrieved fill:#ADD8E6,stroke:#00008B\n    classDef both fill:#9370DB,stroke:#4B0082\n    classDef neither fill:#DCDCDC,stroke:#696969\n    classDef formula fill:#FFFACD,stroke:#8B8B00,stroke-width:2px\n\n    class A both\n    class B relevant\n    class C retrieved\n    class D neither\n    class P,R formula</code></pre> <p>Recall is the percentage of relevant documents that are successfully retrieved. If there are 10 correct documents and your system finds 4 of them, recall is 40%. High recall means your system finds most of the relevant documents, which is crucial when facts are distributed across multiple documents.</p> <p>Precision is the percentage of retrieved documents that are relevant. If your system returns 10 results but only 2 are relevant, precision is 20%. High precision means most retrieved documents are relevant.</p> <p>With advanced models, recall tends to be more important than precision, as these models can better ignore irrelevant information. With simpler models, precision becomes more critical because irrelevant information can cause confusion.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#case-studies-real-world-improvements","title":"Case Studies: Real-World Improvements","text":"<p>Let's examine two case studies where focusing on retrieval metrics led to rapid improvements.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#case-study-1-report-generation-from-expert-interviews","title":"Case Study 1: Report Generation from Expert Interviews","text":"<p>I worked with a company that generates reports from user research interviews. Consultants conduct 15-30 interviews with experts and request AI-generated reports summarizing key findings.</p> <p>Problem: Customers noticed that only a subset of relevant quotes were included in reports. In one case, a consultant knew that 6 experts had expressed similar opinions, but the report only cited 3 of them. This 50% recall rate damaged trust in the system.</p> <p>Approach: We set a goal to dramatically improve recall for these specific use cases. We manually built question-chunk relevance datasets by examining problematic examples and discovered that most issues could be addressed through better pre-processing.</p> <p>Result: By experimenting with text chunking methods before ingestion, we improved recall from 50% to 90% in just a few iterations. This improvement restored customer confidence and established a workflow for continuous enhancement.</p> <p>Key Takeaway: Text pre-processing that aligns with anticipated user queries can dramatically improve retrieval performance.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#case-study-2-blueprint-search-for-construction","title":"Case Study 2: Blueprint Search for Construction","text":"<p>Another client needed to implement AI search for construction blueprints, allowing workers to ask questions about building plans.</p> <p>Problem: Initial tests showed only 27% recall when trying to find the correct blueprint image for specific questions.</p> <p>Approach: We hypothesized that better image captions would improve retrieval. We implemented a visual language model to create detailed captions for blueprints, using chain-of-thought prompting to reason about the images and generate hypothetical questions users might ask.</p> <p>Result: In just four days of experimentation, recall improved from 27% to 85%. This allowed us to launch the feature and collect real user data, which revealed that 20% of queries involved counting objects in blueprints. This insight justified investing in bounding box models to count rooms and features automatically.</p> <p>Key Takeaway: Testing specific subsystems independently enables rapid baseline improvements. Synthetic data generation for specific use cases can dramatically improve retrieval.</p> <p>Chunk Size Best Practices</p> <p>Based on extensive testing across multiple domains and recommendations from Anthropic and OpenAI:</p> <p>Starting Point: 800 tokens with 50% overlap - This configuration works well for most use cases - Provides good context while maintaining relevance - Overlap ensures important information isn't split</p> <p>Why Chunk Optimization Rarely Provides Big Wins: - Changing chunk size typically yields &lt;10% improvements - Time is better spent on:     - Query understanding and expansion     - Metadata filtering     - Contextual retrieval (adding document context to chunks)     - Better embedding models</p> <p>When to Adjust Chunks: - Legal/regulatory: Larger chunks (1500-2000 tokens) to preserve full clauses - Technical docs: Smaller chunks (400-600 tokens) for precise retrieval - Conversational: Page-level chunks to maintain context</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#practical-implementation-building-your-evaluation-framework","title":"Practical Implementation: Building Your Evaluation Framework","text":"<p>With synthetic data in hand, you can build a proper evaluation framework. This framework should:</p> <ol> <li>Run questions through your retrieval system</li> <li>Compare retrieved chunks to ground truth</li> <li>Calculate precision and recall metrics</li> <li>Track performance over time as you make changes</li> </ol> <p>This framework becomes the foundation of your improvement flywheel. Every change to your system\u2014whether a new embedding model, chunking strategy, or ranking approach\u2014should be evaluated against these metrics.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#building-a-practical-evaluation-pipeline","title":"Building a Practical Evaluation Pipeline","text":"<p>Let's walk through building a simple but effective evaluation pipeline:</p> <pre><code>def evaluate_retrieval(evaluation_data, retriever_fn, k=10):\n    \"\"\"\n    Evaluate retrieval performance on a dataset.\n\n    Args:\n        evaluation_data: List of dicts with 'question' and 'relevant_docs' keys\n        retriever_fn: Function that takes question text and returns docs\n        k: Number of top results to consider\n\n    Returns:\n        Dict containing evaluation metrics\n    \"\"\"\n    results = []\n\n    for item in evaluation_data:\n        question = item['question']\n        ground_truth = set(item['relevant_docs'])\n\n        # Call retrieval system\n        retrieved_docs = retriever_fn(question, top_k=k)\n        retrieved_ids = [doc['id'] for doc in retrieved_docs]\n\n        # Calculate metrics\n        retrieved_relevant = set(retrieved_ids) &amp; ground_truth\n        precision = len(retrieved_relevant) / len(retrieved_ids) if retrieved_ids else 0\n        recall = len(retrieved_relevant) / len(ground_truth) if ground_truth else 1.0\n\n        # Store individual result\n        results.append({\n            'question_id': item.get('id', ''),\n            'question': question,\n            'precision': precision,\n            'recall': recall,\n            'retrieved_docs': retrieved_ids,\n            'relevant_docs': list(ground_truth),\n            'metadata': item.get('metadata', {})\n        })\n\n    # Aggregate metrics\n    avg_precision = sum(r['precision'] for r in results) / len(results)\n    avg_recall = sum(r['recall'] for r in results) / len(results)\n\n    return {\n        'avg_precision': avg_precision,\n        'avg_recall': avg_recall,\n        'detailed_results': results\n    }\n</code></pre>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#running-regular-evaluations","title":"Running Regular Evaluations","text":"<p>Make evaluation a regular part of your development cycle:</p> <ol> <li>Continuous testing: Run evaluations with every significant change to your system</li> <li>Weekly benchmarks: Schedule comprehensive evaluations weekly</li> <li>Version comparison: Always compare new changes against previous versions</li> <li>Failure analysis: Regularly review cases with 0% recall to identify patterns</li> <li>Difficulty progression: As scores improve, add more challenging test cases</li> </ol> <p>Production Monitoring Insights</p> <p>Track These Metrics Over Time: - Average cosine distance between queries and retrieved documents - Percentage of queries with no results above threshold - Distribution of retrieval scores (watch for bimodal distributions)</p> <p>Segment Analysis by User Variables: - New vs returning users often have different query patterns - Technical vs non-technical users may need different retrieval strategies - Time-based patterns (e.g., queries during product launches)</p> <p>Real Example: A company noticed cosine distances spiked during their Super Bowl ad campaign. New users asked different questions than existing users, revealing gaps in their content coverage. This led to creating onboarding-specific content that improved new user retention by 25%.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#integrating-with-development-workflow","title":"Integrating with Development Workflow","text":"<p>To maximize the value of your evaluation framework:</p> <ol> <li>Build a dashboard: Create a simple interface showing metrics over time</li> <li>Implement automatic testing: Run evaluations as part of your CI/CD pipeline</li> <li>Set alert thresholds: Get notified if metrics drop below critical levels</li> <li>Document all experiments: Keep track of changes and their impact on metrics</li> <li>Tie to business metrics: Connect retrieval metrics to business outcomes</li> </ol> <p>Remember that your evaluation framework should evolve with your application. Start simple and add complexity as you gain more insights and collect more data.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#creating-synthetic-data-for-evaluation","title":"Creating Synthetic Data for Evaluation","text":"<p>If you're starting a RAG application without user data, synthetic data generation is your best approach to kickstart improvement. While everyone talks about synthetic data, it's not as simple as asking an LLM for more data. You need to make LLMs create diverse, realistic datasets that reflect potential production traffic.</p> <pre><code>flowchart TD\n    A[\"Document Corpus\"] --&gt; B[\"Sample Representative&lt;br&gt;Text Chunks\"]\n    B --&gt; C[\"LLM-Powered&lt;br&gt;Question Generation\"]\n\n    C --&gt; D1[\"Factual&lt;br&gt;Questions\"]\n    C --&gt; D2[\"Inferential&lt;br&gt;Questions\"]\n    C --&gt; D3[\"Comparative&lt;br&gt;Questions\"]\n    C --&gt; D4[\"Hypothetical&lt;br&gt;Questions\"]\n\n    D1 &amp; D2 &amp; D3 &amp; D4 --&gt; E[\"Verify Retrievability&lt;br&gt;with Current System\"]\n\n    E --&gt; F1[\"Easy Questions&lt;br&gt;(80-90% expected recall)\"]\n    E --&gt; F2[\"Medium Questions&lt;br&gt;(50-70% expected recall)\"]\n    E --&gt; F3[\"Hard Questions&lt;br&gt;(30-50% expected recall)\"]\n\n    F1 &amp; F2 &amp; F3 --&gt; G[\"Comprehensive&lt;br&gt;Evaluation Dataset\"]\n\n    H[\"Real User&lt;br&gt;Questions\"] -.-&gt;|\"As they become&lt;br&gt;available\"| G\n\n    style A fill:#CCCCFF,stroke:#0000AA\n    style C fill:#FFD700,stroke:#B8860B\n    style E fill:#90EE90,stroke:#006400\n    style G fill:#FF7F50,stroke:#A52A2A\n    style H fill:#98FB98,stroke:#006400,stroke-dasharray: 5 5</code></pre>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#basic-approach-question-generation","title":"Basic Approach: Question Generation","text":"<p>The simplest approach is:</p> <ol> <li>Take a random text chunk from your corpus</li> <li>Ask a language model to generate a question that this text chunk would answer</li> <li>Verify that when you search with this question, the original text chunk is retrieved</li> </ol> <p>This creates a basic synthetic dataset to test how well your system can find the right information. Recall becomes a binary metric: either you find the source chunk or you don't.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#generating-diverse-synthetic-data","title":"Generating Diverse Synthetic Data","text":"<p>To make your synthetic data truly valuable, you need diversity. Here are several approaches to ensure comprehensive coverage:</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#1-variation-in-question-types","title":"1. Variation in Question Types","text":"<p>Create questions that test different retrieval capabilities:</p> <ul> <li>Factual questions: Direct questions with answers explicitly stated in the text</li> <li>Inferential questions: Questions requiring connecting multiple pieces of information</li> <li>Comparative questions: Queries that involve comparing different entities or concepts</li> <li>Hypothetical questions: \"What if\" scenarios related to the content</li> <li>Clarification questions: Queries asking for elaboration on specific topics</li> </ul>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#2-linguistic-diversity-techniques","title":"2. Linguistic Diversity Techniques","text":"<p>Vary how questions are phrased:</p> <ul> <li>Paraphrasing: Generate multiple phrasings of the same question</li> <li>Terminology variation: Use synonyms and domain-specific language</li> <li>Complexity levels: Mix simple direct questions with complex compound queries</li> <li>Query length: Include both short keyword-style and verbose natural language questions</li> <li>Formatting diversity: Questions, commands, statements that imply questions</li> </ul>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#3-chain-of-thought-generation","title":"3. Chain-of-Thought Generation","text":"<p>Use chain-of-thought prompting to create more nuanced questions:</p> <pre><code>Given this text chunk:\n[CHUNK]\n\nFirst, identify 3-5 key facts or concepts in this text.\nFor each key concept:\n1. Think about different ways someone might ask about it\n2. Consider various levels of prior knowledge the asker might have\n3. Imagine different contexts in which this information might be relevant\n\nNow, generate 5 diverse questions about this text that:\n- Vary in complexity and format\n- Would be asked by users with different backgrounds\n- Target different aspects of the information\n- Require different types of retrieval capabilities to answer correctly\n\nFor each question, explain your reasoning about why this is a realistic user question.\n</code></pre>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#4-few-shot-prompting-for-domain-specificity","title":"4. Few-Shot Prompting for Domain Specificity","text":"<p>If you have even a handful of real user questions, use them as examples:</p> <pre><code>I'm creating questions that users might ask about [DOMAIN].\nHere are some examples of real questions:\n\n1. [REAL QUESTION 1]\n2. [REAL QUESTION 2]\n3. [REAL QUESTION 3]\n\nHere's a text passage:\n[CHUNK]\n\nPlease generate 5 new questions similar in style and intent to the examples above,\nthat would be answered by this text passage.\n</code></pre>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#5-adversarial-question-generation","title":"5. Adversarial Question Generation","text":"<p>Deliberately create challenging questions:</p> <pre><code>Given this text passage:\n[CHUNK]\n\nGenerate 3 challenging questions that:\n1. Use different terminology than what appears in the passage\n2. Require understanding the implications of the content\n3. Might confuse a basic keyword search system\n4. Would still be reasonable questions a user might ask\n\nFor each question, explain why it's challenging and what makes it a good test case.\n</code></pre>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#building-comprehensive-evaluation-sets","title":"Building Comprehensive Evaluation Sets","text":"<p>Once you're generating diverse questions, structure your evaluation sets strategically:</p> <ol> <li>Coverage mapping: Ensure questions cover all document types and topics</li> <li>Difficulty distribution: Include easy (80-90% expected recall), medium (50-70%), and hard (30-50%) questions</li> <li>Ground truth validation: Have subject matter experts verify questions and relevant documents</li> <li>Domain segmentation: Create separate evaluation sets for different use cases or domains</li> <li>Synthetic/real blending: As you collect real data, gradually blend it with synthetic data</li> </ol>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#using-synthetic-data-beyond-evaluation","title":"Using Synthetic Data Beyond Evaluation","text":"<p>Your synthetic data can serve multiple purposes:</p> <ol> <li>Retrieval benchmarking: Measuring search quality</li> <li>Few-shot examples: Enhancing your LLM's understanding in prompts</li> <li>Fine-tuning data: Training specialized embeddings or rerankers</li> <li>User experience testing: Simulating realistic user sessions</li> </ol> <p>By investing time in creating high-quality synthetic data upfront, you establish a foundation that accelerates every aspect of your RAG development process.</p> <p>Model Sensitivity Considerations</p> <p>Key Discovery: Models are often more sensitive to irrelevant information than we expect.</p> <p>Practical Implications: - Even GPT-4 and Claude can be distracted by marginally relevant content - Precision matters more than you might think, especially for:     - Multi-step reasoning tasks     - Numerical calculations     - Tasks requiring specific facts from specific documents</p> <p>Testing Approach: 1. Create test cases with varying amounts of irrelevant information 2. Measure how performance degrades as noise increases 3. Use this to set your precision/recall tradeoffs</p> <p>Example: One team found that including 5 irrelevant documents (even when marked as \"potentially less relevant\") reduced their financial calculation accuracy by 30%. They adjusted their retrieval to favor precision for numerical queries.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#additional-resources","title":"Additional Resources","text":"<p>!!! info \"Tools and Libraries for RAG Evaluation\" - RAGAS: Open-source framework for evaluating RAG applications - LangChain Evaluation: Tools for evaluating retrieval and generation - Prompttools: Toolkit for testing and evaluating LLM applications - MLflow for Experiment Tracking: Open-source platform for managing ML lifecycle</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#reflection-questions","title":"Reflection Questions","text":"<p>!!! question \"Self-Assessment\" 1. What are your leading and lagging metrics for your RAG application? How do they relate to each other?</p> <pre><code>2. How might you generate more diverse and challenging synthetic questions for your specific domain?\n\n3. Where does your current evaluation framework fall short, and what additional metrics might be valuable?\n\n4. What experiment could you run this week to test a hypothesis about improving retrieval quality?\n\n5. How will you incorporate real user feedback into your evaluation framework as it becomes available?\n</code></pre>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#conclusion-and-next-steps","title":"Conclusion and Next Steps","text":"<p>In this chapter, we've established the foundation for systematically improving your RAG application by focusing on proper evaluation frameworks. Rather than making subjective judgments or random changes, you now have the tools to measure progress objectively and make data-driven decisions.</p> <p>By prioritizing retrieval metrics like precision and recall, you can run more experiments in less time, at lower cost, and with greater confidence in the results. This approach sets the stage for incorporating user feedback and advanced fine-tuning techniques.</p> <p>What's Coming Next</p> <p>In Chapter 2, we'll explore how to convert evaluations into training data for fine-tuning, allowing you to create specialized models that better reflect your unique business needs.</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter1/#summary","title":"Summary","text":"<p>Remember these key principles as you move forward:</p> <ol> <li>Focus on leading metrics - Particularly experiment velocity, which you can control directly</li> <li>Do the obvious thing repeatedly - Success often looks boring, like counting calories in and calories out</li> <li>Combat absence blindness - Pay attention to retrieval quality, not just the visible parts like generation</li> <li>Avoid intervention bias - Make targeted changes based on specific hypotheses, not random tweaks</li> <li>Embrace empiricism - There are no universal answers that work for all use cases\u2014test everything</li> </ol> <p>The goal is not to chase the latest AI techniques blindly, but to establish a flywheel of continuous improvement driven by clear metrics aligned with user outcomes. Start with synthetic data, focus on retrieval before generation, and measure everything. These practices will serve as the foundation for all our subsequent improvements to your RAG application.</p> <p>As one of my clients discovered after implementing these principles: \"We spent three months trying to make our system better through prompt engineering and model switching. In just two weeks with proper evaluations, we made more progress than in all that time.\"</p> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>","tags":["evaluation","metrics","synthetic-data","data-flywheel"]},{"location":"workshops/chapter2/","title":"Converting Evaluations into Training Data for Fine-Tuning","text":"<p>Chapter Overview</p> <pre><code>This chapter explores how to transform evaluation data into valuable training assets:\n\n- Converting evaluation examples into few-shot prompts\n- Understanding the limitations of generic embeddings\n- Creating datasets for fine-tuning retrieval models\n- Learning how contrastive learning improves embeddings\n- Testing approaches systematically\n- Building a roadmap for continuous improvement\n</code></pre> <p>Key Insight</p> <p>If you're not fine-tuning, you're Blockbuster, not Netflix. The goal isn't to fine-tune language models (which are expensive and complex), but to fine-tune embedding models that move toward your specific data distributions and improve retrieval, not generation.</p> <p>Fine-Tuning Cost Reality Check</p> <p>Embedding Model Fine-Tuning: - Cost: ~$1.50 for 6,000 examples - Time: 40 minutes on a laptop - Infrastructure: Consumer GPU or cloud notebook - Improvement: 6-10% better performance</p> <p>Language Model Fine-Tuning: - Cost: $100-1000s depending on model size - Time: Hours to days - Infrastructure: Multiple GPUs or specialized services - Complexity: Requires ML expertise</p> <p>This dramatic difference explains why embedding fine-tuning should be your first focus.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#introduction","title":"Introduction","text":"<p>In the previous chapter, we established our evaluation framework and generated synthetic data to benchmark our RAG system. Now we take the crucial next step in our improvement journey: transforming these evaluations into practical training assets that can significantly boost performance.</p> <p>Prerequisites from Previous Chapters</p> <ul> <li>Chapter 0: Understanding the improvement flywheel concept</li> <li>Chapter 1: Creating evaluation datasets with synthetic data</li> </ul> <p>The evaluation examples from Chapter 1 become your training data in this chapter.</p> <p>This chapter bridges the gap between evaluation and production improvement, showing how the same datasets serve both purposes. The fundamental philosophy here is simple but powerful: the data you collect for evaluation should never go to waste. Every question, every relevance judgment, and every performance insight can\u2014and should\u2014be repurposed to train your system.</p> <p>Key Philosophy</p> <p>\"Every evaluation example is a potential training example. The data flywheel transforms what begins as a handful of evaluation examples into few-shot prompts, then into training datasets for fine-tuning embedding models and re-rankers.\"</p> <p>As we'll explore, this transformation process follows a natural progression. What begins as a handful of evaluation examples can evolve into few-shot prompts, then into training datasets for fine-tuning embedding models and re-rankers. This is the essence of the RAG improvement flywheel\u2014data collected for one purpose fuels improvements across your entire system.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#why-generic-embeddings-fall-short","title":"Why Generic Embeddings Fall Short","text":"<p>Before we dive into the transformation process, we need to understand a fundamental challenge in RAG applications: generic embeddings from third-party providers often fall short for specialized applications. This isn't because they're poorly designed\u2014in fact, models like OpenAI's embeddings are remarkably capable. The issue is that they're designed to serve every possible use case, which means they serve no specific use case perfectly.</p> <p>Limitation of Generic Models</p> <p>Generic embedding models inherit assumptions about what \"similarity\" means\u2014assumptions that may not align with your specific needs. They don't know:</p> <pre><code>1. The specific datasets they were trained on\n2. The objective function that defined success during training\n3. The weighting given to different types of similarity\n4. The trade-offs made to accommodate diverse use cases\n</code></pre>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#the-elusive-nature-of-similarity","title":"The Elusive Nature of \"Similarity\"","text":"<p>At the heart of embedding models is a deceptively simple concept: they convert text (or other data) into numerical vectors that capture semantic meaning. The assumption is that items with similar meanings will have vectors that are close to each other when measured by cosine similarity or other distance metrics.</p> <p>Domain-Specific Similarity</p> <pre><code>In e-commerce, what does it mean for two products to be similar? Are they similar because they're substitutes (different brands of red shirts) or complements (a shirt and matching pants)?\n\nFor music recommendations, are songs similar because they share the same genre, appear in the same playlists, or appeal to the same listeners? For a \"add more songs to this playlist\" feature, similarity might mean stylistic consistency, but for a discovery feature like Spotify's Discovery Weekly, it could mean something entirely different.\n\nPerhaps the clearest example comes from dating apps. Should \"I love coffee\" and \"I hate coffee\" be considered similar or different? From a linguistic perspective, they're opposites. From a topic perspective, both profiles care enough about beverages to mention them prominently. \n\nBut there are many other interpretations: Maybe they're different because coffee lovers wouldn't date coffee haters. Maybe they're similar because both indicate people with strong food preferences (foodies). Maybe they're complementary because as long as one loves tea and one loves coffee, they'll actually get along well.\n\nThe key insight: **What matters for a dating app isn't textual similarity at all\u2014it's whether two profiles predict if users will like each other.** This relationship isn't captured in generic embedding models trained on web text.\n</code></pre> <p>But here's the problem: \"similarity\" is poorly defined when we move beyond general language understanding. The correct answer isn't universal\u2014it depends entirely on your application's objectives.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#the-hidden-assumptions-in-provider-models","title":"The Hidden Assumptions in Provider Models","text":"<p>When you use embedding models from providers like OpenAI, Cohere, or others, you're inheriting a set of assumptions about what \"similarity\" means\u2014assumptions that may not align with your specific needs.</p> <p>Legal Document Search Failure</p> <p>One memorable case involved a legal document search application. The generic embeddings performed reasonably well for finding factual information but struggled with procedural questions. The embeddings didn't adequately capture the relationships between legal procedures and their applications\u2014a specific type of similarity vital to legal professionals but not emphasized in general-purpose training data.</p> <p>The key insight here isn't that provider embeddings are bad\u2014they're actually remarkable technical achievements. It's that their definition of similarity is generic, while your application needs are specific. And as we'll see, fine-tuning with your own data bridges this gap.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#from-evaluation-to-few-shot-examples","title":"From Evaluation to Few-Shot Examples","text":"<p>While fine-tuning embedding models can dramatically improve retrieval, we can often make significant improvements using a simpler approach: few-shot examples. Let's explore how to transform evaluation examples into effective few-shot prompts.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#the-power-of-examples-in-context","title":"The Power of Examples in Context","text":"<p>Few-shot learning has emerged as one of the most powerful techniques for guiding language model behavior. Rather than fine-tuning the model itself (which requires specialized infrastructure and significant data), few-shot learning simply includes examples within the prompt.</p> <p>How Few-Shot Learning Works</p> <p>When you provide a language model with examples of how to respond to similar queries, you activate its ability to recognize patterns and apply them to new inputs. It's like showing a human a few examples of a task before asking them to perform it themselves\u2014no specialized training required, just clear demonstrations.</p> <p>This approach is particularly powerful for RAG applications because different query types often need different retrieval and response strategies. A few well-chosen examples can help the model recognize the current query type and apply the appropriate strategy.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#selecting-the-right-examples","title":"Selecting the Right Examples","text":"<p>Not all evaluation examples make good few-shot prompts. I've seen teams simply grab random examples from their evaluation set, only to find the model's performance actually decreased. The problem wasn't the model\u2014it was the examples.</p> <p>Characteristics of Good Examples</p> <p>The best examples for few-shot learning share several characteristics:</p> <pre><code>1. They represent common query patterns your users actually use\n2. They demonstrate clear, step-by-step reasoning paths\n3. They cover a diverse range of topics or question types\n4. They avoid being too specific or unusual\n</code></pre> <p>Remember the synthetic data generation techniques we explored in Chapter 1? You can use those same methods to generate examples specifically for few-shot learning. The key difference is that for few-shot examples, you need not just questions and answers, but also the reasoning process that connects them.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#building-your-few-shot-library","title":"Building Your Few-Shot Library","text":"<p>Creating a comprehensive few-shot example library is a strategic investment that pays dividends across your entire RAG application. I recommend organizing your examples into a structured library:</p> <ol> <li>Start by filtering your evaluation data for the highest-quality examples</li> <li>Group them by query type or intent (factual questions, how-to guides, comparisons, etc.)</li> <li>Select representative examples from each group</li> <li>Format them consistently for inclusion in prompts</li> <li>Test their effectiveness with your RAG pipeline</li> <li>Iterate and refine based on performance</li> </ol> <p>Structured Few-Shot Prompt</p> <pre><code>You are an assistant specialized in answering questions about [domain].\n\n    Here are some examples of how to answer questions:\n\n    Question: [Example Question 1]\n    Thinking: [First, I'll identify the key entities in the question. Then I'll look for information about their relationship...]\n    Answer: [Example Answer 1]\n\n    Question: [Example Question 2]\n    Thinking: [This appears to be a comparison question. I should look for information about both entities and highlight similarities and differences...]\n    Answer: [Example Answer 2]\n\n    Now please answer the following question:\n    Question: [Actual User Query]\n    ```\n\nThis organized approach helps you maintain and evolve your example library as your application grows. You can track which examples work best for different situations, rotate examples to prevent overfitting, and continuously refine based on user interactions.\n\n## Practical Implementation: Building the Data Flywheel for Fine-Tuning\n\nWhile few-shot learning is powerful, fine-tuning your embedding models can create even more dramatic improvements in retrieval quality. The challenge is getting enough high-quality data to make fine-tuning worthwhile. This is where the concept of the data flywheel becomes crucial.\n\n### Starting the Flywheel\n\nI often tell teams that building a RAG application is like practicing martial arts\u2014there's a \"wax on, wax off\" aspect to it. You start with a small set of examples that serve as your evaluation benchmarks. As you collect more data, those examples become few-shot prompts. And as your dataset grows further, it becomes training data for fine-tuning.\n\n!!! info \"Data Collection Milestones\" - With 20 examples, you can build basic evaluation benchmarks - With 30 examples, you can create effective few-shot prompts - With 1000+ examples, you can fine-tune your retrieval models\n\nThe beauty of this approach is that you're continuously repurposing the same data for increasingly sophisticated improvements. Each stage builds on the previous one, creating a virtuous cycle of enhancement.\n\n!!! warning \"Start Collecting Now\"\nYou need to start collecting the right data now, even if you're not ready to fine-tune yet. The sooner you start logging relevant user interactions, the sooner you'll reach the critical mass needed for fine-tuning.\n\n### What Data Should You Log?\n\nFor RAG applications, the most valuable data points to collect include:\n\n1. The actual queries users make\n2. Which retrieved chunks were cited in the final responses\n3. Which responses received positive feedback (and which didn't)\n4. Which queries required refinement or follow-up\n\nThese signals help you understand which documents are truly relevant to which queries\u2014the fundamental relationship you're trying to capture in fine-tuned embeddings.\n\n!!! example \"Domain-Specific Relevance Signals\"\nFor other applications, the relevance signals will differ:\n\n    - In e-commerce: track which items are purchased together, viewed in sequence, or added to the same lists\n    - For music recommendations: log which songs appear in the same playlists or are hearted by the same users\n    - For dating apps: record which profiles match and go on to have meaningful conversations\n\nThe key is defining what \"relevance\" means in your specific context and systematically collecting data that captures this relationship.\n\n!!! warning \"Start Logging Yesterday!\"\nI've seen numerous companies hire machine learning engineers to fine-tune embedding models, only to realize they hadn't started logging relevance data. These teams then have to wait 3-6 months to collect enough data before they can begin the work they intended to do immediately.\n\n**The most important action you can take today is to start logging relevance data**, even if you're not ready to hire ML specialists or begin fine-tuning. Save the top 20-40 chunks for each query and use an LLM to mark relevance if human annotation isn't feasible. This data will be invaluable when you're ready to improve your models.\n\nI worked with one team that built a beautiful RAG application for internal documents but failed to implement any feedback collection mechanisms. Six months later, when they wanted to fine-tune their embeddings, they had to start from scratch with synthetic data because they had no record of which retrieved documents had actually been helpful to users. Don't make this mistake\u2014plan your data collection from day one.\n\n!!! success \"Small Datasets Can Make Big Differences\"\nThe team at Sentence Transformers has demonstrated that even with just 6,000 examples, you can achieve 6-10% better performance. With 40 minutes of fine-tuning on a laptop, you can create significant lifetime value for your application. This makes fine-tuning embedding models accessible even to teams without massive datasets or specialized infrastructure.\n\n## Understanding Contrastive Learning for Embeddings\n\nTo understand how fine-tuning works for embedding models, we need to explore contrastive learning\u2014the technique that powers most modern embedding fine-tuning approaches.\n\n### Learning Through Contrasts\n\nContrastive learning is based on a simple but powerful idea: learning what things are similar by understanding what things are different. Rather than trying to predict absolute values, contrastive learning focuses on relative relationships.\n\n!!! info \"Triplet Structure\"\nThe most common implementation uses a structure called a triplet, which consists of:\n\n    1. An **anchor** (usually the query)\n    2. A **positive example** (a document that's relevant to the query)\n    3. A **negative example** (a document that's not relevant to the query)\n\nThe goal of training is straightforward: adjust the embedding model so that the distance between the anchor and positive example decreases, while the distance between the anchor and negative example increases. In other words, pull similar things closer together and push dissimilar things further apart.\n\n```mermaid\ngraph LR\n    A[Anchor: Query] --- P[Positive: Relevant Document]\n    A --- N[Negative: Irrelevant Document]\n    P -.- |\"Pull Closer\"| A\n    N -.- |\"Push Away\"| A\n</code></pre> <p>This approach is particularly effective for embedding models because it directly optimizes for the distance relationships we care about in retrieval tasks.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#creating-effective-triplets-for-rag","title":"Creating Effective Triplets for RAG","text":"<p>For RAG applications, there are several natural ways to create triplet datasets:</p> <ul> <li>Anchor: The user's query</li> <li>Positive: Document chunks that were cited in the final response or received positive feedback</li> <li>Negative: Document chunks that were retrieved but not cited, or received negative feedback</li> </ul> <p>Healthcare RAG Triplet</p> <p>Imagine a healthcare RAG application where a user asks:</p> <pre><code>```\nWhat are the side effects of medication X?\n```\n\nOur retrieval system might return several documents, including:\n\n```\nDocument A: \"Medication X may cause drowsiness, nausea, and in rare cases, allergic reactions.\"\n\nDocument B: \"Medication X is used to treat high blood pressure and should be taken with food.\"\n```\n\nIf Document A is cited in the response while Document B isn't, we can create a triplet:\n\n```json\n{\n  \"anchor\": \"What are the side effects of medication X?\",\n  \"positive\": \"Medication X may cause drowsiness, nausea, and in rare cases, allergic reactions.\",\n  \"negative\": \"Medication X is used to treat high blood pressure and should be taken with food.\"\n}\n```\n</code></pre> <p>Through many such examples, the model learns that queries about side effects should be closer to texts describing adverse reactions than to texts describing indications or administration instructions.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#the-challenge-of-hard-negatives-and-how-ux-can-help","title":"The Challenge of Hard Negatives and How UX Can Help","text":"<p>The triplet example above introduces an important subtlety in contrastive learning. Notice that our negative example, \"Medication X is used to treat high blood pressure,\" is still about the same medication\u2014it's just not about side effects. This makes it what we call a \"hard negative\"\u2014it's similar to what we're looking for in some ways (same medication) but different in crucial aspects (not about side effects).</p> <p>Hard Negative Mining Strategies</p> <p>Effective Approaches:</p> <ol> <li>Semantic Similarity with Different Intent:</li> <li>\"Software engineer\" vs \"Software engineering recruiter\"</li> <li> <p>Both about software roles, but serving different user needs</p> </li> <li> <p>User Deletion Signals:</p> </li> <li>Track which documents users actively remove from results</li> <li> <p>These are perfect hard negatives - retrieved but explicitly rejected</p> </li> <li> <p>Category Boundaries:</p> </li> <li>Items from adjacent but different categories</li> <li> <p>Example: \"Red running shoes\" vs \"Red dress shoes\"</p> </li> <li> <p>Temporal Relevance:</p> </li> <li>Outdated versions of correct information</li> <li>Example: \"2023 tax rates\" when user needs \"2024 tax rates\"</li> </ol> <p>Agentic Retrieval Perspective</p> <p>Colin Flaherty's work on agentic coding systems reveals a surprising insight: \"We found that for SweeBench tasks, embedding-based retrieval was not the bottleneck - grep and find were sufficient.\" The agent's persistence effectively compensated for less sophisticated tools. This suggests that while fine-tuning embeddings is valuable, the agent layer can sometimes overcome retrieval limitations through persistence. Learn more about agentic approaches \u2192</p> <p>Value of Hard Negatives</p> <p>Hard negatives are much more valuable for training than \"easy negatives.\" If instead our negative example had been about car maintenance\u2014completely unrelated to medications\u2014the model wouldn't learn much from this contrast because it's already obvious that car maintenance isn't relevant to medication side effects.</p> <pre><code>The truly challenging distinction\u2014and the one that will improve our retrieval quality the most\u2014is teaching the model to distinguish between different aspects of related topics.\n</code></pre> <p>This is where hard negative mining becomes crucial. Hard negative mining is the process of finding negative examples that are challenging but instructive for the model.</p> <p>Designing UX for Better Training Data</p> <p>If you're serious about improving your embeddings, consider explicitly designing your UX to capture these signals:</p> <pre><code>1. **Document-level feedback mechanisms**: Add simple thumbs up/down options next to each retrieved document, not just for the final answer\n\n2. **Click tracking**: Record which documents users click on and which they ignore\u2014those ignored despite ranking highly are excellent hard negative candidates\n\n3. **Dwell time analysis**: If a user quickly returns from a document without spending time reading it, that's a strong signal it wasn't relevant\n\n4. **Explicit comparison interfaces**: For critical applications, consider interfaces that ask users to compare documents and select the most relevant one\n\n5. **Query reformulation tracking**: When a user modifies their query slightly and gets better results, you can pair the original query with documents from the improved results to create training pairs\n</code></pre> <p>One particularly effective approach I've seen involved a \"more like this\" button next to helpful documents. This not only improved the immediate user experience but also created clear signals about which documents were semantically related in the users' mental models\u2014relationships that might not be obvious from text content alone.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#the-power-of-re-rankers-in-rag-systems","title":"The Power of Re-Rankers in RAG Systems","text":"<p>While embedding models are the workhorses of retrieval, re-rankers provide an additional layer of refinement that can significantly improve results. Where embedding models (also called bi-encoders) encode queries and documents separately, re-rankers (cross-encoders) process them together to make more nuanced relevance judgments.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#bi-encoders-vs-cross-encoders-understanding-the-trade-offs","title":"Bi-Encoders vs. Cross-Encoders: Understanding the Trade-offs","text":"<p>The fundamental trade-off between embedding models and re-rankers is between speed and accuracy:</p> <p>Model Comparison</p> <p>Bi-encoders (embedding models): - Encode query and document independently - Allow pre-computation of document embeddings - Enable fast vector similarity operations - Work well for first-pass retrieval of candidates - Examples include OpenAI's text-embedding models, SBERT, MPNet</p> <pre><code>**Cross-encoders (re-rankers)**:\n- Process query and document together as a pair\n- Cannot pre-compute relevance scores\n- Provide more accurate relevance judgments\n- Work best for re-ranking a smaller set of candidates\n- Examples include Cohere Rerank, monoT5\n</code></pre> <p>This complementary relationship makes them perfect partners in a two-stage retrieval process: use embeddings to quickly find candidate documents, then use a re-ranker to sort them more accurately.</p> <p>Re-Ranker Success Story</p> <p>One team I worked with was debating whether to invest in fine-tuning their embeddings or implementing a re-ranker. When they tested both approaches, they found that fine-tuning embeddings improved recall from 65% to 78%, while adding a re-ranker (even without fine-tuning) improved it to 82%. Combining both approaches pushed performance to 91%\u2014a transformative improvement from where they started.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#creating-training-data-for-re-rankers","title":"Creating Training Data for Re-Rankers","text":"<p>Re-rankers benefit from more nuanced training data than binary relevant/not-relevant labels. While you can start with the same data you use for embedding fine-tuning, consider enriching it with graded relevance scores:</p> <ol> <li>Create pairs of (query, document) with relevance scores on a scale (often 0-5)</li> <li>Include a range of scores to help the model learn gradations of relevance</li> <li>Train the model to predict these more nuanced relevance scores</li> </ol> <p>Graded Relevance Example</p> <p><code>json     {       \"query\": \"How do I reset my password?\",       \"documents\": [         {\"text\": \"Step-by-step password reset guide\", \"score\": 5},         {\"text\": \"General account management information\", \"score\": 3},         {\"text\": \"Creating a strong password\", \"score\": 2},         {\"text\": \"About our company\", \"score\": 0}       ]     }</code></p> <p>This richer data helps the re-ranker understand not just what's relevant versus irrelevant, but also what's highly relevant versus somewhat relevant\u2014a distinction that can significantly improve user experience.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#testing-different-approaches-systematically","title":"Testing Different Approaches Systematically","text":"<p>With your evaluation framework from Chapter 1 and your growing dataset of examples, you can now test various improvement approaches systematically. This experimental mindset is critical to making steady progress.</p> <p>Good Experimentation Practices</p> <p>Good experimentation requires discipline and structure. For each test:</p> <pre><code>1. Form a clear hypothesis: \"Implementing a re-ranker will improve recall@10 by at least 15%\"\n2. Define success criteria before running the experiment\n3. Isolate a single variable whenever possible\n4. Measure impact on your established metrics\n5. Document both successful and unsuccessful experiments\n</code></pre> <p>Common experiments worth running include:</p> <ol> <li>Embedding model comparisons: Test different models (OpenAI, Cohere, open-source alternatives)</li> <li>Chunking strategy variations: Try different chunk sizes and overlap percentages</li> <li>Retrieval method comparisons: Compare lexical, semantic, and hybrid approaches</li> <li>Re-ranking impact assessment: Measure the effect of adding a re-ranker</li> <li>Few-shot prompt variations: Test different examples and formats</li> </ol> <p>Debate Resolved Through Data</p> <p>One team I worked with spent weeks debating which embedding model to use, with different team members advocating for their preferred option. Instead of continuing the debate, they implemented a simple experiment: they indexed their documents with three different embedding models and measured recall on their evaluation set. The results settled the debate in hours, not weeks, and the team moved forward with data-backed confidence.</p> <p>This cyclical process creates a data-driven improvement flywheel that continuously enhances your system's performance.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#building-a-roadmap-for-continuous-improvement","title":"Building a Roadmap for Continuous Improvement","text":"<p>Based on your experimental results, you can now build a roadmap for ongoing improvements. This isn't just about technical enhancements\u2014it's about creating a systematic process for evolution.</p> <p>Prioritization Framework</p> <p>When deciding what to improve next, consider multiple factors:</p> <pre><code>1. **Impact**: Which changes will most dramatically improve key metrics?\n2. **Effort**: How much work is required to implement each change?\n3. **Dependencies**: Which improvements depend on others being completed first?\n4. **Risk**: What is the chance of negative side effects or regressions?\n</code></pre> <p>I've found that impact/effort prioritization works particularly well for RAG improvements. Plot potential enhancements on a simple 2x2 grid with impact on one axis and effort on the other, then focus on high-impact, low-effort improvements first. These \"quick wins\" build momentum and demonstrate value while you prepare for more complex enhancements.</p> <p>Prioritization in Action</p> <p>In one project, we identified that implementing BM25 hybrid retrieval would be high-impact and medium-effort, while fine-tuning custom embeddings would be high-impact but high-effort. We prioritized the hybrid retrieval first, which gave us immediate gains while we collected data for the eventual embedding fine-tuning.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#linear-adapters-a-cost-effective-alternative","title":"Linear Adapters: A Cost-Effective Alternative","text":"<p>Before diving into full fine-tuning, consider linear adapters - a technique that can deliver significant improvements at a fraction of the cost.</p> <p>What Are Linear Adapters?</p> <p>Linear adapters add a small trainable layer on top of frozen embeddings: - Train only a linear transformation matrix - Keep the base embedding model unchanged - Combine benefits of domain specificity with pre-trained knowledge</p> <p>Cost Comparison: - Full fine-tuning: \\(50-100 for meaningful datasets - Linear adapters: ~\\)12 for the same improvement - Training time: Minutes vs hours</p> <p>When to Use Linear Adapters</p> <p>Perfect for: - Domain-specific terminology mapping - Multi-domain applications (train separate adapters) - Rapid experimentation - Limited computational resources</p> <p>Implementation: <pre><code># Simplified example\nbase_embeddings = model.encode(texts)\nadapted_embeddings = linear_adapter(base_embeddings)\n</code></pre></p> <p>You can train different adapters for different query types or domains, switching between them based on query classification.</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#additional-resources","title":"Additional Resources","text":"<p>Tools and Libraries</p> <pre><code>### Understanding Embedding Models\n\n1. **Sentence Transformers Library** ([https://www.sbert.net/](https://www.sbert.net/)): This library provides easy-to-use implementations for state-of-the-art embedding models, supporting both pairwise datasets and triplets for fine-tuning. It's my recommended starting point for most teams due to its balance of performance and ease of use.\n\n2. **Modern BERT** ([https://huggingface.co/sentence-transformers](https://huggingface.co/sentence-transformers)): These newer models offer 8,000 token sequence lengths and generally outperform classic BERT-based models. The BGE models in particular have shown excellent performance across many domains and are worth testing in your applications.\n\n3. **Cohere Re-ranking Models** ([https://cohere.com/rerank](https://cohere.com/rerank)): Cohere offers state-of-the-art re-ranking capabilities with a fine-tuning API that makes it relatively easy to customize for your specific needs. In my experience, even their base re-ranker without fine-tuning often provides substantial improvements to retrieval quality.\n\n4. **Specialized Domains**: For specific domains like code, science, or legal documents, look for models pre-trained on related corpora. For example, CodeBERT for programming or SciBERT for scientific literature can provide better starting points than general models.\n\n5. **Comparison to Data Labeling**: Everything we're doing today with fine-tuning embedding models is what I used to pay data labeling teams hundreds of thousands of dollars to do annually. The ML playbook that was once only accessible to large companies with significant budgets is now available to teams of all sizes thanks to advances in transfer learning and fine-tuning techniques.\n</code></pre> <p>Key Concepts</p> <pre><code>#### Contrastive Learning In-Depth\n\nContrastive learning trains models to recognize similarities and differences between items by pushing and pulling examples in the embedding space:\n\n- **Triplet Loss**: Optimizes the distance between anchor-positive pairs relative to anchor-negative pairs\n- **InfoNCE Loss**: Contrasts a positive pair against multiple negative examples\n- **Multiple Negatives Ranking Loss**: Handles batches of queries with multiple negatives per query\n\n#### Scaling and Efficiency Considerations\n\nFor large datasets or production workloads:\n\n- Consider parallel processing frameworks (like Modal) to accelerate embedding and training\n- Experiment with multi-GPU training for faster iterations\n- Evaluate the trade-offs between API costs and self-hosting\n- Test multiple model variations simultaneously to find optimal configurations\n</code></pre>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#reflection-questions","title":"Reflection Questions","text":"<p>!!! question \"Self-Assessment\" 1. What specific definition of \"similarity\" is most important for your application's domain?</p> <pre><code>2. How would you create effective few-shot examples from your existing evaluation data?\n\n3. What user interactions in your application could provide valuable training signals for fine-tuning?\n\n4. If you had to prioritize one retrieval improvement for your system, would it be embeddings, re-ranking, or something else? Why?\n\n5. What experiments could you run to test your hypotheses about improving retrieval quality?\n</code></pre>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#conclusion-and-next-steps","title":"Conclusion and Next Steps","text":"<p>In this chapter, we've explored how to transform evaluation data into valuable training assets:</p> <ol> <li>Converting evaluation examples into few-shot prompts</li> <li>Understanding the limitations of generic embeddings</li> <li>Creating datasets for fine-tuning retrieval models</li> <li>Learning how contrastive learning improves embeddings</li> <li>Testing approaches systematically</li> <li>Building a roadmap for continuous improvement</li> </ol> <p>The key insight to take away is that data collection and repurposing form the foundation of systematic RAG improvement. Every question, every piece of feedback, and every evaluation can fuel your improvement flywheel if properly captured and utilized.</p> <p>As we've seen, fine-tuning embedding models can dramatically improve the performance of your RAG application. Unlike fine-tuning large language models (which requires significant expertise and resources), embedding model fine-tuning is accessible to teams of all sizes, with demonstrated benefits from as few as 6,000 examples.</p> <p>What's Coming Next</p> <p>In Chapter 3, we'll dive into deployment strategies, user feedback collection methods, and how to use this feedback to further refine your RAG application. We'll explore practical techniques for gathering implicit and explicit feedback, designing effective user interfaces, and closing the loop between user interactions and system improvements.</p> <p>Related Concepts in Other Chapters</p> <ul> <li>Query Segmentation (Chapter 4): Learn how to identify which queries benefit most from fine-tuning</li> <li>Specialized Models (Chapter 5): See how fine-tuned embeddings power specialized retrievers</li> <li>Router Optimization (Chapter 6): Understand how fine-tuning improves query routing</li> </ul>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter2/#summary","title":"Summary","text":"<p>The data flywheel approach transforms what begins as evaluation into training assets that continuously improve your RAG system. By understanding the limitations of generic models, implementing few-shot examples, and preparing for fine-tuning, you create a foundation for ongoing enhancement.</p> <p>The most critical actions to take immediately are:</p> <ol> <li>Start logging relevancy data today - Even if you're not ready to fine-tune yet</li> <li>Define similarity for your domain - What makes two items \"similar\" in your specific context?</li> <li>Implement feedback mechanisms - Design your UX to collect the signals you'll need later</li> <li>Build your few-shot library - Begin transforming evaluation data into prompt examples</li> <li>Test domain-specific models - Explore fine-tuned models for your specific application area</li> </ol> <p>This systematic approach ensures that every piece of data you collect contributes to a cycle of improvement that makes your application increasingly effective for your specific use case. When done properly, this flywheel effect touches every part of your system\u2014improving clustering, topic modeling, and all other aspects we'll explore in the coming weeks.</p> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>","tags":["few-shot","fine-tuning","training-data","evaluation"]},{"location":"workshops/chapter3-1/","title":"Feedback Collection: Building Your Improvement Flywheel","text":"<p>Chapter Overview</p> <pre><code>This chapter explores the essential role of feedback collection in RAG systems, introducing the concept of a feedback flywheel for systematic improvement. You'll learn practical strategies for making feedback mechanisms visible and engaging to users, techniques for collecting segmented feedback that provides actionable insights, and methods for mining user behavior to generate training data. The chapter emphasizes how effective feedback collection transforms your RAG application from a static tool into a continuously improving system that grows more valuable with every interaction.\n</code></pre>"},{"location":"workshops/chapter3-1/#introduction","title":"Introduction","text":"<p>The true power of RAG isn't in its initial deployment\u2014it's in how the system improves over time through feedback collection. Many RAG implementations focus exclusively on the technical details of retrieval and generation while neglecting the critical infrastructure needed to collect and utilize user feedback.</p> <p>Building on Previous Chapters</p> <ul> <li>Chapter 1: The evaluation framework you built provides the baseline</li> <li>Chapter 2: The fine-tuning techniques need feedback data to be effective</li> </ul> <p>This chapter shows you how to collect the data that powers continuous improvement.</p> <p>In this chapter, we'll explore how to build effective feedback mechanisms that transform your RAG application from a static implementation into a continuously improving system that grows more valuable with every user interaction. This approach creates a \"feedback flywheel\"\u2014a virtuous cycle where user interactions provide the data needed to make the system better, which in turn attracts more users and generates more feedback.</p> <p>The Invisible Feedback Problem</p> <p>Many RAG implementations hide feedback mechanisms in obscure UI locations or use generic \"thumbs up/down\" buttons that provide minimal insight. Research suggests that users interact with these minimal feedback options less than 0.1% of the time, providing insufficient data for meaningful improvements.</p> <p>In my consulting practice, I've seen that simply changing the copy from generic \"How did we do?\" to specific \"Did we answer your question?\" can increase feedback rates by 5x. Well-designed feedback mechanisms don't just collect more data\u2014they accelerate your entire improvement cycle, allowing you to fine-tune 5x faster and deploy with greater confidence.</p> <p>Proven Copy That Works</p> <p>5x Better Feedback Rates: - \u2705 \"Did we answer your question?\" - \u2705 \"Was this information helpful?\" - \u2705 \"Did we take the correct actions?\" (for action-oriented systems) - \u274c \"How did we do?\" - \u274c \"Rate your experience\"</p> <p>Context-Specific Examples: - For coding assistants: \"Did this code solve your problem?\" - For customer support: \"Did we resolve your issue?\" - For research tools: \"Did you find what you were looking for?\" - For data analysis: \"Were these insights useful?\"</p> <p>The key is focusing on the core value proposition rather than generic satisfaction.</p> <p>Feedback collection is the lifeblood of systematic RAG improvement. Without it, you're flying blind\u2014unable to identify which aspects of your system are performing well and which need enhancement. Robust feedback mechanisms tell you:</p> <ul> <li>Which queries your retrieval system handles poorly</li> <li>Which document segments are most valuable for answering specific questions</li> <li>Where your generation step produces inaccurate or unhelpful responses</li> </ul> <p>This chapter focuses on the practical implementation of feedback mechanisms in RAG applications. We'll cover strategies for making feedback visible and engaging, approaches for segmenting feedback to make it more actionable, and techniques for mining user behavior to generate training datasets.</p>"},{"location":"workshops/chapter3-1/#feedback-visibility-make-it-impossible-to-miss","title":"Feedback Visibility: Make It Impossible to Miss","text":"<p>The first principle of effective feedback collection is visibility. Your feedback mechanisms should be prominent and engaging, not hidden in dropdown menus or settings pages. Users should encounter feedback options naturally as part of their interaction flow.</p> <p>High-Visibility Feedback UI</p> <p>Consider the difference between these two approaches:</p> <pre><code>**Low Visibility:** A small thumbs up/down icon in the corner of the response\n\n**High Visibility:**\n\nAfter receiving an answer, users see:\n\n\"Was this answer helpful? [Yes] [Somewhat] [No]\"\n\nIf they click \"Somewhat\" or \"No\":\n\n\"What could be improved?\"\n- [ ] More detailed explanation\n- [ ] More relevant information\n- [ ] Incorrect information\n- [ ] Better formatting\n- [ ] Other: ____________\n</code></pre> <p>The second approach not only makes feedback impossible to miss but also structures it in a way that provides more actionable insights. Research shows that visible, engaging feedback mechanisms can increase feedback rates from less than 1% to over 30%.</p>"},{"location":"workshops/chapter3-1/#implementation-strategies","title":"Implementation Strategies","text":"<p>Here are several patterns for implementing high-visibility feedback mechanisms:</p> <ol> <li>Inline Feedback: Place feedback options directly beneath each response</li> <li>Modal Prompts: Show a feedback modal after a certain number of interactions</li> <li>Follow-up Questions: Include feedback collection as part of conversational flow</li> <li>Email Follow-ups: Send follow-up emails asking for feedback on recent sessions</li> </ol> <p>Each approach has advantages for different use cases. The key is to make feedback collection a natural part of the user experience rather than an afterthought.</p> <p>Streaming and Perceived Performance</p> <p>The Claude Progress Counter Effect:</p> <p>Claude's implementation of progress counters during response generation serves multiple purposes: - Shows \"thinking\" progress (e.g., \"Analyzing document 3 of 5...\") - Reduces perceived latency by up to 45% - Gives users confidence the system is working - Creates natural moments for feedback collection</p> <p>Implementation Pattern: <pre><code>Searching documents... [\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591] 40%\nFound 5 relevant sources\nAnalyzing content... [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591] 80%\nGenerating response... [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588] 100%\n\n[Response appears here]\n\nDid we find the right information? [Yes] [No]\n</code></pre></p> <p>This pattern makes feedback feel like a natural continuation of the interaction rather than an interruption.</p>"},{"location":"workshops/chapter3-1/#enterprise-feedback-collection-with-slack-integration","title":"Enterprise Feedback Collection with Slack Integration","text":"<p>For enterprise applications, especially when working with large customers who have dedicated customer success teams, consider implementing a Slack integration for feedback collection:</p> <ol> <li>Create a shared Slack channel with customer stakeholders</li> <li>Post negative feedback directly to the channel in real-time</li> <li>Allow your team to discuss issues and ask follow-up questions</li> <li>Document how feedback is addressed and integrated into your evaluation suite</li> <li>Report back on improvements during regular sync meetings</li> </ol> <p>This approach creates transparency and builds trust by showing customers that their feedback drives real improvements. In my experience, this method increases feedback by approximately 5x compared to traditional forms, while also improving customer retention through visible responsiveness.</p> <p>Enterprise Feedback Pattern</p> <p>The Most Effective B2B Feedback Flow:</p> <ol> <li>In-App Collection:</li> <li>Binary feedback (thumbs up/down) for quick signals</li> <li>Optional text field appears only after negative feedback</li> <li> <p>Track which employee provided feedback</p> </li> <li> <p>Slack Integration: <pre><code>\ud83d\udea8 Negative Feedback Alert\nUser: sarah@company.com\nQuery: \"Find all contracts with termination clauses\"\nIssue: Missing several key documents\nResponse ID: #12345\n\n[View Full Context] [Reply to User]\n</code></pre></p> </li> <li> <p>Follow-Up:</p> </li> <li>Customer success team can immediately engage</li> <li>Engineering team sees issues in real-time</li> <li>Creates accountability and trust</li> </ol> <p>This pattern has helped teams achieve 30-40% feedback rates in enterprise settings.</p> <p>Slack Webhook Integration Code</p> <p></p> <pre><code>*This code demonstrates how to integrate feedback collection with Slack, automatically posting negative feedback to a shared channel for immediate visibility and follow-up.*\n</code></pre> <p>Feedback UI Implementation</p> <p></p> <pre><code>*This code renders a response with prominent feedback options, automatically showing a more detailed form if the user indicates the response wasn't fully helpful.*\n</code></pre>"},{"location":"workshops/chapter3-1/#segmented-feedback-make-it-actionable","title":"Segmented Feedback: Make It Actionable","text":"<p>Generic feedback like thumbs up/down provides minimal insight for improvement. To make feedback truly actionable, segment it into specific aspects of your RAG pipeline.</p> <p>The Problem with Generic Feedback</p> <p>A simple \"thumbs down\" could mean many things: - The retrieval system found irrelevant documents - The generation step produced inaccurate information - The answer was technically correct but poorly formatted - The answer was too brief or too verbose</p> <pre><code>Without knowing which aspect failed, you can't target improvements effectively.\n</code></pre> <p>Segmented feedback isolates specific parts of your RAG pipeline, helping you identify exactly where issues occur. Instead of asking \"Was this helpful?\" consider questions like:</p> <ul> <li>\"Did this answer directly address your question?\"</li> <li>\"Was the information factually accurate?\"</li> <li>\"Were sources relevant to your query?\"</li> <li>\"Was the response clear and well-organized?\"</li> </ul> <p>Each question targets a different aspect of your system, allowing you to pinpoint areas for improvement.</p>"},{"location":"workshops/chapter3-1/#collecting-segmented-negative-feedback","title":"Collecting Segmented Negative Feedback","text":"<p>Negative feedback is particularly valuable for improvement, but users often abandon interactions after having a bad experience. To maximize the collection of negative feedback:</p> <ol> <li>Make feedback collection immediate\u2014don't wait until the end of a session</li> <li>Use progressive disclosure to collect more detailed feedback after an initial negative response</li> <li>Keep detailed feedback optional but make it easy to provide</li> <li>Explain how feedback will be used to improve the system</li> </ol> <p>Here's how you might implement segmented negative feedback collection:</p>"},{"location":"workshops/chapter3-1/#learning-from-user-behavior-the-implicit-feedback-gold-mine","title":"Learning from User Behavior: The Implicit Feedback Gold Mine","text":"<p>While explicit feedback (ratings, comments) is valuable, users express opinions through their actions even when they don't provide direct feedback. These behavioral signals\u2014often called implicit feedback\u2014can be a gold mine for system improvement.</p> <p>Key implicit feedback signals include:</p> <ul> <li>Query refinements: When users rephrase a query immediately after receiving a response</li> <li>Abandonment: When users abandon a session after receiving a response</li> <li>Engagement time: How long users engage with a response</li> <li>Link clicks: Which citations or references users click on</li> <li>Copypaste actions: What parts of responses users copy to their clipboard</li> <li>Scrolling behavior: Whether users read the entire response or just skim</li> </ul> <p>By tracking these behaviors, you can identify patterns that indicate success or failure even when users don't provide explicit feedback.</p> <p>Implicit Feedback Collection</p> <p></p> <pre><code>*This code tracks key implicit feedback signals including query refinements, citation clicks, and engagement time, providing valuable data even when users don't explicitly rate responses.*\n</code></pre>"},{"location":"workshops/chapter3-1/#mining-hard-negatives-from-user-behavior","title":"Mining Hard Negatives from User Behavior","text":"<p>One particularly valuable form of implicit feedback is the identification of \"hard negatives\"\u2014documents that appear relevant based on keyword or semantic matching but are actually irrelevant or misleading for a particular query.</p> <p>When a user submits a query, views the response and citations, then immediately refines their query or provides negative feedback, there's a good chance that the retrieved documents were not helpful. These interactions provide strong signals about weaknesses in your retrieval system.</p> <p>By tracking these patterns, you can build datasets of queries paired with documents that should NOT be retrieved\u2014invaluable training data for improving embedding models or reranking systems.</p>"},{"location":"workshops/chapter3-1/#creative-ui-patterns-for-hard-negative-collection","title":"Creative UI Patterns for Hard Negative Collection","text":"<p>Consider these UI patterns specifically designed to help collect hard negative examples:</p> <ol> <li> <p>Interactive Citations: Display the source documents used to generate the response and allow users to mark specific citations as irrelevant. This direct feedback creates perfect triplets for contrastive learning (query \u2192 relevant docs \u2192 irrelevant docs).</p> </li> <li> <p>Document Filtering UI: Similar to how social networks show \"People You May Know,\" present a scrollable list of potentially relevant documents and let users remove irrelevant ones. Each removal creates a hard negative training example.</p> </li> <li> <p>Limited Options with Refresh: Show only the top 5 most relevant documents, with options to \"add\" (positive) or \"delete\" (negative) each one. When a user deletes a document to see another option, you've collected a hard negative.</p> </li> <li> <p>Regeneration After Removal: Allow users to remove citation sources and then regenerate the answer. Documents removed before regeneration become strong hard negative candidates for that query.</p> </li> </ol> <p>Interactive Citations UI</p> <p></p> <pre><code>*This UI allows users to mark specific citations as relevant or irrelevant and regenerate answers, creating valuable training data for improving retrieval quality.*\n\nRemember: Hard negatives are the most valuable training examples for improving retrieval quality through embedding model fine-tuning. While standard negatives (completely unrelated documents) are easy to find, hard negatives (seemingly relevant but actually unhelpful documents) are rare and therefore extremely valuable for training.\n</code></pre> <p>Here's a simple algorithm for mining hard negatives from user interactions:</p> <p>By collecting these potential hard negatives over time, you can build a dataset for fine-tuning embedding models or training re-rankers to avoid these problematic documents in future queries.</p>"},{"location":"workshops/chapter3-1/#citations-for-building-trust-and-collecting-feedback","title":"Citations for Building Trust and Collecting Feedback","text":"<p>Citations are a powerful tool that serves multiple purposes in a RAG system:</p> <ol> <li>Building trust: Users want to know where information comes from and how the AI found it</li> <li>Providing transparency: Citations show what data is being used to generate responses</li> <li>Collecting feedback: Citations create opportunities to gather document-level relevance signals</li> </ol> <p>When users can see and interact with the source documents used in responses, they gain confidence in the system and are more likely to provide feedback on the quality and relevance of these sources.</p>"},{"location":"workshops/chapter3-1/#implementing-interactive-citations","title":"Implementing Interactive Citations","text":"<p>There are several approaches to implementing citations in your RAG interface:</p> <ol> <li>Markdown links: A simple implementation using markdown formatting to link to source documents</li> <li>Numbered citations: Academic-style numbered references with hover previews</li> <li>Inline highlights: Highlighting portions of text with the source documents they came from</li> <li>Visual PDF overlays: For document-based applications, highlighting the exact location in a PDF</li> </ol> <p>Markdown-based Citation Implementation</p> <p></p> <pre><code>*This code formats responses with clickable citations and builds a reference list that includes feedback options for each source, helping collect document-level relevance signals.*\n</code></pre>"},{"location":"workshops/chapter3-1/#advanced-visualization-with-bounding-boxes","title":"Advanced Visualization with Bounding Boxes","text":"<p>For document-centric applications, consider implementing bounding box citations that highlight the exact location in the source documents:</p> <ol> <li>Store coordinates of key information in your vector database</li> <li>When generating responses, include these coordinates in citation metadata</li> <li>Render the original document with visual overlays on the cited portions</li> <li>Allow users to click citations in the answer to jump to the exact location in the document</li> </ol> <p>This approach is particularly valuable for PDF-heavy domains like legal, medical, or technical documentation where source verification is critical.</p>"},{"location":"workshops/chapter3-1/#citation-implementation-patterns","title":"Citation Implementation Patterns","text":"<p>Preventing Hallucinations</p> <p>Skylar Payne emphasizes that hallucination remains a critical challenge, especially in sensitive domains. His most effective approach: \"Force the LLM to provide inline citations, validate that each citation exists in the retrieved documents, and semantically validate that each citation actually supports the claimed content.\"</p> <p>This is particularly critical for healthcare, legal, and financial applications. See more anti-patterns to avoid \u2192</p> <p>XML-Based Citation Pattern</p> <p>The Most Robust Approach:</p> <p>Instead of relying on markdown links or footnotes, use XML tags with start/end word anchoring:</p> <pre><code>According to the contract, &lt;cite source=\"doc123\" start=\"450\" end=\"467\"&gt;the termination \nclause requires 30 days notice&lt;/cite&gt; and &lt;cite source=\"doc124\" start=\"122\" end=\"134\"&gt;\nincludes a penalty fee of $10,000&lt;/cite&gt;.\n</code></pre> <p>Benefits: - Survives markdown parsing - Enables precise highlighting - Works well with fine-tuning - Handles abbreviations and technical language</p> <p>Fine-Tuning for Citations: - Train models to generate these XML tags - Use your evaluation data as training examples - Particularly effective for domains with heavy abbreviations (medical, legal, technical)</p>"},{"location":"workshops/chapter3-1/#building-a-feedback-driven-roadmap","title":"Building a Feedback-Driven Roadmap","text":"<p>The ultimate goal of feedback collection is to guide your improvement roadmap. Rather than making enhancement decisions based on intuition or technical interest, you can prioritize based on user needs revealed through feedback.</p> <p>Production Monitoring: Beyond Basic Feedback</p> <p>Ben Hylak and Sidhant Bendre highlight a critical insight: \"There's no exception being thrown when something goes wrong - the model simply produces an inadequate response.\" Their approach combines implicit signals (user frustration, task failures) with explicit signals (ratings, regenerations) to identify issues that traditional monitoring misses. The Trellis framework they present helps organize the \"infinite chaos\" of AI outputs into controllable segments. Learn about production monitoring strategies \u2192</p> <p>A feedback-driven roadmap:</p> <ol> <li>Identifies the most common issues reported by users</li> <li>Quantifies the impact of each issue on user satisfaction</li> <li>Ranks potential improvements by expected impact</li> <li>Establishes clear metrics to evaluate whether changes actually improve the user experience</li> </ol> <p>This approach ensures that engineering efforts focus on changes that will have the greatest impact on user satisfaction rather than on the most technically interesting problems.</p>"},{"location":"workshops/chapter3-1/#conclusion-feedback-as-foundation","title":"Conclusion: Feedback as Foundation","text":"<p>Effective feedback collection is the foundation of systematic RAG improvement. Without robust feedback mechanisms, you're left guessing about which aspects of your system need enhancement and whether your changes actually improve the user experience.</p> <p>By implementing the strategies outlined in this chapter\u2014making feedback visible, segmenting it for actionability, mining user behaviors for implicit signals, and using feedback to drive your roadmap\u2014you establish a data-driven approach to continuous improvement.</p> <p>The investment in well-designed feedback mechanisms pays extraordinary dividends:</p> <ol> <li>Accelerated improvement cycles: With 5x more feedback, you can fine-tune models 5x faster</li> <li>Higher-quality training data: Hard negatives mined from user interactions dramatically improve retrieval quality</li> <li>Increased user trust: Citations and transparency build confidence in your system's outputs</li> <li>Better prioritization: Clear signals about which issues matter most to users</li> <li>Data-driven roadmap: Engineering priorities driven by user needs, not technical curiosity</li> </ol> <p>Remember that small UX changes can make enormous differences in feedback collection rates. The most successful RAG applications aren't always those with the most sophisticated technology\u2014they're the ones that most effectively learn from their users.</p> <p>In the next chapter, we'll explore how to reduce perceived latency through streaming and progressive responses, building on the feedback foundation to create a more engaging user experience.</p> <p>How This Chapter Connects Forward</p> <ul> <li>Chapter 4: The feedback you collect enables query segmentation and analysis</li> <li>Chapter 5: User behavior patterns reveal which specialized retrievers to build</li> <li>Chapter 6: Feedback on router decisions improves tool selection</li> </ul>"},{"location":"workshops/chapter3-1/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>How visible are the feedback mechanisms in your current RAG implementation? What changes could make them more prominent and engaging?</p> </li> <li> <p>What implicit signals could you collect from user interactions with your system? How might these complement explicit feedback?</p> </li> <li> <p>How could you segment feedback to better pinpoint issues in specific parts of your RAG pipeline?</p> </li> <li> <p>What processes would you need to implement to translate feedback into a prioritized improvement roadmap?</p> </li> <li> <p>How might you incentivize users to provide more detailed feedback, especially after negative experiences?</p> </li> </ol>"},{"location":"workshops/chapter3-1/#summary","title":"Summary","text":"<p>Effective feedback collection is essential for systematic improvement of RAG systems. By making feedback mechanisms visible and engaging, segmenting feedback to target specific pipeline components, mining implicit signals from user behavior, and using feedback to drive your improvement roadmap, you create a foundation for continuous enhancement. The feedback flywheel turns raw user interactions into actionable insights that guide your development priorities and measure the impact of your improvements.</p>"},{"location":"workshops/chapter3-1/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Feedback Copy Matters: Changing from generic \"How did we do?\" to specific \"Did we answer your question?\" can increase feedback rates by 5x.</p> </li> <li> <p>Enterprise Patterns: For B2B applications, Slack integrations that post feedback directly to shared channels create transparency and trust while dramatically increasing feedback rates.</p> </li> <li> <p>Hard Negative Mining: Design your UX to collect hard negatives\u2014documents that appear relevant but are actually unhelpful\u2014as they're the most valuable training examples for fine-tuning.</p> </li> <li> <p>Citation Benefits: Interactive citations serve multiple purposes: building trust, providing transparency, and creating opportunities to collect document-level relevance signals.</p> </li> <li> <p>Behavior Tracking: Implicit signals from user behavior (query refinements, dwell time, citation clicks) can provide even more training data than explicit feedback.</p> </li> <li> <p>Start Small: Begin with simple, high-visibility feedback mechanisms and gradually add sophistication as you learn what works for your specific users and use cases.</p> </li> </ol> <p>Quick Implementation Wins</p> <p>Start with these proven patterns:</p> <ol> <li>Change your feedback copy to \"Did we answer your question?\" (immediate 5x improvement)</li> <li>Add streaming progress indicators to reduce perceived latency by 45%</li> <li>Implement XML-based citations for robust source tracking</li> <li>Set up Slack webhooks for enterprise customers</li> <li>Track query refinements as implicit negative signals</li> </ol> <p>These changes can typically be implemented in 1-2 sprints and deliver immediate, measurable improvements.</p>"},{"location":"workshops/chapter3-1/#additional-resources","title":"Additional Resources","text":"<ol> <li> <p>Nielsen Norman Group, \"User Feedback Mechanisms for Mobile and Web\"</p> </li> <li> <p>Google Research, \"Beyond A/B Testing: Implicit Feedback for UI Improvement\"</p> </li> <li> <p>Qualtrics, \"Designing Feedback Forms That Users Actually Complete\"</p> </li> <li> <p>GitHub Repository: RAG-Feedback-Collection - Templates and examples for implementing feedback mechanisms in RAG applications</p> </li> </ol> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"workshops/chapter3-2/","title":"Overcoming Latency: Streaming and Interstitials","text":"<p>Chapter Overview</p> <pre><code>This chapter explores how to overcome the critical challenge of latency in RAG applications. You'll learn strategies for streaming responses, designing meaningful interstitials, and employing various technical optimizations to enhance both actual and perceived performance. The chapter demonstrates how techniques like streaming structured data and dynamic content updates can transform waiting time from a frustrating experience into an engaging one, ultimately improving user satisfaction and feedback collection rates. By implementing these approaches, you'll create RAG applications that feel responsive even during complex processing operations.\n</code></pre>"},{"location":"workshops/chapter3-2/#introduction-the-psychology-of-waiting","title":"Introduction: The Psychology of Waiting","text":"<p>In our quest to build exceptional RAG applications, we often focus on the quality of responses while overlooking a critical aspect of user experience: latency. Even the most accurate and helpful answer loses value if users grow frustrated waiting for it to appear.</p> <p>The reality is that RAG processes\u2014retrieval, generation, validation, citation lookup\u2014take time. This inherent latency creates a fundamental challenge: how do we keep users engaged and confident while these processes run?</p> <p>Perceived performance often matters more than actual performance. Research shows that users perceive responsive systems as faster even when the total completion time is identical. This psychological principle is at the heart of the strategies we'll explore in this chapter.</p> <p>The Perception Gap</p> <p>Studies show that perceived wait times can be up to 25% longer than actual wait times when users have no visibility into system progress. Conversely, showing meaningful progress can make perceived wait times up to 40% shorter than actual wait times.</p> <p>Industry Perspective</p> <p>\"Streaming has become table stakes in modern LLM applications. Users expect responses instantly, and implementing streaming dramatically improves both actual and perceived performance. Only about 20% of companies I work with have a good understanding of how to implement streaming effectively to enhance user experience.\"</p> <p>We'll explore two complementary approaches to addressing latency:</p> <ol> <li>Streaming responses to show progress and deliver content incrementally</li> <li>Designing meaningful interstitials that engage users while processing occurs</li> </ol> <p>These techniques not only improve user experience but also lead to higher engagement and more feedback collection, strengthening the improvement flywheel we established in the previous chapter.</p> <p>Implementation Timing Insight</p> <p>If you're on the fence about implementing streaming in your RAG application, do it early. Migrating from a non-streaming to a streaming application is significantly more complex than building with streaming from the start. It can add weeks to your development cycle if attempted later in the project lifecycle.</p> <p>Impact of Visual Feedback</p> <p>- Users perceive animated progress bars as 11% faster even when wait times are identical - Users will tolerate up to 8 seconds of waiting when given visual feedback, reducing abandonment rates - Applications with engaging loading screens report higher satisfaction scores - Facebook discovered that skeleton screens significantly reduced perceived load times, resulting in better user retention and engagement</p> <p>The strategies we'll cover in this chapter aren't just enhancements\u2014they're becoming essential components of modern LLM applications. By the end of this chapter, you'll understand how to transform waiting time from a point of frustration to an opportunity for engagement and trust-building.</p>"},{"location":"workshops/chapter3-2/#the-power-of-animation-creating-the-illusion-of-speed","title":"The Power of Animation: Creating the Illusion of Speed","text":"<p>Before diving into streaming implementations, let's understand why animated indicators are so effective at improving perceived performance. Research in cognitive psychology reveals that humans perceive time differently when observing movement.</p> <p>The Power of Progress Indicators</p> <p>In a study by the Nielsen Norman Group, users reported a 15-20% faster perceived load time when shown an animated progress indicator compared to a static wait screen, even though the actual load times were identical.</p> <p>Animated indicators work by:</p> <ol> <li>Giving users confidence that the system is actively working</li> <li>Drawing attention away from the passage of time</li> <li>Setting expectations about progress and completion</li> </ol> <p>The most effective indicators for RAG systems are those that convey meaningful information about what's happening behind the scenes, not just generic loading animations.</p> <p>Consider how differently users perceive these three waiting experiences:</p> <ol> <li>A static screen with no feedback</li> <li>A generic spinning wheel</li> <li>A step-by-step indicator showing \"Searching relevant documents (2/5 complete)...\"</li> </ol> <p>The third approach not only feels faster but also builds trust by providing transparency into the process.</p>"},{"location":"workshops/chapter3-2/#streaming-responses-the-ultimate-progress-indicator","title":"Streaming Responses: The Ultimate Progress Indicator","text":"<p>Streaming takes the concept of progress indicators to its logical conclusion by delivering content to users as it's generated, rather than waiting for the entire response to complete. This creates a dramatically better user experience by:</p> <ol> <li>Showing immediate activity, reducing uncertainty</li> <li>Providing useful content while generation continues</li> <li>Allowing users to begin reading before the full response is ready</li> </ol> <p>In a traditional RAG implementation, users submit a query and wait in silence until the full response appears. With streaming, they see the response unfold in real-time\u2014a far more engaging experience.</p> <p>When to Implement Streaming</p> <p>My recommendation is to stream everything when possible. You can:</p> <pre><code>- Stream interstitials to explain latency and help users understand what's happening\n- Stream different results and UI components so users don't have to wait for completion\n- Stream tool calls and function arguments to show intermediate states\n- Implement skeleton screens (like those used by Facebook, LinkedIn, and Slack) to improve perceived latency\n</code></pre> <p>Industry Experience</p> <p>\"I've seen companies experience 30-40% higher feedback collection rates after implementing effective streaming compared to traditional 'wait and display' approaches. This creates a virtuous cycle where better performance leads to more feedback, which enables more targeted improvements, which in turn enhances performance.\"</p> <pre><code>sequenceDiagram\n    participant User\n    participant Frontend\n    participant Backend\n    participant Retriever\n    participant Generator\n\n    User-&gt;&gt;Frontend: Submits query\n    Frontend-&gt;&gt;Backend: Sends query\n    Note over Frontend: Shows \"Thinking...\" animation\n\n    Backend-&gt;&gt;Retriever: Requests relevant documents\n    Retriever-&gt;&gt;Backend: Returns documents\n    Note over Backend: Documents retrieved\n\n    Backend-&gt;&gt;Generator: Generates response with documents\n    Note over Frontend: Shows \"Generating response...\"\n\n    loop Streaming\n        Generator-&gt;&gt;Backend: Streams token chunks\n        Backend-&gt;&gt;Frontend: Forwards token chunks\n        Frontend-&gt;&gt;User: Displays incremental response\n    end\n\n    Note over Frontend: Full response displayed</code></pre> <p>This sequence diagram illustrates how streaming transforms the user experience from a binary \"waiting/complete\" pattern to a continuous flow of information. Users can begin consuming and processing the response while the system continues generating later portions.</p>"},{"location":"workshops/chapter3-2/#technical-implementation-of-streaming","title":"Technical Implementation of Streaming","text":"<p>Implementing streaming requires coordination across your entire stack:</p> <ol> <li>A generation endpoint that supports streaming</li> <li>Backend routes that maintain open connections</li> <li>Frontend components that render incremental updates</li> </ol> <p>Implementation Timing</p> <p>If you're on the fence about implementing streaming, do it early. Migrating from a non-streaming to a streaming application is significantly more complex than building it from the start. It can add weeks to your development cycle if attempted later in the project lifecycle.</p> <p>Most modern language models and APIs support streaming, though the specific implementation varies. The effort is well worth it - side-by-side comparisons show dramatically improved user experience, with streaming responses feeling much more responsive than waiting for complete responses:</p> <pre><code># Example using OpenAI's API for streaming\nimport openai\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import StreamingResponse\nimport asyncio\n\napp = FastAPI()\n\n@app.post(\"/query/stream\")\nasync def stream_query_response(request: Request):\n    \"\"\"\n    Stream a response to a user query.\n\n    This endpoint:\n    1. Processes the incoming query\n    2. Retrieves relevant documents\n    3. Streams the generated response\n    \"\"\"\n    # Parse the incoming request\n    data = await request.json()\n    query = data.get(\"query\")\n\n    # Retrieve relevant documents (non-streaming part)\n    documents = retrieve_documents(query)\n    context = prepare_context(documents)\n\n    # Set up streaming response\n    async def event_generator():\n        # Create a streaming completion\n        response = await openai.ChatCompletion.acreate(\n            model=\"gpt-4\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                {\"role\": \"user\", \"content\": f\"Query: {query}\\n\\nContext: {context}\"}\n            ],\n            stream=True  # Enable streaming\n        )\n\n        # Yield chunks as they arrive\n        async for chunk in response:\n            if chunk.choices[0].delta.get(\"content\"):\n                yield f\"data: {chunk.choices[0].delta.content}\\n\\n\"\n            await asyncio.sleep(0.01)  # Small delay to control flow rate\n\n        yield \"data: [DONE]\\n\\n\"\n\n    # Return a streaming response\n    return StreamingResponse(\n        event_generator(),\n        media_type=\"text/event-stream\"\n    )\n</code></pre> <p>On the frontend, you'll need to handle Server-Sent Events (SSE) or WebSockets to receive and display the streamed content:</p> <p>Frontend Streaming Implementation</p> <p></p> <pre><code>*This code shows how to handle streaming responses on the frontend, creating a reader for the response stream, decoding chunks as they arrive, and updating the UI in real-time to display incremental results.*\n</code></pre>"},{"location":"workshops/chapter3-2/#showing-function-call-arguments","title":"Showing Function Call Arguments","text":"<p>One unique advantage of streaming is the ability to show users not just the final response but also the thinking and processing that led to it. This creates engagement and builds trust by making the system's operation more transparent.</p> <p>For example, you can stream the function calls and arguments that your RAG system is using:</p> <p>This approach gives users insight into how their query is being processed, creating engagement during what would otherwise be idle waiting time.</p>"},{"location":"workshops/chapter3-2/#streaming-structured-data","title":"Streaming Structured Data","text":"<p>Streaming isn't limited to plain text\u2014you can stream structured data like citations, follow-up questions, or data visualizations. This technique is especially valuable for complex RAG applications where responses have multiple components.</p> <p>Streaming in Modern Applications</p> <p>Libraries like Instruct and modern LLM frameworks now support streaming structured data. This allows applications to:</p> <pre><code>- Stream citations with IDs and titles\n- Stream different response components in parallel\n- Stream function calls and their arguments \n- Build dynamic UI that renders each component as it becomes available\n</code></pre> <p>Here's how you might implement structured streaming for a response that includes an answer, citations, and follow-up questions:</p> <pre><code>async def stream_structured_response(query: str):\n    \"\"\"\n    Stream a structured response with multiple components.\n\n    Parameters:\n    - query: The user's question\n\n    Returns:\n    - A streaming response with structured components\n    \"\"\"\n    # Retrieve documents (non-streaming)\n    documents = retrieve_documents(query)\n\n    # Start streaming response components\n    async def generate_stream():\n        # Send response type indicator\n        yield json.dumps({\"type\": \"start\", \"components\": [\"answer\", \"citations\", \"followup\"]}) + \"\\n\"\n\n        # Stream the answer generation\n        answer_chunks = generate_answer_stream(query, documents)\n        async for chunk in answer_chunks:\n            yield json.dumps({\"type\": \"answer\", \"content\": chunk}) + \"\\n\"\n            await asyncio.sleep(0.02)\n\n        # Stream citations after the answer\n        citations = extract_citations(documents)\n        for citation in citations:\n            yield json.dumps({\n                \"type\": \"citation\",\n                \"id\": citation[\"id\"],\n                \"title\": citation[\"title\"],\n                \"text\": citation[\"text\"][:100] + \"...\",\n                \"relevance\": citation[\"relevance\"]\n            }) + \"\\n\"\n            await asyncio.sleep(0.05)\n\n        # Generate and stream follow-up questions\n        followups = generate_followup_questions(query, documents)\n        yield json.dumps({\"type\": \"followup\", \"questions\": followups}) + \"\\n\"\n\n        # Signal completion\n        yield json.dumps({\"type\": \"end\"}) + \"\\n\"\n\n    return StreamingResponse(generate_stream(), media_type=\"application/json\")\n</code></pre> <p>On the frontend, you'd handle this structured stream by updating different UI components based on the message type:</p> <p>Structured Data Streaming Handler</p> <p></p> <pre><code>*This code processes a structured data stream, separating different components (answer chunks, citations, follow-up questions) and rendering each in their appropriate UI sections. This creates a dynamic, engaging experience where different parts of the response appear progressively.*\n</code></pre> <p>This approach creates a dynamic, engaging experience where different parts of the response appear progressively, keeping users engaged throughout the generation process.</p>"},{"location":"workshops/chapter3-2/#meaningful-interstitials-making-waiting-engaging","title":"Meaningful Interstitials: Making Waiting Engaging","text":"<p>For situations where some processing must happen before any content can be displayed, well-designed interstitials can transform waiting time from a frustrating experience into an engaging one.</p> <p>The key principle is to make interstitials meaningful rather than generic. Instead of a simple spinning wheel, show information that helps users understand what's happening and build confidence that their query is being handled effectively.</p>"},{"location":"workshops/chapter3-2/#skeleton-screens-the-illusion-of-progress","title":"Skeleton Screens: The Illusion of Progress","text":"<p>Skeleton screens are placeholder UI elements that mimic the structure of content while it loads. Unlike traditional spinners or progress bars, they create the impression that content is almost ready by showing its outline.</p> <p>Facebook's Discovery</p> <p>Facebook's user experience research discovered that skeleton screens significantly reduced perceived load times, resulting in better user retention and engagement. Users reported that the experience \"felt faster\" even when actual load times were identical to spinner-based approaches.</p> <p>Skeleton screens work because they:</p> <ol> <li>Set clear expectations about what content is loading</li> <li>Provide a sense of progress without requiring actual progress data</li> <li>Create the impression that the system is actively working on the request</li> <li>Give users visual stimulation during the waiting period</li> </ol> <p>For RAG applications, skeleton screens can be particularly effective when showing:</p> <ul> <li>The structure of the answer before content loads</li> <li>Citation placeholders that will be filled</li> <li>Follow-up question button outlines</li> <li>Tool usage summaries that will appear</li> </ul> <p>Meaningful vs. Generic Interstitials</p> <p>Generic Interstitial: \"Loading...\"</p> <pre><code>**Meaningful Interstitial:**\n\"Searching 382,549 documents in our knowledge base...\"\n\"Finding relevant precedent cases from 2021-2022...\"\n\"Analyzing 3 legal frameworks that might apply to your question...\"\n</code></pre> <p>Meaningful interstitials should:</p> <ol> <li>Be specific about what the system is doing</li> <li>Include actual metrics when possible (number of documents, etc.)</li> <li>Update dynamically to show progress</li> <li>Maintain a confident, authoritative tone</li> </ol> <p>Here's how you might implement meaningful interstitials:</p> <pre><code>async def generate_interstitials(query: str):\n    \"\"\"\n    Generate meaningful interstitial messages for a query.\n\n    Parameters:\n    - query: The user's question\n\n    Returns:\n    - A sequence of interstitial messages\n    \"\"\"\n    # Analyze the query to determine appropriate interstitials\n    category = classify_query(query)\n\n    # Define category-specific interstitials\n    interstitials = {\n        \"technical\": [\n            \"Scanning documentation and code repositories...\",\n            \"Identifying relevant code examples and patterns...\",\n            \"Analyzing technical specifications and requirements...\",\n        ],\n        \"legal\": [\n            \"Searching legal databases and precedents...\",\n            \"Reviewing relevant case law and statutes...\",\n            \"Analyzing jurisdictional applicability...\",\n        ],\n        \"medical\": [\n            \"Consulting medical literature and guidelines...\",\n            \"Reviewing clinical studies and research papers...\",\n            \"Analyzing treatment protocols and best practices...\",\n        ],\n        # Add other categories as needed\n    }\n\n    # Add domain-specific metrics if available\n    try:\n        # For technical queries, add repository info\n        if category == \"technical\":\n            repo_count = get_repository_count()\n            interstitials[\"technical\"].append(f\"Searching across {repo_count} code repositories...\")\n\n        # For legal queries, add document counts\n        elif category == \"legal\":\n            case_count = get_case_count()\n            interstitials[\"legal\"].append(f\"Analyzing {case_count} potentially relevant cases...\")\n    except:\n        # Fall back to generic but still domain-specific messages\n        pass\n\n    # Get the relevant list based on category, or use default\n    message_list = interstitials.get(category, [\n        \"Processing your query...\",\n        \"Searching for relevant information...\",\n        \"Analyzing related documents...\"\n    ])\n\n    # Return the message list\n    return message_list\n</code></pre> <p>On the frontend, you'd display these interstitials in sequence during the waiting period:</p> <p>Meaningful Interstitials Implementation</p> <p></p> <pre><code>*This code shows how to fetch and display domain-specific interstitial messages that rotate every few seconds. The animation and context-specific messages engage users during waiting time, making the system feel more responsive.*\n</code></pre>"},{"location":"workshops/chapter3-2/#optimizing-actual-performance","title":"Optimizing Actual Performance","text":"<p>While perceived performance is critical, we shouldn't neglect actual performance optimizations. Here are several strategies for reducing real latency in RAG applications:</p>"},{"location":"workshops/chapter3-2/#1-optimize-your-retrieval-pipeline","title":"1. Optimize Your Retrieval Pipeline","text":"<p>The retrieval phase is often the most time-consuming part of a RAG system. Consider these optimizations:</p> <ul> <li>Use approximate nearest neighbor search instead of exact search for large collections</li> <li>Implement a tiered retrieval approach that filters candidates quickly before precise ranking</li> <li>Pre-compute and cache embeddings for your document collection</li> <li>Shard your vector database to distribute search across multiple instances</li> </ul>"},{"location":"workshops/chapter3-2/#2-implement-caching","title":"2. Implement Caching","text":"<p>Caching dramatically improves performance for repeated or similar queries:</p> <ul> <li>Semantic caching: Cache results based on embedding similarity, not just exact matches</li> <li>Fragment caching: Cache individual retrieved documents even if the full query is new</li> <li>Result caching: Store complete responses for common queries</li> </ul> <p>Here's a simple implementation of semantic caching:</p>"},{"location":"workshops/chapter3-2/#3-implement-progressive-loading","title":"3. Implement Progressive Loading","text":"<p>Load different components of your response progressively, with the most important parts first:</p> <ul> <li>Show the direct answer before loading citations</li> <li>Display key findings before detailed explanations</li> <li>Show high-confidence sections before speculative ones</li> </ul>"},{"location":"workshops/chapter3-2/#4-optimize-model-usage","title":"4. Optimize Model Usage","text":"<p>Language model inference can be optimized through:</p> <ul> <li>Quantization: Use 8-bit or 4-bit quantized models where appropriate</li> <li>Distillation: Train smaller, faster models for specific query types</li> <li>Parallel inference: Process multiple documents or query components simultaneously</li> <li>Model selection: Use smaller models for simpler tasks, reserving larger models for complex reasoning</li> </ul>"},{"location":"workshops/chapter3-2/#platform-specific-implementations","title":"Platform-Specific Implementations","text":""},{"location":"workshops/chapter3-2/#streaming-in-slack-bots","title":"Streaming in Slack Bots","text":"<p>Implementing streaming in a Slack bot environment presents unique challenges and opportunities. While Slack doesn't support true streaming in the same way as a web interface, you can create the illusion of progress and responsiveness through careful interaction design.</p> <p>Here's a simple but effective approach for Slack bots:</p> <ol> <li> <p>Initial Acknowledgment: React with the \ud83d\udc40 emoji immediately when receiving a message to indicate that the bot has seen the request and is processing it.</p> </li> <li> <p>Progress Updates: Use message updates or threading to show progress, such as:</p> </li> </ol> <pre><code>Searching through knowledge base...\nFound 5 relevant documents...\nGenerating response...\n</code></pre> <ol> <li> <p>Completion Indicator: Mark the message with a \u2705 emoji when the response is complete.</p> </li> <li> <p>Feedback Collection: Pre-fill emoji reactions (\ud83d\udc4d \ud83d\udc4e \u2b50) to prompt users for feedback on the response quality.</p> </li> </ol> <p>Slack Bot Pseudo-Streaming Implementation</p> <p></p> <pre><code>*This code shows how to implement pseudo-streaming in a Slack bot environment, using message updates, emoji reactions, and staged processing to create the illusion of progress and maintain user engagement.*\n</code></pre> <p>Slack Feedback Collection</p> <p>By pre-filling emoji reactions (\ud83d\udc4d \ud83d\udc4e \u2b50), you dramatically increase the likelihood of receiving user feedback. This approach places feedback options directly in the user's view, rather than requiring them to take additional steps. In testing, this approach increased feedback collection rates by up to 5x compared to text-based feedback prompts.</p>"},{"location":"workshops/chapter3-2/#the-connection-between-streaming-performance-and-feedback","title":"The Connection Between Streaming, Performance, and Feedback","text":"<p>The techniques discussed in this chapter aren't just about improving user experience\u2014they directly strengthen the feedback collection mechanisms we established in Chapter 3.1.</p> <p>Research consistently shows that users provide more feedback when systems feel responsive and engaging. When users abandon sessions due to perceived slowness, you lose valuable feedback opportunities. By implementing streaming and meaningful interstitials, you create an experience that keeps users engaged, increasing the likelihood they'll provide feedback.</p> <p>In our experience, implementations with effective streaming collect 30-40% more feedback compared to traditional \"wait and display\" approaches. This creates a virtuous cycle where better performance leads to more feedback, which enables more targeted improvements, which in turn enhances performance.</p> <p>The most successful RAG applications aren't just accurate\u2014they're responsive, engaging, and transparent. By applying the techniques in this chapter, you create an experience that keeps users engaged throughout the interaction, building trust and encouraging the feedback that fuels continuous improvement.</p> <p>Real-world Impact</p> <p>\"For a customer support RAG application, implementing streaming and feedback-optimized interstitials increased our feedback collection rate from 5.6% to over 25%. This allowed us to fine-tune five times faster and quickly identify the most problematic query types. Within six weeks, we improved customer satisfaction scores by 34% by addressing these specific failure modes.\"</p>"},{"location":"workshops/chapter3-2/#conclusion-performance-as-experience-design","title":"Conclusion: Performance as Experience Design","text":"<p>Throughout this chapter, we've explored how to overcome latency through a combination of streaming responses, meaningful interstitials, skeleton screens, platform-specific implementations, and technical optimizations. The key insight is that performance isn't just a technical concern\u2014it's a fundamental aspect of experience design that directly impacts your feedback collection rates.</p> <p>By implementing streaming, you transform the user experience from a binary \"waiting/complete\" pattern to a continuous flow of information. With skeleton screens, you set clear expectations about what content is loading. By designing meaningful interstitials, you make waiting time both informative and engaging. And by optimizing actual performance, you reduce the waiting time itself.</p> <p>These approaches work in concert to create a responsive, engaging RAG experience that keeps users invested and encourages feedback. The research is clear: users provide up to 5x more feedback when your application feels responsive and engaging. This creates a powerful feedback flywheel where better performance leads to more feedback, which enables more targeted improvements, which in turn enhances performance further.</p> <p>Implementation Priority</p> <p>If you're at the start of your RAG implementation journey, prioritize streaming first. It's much easier to integrate from the beginning than to retrofit later. Next, focus on meaningful interstitials and skeleton screens. Finally, implement platform-specific optimizations for your particular usage context (web, Slack, mobile, etc.).</p> <p>In the next chapter, we'll build on these foundations by exploring quality-of-life improvements like interactive citations, chain-of-thought reasoning, and validation patterns. These elements further enhance the user experience while creating additional opportunities for feedback collection.</p>"},{"location":"workshops/chapter3-2/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>What aspects of your RAG application's user experience are most affected by latency?</p> </li> <li> <p>How could you modify your current interface to show meaningful progress during retrieval and generation?</p> </li> <li> <p>What information could you stream incrementally to improve perceived performance?</p> </li> <li> <p>Which components of your RAG pipeline are the biggest contributors to actual latency? How might you optimize them?</p> </li> <li> <p>How would implementing streaming affect your feedback collection mechanisms?</p> </li> <li> <p>Is your feedback collection UI too subtle? How could you improve its visibility and clarity?</p> </li> <li> <p>How might you implement skeleton screens in your particular application context?</p> </li> <li> <p>If your application runs on platforms like Slack or Teams, what platform-specific techniques could you use to improve perceived latency?</p> </li> <li> <p>How could you use interstitials to educate users about your system's capabilities and build trust?</p> </li> <li> <p>What metrics would you track to measure the impact of your latency improvements on user satisfaction and feedback collection?</p> </li> </ol>"},{"location":"workshops/chapter3-2/#summary","title":"Summary","text":"<p>Latency is a critical challenge in RAG applications that directly impacts both user experience and feedback collection rates. In this chapter, we've explored a comprehensive approach to overcoming latency challenges:</p> <p>Streaming responses transform waiting into an engaging experience where users see answers unfold in real time, dramatically improving perceived performance and user engagement. Research shows that streaming can increase feedback collection rates by 30-40% compared to traditional approaches.</p> <p>Skeleton screens create the illusion of progress by showing content outlines before the actual content loads. Companies like Facebook have found that skeleton screens significantly reduce perceived load times and improve user retention.</p> <p>Meaningful interstitials make necessary waiting periods informative and less frustrating by communicating what's happening behind the scenes. Well-designed interstitials can make perceived wait times up to 40% shorter than actual wait times.</p> <p>Platform-specific implementations like Slack bots with emoji reactions can create pseudo-streaming experiences and dramatically increase feedback collection, with pre-filled emoji reactions driving up to 5x more feedback.</p> <p>These techniques, combined with actual performance optimizations like caching and progressive loading, create RAG applications that feel responsive and trustworthy even when complex processing is occurring. The result is not just better user experience but also significantly more feedback, fueling a powerful continuous improvement cycle.</p> <p>Remember: If you only implement one improvement from this chapter, make it streaming. It's substantially easier to build streaming from the start than to retrofit it later, and it has the biggest impact on both perceived performance and feedback collection rates.</p>"},{"location":"workshops/chapter3-2/#additional-resources","title":"Additional Resources","text":"<ol> <li> <p>Nielsen Norman Group, \"Progress Indicators Make a Slow System Less Insufferable\" - Research on how progress indicators affect perceived wait times</p> </li> <li> <p>Google Developers, \"Measuring Perceived Performance\" - Metrics and techniques for measuring how users perceive application performance</p> </li> <li> <p>OpenAI Documentation, \"Streaming API Best Practices\" - Implementation details for streaming with OpenAI models</p> </li> <li> <p>GitHub Repository: Streaming-RAG-Implementation - Example implementation of a streaming RAG application</p> </li> <li> <p>Facebook Engineering, \"Building Skeleton Screens\" - Facebook's approach to implementing skeleton screens for improved perceived performance</p> </li> <li> <p>Anthropic Structured Outputs Guide - Guide for generating structured data with Claude that can be streamed incrementally</p> </li> <li> <p>Slack API Documentation, \"Adding Reactions to Messages\" - How to programmatically add emoji reactions to messages for feedback collection</p> </li> <li> <p>Article: \"The Psychology of Waiting Lines\" - David Maister's research on the psychological aspects of waiting</p> </li> <li> <p>GitHub Repository: React Skeleton Screens - Open-source library for implementing skeleton screens in React applications</p> </li> </ol> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>"},{"location":"workshops/chapter3-3/","title":"3.3 Quality of Life Improvements: Citations, Chain of Thought, and Validation","text":"<p>Chapter Overview</p> <pre><code>This chapter explores essential quality-of-life improvements that transform RAG systems from functional to exceptional. You'll learn how to implement interactive citations that build trust while collecting valuable feedback, techniques for making reasoning transparent through chain-of-thought approaches, and validation methods that catch errors before they reach users. These enhancements not only improve immediate user experience but also strengthen your feedback flywheel by creating additional opportunities for collecting insights. By implementing these techniques, you'll create a RAG system that users genuinely enjoy using\u2014one that explains its reasoning, justifies its answers, and demonstrates reliability through self-correction.\n</code></pre>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#introduction-from-functional-to-exceptional","title":"Introduction: From Functional to Exceptional","text":"<p>In the previous two chapters, we established critical foundations for successful RAG applications: robust feedback collection mechanisms in Chapter 3.1 and techniques to overcome latency in Chapter 3.2. Now we're ready to elevate our system from merely functional to truly exceptional by addressing the subtle but powerful elements that transform user experience.</p> <p>Think of a RAG system as similar to a home. The feedback mechanisms are like the foundation\u2014essential but invisible to most visitors. The streaming capabilities are like the utilities\u2014noticed primarily when they're not working well. What we'll cover in this chapter are the finishing touches that visitors actually see and interact with directly: the paint, the furniture, the artwork\u2014the elements that transform a house into a home.</p> <p>These \"quality of life\" improvements may seem like optional enhancements, but they often make the difference between systems that are occasionally useful and those that become indispensable tools that users rely on daily. They build trust through transparency, improve reasoning through explicit thinking processes, and prevent errors before they reach users.</p> <p>Industry Insight</p> <p>\"These quality of life improvements represent massively missed opportunities for many RAG teams. Implementing chain of thought in ways that matter to your business has been one of the highest-impact changes we've seen, consistently producing a 10% bump in performance. That might not sound dramatic, but it's often the difference between something that's usable and something that's impossible to deploy in production.\"</p> <p>In this chapter, we'll explore three categories of improvements:</p> <ol> <li>Citations: How to transform static references into interactive elements that build trust while providing valuable feedback signals</li> <li>Chain of Thought: Techniques to make reasoning transparent, improving both accuracy and user confidence</li> <li>Validation: Methods to catch errors before they reach users, creating more reliable experiences</li> </ol> <p>Each of these approaches not only enhances immediate user experience but also strengthens the feedback flywheel we've been building throughout these chapters. By implementing these techniques, you'll create a RAG system that users not only tolerate but genuinely enjoy using\u2014a system that explains its reasoning, justifies its answers, and catches its own mistakes.</p> <p>Real-world Impact</p> <p>One healthcare company implementing the techniques in this chapter saw their user satisfaction scores increase by 34% in just six weeks. More importantly, their user trust metrics\u2014measuring how much users believed and acted on the system's recommendations\u2014increased by 62%. This wasn't just about making users happy; it fundamentally changed how their system influenced real-world decisions.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#beyond-the-basics-elevating-user-experience","title":"Beyond the Basics: Elevating User Experience","text":"<p>When I first started working with RAG systems, I viewed success primarily through the lens of retrieval accuracy and response relevance. If a system could find the right information and generate a coherent answer, I considered it a job well done. But as I gained experience deploying these systems to real users, I realized that truly exceptional RAG applications go beyond these basic capabilities to deliver experiences that build trust, showcase reasoning, and prevent errors before they happen.</p> <p>In this chapter, we'll explore what I've come to think of as \"quality of life improvements\"\u2014techniques that transform a technically sound RAG system into a delightful user experience. These approaches don't necessarily improve retrieval or generation in a fundamental way, but they dramatically enhance how users perceive and interact with your system. And as we'll see, many of these techniques create valuable opportunities for additional feedback collection, strengthening the flywheel we established in Chapter 3.1.</p> <p>After implementing the feedback collection mechanisms from Chapter 3.1 and the streaming techniques from Chapter 3.2, you've already built a solid foundation. Now we'll add the finishing touches that create a truly exceptional experience.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#citations-building-trust-through-transparency","title":"Citations: Building Trust Through Transparency","text":"","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#the-dual-purpose-of-citations","title":"The Dual Purpose of Citations","text":"<p>When I first started working with RAG systems, I viewed citations as primarily a trust-building mechanism\u2014a way to show users that the system's responses were grounded in actual documents rather than fabricated. While this is certainly valuable, I've come to realize that citations serve an equally important purpose: they're golden opportunities for feedback collection.</p> <p>Think about it this way. When a user sees a citation in a response, they're naturally curious about the source. By making citations interactive and engaging, you create touchpoints for feedback that feel natural and contextual. This isn't just about slapping a thumbs-up button on your interface; it's about integrating feedback collection into the core user experience.</p> <p>The most effective approach turns citations from static references into interactive elements that users can engage with:</p> <ol> <li>Quote different parts of responses and visually link them to specific citations</li> <li>Allow users to expand citations to review the full context</li> <li>Enable users to provide feedback on individual citations</li> <li>Let users remove irrelevant citations and request regeneration</li> </ol> <pre><code>graph TD\n    A[Generated Response] --&gt;|Contains| B[Interactive Citations]\n    B --&gt;|User Expands| C[Citation Content]\n    B --&gt;|User Marks Relevant| D[Positive Training Example]\n    B --&gt;|User Marks Irrelevant| E[Negative Training Example]\n    B --&gt;|User Removes| F[Regeneration Request]\n\n    style B fill:#f9d77e,stroke:#333,stroke-width:2px</code></pre> <p>I worked with a legal research team that implemented this approach for their in-house attorneys. Each generated response included interactive citations linked to specific case law or statutes. Attorneys could click on citations to see the full context, and importantly, they could mark citations as relevant or irrelevant to their query. When they marked a citation as irrelevant, the system would regenerate the response without using that source.</p> <p>This interaction served two purposes: it immediately improved the user experience by removing unhelpful information, and it generated invaluable training data for our retrieval system. Each marked citation became labeled data that helped us fine-tune our embedding models. Within three months, we had collected over 50,000 labeled examples\u2014a dataset that would have been prohibitively expensive to create manually.</p> <p>Citations as UI Elements</p> <p>Design your citations not just as references but as interactive UI elements. When users can explore, evaluate, and modify citations, they become active participants in improving your system rather than passive consumers of information.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#crafting-citation-rich-responses","title":"Crafting Citation-Rich Responses","text":"<p>Creating effective citations begins with how you prompt your language model. Instead of treating citations as an afterthought, build them into your response generation process from the ground up.</p> <p>Here's a prompt template that encourages detailed, well-structured citations:</p> <pre><code>def create_citation_prompt(query: str, documents: list):\n    \"\"\"\n    Create a prompt that encourages detailed citation usage.\n\n    Parameters:\n    - query: The user's question\n    - documents: Retrieved documents for context\n\n    Returns:\n    - A structured prompt that will generate well-cited responses\n    \"\"\"\n    # Format document context with identifiers\n    formatted_docs = []\n    for i, doc in enumerate(documents):\n        formatted_docs.append(f\"DOCUMENT [{i+1}]: {doc.title}\\n{doc.content}\")\n\n    context = \"\\n\\n\".join(formatted_docs)\n\n    prompt = f\"\"\"\n    Answer the following question based ONLY on the provided documents.\n    For each piece of information in your answer, include a citation to the specific document it came from using the format [X] where X is the document number.\n\n    If the documents don't contain enough information to fully answer the question, say so clearly and cite which documents you used for the partial answer.\n\n    At the end of your answer, include a \"Sources\" section that lists all the documents you cited.\n\n    QUESTION: {query}\n\n    DOCUMENTS:\n    {context}\n\n    ANSWER (with citations):\n    \"\"\"\n\n    return prompt\n</code></pre> <p>On the frontend, you can transform these citations into interactive elements:</p> <p>Interactive Citations Rendering</p> <p></p> <pre><code>*This code demonstrates how to transform a response with citation markers into an interactive UI where citations are clickable elements, and sources can be rated for relevance.*\n</code></pre> <p>This creates an interactive experience where citations are visually distinct, clickable elements. When users engage with these elements, you can collect valuable feedback while enhancing their understanding of the response.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#chain-of-thought-making-thinking-visible","title":"Chain of Thought: Making Thinking Visible","text":"","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#the-underutilized-superpower","title":"The Underutilized Superpower","text":"<p>One of the most underutilized yet powerful techniques for improving RAG responses is chain of thought prompting\u2014asking the model to reason step by step before providing its final answer. This approach typically provides a 10% performance improvement for classification and reasoning tasks, which might sound modest until you realize it's often the difference between a system that's occasionally helpful and one that's consistently reliable.</p> <p>Expert Insight</p> <p>\"Chain of thought is a massively missed opportunity for many RAG teams. With the advent of models like Claude 3 Opus and GPT-4o, we know this approach is a game-changer for performance. Even without these advanced models, implementing chain of thought in ways that matter to your business has consistently been one of the highest-impact improvements we've seen.\"</p> <p>I've found chain of thought particularly valuable for complex retrieval tasks where multiple documents need to be synthesized or where subtle judgments about relevance are required. By making the reasoning explicit, you can identify where things might be going wrong and provide more targeted guidance.</p> <p>Performance Impact</p> <p>In our testing across multiple domains, chain of thought prompting consistently improved answer accuracy by 8-15%, with the biggest gains coming in complex reasoning scenarios like multi-hop questions and comparative analyses. This improvement can be the difference between a system that's deployable in production versus one that fails to meet quality thresholds.</p> <p>When implementing chain of thought, structure it clearly to separate the thinking process from the final response. XML tags work well for this purpose, creating distinct sections that can be processed differently by your application.</p> <p>Chain of thought also serves another purpose: it can become an engaging loading interstitial. By streaming the reasoning process, you transform waiting time into a transparent window into how the system is working through the problem, building both engagement and trust.</p> <pre><code>def chain_of_thought_prompt(query: str, documents: list):\n    \"\"\"\n    Create a prompt that encourages step-by-step reasoning.\n\n    Parameters:\n    - query: The user's question\n    - documents: Retrieved documents for context\n\n    Returns:\n    - A prompt that will generate reasoning steps and a final answer\n    \"\"\"\n    context = \"\\n\\n\".join([f\"DOCUMENT: {doc.content}\" for doc in documents])\n\n    prompt = f\"\"\"\n    You will answer the user's question based on the provided documents.\n    First, think step by step about how to answer the question using the documents.\n    Then provide your final answer.\n\n    Structure your response like this:\n    &lt;thinking&gt;\n    Your step-by-step reasoning process here...\n    &lt;/thinking&gt;\n\n    &lt;answer&gt;\n    Your final answer here, with citations to specific documents...\n    &lt;/answer&gt;\n\n    USER QUESTION: {query}\n\n    DOCUMENTS:\n    {context}\n    \"\"\"\n\n    return prompt\n</code></pre> <p>Taking this a step further, you can stream the thinking process as a separate UI component or interstitial. This serves two purposes: it makes the waiting time more engaging by showing users that complex reasoning is happening, and it allows users to intervene if they notice the reasoning going astray.</p> <p>Chain of Thought Streaming Implementation</p> <p></p> <pre><code>*This code processes streamed tokens containing XML-tagged thinking and answer sections, rendering them in separate UI components. This makes the reasoning process transparent and engaging for users.*\n</code></pre> <p>I worked with a financial advisory firm that implemented this approach for their investment recommendation system. As the model reasoned through market conditions, client preferences, and portfolio considerations, this thinking was streamed to the advisor in a separate panel. If the advisor noticed a misunderstanding or faulty assumption in the reasoning, they could pause generation and refine their query before a final recommendation was produced.</p> <p>This interactive approach not only improved recommendation quality but also created a valuable feedback loop where advisors could correct misunderstandings early in the process. Each correction became training data that helped the system learn and improve over time.</p> <p>On the frontend, you can implement this with an expandable \"See reasoning\" section that users can toggle to view the model's step-by-step analysis. This transparency builds trust by demystifying the AI process and gives users insight into how conclusions were reached.</p> <p>The beauty of chain of thought isn't just that it improves immediate response quality\u2014it also creates a more explainable, trustworthy system that users feel comfortable relying on. In domains where decisions matter and consequences are real, this transparency can be the difference between a system that's used occasionally and one that becomes an indispensable tool.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#monologues-solving-the-context-management-problem","title":"Monologues: Solving the Context Management Problem","text":"","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#reasoning-in-limited-windows","title":"Reasoning in Limited Windows","text":"<p>As context windows grow larger, one might think that managing complex information would become easier. Counterintuitively, though, larger context windows often create new challenges for language models, which can struggle to attend to the most relevant information among thousands of tokens.</p> <p>Monologuing\u2014having the model explicitly reiterate key information before generating a response\u2014has emerged as a powerful technique to enhance reasoning and quality, especially with large contexts and complex documents.</p> <p>Key Insight</p> <p>When dealing with very long contexts, language models often struggle with recall and fully processing all instructions. By having the model monologue - explicitly reiterate key information before answering - we reorganize the context in a way that allows the model to effectively \"re-read\" the prompt, improving reasoning dramatically without complex architectural changes.</p> <p>The process is wonderfully simple: ask the model to \"think out loud\" about what information is relevant before generating the final answer. This serves several purposes:</p> <ol> <li>It helps the model re-read and reinforce important context</li> <li>It allows the model to organize scattered information into a coherent structure</li> <li>It creates natural separation between reasoning and response</li> <li>It produces valuable data for future fine-tuning</li> <li>It can replace more complex multi-stage agents for many use cases</li> <li>It can improve consistency by ensuring the model considers all relevant factors</li> </ol> <p>Monologues have proven particularly effective at replacing complex agent architectures for many use cases. Rather than building intricate multi-stage processes, you can often achieve similar results with a single thoughtfully constructed monologue prompt, saving both development time and computational resources.</p> <p>Here's an example prompt for implementing monologues:</p> <pre><code>def monologue_prompt(query: str, documents: list, pricing_data: str):\n    \"\"\"\n    Create a prompt that encourages monologuing for improved comprehension.\n\n    Parameters:\n    - query: The user's question about pricing options\n    - documents: Relevant call transcripts or customer information\n    - pricing_data: Pricing documentation and guidelines\n\n    Returns:\n    - A prompt that will generate a structured monologue before answering\n    \"\"\"\n    context = \"\\n\\n\".join([f\"TRANSCRIPT: {doc.content}\" for doc in documents])\n\n    prompt = f\"\"\"\n    You'll help generate a pricing quote based on the call transcript and pricing documentation.\n\n    First, reiterate the key variables that determine pricing options according to the documentation.\n    Then, identify specific parts of the transcript that relate to these variables.\n    Next, determine which pricing options from the documentation are most relevant.\n    Finally, provide a recommended pricing quote with justification.\n\n    QUESTION: {query}\n\n    TRANSCRIPT:\n    {context}\n\n    PRICING DOCUMENTATION:\n    {pricing_data}\n\n    MONOLOGUE AND ANSWER:\n    \"\"\"\n\n    return prompt\n</code></pre> <p>I want to share a specific case study that demonstrates the power of monologues. We were working with a SaaS company that needed to generate pricing quotes based on sales call transcripts and a complex pricing document. The initial approach\u2014simply providing the transcript and pricing document as context\u2014resulted in quotes that were often inconsistent or missed key information from the transcript.</p> <p>Monologue Implementation</p> <p>Rather than building a complex multi-stage agent, we found it was much more effective to simply cache-prompt the pricing data and inject a single call transcript with specific monologue instructions.</p> <p>We implemented a monologue-based approach that instructed the model to:</p> <ol> <li>First reiterate the variables that determine pricing options</li> <li>Then identify relevant parts of the transcript that mention these variables</li> <li>Next determine which pricing tiers and options apply</li> <li>Finally generate the actual quote with justification</li> </ol> <p>The results were remarkable\u2014quote accuracy improved from 62% to 94%, and sales representatives reported much greater confidence in the system's recommendations. The monologue approach helped the model connect scattered information from the transcript with the precise rules in the pricing document, creating much more coherent reasoning.</p> <p>Business Impact</p> <p>After implementing this approach, 90% of the follow-up emails were accepted by sales reps without any edits. This generated a wealth of training data that we used to fine-tune a smaller, more efficient model. Eventually, the fine-tuned model achieved similar accuracy without needing the explicit monologue step, demonstrating how monologues can be used as a stepping stone to more efficient solutions.</p> <p>Monologue in Action</p> <p>Query: What pricing should we offer based on this call transcript?</p> <pre><code>**Monologue:**\nLet me identify the key pricing variables from our documentation:\n1. Number of users (determines tier)\n2. Required features (basic, professional, enterprise)\n3. Length of contract commitment (monthly vs. annual)\n4. Add-on options (API access, premium support)\n\nFrom the transcript, I can see:\n- The customer mentioned needing \"around 50 user seats\" (line 127)\n- They specifically asked about the \"workflow automation feature\" (line 213)\n- They stated \"we're looking for an annual commitment\" (line 289)\n- They expressed interest in \"API access for integration\" (line 342)\n\nAccording to our pricing documentation:\n- 50 users falls into the Medium Business tier ($30/user/month)\n- Workflow automation is part of the Professional plan\n- Annual commitment provides a 20% discount\n- API access is a $500/month add-on\n\n**Answer:**\nBased on the call transcript, I recommend offering the Professional plan with API access add-on:\n- 50 users \u00d7 $30/user/month = $1,500/month\n- API access add-on: $500/month\n- Subtotal: $2,000/month\n- Annual commitment (20% discount): $19,200/year\n\nThis aligns with their needs for workflow automation and API access while providing the annual discount they're expecting.\n</code></pre> <p>This example shows how monologues can dramatically improve comprehension and reasoning, especially for complex tasks with multiple documents. The approach doesn't require any special architecture\u2014just thoughtful prompting that encourages the model to organize information before generating a response.</p> <p>Monologues can also improve tonality and quality by separating reasoning from response generation. Have the model first reason about what to say, then say it in the desired tone. This creates datasets for future fine-tuning without reasoning steps, allowing you to eventually distill the reasoning process into more efficient models.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#validation-patterns-the-safety-net-approach","title":"Validation Patterns: The Safety Net Approach","text":"","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#catching-errors-before-they-reach-users","title":"Catching Errors Before They Reach Users","text":"<p>In the early days of RAG systems, there was a tendency to treat the language model's response as the final word\u2014whatever it generated went straight to the user, for better or worse. As these systems have evolved and stakes have increased, we've learned the value of adding validation layers that catch issues before they reach users.</p> <p>Industry Perspective</p> <p>\"As language models get more sophisticated, we're finding that a single well-designed prompt combined with simple validation often outperforms complex multi-stage agent behaviors. I recommend implementing validation patterns before building elaborate agent architectures - they're simpler to deploy, easier to debug, and frequently just as effective.\"</p> <p>Think of validation patterns as safety nets for your RAG system. Just as a circus performer feels more confident with a net below, your system can be more ambitious knowing that validation checks will catch potential errors.</p> <p>Before implementing complex agent systems or multi-step pipelines, consider adding simple validation patterns to your RAG application. For latency-insensitive applications\u2014where an extra second or two of processing won't harm the user experience\u2014validators can significantly increase trust and satisfaction by ensuring responses meet quality standards.</p> <p>When to Use Validators</p> <p>Validators are particularly valuable in:</p> <pre><code>1. High-stakes domains where errors could have significant consequences\n2. Applications where users make important decisions based on system output\n3. Scenarios where specific constraints must be enforced (like valid URLs or specific data formats)\n4. Cases where you need to increase user trust in system outputs\n\nThe slight latency increase is often well worth the improved reliability and user confidence.\n</code></pre> <pre><code>sequenceDiagram\n    participant User\n    participant RAG as RAG System\n    participant Validator\n\n    User-&gt;&gt;RAG: Submits Query\n    RAG-&gt;&gt;RAG: Retrieves Documents\n    RAG-&gt;&gt;RAG: Generates Response\n    RAG-&gt;&gt;Validator: Submits Response for Validation\n\n    alt Response Passes Validation\n        Validator-&gt;&gt;RAG: Approves Response\n        RAG-&gt;&gt;User: Delivers Validated Response\n    else Response Fails Validation\n        Validator-&gt;&gt;RAG: Returns Specific Issues\n        RAG-&gt;&gt;RAG: Regenerates Response\n        RAG-&gt;&gt;Validator: Submits Revised Response\n        Validator-&gt;&gt;RAG: Approves Response\n        RAG-&gt;&gt;User: Delivers Validated Response\n    end</code></pre> <p>Validators act as a quality control layer that checks responses before they reach the user. The process is straightforward:</p> <ol> <li>Generate your reasoning, citations, and response as usual</li> <li>Pass the results to a secondary system (LLM or simple programmatic tests)</li> <li>Evaluate whether the response meets quality criteria</li> <li>If issues are found, provide specific feedback and regenerate</li> </ol> <p>I worked with a healthcare information provider that implemented a simple factual consistency validator for their patient-facing RAG system. After generating a response about treatment options, the validator would check whether all mentioned treatments were actually present in the retrieved documents and whether any contraindications or warnings had been omitted. If discrepancies were found, the response would be regenerated with specific instructions to correct the issues.</p> <p>This approach reduced factual errors by over 80% with minimal impact on latency\u2014a trade-off well worth making given the sensitive nature of healthcare information. The validator wasn't complex or expensive to implement, but it dramatically improved reliability and trustworthiness.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#a-practical-example-url-validation","title":"A Practical Example: URL Validation","text":"<p>Let me share a concrete example that illustrates the power of simple validators. I worked with a marketing team building a system to generate personalized follow-up emails that included links to case studies and marketing materials. The language model was excellent at crafting personalized messages, but we encountered a persistent problem: about 4% of generated emails contained URLs that either didn't exist or linked to internal resources that weren't publicly accessible.</p> <p>Rather than scrapping the approach or implementing a complex agent system, we added a straightforward validator that ran after response generation:</p> <pre><code>def validate_urls_in_email(email_body: str, allowed_domains: list):\n    \"\"\"\n    Validate that all URLs in an email are valid and from allowed domains.\n\n    Parameters:\n    - email_body: The generated email content\n    - allowed_domains: List of allowed domains for links\n\n    Returns:\n    - (is_valid, issues): Tuple of validation result and list of issues\n    \"\"\"\n    # Extract all URLs using regex\n    url_regex = r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n    urls = re.findall(url_regex, email_body)\n\n    issues = []\n\n    # Check each URL\n    for url in urls:\n        # Check if the domain is allowed\n        domain = urlparse(url).netloc\n        if domain not in allowed_domains:\n            issues.append(f\"URL {url} contains disallowed domain {domain}\")\n            continue\n\n        # Check if the URL exists (returns 200)\n        try:\n            response = requests.head(url, timeout=3)\n            if response.status_code != 200:\n                issues.append(f\"URL {url} returned status code {response.status_code}\")\n        except Exception as e:\n            issues.append(f\"URL {url} failed to connect: {str(e)}\")\n\n    return len(issues) == 0, issues\n\ndef regenerate_email_if_needed(query: str, initial_email: str, allowed_domains: list):\n    \"\"\"\n    Validate and potentially regenerate an email if URLs are problematic.\n    \"\"\"\n    is_valid, issues = validate_urls_in_email(initial_email, allowed_domains)\n\n    if is_valid:\n        return initial_email\n\n    # If validation failed, regenerate with specific guidance\n    issues_text = \"\\n\".join(issues)\n    regeneration_prompt = f\"\"\"\n    The previously generated email contained the following URL issues:\n    {issues_text}\n\n    Please regenerate the email, either:\n    1. Removing any problematic URLs entirely, or\n    2. Replacing them with valid URLs from these domains: {', '.join(allowed_domains)}\n\n    Original request: {query}\n    \"\"\"\n\n    regenerated_email = generate_email(regeneration_prompt)\n    return regenerated_email\n</code></pre> <p>The results were remarkable. After implementing this validator, the error rate dropped from 4% to 0% after just one retry.</p> <p>Beyond Validation: Fine-tuning from Corrections</p> <p>Even more interestingly, we took the validation process a step further. After collecting sufficient examples of corrections, we fine-tuned our model (distilling GPT-4 into a smaller model) using this dataset of corrected responses. The result was astonishing - the base error rate before validation dropped to nearly zero. The model had effectively learned from its corrections, internalizing the patterns of valid URLs and avoiding problematic ones altogether.</p> <pre><code>This entire validation and fine-tuning process took just three days to implement and resulted in a much faster application since we no longer needed the retry loop. The model now produces valid URLs in a single pass.\n</code></pre> <p>This example illustrates a broader principle: validation doesn't just catch errors\u2014it creates valuable training data that can improve your system over time. Each correction becomes a learning opportunity, gradually reducing the need for the validator itself.</p> <p>Persistent Challenges</p> <p>It's worth noting that even in early 2025, even the most advanced models can still produce hallucinated URLs when given the opportunity. Simple validators remain valuable safeguards even as models continue to improve.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#strategic-rejection-of-work","title":"Strategic Rejection of Work","text":"","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#when-i-dont-know-is-the-right-answer","title":"When \"I Don't Know\" is the Right Answer","text":"<p>One of the most overlooked strategies for improving RAG application reliability is knowing when to reject work. Rather than delaying deployment until all edge cases are solved, implement strategic rejection for scenarios where your system isn't yet strong enough. This allows you to deploy sooner while collecting data to improve problematic segments.</p> <p>Industry Insight</p> <p>\"One of the things you'll realize as you analyze your RAG system's performance is that oftentimes you can make your application much more reliable just by rejecting certain types of work. This is a massively underutilized strategy - many teams try to handle every query thrown at them rather than focusing on what they can reliably deliver.\"</p> <p>The approach is simple but powerful:</p> <ol> <li>Identify segments where performance is consistently poor</li> <li>Create rejection messages that set appropriate expectations</li> <li>Provide feedback forms to gather information about rejected queries</li> <li>Give users the option to proceed with caution if they wish</li> </ol> <p>This pattern works particularly well for specialized domains where some questions might require expertise your system hasn't yet developed. By acknowledging limitations transparently, you build trust while focusing on the areas where you can deliver value reliably.</p> <p>Rejection in Practice</p> <p>One enterprise RAG application we built for legal research would explicitly reject certain types of complex regulatory analysis questions with a message like:</p> <pre><code>\"I notice you're asking about cross-jurisdictional implications of regulation X. Currently, I'm not confident in my ability to analyze multi-jurisdictional regulatory conflicts accurately. Would you like me to instead focus on the requirements within your primary jurisdiction, or connect you with a regulatory specialist?\"\n\nThis approach was far better received than attempting answers that might contain subtle but critical errors.\n</code></pre> <pre><code>def should_reject_query(query: str, confidence_threshold: float = 0.85):\n    \"\"\"\n    Determine if a query should be politely rejected.\n\n    Parameters:\n    - query: The user's question\n    - confidence_threshold: Minimum confidence to accept the query\n\n    Returns:\n    - (should_reject, reason): Whether to reject and why\n    \"\"\"\n    # Analyze the query\n    query_category = classify_query(query)\n    query_complexity = assess_complexity(query)\n    expected_confidence = predict_confidence(query, query_category, query_complexity)\n\n    # Check against thresholds\n    if expected_confidence &lt; confidence_threshold:\n        reason = f\"This appears to be a {query_category} question with {query_complexity} complexity. \" \\\n                 f\"Based on similar questions, our confidence is {expected_confidence:.2f}, \" \\\n                 f\"which is below our threshold of {confidence_threshold:.2f}.\"\n        return True, reason\n\n    return False, None\n\ndef handle_query_with_rejection(query: str):\n    \"\"\"\n    Process a query with potential rejection if the system isn't confident.\n    \"\"\"\n    should_reject, reason = should_reject_query(query)\n\n    if should_reject:\n        return {\n            \"type\": \"rejection\",\n            \"message\": f\"I'm not confident I can answer this question accurately. {reason}\",\n            \"allow_override\": True,\n            \"feedback_requested\": True\n        }\n    else:\n        # Process normally\n        documents = retrieve_documents(query)\n        response = generate_response(query, documents)\n        return {\n            \"type\": \"answer\",\n            \"message\": response\n        }\n</code></pre> <p>Design your rejection system with precision-recall tradeoffs in mind - avoid rejecting questions you can actually answer well. The rejection should always be polite, explain the limitation, and where possible, suggest alternative approaches or questions the system can handle.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#showcasing-capabilities","title":"Showcasing Capabilities","text":"","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#guide-users-to-what-you-do-well","title":"Guide Users to What You Do Well","text":"<p>While RAG systems can theoretically answer a wide range of questions, most excel at particular types of queries. Explicitly highlighting what your system does well guides user behavior toward successful interactions.</p> <p>UX Design Insight</p> <p>\"Not all prompting should be for the language model - we should also prompt the user. People are generally lazy and often don't know exactly what they want. By giving them examples early on, you make their lives easier while showcasing capabilities they might not have known were possible.\"</p> <p>Implement these strategies to showcase your system's strengths:</p> <ul> <li>Show suggested query types that leverage your strengths</li> <li>Create UI elements that highlight special capabilities</li> <li>Provide examples of successful interactions</li> <li>Use white space to create different blocks showcasing specialized capabilities</li> </ul> <p>Perplexity provides an excellent example of this approach. Their interface showcases different capabilities (web search, academic papers, math equations) with specific UI elements, guiding users toward interactions that will be successful.</p> <p>Capability Demonstration</p> <p>When Perplexity added their \"Social\" search capability, many users didn't even know this was possible. By prominently featuring this option in the interface, they not only educated users about a new capability but also increased engagement with a feature they wanted to promote.</p> <p>By highlighting certain capabilities, you not only improve user satisfaction by focusing on strengths, but you also set appropriate expectations about what the system doesn't handle well. This creates a more predictable experience where users know what to expect.</p> <p>This approach also complements the strategic rejection strategy - when users are guided toward your strengths, they're less likely to attempt queries that would trigger rejection responses.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#putting-it-all-together-the-complete-experience","title":"Putting It All Together: The Complete Experience","text":"<p>When implemented together, these quality of life improvements create a comprehensive, trustworthy experience that elevates your RAG application above typical implementations:</p> <ol> <li>Streaming creates an engaging, responsive experience that keeps users engaged</li> <li>Citations build trust and provide opportunities for feedback collection</li> <li>Chain of thought makes reasoning transparent and improves accuracy</li> <li>Monologues enhance comprehension of complex information</li> <li>Validation catches errors before they reach users</li> <li>Strategic rejection sets appropriate expectations</li> <li>Capability showcasing guides users to successful interactions</li> </ol> <p>Each element reinforces the others, creating a system that feels polished, trustworthy, and genuinely helpful. Users don't just get answers\u2014they understand where those answers come from, see the reasoning behind them, and trust that they've been validated for accuracy.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#preparing-for-the-next-chapter","title":"Preparing for the Next Chapter","text":"<p>With these quality of life improvements in place, your RAG system now provides an exceptional user experience that builds trust, encourages engagement, and generates valuable feedback. In the next chapter, we'll explore how to make sense of all the data you're collecting through topic modeling and clustering techniques. These approaches will help you identify patterns in user queries and system performance, revealing the highest-impact opportunities for improvement.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter3-3/#conclusion-the-complete-rag-experience","title":"Conclusion: The Complete RAG Experience","text":"<p>Throughout this chapter, we've explored techniques that transform a technically sound RAG system into an exceptional user experience. Let's recap the key principles we've covered:</p> <ol> <li> <p>Interactive citations build trust and collect feedback - By making citations explorable and interactive, you simultaneously build confidence and gather valuable training signals, allowing users to delete irrelevant citations and regenerate better answers.</p> </li> <li> <p>Chain of thought reasoning improves accuracy and transparency - Making thinking visible not only leads to better answers (with a consistent 10% performance improvement) but also helps users understand how conclusions were reached, building trust in the system's outputs.</p> </li> <li> <p>Monologues enhance comprehension of complex information - Encouraging the model to organize and reiterate key information improves reasoning in complex contexts without requiring elaborate multi-stage agent architectures.</p> </li> <li> <p>Validation patterns catch errors before they reach users - Simple validation checks dramatically improve reliability, creating both immediate value and generating training data that can improve base model performance over time.</p> </li> <li> <p>Strategic rejection sets appropriate expectations - Being transparent about limitations builds trust while collecting data for future improvements, making your system more reliable by focusing on what it can do well.</p> </li> <li> <p>Capability showcasing guides users effectively - Explicitly highlighting your system's strengths improves user satisfaction and engagement while setting appropriate expectations.</p> </li> </ol> <p>Practical Implementation Strategy</p> <p>\"When implementing these improvements, I recommend starting with citations and validation patterns, as they provide the most immediate reliability gains. Then add chain of thought for complex reasoning scenarios, followed by strategic rejection for edge cases. These foundational elements will deliver the most value for your development time while setting the stage for more advanced techniques.\"</p> <p>These improvements work in concert with the feedback mechanisms from Chapter 3.1 and the streaming techniques from Chapter 3.2 to create a comprehensive, user-centered RAG experience. Each element reinforces the others: citations provide opportunities for feedback, streaming makes the thinking process engaging, and validation ensures that what users see is reliable.</p> <p>This completes our exploration of deployment and feedback collection. We've now built a robust system that not only delivers accurate information but does so in a way that users find trustworthy, engaging, and helpful. The system collects feedback naturally, feels responsive despite complex processing, and provides transparency into its reasoning and sources.</p> <p>In Chapter 4, we'll shift our focus to analyzing the wealth of data you're now collecting. Through topic modeling and clustering techniques, you'll learn to identify patterns in user queries and system performance, revealing focused opportunities for improvement. This marks an exciting transition from building a great system to understanding how it's being used in the real world and systematically enhancing its capabilities based on that understanding.</p> <p>By implementing the techniques from all three parts of Chapter 3, you've built the foundation for a continuous improvement cycle driven by user feedback and data analysis\u2014a system that doesn't just answer questions but gets better with every interaction.</p>","tags":["citations","chain-of-thought","validation","prompting"]},{"location":"workshops/chapter4-1/","title":"Topic Modeling and Analysis: Finding Patterns in User Feedback","text":"<p>Chapter Overview</p> <p>This chapter explores how to identify patterns in user queries and feedback using topic modeling techniques:</p> <pre><code>- Understanding the difference between topics and capabilities\n- Applying topic modeling techniques to user queries\n- Categorizing queries for targeted improvements\n- Building production-ready classifiers\n- Monitoring performance by segment\n</code></pre>","tags":["topic-modeling","clustering","classification","query-analysis"]},{"location":"workshops/chapter4-1/#introduction-from-collecting-feedback-to-understanding-it","title":"Introduction: From Collecting Feedback to Understanding It","text":"<p>In Chapter 3, we built a comprehensive system for deployment and feedback collection: we designed mechanisms to capture valuable user signals in Chapter 3.1, implemented streaming to create engaging experiences in Chapter 3.2, and added quality-of-life improvements to enhance trust and transparency in Chapter 3.3. Now we're faced with a new challenge\u2014one that nearly every team encounters after successful deployment: making sense of all that feedback data.</p> <p>The first time I deployed a successful RAG system with robust feedback collection, I felt a mix of triumph and panic. Within weeks, we had thousands of queries, ratings, and interaction signals. But when my manager asked, \"So what should we improve next?\" I realized I had no systematic way to answer that question. We were drowning in data but struggling to extract actionable insights.</p> <p>This is where topic modeling and clustering become transformative. While it's tempting to dive into individual feedback instances or fixate on particularly negative comments, the real power comes from identifying patterns that reveal systematic opportunities for improvement. By grouping similar queries and analyzing performance patterns, you move from reacting to individual feedback to making strategic decisions about where to invest your limited resources.</p> <p>Key Philosophy</p> <p>\"Not all improvements are created equal. The art of systematic RAG development is identifying which capabilities will deliver the most value to your users.\"</p> <p>This chapter may be my favorite in the entire book, as it transforms the vague directive of \"make the AI better\" into a structured, data-driven approach for identifying exactly what to improve and where to allocate your limited resources.</p> <p>Think of this process as similar to a product manager analyzing customer segments. Just as not all customers have the same needs or value, not all query types deserve the same attention. Some query categories might represent a small percentage of volume but be critical to your most valuable users. Others might be frequent but easily satisfied with simple improvements. The goal is to move beyond \"making the AI better\" to precisely targeting your efforts where they'll have the maximum impact.</p> <p>In this chapter, we'll explore practical techniques for segmenting and analyzing user queries, identifying patterns in performance, and creating a strategic roadmap for improvement. By the end, you'll have a data-driven framework for deciding exactly where to focus your efforts and which capabilities to develop next.</p>","tags":["topic-modeling","clustering","classification","query-analysis"]},{"location":"workshops/chapter4-2/","title":"Prioritization and Roadmapping: From Insights to Action","text":"<p>Chapter Overview</p> <p>This chapter explores how to turn query pattern insights into strategic action plans:</p> <pre><code>- Identifying high-value, high-impact opportunities\n- Applying prioritization frameworks to improvement decisions\n- Creating a strategic roadmap for RAG enhancement\n- Analyzing failure modes and root causes\n- Building systems that continuously improve\n</code></pre>","tags":["prioritization","roadmapping","impact-analysis","strategic-planning"]},{"location":"workshops/chapter4-2/#understanding-topics-and-capabilities","title":"Understanding Topics and Capabilities","text":"<p>Before diving into prioritization techniques, let's clarify two key concepts: topics and capabilities.</p> <p>Foundations from Earlier Chapters</p> <ul> <li>Chapter 1: The evaluation metrics help identify performance by topic</li> <li>Chapter 3: Feedback collection provides the raw data for analysis</li> </ul> <p>This chapter shows you how to turn that data into actionable insights.</p>","tags":["prioritization","roadmapping","impact-analysis","strategic-planning"]},{"location":"workshops/chapter4-2/#topics-vs-capabilities","title":"Topics vs. Capabilities","text":"<p>Key Definitions</p> <p>- Topics: Subject matter domains in your content (e.g., \"account management,\" \"pricing,\" \"technical specifications\") - Capabilities: Functional abilities your system should have (e.g., \"summarization,\" \"comparison,\" \"step-by-step instructions\")</p> <p>These concepts provide two complementary lenses for analyzing user needs. When I work with teams to improve their RAG systems, I often find they conflate these two dimensions, leading to unfocused efforts. A topic tells you what users are asking about, while a capability tells you what they want the system to do with that information.</p> <pre><code>graph LR\n    A[User Queries] --&gt; B[Topic Analysis]\n    A --&gt; C[Capability Analysis]\n\n    B --&gt; D[What users ask about]\n    C --&gt; E[What users want the system to do]\n\n    D --&gt; F[Content Coverage Improvements]\n    E --&gt; G[Functional Improvements]\n\n    F --&gt; H[System Enhancements]\n    G --&gt; H</code></pre> <p>I remember working with a healthcare company whose initial analysis simply categorized queries by medical conditions. While helpful, this one-dimensional view missed critical patterns. When we added capability analysis, we discovered that queries about common conditions like diabetes primarily needed comparison capabilities (\"How does medication A compare to B?\"), while queries about rare conditions needed comprehensive summarization (\"Give me an overview of treatment options for X\"). This insight completely changed their improvement priorities.</p> <p>Topics vs. Capabilities Example</p> <p>Consider these user queries in a product support RAG:</p>","tags":["prioritization","roadmapping","impact-analysis","strategic-planning"]},{"location":"workshops/chapter4-2/#-how-do-i-reset-my-password-topic-account-security-capability-step-by-step-instructions-compare-the-pro-and-basic-plans-topic-pricingplans-capability-comparison-summarize-the-latest-product-release-notes-topic-product-updates-capability-summarization","title":"<pre><code>- \"How do I reset my password?\"\n  - **Topic**: Account security\n  - **Capability**: Step-by-step instructions\n\n- \"Compare the Pro and Basic plans\"\n  - **Topic**: Pricing/Plans\n  - **Capability**: Comparison\n\n- \"Summarize the latest product release notes\"\n  - **Topic**: Product updates\n  - **Capability**: Summarization\n</code></pre>","text":"<p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>","tags":["prioritization","roadmapping","impact-analysis","strategic-planning"]},{"location":"workshops/chapter5-1/","title":"Understanding Specialized Retrieval: Beyond Basic RAG","text":"<p>Chapter Overview</p> <pre><code>This part explores the foundations of specialized retrieval and its importance:\n\n- Understanding why specialized retrievers outperform general-purpose solutions\n- Learning key strategies for enhancing retrievability\n- Measuring performance at both router and retriever levels\n- Implementing metadata extraction and synthetic summaries\n</code></pre>","tags":["specialized-indices","retrieval-strategies","extraction","synthetic-text"]},{"location":"workshops/chapter5-1/#introduction","title":"Introduction","text":"<p>In our journey through systematically improving RAG applications, we've reached a pivotal moment. The previous sessions have equipped us with the fundamental building blocks: the RAG playbook, synthetic data generation, fine-tuning approaches, user experience design for data collection, and segmentation techniques. Now, in Session 5, we turn our attention to a concept that often separates basic implementations from truly exceptional ones\u2014multimodal RAG and specialized search indices.</p> <p>Building on the Foundation</p> <ul> <li>Chapter 1: Evaluation metrics for each specialized retriever</li> <li>Chapter 2: Fine-tuning embeddings for specific domains</li> <li>Chapter 3: Collecting feedback on retrieval quality</li> <li>Chapter 4: Identifying which capabilities need specialization</li> </ul> <p>Key Insight</p> <p>The fundamental insight that drives this session is deceptively simple yet profound: not all queries are created equal. Different types of information require different approaches to retrieval. Just as you wouldn't use a hammer for every home repair task, you shouldn't rely on a single retrieval mechanism for every type of query your users might have.</p> <p>Today, we'll explore how to identify distinct capabilities through segmentation and address each with tailored approaches. We'll see why specialized models solving specific problems consistently outperform general-purpose solutions, and how this paradigm shift can transform your RAG implementation from adequate to exceptional.</p>","tags":["specialized-indices","retrieval-strategies","extraction","synthetic-text"]},{"location":"workshops/chapter5-1/#the-power-of-specialization","title":"The Power of Specialization","text":"","tags":["specialized-indices","retrieval-strategies","extraction","synthetic-text"]},{"location":"workshops/chapter5-1/#beyond-the-monolithic-approach","title":"Beyond the Monolithic Approach","text":"<p>Traditional RAG implementations often begin with a single, monolithic index\u2014a one-size-fits-all approach that attempts to handle every query with the same retrieval mechanism. While this approach can work for simple use cases, it quickly reaches its limits when confronted with the rich diversity of real-world queries.</p> <p>Diverse Query Needs</p> <p>Consider a hardware store's knowledge base. A customer searching for a specific product by model number requires a fundamentally different search approach than someone asking about the durability of various power tools, or another customer trying to find items within a specific weight range. The first query is best served by lexical search matching exact strings, the second by semantic search understanding concepts and opinions, and the third by structured data queries.</p> <p>This diversity of information needs is why major search engines like Google have developed specialized tools\u2014Maps for location-based queries, Photos for visual search, YouTube for video content, and classic web search for text-based information. While these began as separate products, the true innovation came when Google learned to seamlessly route users to the appropriate tool based on the nature of their query.</p> <p>From Previous Cohort</p> <p>\"I've been building separate indices for years without realizing that's what I was doing. This framework just helps me do it more systematically.\"</p>","tags":["specialized-indices","retrieval-strategies","extraction","synthetic-text"]},{"location":"workshops/chapter5-1/#the-mathematics-of-specialization","title":"The Mathematics of Specialization","text":"<p>The superiority of specialized approaches isn't just theoretical\u2014it's mathematically demonstrable. When distinct segments exist within a population of queries, a collection of local decision models will consistently outperform a global model trying to handle all cases.</p> <p>This principle manifests in modern machine learning in multiple ways. We see it in the evolution from monolithic models to mixtures of experts, where specialized sub-models handle different types of inputs. We see it in the trend toward decomposing complex tasks into simpler subtasks that can be solved independently before being recombined.</p> <pre><code>graph TD\n    A[Monolithic Approach] --&gt; B[One-size-fits-all]\n    C[Specialized Approach] --&gt; D[Domain-specific Models]\n\n    B --&gt;|Limited Performance| E[General Coverage]\n    D --&gt;|Optimized Performance| F[Targeted Coverage]\n\n    F --&gt; G[Better Overall Results]\n    E --&gt; G\n\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style C fill:#bbf,stroke:#333,stroke-width:2px</code></pre> <p>Organizational Benefits</p> <p>Beyond the performance benefits, specialized indices offer practical organizational advantages:</p> <pre><code>1. **Division of labor**: Teams can work on isolated, well-defined problems rather than tangling with the entire system\n2. **Incremental improvement**: Adding a new specialized index is less disruptive than rebuilding an entire system\n3. **Targeted innovation**: Teams can innovate within their specific domain without risking the stability of the whole\n</code></pre> <p>Industry Perspective</p> <p>\"Building specialized indices isn't just about performance\u2014it's about creating a sustainable path for continuous improvement.\"</p>","tags":["specialized-indices","retrieval-strategies","extraction","synthetic-text"]},{"location":"workshops/chapter5-1/#two-paths-to-better-retrieval","title":"Two Paths to Better Retrieval","text":"<p>When improving retrieval capabilities for RAG applications, two complementary strategies emerge. Think of them as opposite sides of the same coin\u2014one extracting structure from the unstructured, the other creating retrieval-optimized representations of structured data.</p> <p>Key Insight</p> <p>If you remember nothing else from this chapter, remember this: These strategies essentially create materialized views of your existing data, processed by AI through either structuring or rewriting.</p>","tags":["specialized-indices","retrieval-strategies","extraction","synthetic-text"]},{"location":"workshops/chapter5-1/#strategy-1-extracting-metadata","title":"Strategy 1: Extracting Metadata","text":"<p>The first approach involves defining and extracting more metadata from your text chunks. Instead of viewing your content as an undifferentiated mass of text, you identify valuable structured information that can be exposed to search engines.</p> <p>Metadata Extraction Examples</p> <p>- In finance applications, distinguishing between fiscal years and calendar years - For legal document systems, classifying contracts as signed or unsigned and extracting payment dates and terms - When processing call transcripts, categorizing them by type (job interviews, stand-ups, design reviews) - For product documentation, identifying specifications, compatibility information, and warranty details</p> <p>This approach essentially asks: \"What structured information is hiding within our unstructured content that would make it easier to search?\" Once extracted, this metadata can be stored in traditional databases like PostgreSQL, enabling powerful filtering and structured queries beyond what's possible with pure vector search.</p> <p>Practical Application</p> <p>When consulting with financial clients, we discovered that simply being able to distinguish between fiscal years and calendar years dramatically improved search accuracy for financial metrics. Similarly, for legal teams, identifying whether a contract was signed or unsigned allowed for immediate filtering that saved hours of manual review.</p> <p>Financial Metadata Model</p> <pre><code>```python\nfrom pydantic import BaseModel\nfrom datetime import date\nfrom typing import Optional, List\n\nclass FinancialStatement(BaseModel):\n    \"\"\"Structured representation of a financial statement document.\"\"\"\n    company: str\n    period_ending: date\n    revenue: float\n    net_income: float\n    earnings_per_share: float\n    fiscal_year: bool = True  # Is this fiscal year (vs calendar year)?\n    # Additional fields that might be valuable:\n    sector: Optional[str] = None\n    currency: str = \"USD\"\n    restated: bool = False  # Has this statement been restated?\n\ndef extract_financial_data(document_text: str) -&gt; FinancialStatement:\n    \"\"\"\n    Extract structured financial data from document text using LLM.\n\n    Args:\n        document_text: Raw text from financial document\n\n    Returns:\n        Structured FinancialStatement object with extracted data\n    \"\"\"\n    # Define a structured extraction prompt\n    system_prompt = \"\"\"\n    Extract the following financial information from the document:\n    - Company name\n    - Period end date\n    - Whether this is a fiscal year report (vs calendar year)\n    - Revenue amount (with currency)\n    - Net income amount\n    - Earnings per share\n    - Business sector\n    - Whether this statement has been restated\n\n    Format your response as a JSON object with these fields.\n    \"\"\"\n\n    # Use LLM to extract the structured information\n    # Implementation depends on your LLM framework\n    extracted_json = call_llm(system_prompt, document_text)\n\n    # Parse the extracted JSON into our Pydantic model\n    return FinancialStatement.parse_raw(extracted_json)\n```\n</code></pre> <p>By extracting these structured elements from quarterly reports, organizations can enable precise filtering and comparison that would have been impossible with text-only search. For instance, you can easily query \"Show me all companies in the tech sector with revenue growth over 10% in fiscal year 2024\" or \"Find all restated financial statements from the last quarter.\"</p>","tags":["specialized-indices","retrieval-strategies","extraction","synthetic-text"]},{"location":"workshops/chapter5-1/#strategy-2-building-synthetic-text-chunks","title":"Strategy 2: Building Synthetic Text Chunks","text":"<p>The second approach reverses the flow: taking structured data (or even unstructured data) and producing synthetic text chunks optimized for retrieval. These chunks serve as semantic pointers to the original content, enabling more effective recall.</p> <p>Practical Perspective</p> <p>\"If the first approach is to extract structured data from text chunks for easier structured search, the second approach is to create synthetic text chunks optimized specifically for embedding and retrieval.\"</p> <p>Synthetic Text Applications</p> <p>- For image collections: Generate detailed descriptions capturing searchable aspects - For research interviews: Extract common questions and answers to form an easily searchable FAQ - For numerical data: Create natural language descriptions of key trends and outliers - For product documentation: Generate comprehensive feature summaries that anticipate user queries - For customer service transcripts: Create problem-solution pairs that capture resolution patterns</p> <p>These synthetic chunks become intermediaries\u2014easier to search than the original content, but pointing back to that source material when needed for the final response. When implemented correctly, you gain the benefits of both worlds: optimized retrieval via the synthetic chunks, and full fidelity from the source content.</p>","tags":["specialized-indices","retrieval-strategies","extraction","synthetic-text"]},{"location":"workshops/chapter5-1/#measuring-what-matters","title":"Measuring What Matters","text":"<p>As we introduce specialized indices, our measurement framework expands to assess performance at two levels:</p> <p>Two-Level Measurement Framework</p> <pre><code>1. Are we selecting the right retrieval method for each query? \n2.  Is each retrieval method finding the right information?\n</code></pre> <p>The overall probability of finding the correct information becomes a product of these two probabilities:</p> <p>Performance Formula</p> <p>P(finding correct data) = P(selecting correct retriever) \u00d7 P(finding correct data | correct retriever)</p> <p>This formula provides a powerful diagnostic tool. When your system underperforms, it helps identify whether the issue lies with retriever selection or with the individual retrievers themselves.</p> <p>Diagnostic Example</p> <p>If you find that your system correctly routes 95% of queries to the appropriate retriever, but those retrievers only find relevant information 60% of the time, your priority should be improving retrieval quality rather than router accuracy.</p> <p>This two-level evaluation framework ensures you invest your improvement efforts where they'll have the greatest impact.</p> <p>Next Steps</p> <p>In Chapter 6, we'll explore how to bring these specialized components together through intelligent routing, creating a unified system that seamlessly directs queries to the appropriate retrievers.</p> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>","tags":["specialized-indices","retrieval-strategies","extraction","synthetic-text"]},{"location":"workshops/chapter5-2/","title":"Implementing Multimodal Search: Specialized Retrieval Techniques","text":"<p>Chapter Overview</p> <pre><code>This part explores the practical implementation of specialized retrieval techniques:\n\n- Implementing strategies for document, image, and table retrieval\n- Building effective SQL generation capabilities\n- Creating rich descriptions for non-text content\n- Combining specialized retrievers into a cohesive system\n</code></pre>","tags":["multimodal","image-search","table-search","sql-generation"]},{"location":"workshops/chapter5-2/#specialized-approaches-for-different-modalities","title":"Specialized Approaches for Different Modalities","text":"<p>Different types of content demand different retrieval strategies. Let's explore approaches for three common modalities: documents, images, and tables.</p>","tags":["multimodal","image-search","table-search","sql-generation"]},{"location":"workshops/chapter5-2/#document-search-beyond-basic-chunking","title":"Document Search: Beyond Basic Chunking","text":"<p>For document retrieval, the foundation remains chunking documents with appropriate metadata and applying both lexical and semantic search techniques. However, several refinements can dramatically improve performance:</p> <p>Page-Level Chunking for Documentation</p> <p>When to Use Page-Level Chunks: - Documentation websites (respect page boundaries) - User manuals (preserve context) - Legal documents (maintain clause integrity) - Academic papers (keep sections together)</p> <p>Why It Works: - Documentation is carefully organized by authors - Semantic boundaries align with page/section breaks - Users expect complete answers from single pages - Reduces context fragmentation</p> <p>Implementation: <pre><code># Instead of arbitrary chunking:\nchunks = chunk_by_tokens(doc, size=800)\n\n# Use page-aware chunking:\nchunks = chunk_by_pages(doc, \n                      respect_sections=True,\n                      min_size=200,\n                      max_size=2000)\n</code></pre></p> <p>Advanced Document Retrieval Techniques</p> <p>- Contextual Retrieval: Rather than using fixed chunks, dynamically rewrite or expand chunks based on the query context. This creates \"query-aware\" text representations that better match user intent.</p> <p>- Hybrid Retrieval Signals: Combine semantic similarity with other signals like recency, authority, and citation frequency to create a more nuanced ranking function.</p> <p>- Multi-stage Retrieval: Implement a cascade of increasingly sophisticated (and computationally expensive) retrieval and ranking steps, filtering out irrelevant content at each stage.</p> <p>Contextual Retrieval Implementation</p> <p>The Power of Context-Aware Chunks:</p> <p>Original chunk: \"Jason the doctor is unhappy with Patient X\"</p> <p>Without context, this is ambiguous: - Is Jason a medical doctor unhappy with a patient? - Is a doctor named Jason unhappy? - Is someone consulting Dr. Jason about Patient X?</p> <p>Solution: Rewrite chunks with full document context:</p> <pre><code>def create_contextual_chunk(chunk, document):\n    \"\"\"Rewrite chunk with document context.\"\"\"\n    prompt = f\"\"\"\n    Document context: {document.title}\n    Section: {chunk.section}\n\n    Original chunk: {chunk.text}\n\n    Rewrite this chunk to include necessary context \n    so it can be understood in isolation.\n    \"\"\"\n\n    return llm.complete(prompt)\n</code></pre> <p>Result: \"In this employee feedback document, Jason (the medical doctor  on our staff) expressed dissatisfaction with the Patient X project  management software due to frequent crashes.\"</p> <p>Key Decision: Compute at write-time vs read-time - Write-time: Higher storage cost, faster retrieval - Read-time: Lower storage cost, slower retrieval - Most teams should compute at write-time for production</p> <pre><code>flowchart LR\n    A[Query] --&gt; B[Initial Retrieval]\n    B --&gt; C[Candidate Chunks]\n    C --&gt; D[Re-ranking]\n    D --&gt; E[Dynamic Expansion]\n    E --&gt; F[Final Context]</code></pre> <p>The result is a document retrieval system that might return different types of content depending on the query:</p> <ul> <li>For some queries, concise summaries of key information</li> <li>For others, entire documents leveraging long-context models</li> <li>For yet others, specific text chunks or structured data extracts</li> </ul> <p>This flexibility allows the system to balance precision, recall, and presentation based on what best serves each query.</p> <p>Document Processor with Contextual Retrieval</p> <pre><code>```python\nfrom typing import List, Dict, Any\nimport re\n\ndef process_document_for_retrieval(document: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Process a document for enhanced retrieval capabilities.\n\n    Args:\n        document: The raw document text\n\n    Returns:\n        Dictionary with processed document components\n    \"\"\"\n    # Extract structured metadata\n    metadata = extract_document_metadata(document)\n\n    # Create standard chunks with overlap\n    chunks = chunk_document(document, chunk_size=800, overlap=0.5)\n\n    # Generate summaries at different levels\n    document_summary = summarize_document(document)\n    section_summaries = [summarize_section(section) for section in extract_sections(document)]\n\n    # Extract any structured data tables\n    tables = extract_tables(document)\n\n    return {\n        \"metadata\": metadata,\n        \"chunks\": chunks,\n        \"document_summary\": document_summary,\n        \"section_summaries\": section_summaries,\n        \"tables\": tables,\n        \"full_document\": document  # Keep original for potential long-context processing\n    }\n\ndef contextual_retrieval(query: str, document_store: List[Dict[str, Any]]) -&gt; List[str]:\n    \"\"\"\n    Perform contextual retrieval that adapts based on query type.\n\n    Args:\n        query: User query\n        document_store: Processed document store\n\n    Returns:\n        List of most relevant text chunks for the query\n    \"\"\"\n    # Analyze query to determine retrieval strategy\n    query_analysis = analyze_query(query)\n\n    if query_analysis[\"requires_specific_detail\"]:\n        # Use chunk-level retrieval for specific information\n        return retrieve_relevant_chunks(query, document_store)\n\n    elif query_analysis[\"requires_overview\"]:\n        # Use summary-level retrieval for broader questions\n        return retrieve_relevant_summaries(query, document_store)\n\n    elif query_analysis[\"requires_structured_data\"]:\n        # Use table retrieval for data-oriented questions\n        return retrieve_relevant_tables(query, document_store)\n\n    else:\n        # Fall back to hybrid approach\n        chunks = retrieve_relevant_chunks(query, document_store)\n        summaries = retrieve_relevant_summaries(query, document_store)\n        return rerank_combined_results(query, chunks + summaries)\n```\n</code></pre>","tags":["multimodal","image-search","table-search","sql-generation"]},{"location":"workshops/chapter5-2/#image-search-bridging-visual-and-textual-understanding","title":"Image Search: Bridging Visual and Textual Understanding","text":"<p>Image search presents unique challenges. Visual language models were trained primarily on captioning data, creating a potential mismatch between how queries are phrased and how images are represented.</p> <p>Embedding Spaces Mismatch</p> <p>The naive approach\u2014applying the same embedding strategy used for text\u2014often fails because question embeddings and image caption embeddings exist in fundamentally different semantic spaces. Simply embedding captions like \"two people\" will not retrieve well when users search for \"business meeting\" or \"team collaboration.\"</p> <p>When to Use Vision Language Models</p> <p>According to Adit from Reducto, VLMs excel at \"things that traditional OCR has always been horrible at\" - handwriting, charts, figures, and diagrams. However, for clean structured information, traditional CV provides better precision and token efficiency. Learn about their hybrid approach \u2192</p> <p>To bridge this gap, more sophisticated image summarization techniques are essential:</p> <p>Advanced Image Description Techniques</p> <p>Rich Prompting: Move beyond simple \"what's in this image?\" prompts to detailed instructions that anticipate likely queries. Compare:</p> <pre><code>*Basic*: \"Describe this image.\"\n\u2192 Result: \"Two people at a table.\"\n\n*Better*: \"Describe this image in detail, noting the number of people, their apparent relationship, the setting, lighting conditions, objects present, and any text visible in the image.\"\n\u2192 Result: \"Two people arguing across a dinner table in a dimly lit room. One person appears agitated while the other looks defensive. A knife is visible on the table.\"\n\n*Optimal*: \"Analyze this image comprehensively as if you were making it searchable in a database. Include details about the people, their emotions, the environment, lighting, objects, potential context, and any visible text. Consider how someone might search for this specific image.\"\n\u2192 Result: \"This dramatic image shows two business professionals in a tense negotiation across a polished conference table in a corporate boardroom with floor-to-ceiling windows overlooking a city skyline. The older man in a gray suit appears frustrated, gesturing emphatically with papers in hand, while the younger woman in a black blazer maintains a composed but firm expression. Multiple financial reports and what appears to be a contract are spread across the table. The scene is captured in natural lighting with dramatic shadows, suggesting a high-stakes discussion or disagreement over business terms.\"\n</code></pre> <p>From Industry Experience</p> <p>\"We found that the difference between basic image descriptions and optimized ones led to a 40% increase in successful retrievals. The key was training our team to create prompts that anticipated the vocabulary users would actually employ in their searches.\"</p> <p>Additional Image Enhancement Approaches</p> <p>- Contextual Enrichment: Incorporate surrounding text, OCR results from the image, and metadata about the image's source and purpose. For example, if an image appears in a product manual, include the product name and function in the description.</p> <pre><code>- **Visual Reasoning**: Use chain-of-thought prompting to guide the model through a reasoning process about the image content, resulting in more comprehensive descriptions. For example: \"First identify all objects in the image. Then consider how they relate to each other. Finally, determine what activity or process is being depicted.\"\n\n- **Bounding Boxes and Visual Grounding**: For applications where precise location or counting is important, supplement descriptions with information about the spatial arrangement of elements. This is particularly valuable in construction, manufacturing, and retail contexts where users often need to locate or count specific items.\n</code></pre> <p>Construction Site Image Analysis</p> <p>For a construction company's image database, users frequently needed to count specific items (\"How many support beams are installed?\") or locate defects (\"Show me images of cracked foundations\"). By implementing bounding box detection alongside rich descriptions, retrieval accuracy for these queries improved by 65% compared to using only semantic descriptions.</p> <p>Rich Image Description Prompt</p> <p>```python def generate_rich_image_description(image, ocr_text=None, surrounding_text=None): \"\"\" Generate a comprehensive description optimized for retrieval.</p> <pre><code>    Args:\n        image: Image data or path\n        ocr_text: Optional text extracted from the image\n        surrounding_text: Optional text surrounding the image in its original context\n\n    Returns:\n        Detailed description of the image\n    \"\"\"\n    prompt = f\"\"\"\n    # Image Analysis Task\n\n    ## Context Information\n    {\"OCR Text from image: \" + ocr_text if ocr_text else \"No OCR text available.\"}\n    {\"Surrounding context: \" + surrounding_text if surrounding_text else \"No surrounding context available.\"}\n\n    ## Analysis Instructions\n    Analyze the following image in extreme detail:\n\n    1. First, describe the visual scene, setting, and overall composition\n    2. List all people visible, their approximate positions, actions, and expressions\n    3. Enumerate all objects visible in the image\n    4. Note any text visible in the image\n    5. Describe colors, lighting, and visual style\n    6. If applicable, identify the type of image (photograph, diagram, screenshot, etc.)\n    7. Use chain-of-thought reasoning: think about what is happening and why\n    8. Generate 5-7 potential questions someone might ask when searching for this image\n    9. Suggest 5-10 relevant tags for this image\n\n    ## Final Description\n    Based on your analysis, provide a comprehensive 3-5 sentence description that would\n    help people find this image when searching with natural language queries.\n    \"\"\"\n\n    # Use this prompt with your vision model implementation\n    # ...\n```\n</code></pre> <p>The enhanced description dramatically improves retrieval capability when troubleshooting specific defects or components.</p>","tags":["multimodal","image-search","table-search","sql-generation"]},{"location":"workshops/chapter5-2/#table-search-structured-data-in-context","title":"Table Search: Structured Data in Context","text":"<p>Tables present a dual challenge: they contain structured data but exist within unstructured contexts. Two main approaches prove effective:</p> <p>Expert Insight: Document Parsing Challenges</p> <p>Adit from Reducto emphasizes that tables are particularly challenging: \"Tables are particularly challenging because they represent two-dimensional associations of data that can be formatted in countless ways. The failures are often subtle - a model might extract what appears to be a valid table but silently drop rows, columns, or individual values.\"</p> <p>For production-ready table extraction, consider specialized tools. Learn more about document ingestion best practices \u2192</p> <p>Markdown Tables: The Surprising Winner</p> <p>Performance Comparison for Table Lookups: - Markdown tables: 85% accuracy - CSV format: 73% accuracy - JSON format: 71% accuracy - YAML format: 69% accuracy</p> <p>Why Markdown Tables Win: - Visual structure helps LLMs understand relationships - Column alignment provides natural grouping - Headers are clearly distinguished - Less token overhead than JSON/YAML</p> <p>Best Practices: <pre><code>| Product ID | Name           | Price  | Stock |\n|------------|----------------|--------|-------|\n| SKU-001    | Widget Pro     | $29.99 | 150   |\n| SKU-002    | Widget Basic   | $19.99 | 0     |\n| SKU-003    | Widget Premium | $49.99 | 75    |\n</code></pre></p> <p>Pro Tip: For financial data, beware of spacing in numbers! - Bad: <code>1 234 567</code> (tokenizes as three separate numbers) - Good: <code>1234567</code> or <code>1,234,567</code></p> <p>Production Table Extraction</p> <p>Reducto's approach to complex tables includes: - Using HTML for tables with 3+ merged cells - Traditional CV for initial extraction, VLMs for correction - Creating natural language summaries for better retrieval</p> <p>See their complete document parsing methodology for handling PDFs, Excel files, and complex layouts.</p> <p>Table Retrieval Approaches</p> <p>Approach 1: Table as Document</p> <pre><code>For finding specific rows or comparing data across tables, chunk the table (preserving headers) and apply semantic search techniques. Generate summaries that capture the table's purpose and key insights to improve retrieval.\n\nThis works well for questions like \"Which product had the highest Q3 sales?\" or \"Show me all tables with warranty information.\"\n\n**Approach 2: Table as Database**\n\nFor detailed data analysis, treat tables as queryable databases. The key challenge becomes identifying which table(s) to query for a given question.\n\nStandardize schemas using CREATE TABLE statements or table descriptions, then build semantic search against these table representations. Include sample data when possible to help clarify the table's contents.\n</code></pre> <p>Table Processor Implementation</p> <p>```python from typing import List, Dict, Any, Optional import pandas as pd</p> <pre><code>class TableProcessor:\n    \"\"\"Process tables for enhanced retrievability and querying.\"\"\"\n\n    def process_table(self, table_data: pd.DataFrame, table_name: str,\n                    source_doc: Optional[str] = None) -&gt; Dict[str, Any]:\n        \"\"\"\n        Process a table for both document-like and database-like retrieval.\n\n        Args:\n            table_data: The table as a pandas DataFrame\n            table_name: Name of the table\n            source_doc: Optional source document information\n\n        Returns:\n            Dictionary with processed table components\n        \"\"\"\n        # Generate schema representation\n        schema = self._generate_schema_representation(table_data)\n\n        # Generate natural language summary\n        summary = self._generate_table_summary(table_data, table_name)\n\n        # Generate sample queries this table could answer\n        sample_queries = self._generate_sample_queries(table_data, table_name)\n\n        # Convert to text chunks for semantic search\n        text_chunks = self._table_to_text_chunks(table_data)\n\n        return {\n            \"table_name\": table_name,\n            \"schema\": schema,\n            \"summary\": summary,\n            \"sample_queries\": sample_queries,\n            \"text_chunks\": text_chunks,\n            \"raw_data\": table_data,\n            \"source_document\": source_doc\n        }\n\n    def _generate_schema_representation(self, df: pd.DataFrame) -&gt; str:\n        \"\"\"Generate a SQL-like schema representation.\"\"\"\n        types = []\n        for col in df.columns:\n            dtype = df[col].dtype\n            if pd.api.types.is_numeric_dtype(dtype):\n                sql_type = \"NUMERIC\"\n            elif pd.api.types.is_datetime64_dtype(dtype):\n                sql_type = \"TIMESTAMP\"\n            else:\n                sql_type = \"TEXT\"\n\n            # Add sample values for better understanding\n            sample_values = df[col].dropna().unique()[:3]\n            sample_str = f\"Sample values: {', '.join(str(x) for x in sample_values)}\"\n\n            types.append(f\"{col} {sql_type} -- {sample_str}\")\n\n        return f\"CREATE TABLE table (\\n  \" + \",\\n  \".join(types) + \"\\n);\"\n\n    def _generate_table_summary(self, df: pd.DataFrame, table_name: str) -&gt; str:\n        \"\"\"Generate a natural language summary of the table.\"\"\"\n        # Use an LLM to summarize the table contents\n        # Implementation depends on your LLM framework\n        # ...\n\n    def _generate_sample_queries(self, df: pd.DataFrame, table_name: str) -&gt; List[str]:\n        \"\"\"Generate sample natural language queries this table could answer.\"\"\"\n        # Use an LLM to generate sample queries\n        # ...\n\n    def _table_to_text_chunks(self, df: pd.DataFrame) -&gt; List[str]:\n        \"\"\"Convert table to text chunks for semantic search.\"\"\"\n        # Implementation for chunking table content\n        # ...\n```\n</code></pre> <p>Once the right table is identified, either:</p> <ul> <li>Place the table directly into the context for simple analysis</li> <li>Generate SQL queries or pandas code for more complex analysis</li> </ul>","tags":["multimodal","image-search","table-search","sql-generation"]},{"location":"workshops/chapter5-2/#sql-query-generation-a-case-study-in-capability-building","title":"SQL Query Generation: A Case Study in Capability Building","text":"<p>SQL query generation exemplifies many of the principles we've discussed. It involves both an inventory challenge (finding the right tables) and a capability challenge (writing effective queries).</p> <p>Limitations of Direct Translation</p> <p>The classical approach\u2014training a model to translate natural language directly to SQL\u2014often struggles with complex schemas and business-specific query patterns. The limitations become especially apparent with:</p> <pre><code>- Complex schemas with dozens or hundreds of tables\n- Business-specific definitions of common terms like \"active user\" or \"revenue\"\n- SQL patterns that require specific business rules like fiscal calendars\n- Performance considerations that require specific optimization techniques\n</code></pre> <p>Data Science Experience</p> <p>\"We spent months trying to fine-tune models for SQL generation with limited success. Once we switched to retrieving exemplar queries from our analytics repository, accuracy jumped by 30% overnight.\"</p> <p>RAPTOR: Recursive Summarization for Long Documents</p> <p>The RAPTOR Approach:</p> <p>When dealing with concepts that span multiple pages or sections:</p> <ol> <li> <p>Cluster Related Chunks: <pre><code># Embed all chunks\nembeddings = [embed(chunk) for chunk in chunks]\n\n# Cluster similar chunks\nclusters = cluster_embeddings(embeddings, \n                            method='hierarchical',\n                            threshold=0.8)\n</code></pre></p> </li> <li> <p>Summarize Each Cluster: <pre><code>for cluster in clusters:\n    summary = summarize_chunks(cluster.chunks)\n    cluster.summary = summary\n</code></pre></p> </li> <li> <p>Build Hierarchical Index:</p> </li> <li>Leaf nodes: Original chunks</li> <li>Internal nodes: Cluster summaries</li> <li> <p>Root node: Document summary</p> </li> <li> <p>Multi-Level Retrieval:</p> </li> <li>Start with high-level summaries</li> <li>Drill down to specific chunks as needed</li> </ol> <p>Use Cases: - Academic papers (methodology across sections) - Legal documents (related clauses) - Technical documentation (feature descriptions) - Books and long-form content</p> <p>This approach handles the \"information spread\" problem where relevant content is distributed across multiple non-contiguous sections.</p> <p>When Simple Tools Beat Embeddings</p> <p>Colin Flaherty's experience building top-performing coding agents reveals that sometimes simple tools like grep and find can outperform embedding-based retrieval: \"The agent's persistence compensated for less sophisticated tools.\" However, he notes this works best for: - Highly structured content like code - Small to medium-sized repositories - When distinctive keywords exist</p> <p>For larger codebases or unstructured content, embeddings become essential. Explore agentic retrieval patterns \u2192</p> <p>RAG Playbook for SQL Generation</p> <p>A more effective strategy applies our RAG playbook:</p> <pre><code>1. **Build an inventory of tables and their descriptions** - Create detailed schema documentation including sample data\n2. **Create synthetic questions targeting this inventory** - Generate diverse questions that test different joining patterns\n3. **Measure retrieval performance for table selection** - Evaluate if the right tables are being identified\n4. **Collect exemplar SQL queries demonstrating important capabilities** - Curate a library of well-written, optimized queries\n5. **Include these exemplars when generating new queries** - Dynamically retrieve and include relevant examples\n</code></pre> <p>This approach addresses a fundamental challenge in SQL generation: the same question can be interpreted in multiple valid ways. Consider \"Show me month-over-month revenue growth\":</p> <ul> <li>Does \"month\" mean calendar month or a 28-day period?</li> <li>Should weekends be excluded for B2B applications?</li> <li>Is \"growth\" absolute or percentage?</li> <li>Should the calculation include or exclude certain revenue types?</li> <li>Should the comparison use the same day of month or the last day of each month?</li> <li>How should partial months be handled when the current month isn't complete?</li> </ul> <p>Subjective Query Interpretations</p> <p>| Question | Possible Interpretation 1 | Possible Interpretation 2 | Possible Interpretation 3 | |----------|---------------------------|---------------------------|---------------------------| | \"Monthly active users\" | Users who logged in during calendar month | Users who performed an action in last 30 days | Users who made a purchase in billing cycle | | \"Revenue by region\" | Geographic sales regions | Product categories | Customer segments | | \"Top performing products\" | Highest revenue | Highest profit margin | Highest growth rate |</p> <p>Without business context, even the most advanced models can only guess. By including relevant exemplars that demonstrate how your organization typically answers such questions, you guide the model toward your preferred interpretations.</p>","tags":["multimodal","image-search","table-search","sql-generation"]},{"location":"workshops/chapter5-2/#bringing-it-all-together","title":"Bringing It All Together","text":"<p>As we prepare for our final session on routing and unified systems, let's solidify the key insights from today's exploration of multimodal RAG:</p> <p>Key Takeaways</p> <p>1. The power of specialization: Building dedicated retrieval mechanisms for different content types and query patterns consistently outperforms monolithic approaches. Specialized models solving specific problems will outperform general-purpose solutions.</p> <p>2. Two complementary strategies: Extract structured data from unstructured content, or create synthetic text chunks that point to source data\u2014both serve as AI-powered materialized views that optimize retrievability.</p> <p>3. Measurement drives improvement: Use precision and recall at both the router and retriever levels to identify your system's limiting factors using the formula: P(finding correct data) = P(selecting correct retriever) \u00d7 P(finding correct data | correct retriever).</p> <p>4. Modality-specific optimizations: Each content type requires tailored approaches, from contextual retrieval for documents to rich descriptions for images to exemplar-based generation for SQL.</p> <p>5. Organizational benefits: Beyond performance, specialized indices enable division of labor, incremental improvement, and targeted innovation without disrupting the entire system.</p> <p>Combining Lexical and Semantic Search</p> <p>The Power of Hybrid Search:</p> <p>Don't abandon lexical search! It excels at: - Exact matches (product codes, names) - Technical terms and abbreviations - Queries with specific keywords</p> <p>Implementation Strategy: <pre><code>def hybrid_search(query, k=10):\n    # Get results from both systems\n    semantic_results = semantic_search(query, k=k*2)\n    lexical_results = bm25_search(query, k=k*2)\n\n    # Combine with weighted scores\n    combined = merge_results(\n        semantic_results, \n        lexical_results,\n        semantic_weight=0.7,\n        lexical_weight=0.3\n    )\n\n    return combined[:k]\n</code></pre></p> <p>Pro Tip: Adjust weights based on query type: - Technical queries: Increase lexical weight - Conceptual queries: Increase semantic weight - Let user behavior guide the optimization</p> <pre><code>flowchart TD\n    A[User Query] --&gt; B[Query Analyzer]\n    B --&gt; C[Query Router]\n\n    C --&gt;|Document Query| D[Document Retriever]\n    C --&gt;|Image Query| E[Image Retriever]\n    C --&gt;|Table Query| F[Table Retriever]\n    C --&gt;|SQL Query| G[SQL Generator]\n\n    D --&gt; H[Result Combiner]\n    E --&gt; H\n    F --&gt; H\n    G --&gt; H\n\n    H --&gt; I[Response Generator]\n    I --&gt; J[User Response]</code></pre> <p>The beauty of this framework is its recursive nature. The same playbook\u2014synthetic data generation, segmentation, capability identification\u2014applies whether you're building your first retrieval system or your fifth specialized index.</p> <p>Implementation Strategy</p> <p>1. Start small: Begin with one or two specialized retrievers for your highest-impact query types 2. Measure relentlessly: Track performance metrics for each retriever and overall system 3. Expand incrementally: Add new retrievers as you identify segments that would benefit 4. Refine continuously: Use user feedback to improve both routing and retrieval quality 5. Optimize alignment: Ensure that your synthetic text and metadata extraction aligns with actual user query patterns</p> <p>Engineering Insight</p> <p>\"No matter how much better AI gets, you'll always be responsible for retrieval. Understanding what to retrieve and how to retrieve it remains the core challenge even as models become more capable.\"</p> <p>Cross-Reference</p> <p>In Chapter 6, we'll explore how to bring these specialized components together through effective routing strategies, creating a unified system that seamlessly directs users to the appropriate retrievers based on their queries.</p> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>","tags":["multimodal","image-search","table-search","sql-generation"]},{"location":"workshops/chapter6-1/","title":"Query Routing Foundations: Building a Cohesive RAG System","text":"<p>Chapter Overview</p> <pre><code>This part explores the fundamental principles of unified RAG architecture:\n\n- Understanding why unified architecture is essential for advanced RAG systems\n- Designing tool interfaces that bridge language models and specialized indices\n- Learning key principles of effective query routing\n- Measuring performance at the system level\n</code></pre>","tags":["query-routing","unified-architecture","tool-interfaces"]},{"location":"workshops/chapter6-1/#introduction-beyond-specialized-retrievers","title":"Introduction: Beyond Specialized Retrievers","text":"<p>In the previous chapter, we explored how to build specialized retrievers for different content types. We discussed strategies for handling documents, images, tables, and other specialized data formats. While these specialized components improve retrieval quality dramatically, they create a new challenge: how do we build a cohesive system that knows when to use each specialized component?</p> <p>This is the challenge of query routing\u2014the process of understanding what a user is asking for and directing their query to the most appropriate retrieval tool or combination of tools. Effective query routing is what transforms a collection of specialized capabilities into a unified, seamless product experience.</p> <p>Key Insight</p> <pre><code>\"The quality of your RAG system isn't just determined by how well each individual retriever performs, but by how effectively your system routes queries to the right retrievers at the right time. Even perfect retrievers fail if they're used for the wrong queries.\"\n</code></pre> <p>The unified architecture approach we'll explore in this chapter completes our improvement flywheel by:</p> <ol> <li>Using the specialized capabilities we built based on user segmentation</li> <li>Implementing intelligent routing between these components</li> <li>Creating interfaces that help users understand system capabilities</li> <li>Building feedback loops that continuously improve both routing and retrieval</li> </ol> <p>Let's begin by examining the architectural patterns that enable effective query routing in RAG systems.</p>","tags":["query-routing","unified-architecture","tool-interfaces"]},{"location":"workshops/chapter6-1/#the-api-mindset-tools-as-interfaces-between-models-and-data","title":"The API Mindset: Tools as Interfaces Between Models and Data","text":"<p>At the heart of unified RAG architecture is a simple but powerful pattern: treating each specialized retriever as an API that language models can call. This \"tools as APIs\" approach creates a clear separation of concerns between:</p> <ol> <li>Tool Interfaces: The definitions that describe what each tool does and what parameters it accepts</li> <li>Tool Implementations: The specialized code that performs retrieval against specific indices</li> <li>Routing Logic: The system that determines which tools to call for a given query</li> </ol> <p>Framework Development Perspective</p> <p>\"You're effectively a framework developer for the language model. I spent many years developing multiple microservices to do retrieval for other teams, and moving forward it's going to feel a lot like building distributed microservices.\"</p> <p>History of Tool Interfaces</p> <p>The tool interface pattern has evolved rapidly in AI systems. What began as simple \"function calling\" in APIs like OpenAI's functions or Anthropic's tools has now developed into more sophisticated frameworks with multiple tool selection strategies. This pattern mimics the development of web API frameworks like REST and GraphQL, but with language models as the primary \"clients\" of these APIs.</p>","tags":["query-routing","unified-architecture","tool-interfaces"]},{"location":"workshops/chapter6-1/#why-the-api-approach-works","title":"Why the API Approach Works","text":"<p>Treating specialized retrievers as APIs offers several key advantages:</p> <ol> <li>Clear Boundaries: Teams can work independently on different tools</li> <li>Testability: Each component can be tested in isolation</li> <li>Reusability: Tools can be used by both language models and developers</li> <li>Scalability: New capabilities can be added without changing existing components</li> <li>Performance: Parallel execution becomes easier to implement</li> <li>Organizational Alignment: Different teams can own different aspects of the system</li> </ol> <p>Organizational Structure</p> <p>One effective team structure: - Interface Team: Designs the API contracts and tool specifications based on user needs - Implementation Team: Builds and optimizes individual retrievers for specific content types - Router Team: Creates and optimizes the query routing system - Evaluation Team: Tests the performance of the entire system and identifies bottlenecks</p> <pre><code>graph TD\n    A[User Query] --&gt; B[Query Router]\n    B --&gt; C[Tool Selection]\n    C --&gt; D[Document Tool]\n    C --&gt; E[Image Tool]\n    C --&gt; F[Table Tool]\n    D --&gt; G[Ranking]\n    E --&gt; G\n    F --&gt; G\n    G --&gt; H[Context Assembly]\n    H --&gt; I[Response Generation]\n    I --&gt; J[User Interface]</code></pre> <p>This architecture resembles modern microservice patterns where specialized services handle specific tasks. The difference is that the \"client\" making API calls is often a language model rather than another service.</p>","tags":["query-routing","unified-architecture","tool-interfaces"]},{"location":"workshops/chapter6-1/#from-monolithic-to-modular-the-evolution-of-rag-architecture","title":"From Monolithic to Modular: The Evolution of RAG Architecture","text":"<p>Many RAG implementations start with a monolithic approach: a single vector database containing all content types, a unified chunking strategy, and a single retrieval mechanism. While simple to implement, this approach quickly reaches its limits as content diversity grows.</p> <p>The transition to a modular, API-based architecture typically follows these stages:</p> <ol> <li>Recognition Phase: Identifying that different query types need different retrieval approaches</li> <li>Separation Phase: Breaking the monolithic system into specialized components</li> <li>Interface Phase: Defining clear boundaries and contracts between components</li> <li>Orchestration Phase: Building a routing layer that knows when to use each component</li> </ol> <p>Real-World Transition</p> <p>A client in the financial services sector initially implemented RAG with a single vector database containing everything from market reports to customer communications. When they transitioned to specialized retrieval components with clear API boundaries, they saw:</p> <pre><code>- **Development Velocity**: 40% increase in feature delivery speed\n- **Retrieval Quality**: 25-35% improvement across different query types\n- **Team Coordination**: Reduced cross-team dependencies and bottlenecks\n- **Scaling**: Ability to add new content types without disrupting existing functionality\n</code></pre> <p>The key insight was treating each specialized retriever not just as an implementation detail, but as a well-defined service with a clear contract.</p>","tags":["query-routing","unified-architecture","tool-interfaces"]},{"location":"workshops/chapter6-2/","title":"Tool Interfaces and Implementation: Building the Components","text":"<p>Chapter Overview</p> <pre><code>This part explores how to implement the key components of a unified RAG system:\n\n- Implementing tool interfaces for different content types\n- Building an effective query router using few-shot examples\n- Creating a feedback loop that improves routing over time\n- Measuring router performance separately from retriever performance\n</code></pre>","tags":["tool-interfaces","implementation","few-shot-learning","microservices"]},{"location":"workshops/chapter6-2/#implementing-tool-interfaces-for-retrieval","title":"Implementing Tool Interfaces for Retrieval","text":"<p>Let's look at how to implement this pattern with a concrete example. Imagine we're building a construction information system that includes blueprints, text documents, and project schedules.</p> <p>Drawing from Previous Chapters</p> <ul> <li>Chapter 1: Evaluation metrics help test router accuracy</li> <li>Chapter 3: Feedback reveals which tools users need</li> <li>Chapter 4: Query analysis identifies tool requirements</li> <li>Chapter 5: Specialized retrievers become the tools</li> </ul>","tags":["tool-interfaces","implementation","few-shot-learning","microservices"]},{"location":"workshops/chapter6-2/#building-a-blueprint-search-tool","title":"Building a Blueprint Search Tool","text":"<p>Based on our analysis in Chapter 5, we've determined that users often search for blueprints by description and date range. We'll define a tool interface that captures this functionality:</p> <pre><code>from pydantic import BaseModel\n\nclass SearchBlueprint(BaseModel):\n    description: str\n    start_date: str | None = None\n    end_date: str | None = None\n\n    def execute(\n        self,\n    ) -&gt; List[BlueprintResult]:\n        \"\"\"\n        Search for blueprints matching the description and date range.\n\n        Args:\n            description: Text to search for in blueprint descriptions\n            start_date: Optional start date in YYYY-MM-DD format\n            end_date: Optional end date in YYYY-MM-DD format\n\n        Returns:\n            List of matching blueprint documents\n        \"\"\"\n        # Implementation details would depend on your database\n        query = self._build_query(\n            query=self.description,\n            start_date=self.start_date,\n            end_date=self.end_date)\n        results = self._execute_query(query)\n        return self._format_results(results)\n\n        ...\n</code></pre>","tags":["tool-interfaces","implementation","few-shot-learning","microservices"]},{"location":"workshops/chapter6-2/#building-a-document-search-tool","title":"Building a Document Search Tool","text":"<p>Similarly, we can define a tool for searching text documents:</p> <pre><code>from pydantic import BaseModel\n\nclass SearchText(BaseModel):\n    query: str\n    document_type: Literal[\"contract\", \"proposal\", \"bid\"] | None = None\n\n    def execute(\n        self,\n    ) -&gt; List[DocumentResult]:\n        if self.document_type:\n            filter_params[\"type\"] = self.document_type\n\n        results = self._search_database(\n            query=self.query,\n            filters=filter_params)\n        return self._format_results(results)\n</code></pre>","tags":["tool-interfaces","implementation","few-shot-learning","microservices"]},{"location":"workshops/chapter6-2/#the-power-of-tool-documentation","title":"The Power of Tool Documentation","text":"<p>Notice the detailed docstrings and examples in these tool definitions. These aren't just for human developers\u2014they're critical for language models to understand how and when to use each tool. The examples in particular help models recognize the patterns of queries that should trigger each tool.</p> <p>Tool Portfolio Design Principles</p> <p>Tools vs Retrievers: - Tools are NOT one-to-one with retrievers - Think of tools like command-line utilities: multiple ways to access the same data - A single retriever might power multiple tools with different interfaces</p> <p>Example: Document Retriever, Multiple Tools <pre><code># One retriever, multiple access patterns\nclass DocumentRetriever:\n    \"\"\"Core retrieval engine for all documents\"\"\"\n    pass\n\n# Tool 1: Search by keyword\nclass SearchDocuments(BaseModel):\n    query: str\n\n# Tool 2: Find by metadata\nclass FindDocumentsByMetadata(BaseModel):\n    author: Optional[str]\n    date_range: Optional[DateRange]\n    document_type: Optional[str]\n\n# Tool 3: Get related documents\nclass GetRelatedDocuments(BaseModel):\n    document_id: str\n    similarity_threshold: float = 0.8\n</code></pre></p> <p>This separation allows users to access the same underlying data in ways that match their mental models.</p>","tags":["tool-interfaces","implementation","few-shot-learning","microservices"]},{"location":"workshops/chapter6-2/#aside-on-mcp","title":"Aside on MCP","text":"<p>The Model Context Protocol (MCP) is an open standard developed by Anthropic that standardizes how applications provide context to large language models. Conceptually similar to the tool interface pattern we've discussed, MCP creates a universal protocol for connecting AI systems to various data sources and tools.</p> <p>Think of MCP like a \"USB-C port for AI applications\" \u2013 just as USB-C provides a standardized way to connect devices to various peripherals, MCP provides a standardized way for AI models to interact with different data sources and tools.</p> <p>Key benefits of MCP include:</p> <ol> <li>Standardization: Developers can build against a single protocol instead of maintaining separate connectors for each data source</li> <li>Interoperability: AI systems can maintain context as they move between different tools and datasets</li> <li>Ecosystem: Pre-built connectors for popular systems like GitHub, Slack, and databases can be shared and reused</li> <li>Security: The protocol is designed with security considerations for connecting AI to sensitive data sources</li> </ol> <p>MCP represents an important step toward the unified architecture vision we've discussed in this chapter, offering a standardized way to implement the \"tools as APIs\" pattern across different AI systems and data sources.</p> <p>MCP is Still Emerging</p> <pre><code>While MCP represents a promising approach to standardizing AI tool interfaces, it's important to note that it's still very new. As of now, there aren't many production-ready MCP implementations available, and the ecosystem of useful MCPs is still in its early stages of development. Organizations adopting MCP should be prepared for an evolving standard and limited availability of pre-built connectors. As with any emerging technology, early adopters will need to invest in building custom implementations and should expect the standard to evolve over time.\n</code></pre>","tags":["tool-interfaces","implementation","few-shot-learning","microservices"]},{"location":"workshops/chapter6-2/#building-the-routing-layer","title":"Building the Routing Layer","text":"<p>Once we have defined our specialized retrieval tools, we need a system that can route queries to the appropriate tools. This routing layer is responsible for:</p> <ol> <li>Understanding the user's query</li> <li>Determining which tool(s) to call</li> <li>Extracting the necessary parameters from the query</li> <li>Calling the appropriate tools with those parameters</li> <li>Combining results when multiple tools are used</li> </ol> <p>Modern language models excel at this kind of task, especially when provided with clear tool definitions and examples.</p> <p>Router vs. Individual Retrievers</p> <p>It's critical to distinguish between the performance of your router (selecting the right tools) and the performance of each individual retriever (finding relevant information). A perfect router with mediocre retrievers will still yield mediocre results, while a mediocre router with perfect retrievers might miss capabilities entirely.</p> <p>Multi-Agent vs Single-Agent Architecture</p> <p>When to Use Multi-Agent Systems:</p> <p>Coordination Challenges: - Agents sharing state is complex - Message passing adds latency - Debugging becomes harder - Error cascades are common</p> <p>Primary Benefits: 1. Token Efficiency: Each agent sees only relevant context 2. Specialization: Different models for different tasks 3. Read/Write Separation: Critical for safety</p> <p>Read-Only vs Write Operations: - Keep read operations in single agent when possible - Separate write operations into specialized agents - Example: Reading code (safe) vs modifying code (requires careful agent)</p> <p>Real-World Example: A coding assistant might use: - Single agent for code reading, analysis, explanation - Specialized agent for code generation with guardrails - Separate agent for file system operations</p> <p>This separation ensures safety while maintaining efficiency.</p>","tags":["tool-interfaces","implementation","few-shot-learning","microservices"]},{"location":"workshops/chapter6-2/#implementing-a-simple-router","title":"Implementing a Simple Router","text":"<p>Here's a basic implementation of a query router using the Instructor library for structured outputs:</p> <pre><code>import instructor\nfrom typing import List, Literal, Iterable\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = OpenAI()\nclient = instructor.from_openai(client)\n\nclass ClarifyQuestion(BaseModel):\n    \"\"\"Use this when you need more information from the user to understand their request.\"\"\"\n    question: str\n\nclass AnswerQuestion(BaseModel):\n    \"\"\"Use this when you can answer directly without retrieving documents.\"\"\"\n    content: str\n    follow_ups: List[str] | None = None\n\nclass SearchBlueprint(BaseModel):\n    \"\"\"Use this to search for building plans and blueprints.\"\"\"\n    blueprint_description: str\n    start_date: str | None = None\n    end_date: str | None = None\n\nclass SearchText(BaseModel):\n    \"\"\"Use this to search for text documents like contracts, proposals, and bids.\"\"\"\n    query: str\n    document_type: Literal[\"contract\", \"proposal\", \"bid\"] | None = None\n\ndef route_query(query: str) -&gt; Iterable[SearchBlueprint | SearchText | AnswerQuestion | ClarifyQuestion]:\n    \"\"\"\n    Routes a user query to the appropriate tool(s) based on the query content.\n\n    This function analyzes the user's query and determines which tool or tools \n    would be most appropriate to handle it. Multiple tools can be returned if needed.\n\n    Args:\n        query: The user's natural language query\n\n    Returns:\n        An iterable of tool objects that should be used to process this query\n    \"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\n                You are a query router for a construction information system.\n\n                Your job is to analyze the user's query and decide which tool(s) should handle it.\n                You can return multiple tools if the query requires different types of information.\n\n                Available tools:\n                - SearchBlueprint: For finding building plans and blueprints\n                - SearchText: For finding text documents like contracts and proposals\n                - AnswerQuestion: For directly answering conceptual questions without retrieval\n                - ClarifyQuestion: For asking follow-up questions when the query is unclear\n\n                Here are examples of how to route different types of queries:\n\n                &lt;examples&gt;\n                ...\n                &lt;/examples&gt;\n                \"\"\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": query\n            }\n        ],\n        response_model=Iterable[SearchBlueprint | SearchText | AnswerQuestion | ClarifyQuestion]\n    )\n\n# Example usage\ndef process_user_query(query: str):\n    \"\"\"Process a user query by routing it to the appropriate tools and executing them.\"\"\"\n    # Step 1: Route the query to appropriate tools\n    tools = route_query(query)\n\n    # Step 2: Execute each tool and collect results\n    results = []\n    for tool in tools:\n        if isinstance(tool, SearchBlueprint):\n            # Execute blueprint search\n            blueprints = search_blueprints(\n                description=tool.blueprint_description,\n                start_date=tool.start_date,\n                end_date=tool.end_date\n            )\n            results.append({\"type\": \"blueprints\", \"data\": blueprints})\n\n        elif isinstance(tool, SearchText):\n            # Execute text search\n            documents = search_documents(\n                query=tool.query,\n                document_type=tool.document_type\n            )\n            results.append({\"type\": \"documents\", \"data\": documents})\n\n        elif isinstance(tool, AnswerQuestion):\n            # Direct answer without retrieval\n            results.append({\"type\": \"answer\", \"data\": tool.content})\n\n        elif isinstance(tool, ClarifyQuestion):\n            # Return clarification question to user\n            return {\"action\": \"clarify\", \"question\": tool.question}\n\n    # Step 3: Generate a response using the collected results\n    return {\"action\": \"respond\", \"results\": results}\n</code></pre>","tags":["tool-interfaces","implementation","few-shot-learning","microservices"]},{"location":"workshops/chapter6-2/#using-few-shot-examples-to-improve-routing","title":"Using Few-Shot Examples to Improve Routing","text":"<p>The effectiveness of the router depends significantly on providing good examples of when to use each tool. These few-shot examples help the model understand the patterns that should trigger different tools.</p> <p>Evolution of RAG Architectures</p> <p>From Embeddings to Tools:</p> <p>The progression of RAG architectures follows a predictable pattern:</p> <ol> <li>Generation 1: Pure Embeddings</li> <li>Single vector database</li> <li>Semantic search only</li> <li> <p>Limited to similarity matching</p> </li> <li> <p>Generation 2: Hybrid Search</p> </li> <li>Combine semantic + lexical</li> <li>Add metadata filtering</li> <li> <p>Still retrieval-focused</p> </li> <li> <p>Generation 3: Tool-Based</p> </li> <li>Multiple specialized tools</li> <li>Goes beyond retrieval</li> <li>Includes actions and computations</li> </ol> <p>Why This Evolution Happens: - Users don't just want to find information - They want to analyze, compare, compute - Tools enable richer interactions - Better matches user mental models</p> <p>Example Evolution: <pre><code>V1: \"Find documents about project X\"\nV2: \"Find recent documents about project X by John\"\nV3: \"Compare project X budget vs actuals and identify variances\"\n</code></pre></p> <p>The third query requires tools that can compute, not just retrieve.</p> <p>Complete the Journey</p> <p>This chapter brings together all the concepts from the book: - The improvement flywheel from Chapter 0 - Evaluation frameworks from Chapter 1 - Fine-tuning from Chapter 2 - Feedback loops from Chapter 3 - Query understanding from Chapter 4 - Specialized capabilities from Chapter 5</p> <p>The unified architecture is where everything comes together into a cohesive product.</p> <p>Effective Few-Shot Examples</p> <p>When creating few-shot examples for query routing:</p> <pre><code>1. **Cover edge cases**: Include examples of ambiguous queries that could be interpreted multiple ways\n2. **Include multi-tool examples**: Show when multiple tools should be used together\n3. **Demonstrate hard decisions**: Show when similar-sounding queries should route to different tools\n4. **Use real user queries**: Whenever possible, use actual queries from your users\n5. **Maintain diversity**: Ensure examples cover all tools and important parameter combinations\n</code></pre> <p>For instance, a system prompt for routing might include examples like:</p> <pre><code>&lt;examples&gt;\n- \"Find blueprints for the city hall built in 2010.\"\n{\n    \"blueprint_description\": \"city hall blueprints\",\n    \"start_date\": \"2010-01-01\",\n    \"end_date\": \"2010-12-31\"\n}\n- \"I need plans for residential buildings constructed after 2015.\"\n{\n    \"blueprint_description\": \"residential building plans\",\n    \"start_date\": \"2015-01-01\",\n    \"end_date\": null\n}\n- \"Can you find me the plans for a the 123 main st building?\"\n{\n    \"blueprint_description\": \"123 main st building\",\n    \"start_date\": null,\n    \"end_date\": null\n}\n- \"Show me blueprints for schools built between 2018 and 2020.\"\n{\n    \"blueprint_description\": \"school blueprints\",\n    \"start_date\": \"2018-01-01\",\n    \"end_date\": \"2020-12-31\"\n}\n- \"I need the contract for the Johnson project.\"\n{\n    \"query\": \"Johnson project contract\",\n    \"document_type\": \"contract\"\n}\n- \"What's the difference between a blueprint and a floor plan?\"\n{\n    \"content\": \"Blueprints are technical architectural drawings that include detailed specifications for construction, while floor plans focus primarily on the layout and dimensions of rooms and spaces within a building.\",\n    \"follow_ups\": [\"How do I read a blueprint?\", \"Can you show me examples of floor plans?\"]\n}\n- \"Can you explain what a load-bearing wall is?\"\n{\n    \"content\": \"A load-bearing wall is a structural element that supports the weight of the building above it, helping to transfer the load to the foundation. Removing or modifying load-bearing walls requires careful engineering considerations.\",\n    \"follow_ups\": [\"How can I identify a load-bearing wall?\", \"What happens if you remove a load-bearing wall?\"]\n}\n- \"I'm not sure what kind of building plans I need for my renovation.\"\n{\n    \"question\": \"Could you tell me more about your renovation project? What type of building is it, what changes are you planning to make, and do you need plans for permits or for construction guidance?\"\n}\n- \"Find me school building plans from 2018-2020 and any related bid documents.\"\n[\n    {\n        \"blueprint_description\": \"school building plans\",\n        \"start_date\": \"2018-01-01\",\n        \"end_date\": \"2020-12-31\"\n    },\n    {\n        \"query\": \"school building bids\",\n        \"document_type\": \"bid\"\n    }\n]\n&lt;/examples&gt;\n</code></pre>","tags":["tool-interfaces","implementation","few-shot-learning","microservices"]},{"location":"workshops/chapter6-2/#dynamic-few-shot-example-selection","title":"Dynamic Few-Shot Example Selection","text":"<p>As your system collects more data about successful interactions, you can move beyond static examples to a dynamic approach that selects the most relevant few-shot examples for each query:</p> <pre><code>def get_dynamic_examples(query: str, example_database: List[dict], num_examples: int = 5) -&gt; List[dict]:\n    \"\"\"\n    Select the most relevant examples for a given query from an example database.\n\n    Args:\n        query: The user's query\n        example_database: Database of previous successful interactions\n        num_examples: Number of examples to return\n\n    Returns:\n        List of the most relevant examples for this query\n    \"\"\"\n    # Embed the query\n    query_embedding = get_embedding(query)\n\n    # Calculate similarity with all examples in database\n    similarities = []\n    for example in example_database:\n        example_embedding = example[\"embedding\"]\n        similarity = cosine_similarity(query_embedding, example_embedding)\n        similarities.append((similarity, example))\n\n    # Sort by similarity and return top examples\n    similarities.sort(reverse=True)\n    return [example for _, example in similarities[:num_examples]]\n\ndef route_query_with_dynamic_examples(query: str) -&gt; Iterable[Tool]:\n    \"\"\"Route query using dynamically selected examples.\"\"\"\n    # Get relevant examples for this query\n    relevant_examples = get_dynamic_examples(query, example_database)\n\n    # Format examples for inclusion in prompt\n    examples_text = format_examples(relevant_examples)\n\n    # Create prompt with dynamic examples\n    system_prompt = f\"\"\"\n    You are a query router for a construction information system.\n    Your job is to analyze the user's query and decide which tool(s) should handle it.\n\n    Available tools:\n    - SearchBlueprint: For finding building plans and blueprints\n    - SearchText: For finding text documents like contracts and proposals\n    - AnswerQuestion: For directly answering conceptual questions without retrieval\n    - ClarifyQuestion: For asking follow-up questions when the query is unclear\n\n    Here are examples of how to route different types of queries:\n\n    {examples_text}\n    \"\"\"\n\n    # Perform routing with dynamic prompt\n    return client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": query}\n        ],\n        response_model=Iterable[SearchBlueprint | SearchText | AnswerQuestion | ClarifyQuestion]\n    )\n</code></pre> <p>This approach ensures that your routing layer continuously improves as you collect more examples of successful interactions, creating a learning system that adapts to your users' query patterns.</p> <p>IF you want to get discounts and 6 day email source on the topic make sure to subscribe to</p>","tags":["tool-interfaces","implementation","few-shot-learning","microservices"]},{"location":"workshops/chapter6-3/","title":"Performance Measurement and Improvement: Building Learning Systems","text":"<p>Chapter Overview</p> <pre><code>This part explores how to measure, test, and continuously improve a unified RAG system:\n\n- Testing and measuring performance of both retrieval and routing components\n- Creating user interfaces that leverage both AI and direct tool access\n- Building systems that scale across teams and complexity levels\n- Creating continuous improvement cycles through user feedback\n</code></pre>","tags":["performance-metrics","testing","user-interfaces","feedback-loops"]},{"location":"workshops/chapter6-3/#testing-query-routing-effectiveness","title":"Testing Query Routing Effectiveness","text":"<p>Just as we need metrics for retrieval quality, we need metrics for routing quality. The fundamental question is: are we selecting the right tools for each query?</p>","tags":["performance-metrics","testing","user-interfaces","feedback-loops"]},{"location":"workshops/chapter6-3/#tool-selection-metrics","title":"Tool Selection Metrics","text":"<p>To evaluate tool selection, we need a test dataset with queries annotated with the correct tool(s) to use. From there, we can calculate:</p> <ol> <li>Tool Precision: When we select a tool, how often is it actually the right one?</li> <li>Tool Recall: How often do we select all the tools that should be selected?</li> <li>Tool F1 Score: The harmonic mean of precision and recall</li> <li>Per-Tool Recall: How often each specific tool is correctly selected when it should be</li> </ol> <p>Data Leakage Risk</p> <p>When creating test datasets for router evaluation, be vigilant about data leakage. If your few-shot examples appear in your test set, you'll get artificially high performance that won't generalize to real queries. Always maintain separate development and test sets with distinct query patterns.</p> <p>Here's a sample evaluation for a construction information system's query router:</p> Query ID Query Text Expected Tools Realized Tools Precision Recall 1 Retrieve blueprints for the museum expansion SearchBlueprint SearchBlueprint 100% 1/1 2 Find schedule and documents for the library renovation SearchSchedule, SearchText SearchSchedule 100% 1/2 3 Get both blueprints and schedule for campus construction SearchBlueprint, SearchSchedule SearchBlueprint, SearchSchedule 100% 2/2 4 Show me contract details and permit requirements for the new office SearchText, SearchBlueprint SearchText, SearchBlueprint, SearchSchedule 67% 2/2 5 Identify materials and design specs for the downtown skyscraper SearchText, SearchBlueprint SearchBlueprint, SearchText 100% 2/2 6 Get full details on industrial park planning SearchBlueprint, SearchText, SearchSchedule SearchText, SearchInvoice, SearchPermit 33% 1/3 7 Find emergency repair guidelines for the abandoned warehouse SearchRepair, SearchBlueprint SearchText 0% 0/2 8 Obtain comprehensive analysis for the urban redevelopment project SearchBlueprint, SearchText, SearchSchedule, SearchPermit SearchBlueprint 100% 1/4 9 Explain zoning regulations for the new industrial area SearchZoning SearchBlueprint, SearchText 0% 0/1 <p>Looking at overall metrics, this system achieves:</p> <ul> <li>Average Precision: 67%</li> <li>Average Recall: 56%</li> <li>Average F1 Score: 61%</li> </ul> <p>These aggregate metrics are useful, but they don't tell the complete story. What's often more revealing is the per-tool recall:</p> Tool Times Expected Times Selected Correctly Per-Tool Recall SearchBlueprint 6 4 67% SearchText 5 3 60% SearchSchedule 4 2 50% SearchPermit 1 0 0% SearchZoning 1 0 0% SearchRepair 1 0 0% <p>This breakdown shows that less common tools (Permit, Zoning, Repair) have extremely low recall, suggesting that our router doesn't have enough examples of these tools to recognize when they should be used.</p>","tags":["performance-metrics","testing","user-interfaces","feedback-loops"]},{"location":"workshops/chapter6-3/#automating-router-evaluation","title":"Automating Router Evaluation","text":"<p>Here's a code example for evaluating router performance:</p> <pre><code>def evaluate_router(router_function, test_dataset):\n    \"\"\"\n    Evaluate a routing function against a test dataset.\n\n    Args:\n        router_function: Function that takes a query and returns tool selections\n        test_dataset: List of {query, expected_tools} pairs\n\n    Returns:\n        Dictionary of evaluation metrics\n    \"\"\"\n    results = []\n    tool_expected_count = {}\n    tool_selected_count = {}\n    tool_correct_count = {}\n\n    for test_case in test_dataset:\n        query = test_case[\"query\"]\n        expected_tools = set(test_case[\"expected_tools\"])\n\n        # Track expected tools\n        for tool in expected_tools:\n            tool_expected_count[tool] = tool_expected_count.get(tool, 0) + 1\n\n        # Get router predictions\n        selected_tools = set(router_function(query))\n\n        # Track selected tools\n        for tool in selected_tools:\n            tool_selected_count[tool] = tool_selected_count.get(tool, 0) + 1\n\n        # Calculate precision and recall for this query\n        correct_tools = expected_tools.intersection(selected_tools)\n        for tool in correct_tools:\n            tool_correct_count[tool] = tool_correct_count.get(tool, 0) + 1\n\n        precision = len(correct_tools) / len(selected_tools) if selected_tools else 1.0\n        recall = len(correct_tools) / len(expected_tools) if expected_tools else 1.0\n        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) &gt; 0 else 0\n\n        results.append({\n            \"query\": query,\n            \"expected_tools\": expected_tools,\n            \"selected_tools\": selected_tools,\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1\": f1\n        })\n\n    # Calculate overall metrics\n    avg_precision = sum(r[\"precision\"] for r in results) / len(results)\n    avg_recall = sum(r[\"recall\"] for r in results) / len(results)\n    avg_f1 = sum(r[\"f1\"] for r in results) / len(results)\n\n    # Calculate per-tool recall\n    per_tool_recall = {}\n    for tool in tool_expected_count:\n        if tool_expected_count[tool] &gt; 0:\n            per_tool_recall[tool] = tool_correct_count.get(tool, 0) / tool_expected_count[tool]\n        else:\n            per_tool_recall[tool] = 0\n\n    return {\n        \"detailed_results\": results,\n        \"avg_precision\": avg_precision,\n        \"avg_recall\": avg_recall,\n        \"avg_f1\": avg_f1,\n        \"per_tool_recall\": per_tool_recall,\n        \"tool_expected_count\": tool_expected_count,\n        \"tool_selected_count\": tool_selected_count,\n        \"tool_correct_count\": tool_correct_count\n    }\n</code></pre>","tags":["performance-metrics","testing","user-interfaces","feedback-loops"]},{"location":"workshops/chapter6-3/#analyzing-tool-selection-failures","title":"Analyzing Tool Selection Failures","text":"<p>When tool selection fails, we need to understand why. A confusion matrix is particularly useful here, showing which tools are being confused with one another.</p> <p>For example, if we find that the <code>SearchBlueprint</code> tool is never being selected even when it should be, we might need to improve its description or add more examples to the system prompt.</p> <p>Confusion Matrix Analysis</p> <pre><code>Imagine our evaluation produces this confusion matrix:\n\n| Expected\\Selected | SearchText | SearchBlueprint | SearchSchedule |\n| ----------------- | ---------- | --------------- | -------------- |\n| SearchText        | 85         | 5               | 10             |\n| SearchBlueprint   | 40         | 50              | 10             |\n| SearchSchedule    | 15         | 5               | 80             |\n\nThis shows that SearchBlueprint is frequently mistaken for SearchText, indicating that we need to better differentiate these tools.\n</code></pre>","tags":["performance-metrics","testing","user-interfaces","feedback-loops"]},{"location":"workshops/chapter6-3/#targeted-improvement-strategy","title":"Targeted Improvement Strategy","text":"<p>Once you've identified specific weaknesses in your router, you can implement targeted improvements:</p> <ol> <li> <p>For low-recall tools:</p> </li> <li> <p>Add more few-shot examples for these tools</p> </li> <li>Improve tool descriptions to more clearly differentiate them</li> <li> <p>Consider whether these tools are truly distinct or should be merged</p> </li> <li> <p>For commonly confused tools:</p> </li> <li> <p>Analyze failure cases to understand what's causing the confusion</p> </li> <li>Create \"contrast examples\" that explicitly show why similar queries go to different tools</li> <li> <p>Refine tool interfaces to have clearer boundaries</p> </li> <li> <p>For overall improvement:</p> </li> <li> <p>Balance your few-shot examples across all tools</p> </li> <li>Include edge cases that test the boundaries between tools</li> <li>Add multi-tool examples that show when multiple tools should be used together</li> </ol> <p>Synthetic Data Generation for Router Testing</p> <p>You can use synthetic data techniques to create comprehensive test cases for your router:</p> <pre><code>1. Start with clear definitions of each tool's purpose\n2. Use an LLM to generate diverse queries that should trigger each tool\n3. Include variants of each query with slightly different wording\n4. Generate ambiguous queries that could reasonably go to multiple tools\n5. Create a balanced dataset that covers all tools proportionally\n\nThis approach ensures comprehensive coverage of your router's decision space without requiring extensive manual labeling.\n</code></pre>","tags":["performance-metrics","testing","user-interfaces","feedback-loops"]},{"location":"workshops/chapter6-3/#user-interfaces-direct-tool-access","title":"User Interfaces: Direct Tool Access","text":"<p>One powerful insight from the routing architecture is that tools designed for language models can often be exposed directly to users as well. Just as Google offers specialized interfaces like Google Maps, YouTube, and Google Images alongside its main search, your RAG application can offer both:</p> <ol> <li>A natural language interface using the router</li> <li>Direct access to specialized tools for specific needs</li> </ol> <p>Expert User Perspective</p> <p>\"When I know exactly what I need, a specialized tool is much faster than explaining it to a chatbot. But when I'm exploring new areas or have complex needs, the chat interface helps me discover what's possible.\"</p> <p>Dual-Mode UI</p> <p>Imagine a construction information system that offers:</p> <pre><code>- A chat interface for general questions\n- A blueprint search interface with date filters\n- A document search interface with type filters\n- A schedule search with timeline visualization\n- A permit lookup tool with status tracking\n\nThese specialized interfaces map directly to the specialized retrievers we've built.\n</code></pre> <p>This dual-mode interface has several advantages:</p> <ol> <li>Expert users can go directly to the tool they need</li> <li>New users can use natural language until they learn the system</li> <li>User interactions with direct tools provide training data for routing</li> <li>Clear capabilities help users understand what the system can do</li> <li>Control and transparency give users confidence in the results</li> <li>Performance optimization for common, well-defined tasks</li> </ol> <p>UI Implementation Strategy</p> <p>When implementing a dual-mode interface:</p> <pre><code>1. Design specialized interfaces that match your existing tools' parameters\n2. Create a unified entry point that offers both chat and specialized tool options\n3. Add suggestions in chat responses that link to relevant specialized tools\n4. Maintain consistent terminology between chat responses and tool interfaces\n5. Track which interface users prefer for different query types\n</code></pre>","tags":["performance-metrics","testing","user-interfaces","feedback-loops"]},{"location":"workshops/chapter6-3/#specialized-interface-examples","title":"Specialized Interface Examples","text":"<p>Here's how specialized interfaces might look for our construction information system:</p>","tags":["performance-metrics","testing","user-interfaces","feedback-loops"]},{"location":"workshops/chapter6-3/#blueprint-search-interface","title":"Blueprint Search Interface","text":"<pre><code>&lt;form action=\"/search/blueprints\" method=\"GET\"&gt;\n  &lt;h2&gt;Blueprint Search&lt;/h2&gt;\n\n  &lt;div class=\"form-group\"&gt;\n    &lt;label for=\"description\"&gt;Description:&lt;/label&gt;\n    &lt;input type=\"text\" id=\"description\" name=\"description\" \n           placeholder=\"e.g., residential building, hospital, school\"&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"form-group\"&gt;\n    &lt;label for=\"start-date\"&gt;Start Date:&lt;/label&gt;\n    &lt;input type=\"date\" id=\"start-date\" name=\"start_date\"&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"form-group\"&gt;\n    &lt;label for=\"end-date\"&gt;End Date:&lt;/label&gt;\n    &lt;input type=\"date\" id=\"end-date\" name=\"end_date\"&gt;\n  &lt;/div&gt;\n\n  &lt;button type=\"submit\"&gt;Search Blueprints&lt;/button&gt;\n&lt;/form&gt;\n</code></pre>","tags":["performance-metrics","testing","user-interfaces","feedback-loops"]},{"location":"workshops/chapter6-3/#document-search-interface","title":"Document Search Interface","text":"<pre><code>&lt;form action=\"/search/documents\" method=\"GET\"&gt;\n  &lt;h2&gt;Document Search&lt;/h2&gt;\n\n  &lt;div class=\"form-group\"&gt;\n    &lt;label for=\"query\"&gt;Search Terms:&lt;/label&gt;\n    &lt;input type=\"text\" id=\"query\" name=\"query\" \n           placeholder=\"e.g., Johnson project, HVAC specifications\"&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"form-group\"&gt;\n    &lt;label for=\"document-type\"&gt;Document Type:&lt;/label&gt;\n    &lt;select id=\"document-type\" name=\"document_type\"&gt;\n      &lt;option value=\"\"&gt;All Documents&lt;/option&gt;\n      &lt;option value=\"contract\"&gt;Contracts&lt;/option&gt;\n      &lt;option value=\"proposal\"&gt;Proposals&lt;/option&gt;\n      &lt;option value=\"bid\"&gt;Bids&lt;/option&gt;\n    &lt;/select&gt;\n  &lt;/div&gt;\n\n  &lt;button type=\"submit\"&gt;Search Documents&lt;/button&gt;\n&lt;/form&gt;\n</code></pre> <p>These interfaces directly map to the tool interfaces we defined earlier, providing users with a clear, structured way to access the same capabilities available to the language model.</p> <p>The key insight is that RAG isn't just about adding chat to your product\u2014it's about building a comprehensive information discovery system where chat is just one interface option among many specialized tools that help users access information efficiently.</p> <p>Beyond Simple Forms</p> <p>These specialized interfaces don't have to be simple forms. They can include rich visualizations, interactive elements, and specialized displays for different content types. For example, a blueprint search might display results on a timeline or a map, while a document search might offer faceted filters and previews. The key is that they map directly to your underlying retrieval tools.</p>","tags":["performance-metrics","testing","user-interfaces","feedback-loops"]},{"location":"workshops/chapter6-3/#user-feedback-as-training-data","title":"User Feedback as Training Data","text":"<p>A particularly valuable aspect of direct tool access is that user interactions can provide high-quality training data for improving both retrieval and routing:</p> <ol> <li>When users select a specific tool, that's a signal about their intent</li> <li>When users click on search results, that's a signal about relevance</li> <li>When users refine their search, that's a signal about what was missing</li> <li>When users explicitly rate or save results, that's direct feedback on quality</li> </ol> <p>User Feedback Collection Mechanisms</p> <p>To maximize the value of user feedback, consider implementing:</p> <pre><code>- **Tool Selection Tracking**: Record which specialized tool a user chooses for each query\n- **Click Tracking**: Monitor which search results users engage with\n- **Query Refinement Analysis**: Capture how users modify queries that didn't yield useful results\n- **Explicit Feedback Buttons**: Add \"Was this helpful?\" buttons to results\n- **Result Saving**: Allow users to save or bookmark useful results\n- **Session Analysis**: Examine session patterns to identify successful vs. unsuccessful paths\n</code></pre> <p>These interactions can be logged and used to:</p> <ul> <li>Fine-tune embedding models with user-confirmed relevant documents</li> <li>Improve router accuracy by learning from user tool selections</li> <li>Create better few-shot examples based on successful interactions</li> <li>Prioritize development efforts based on usage patterns</li> <li>Identify gaps in your retrieval capabilities</li> </ul>","tags":["performance-metrics","testing","user-interfaces","feedback-loops"]},{"location":"workshops/chapter6-3/#implementing-a-feedback-loop","title":"Implementing a Feedback Loop","text":"<p>Here's how you might implement a feedback collection and utilization system:</p> <pre><code>def record_user_feedback(user_id, query, selected_tool, results, clicked_result_ids, explicit_rating=None):\n    \"\"\"\n    Record user feedback for future training data collection.\n\n    Args:\n        user_id: Identifier for the user\n        query: The user's original query\n        selected_tool: Which tool they used (or 'chat' if they used the chat interface)\n        results: The results returned to the user\n        clicked_result_ids: Which result IDs the user clicked on\n        explicit_rating: Optional explicit rating (1-5) provided by the user\n    \"\"\"\n    feedback_entry = {\n        \"user_id\": user_id,\n        \"timestamp\": datetime.now().isoformat(),\n        \"query\": query,\n        \"selected_tool\": selected_tool,\n        \"results\": results,\n        \"clicked_result_ids\": clicked_result_ids,\n        \"explicit_rating\": explicit_rating,\n    }\n\n    # Store feedback in database\n    feedback_collection.insert_one(feedback_entry)\n\n    # If this was a highly-rated interaction, consider adding it to examples\n    if explicit_rating and explicit_rating &gt;= 4:\n        consider_adding_to_examples(feedback_entry)\n\ndef generate_training_data_from_feedback(min_clicks=1, min_rating=None, date_range=None):\n    \"\"\"\n    Generate training data from collected user feedback.\n\n    Args:\n        min_clicks: Minimum number of clicks a result must have received\n        min_rating: Minimum explicit rating (if available)\n        date_range: Optional date range to filter feedback\n\n    Returns:\n        Dictionary with router_training_data and retrieval_training_data\n    \"\"\"\n    # Query conditions\n    conditions = {}\n    if min_rating:\n        conditions[\"explicit_rating\"] = {\"$gte\": min_rating}\n    if date_range:\n        conditions[\"timestamp\"] = {\"$gte\": date_range[0], \"$lte\": date_range[1]}\n\n    # Retrieve feedback entries\n    feedback_entries = feedback_collection.find(conditions)\n\n    router_examples = []\n    retrieval_examples = []\n\n    for entry in feedback_entries:\n        # Generate router training examples\n        if entry[\"selected_tool\"] != \"chat\":\n            router_examples.append({\n                \"query\": entry[\"query\"],\n                \"tool\": entry[\"selected_tool\"]\n            })\n\n        # Generate retrieval training examples\n        for result_id in entry[\"clicked_result_ids\"]:\n            if len(entry[\"clicked_result_ids\"]) &gt;= min_clicks:\n                retrieval_examples.append({\n                    \"query\": entry[\"query\"],\n                    \"relevant_doc_id\": result_id\n                })\n\n    return {\n        \"router_training_data\": router_examples,\n        \"retrieval_training_data\": retrieval_examples\n    }\n\ndef update_few_shot_examples(router_examples, max_examples_per_tool=5):\n    \"\"\"\n    Update the few-shot examples used in the router based on user feedback.\n\n    Args:\n        router_examples: Router examples generated from feedback\n        max_examples_per_tool: Maximum number of examples to keep per tool\n    \"\"\"\n    # Group examples by tool\n    examples_by_tool = {}\n    for example in router_examples:\n        tool = example[\"tool\"]\n        if tool not in examples_by_tool:\n            examples_by_tool[tool] = []\n        examples_by_tool[tool].append(example)\n\n    # Select the best examples for each tool\n    selected_examples = []\n    for tool, examples in examples_by_tool.items():\n        # Sort by frequency or other quality metric\n        sorted_examples = sort_examples_by_quality(examples)\n        selected_examples.extend(sorted_examples[:max_examples_per_tool])\n\n    # Update the router's few-shot examples\n    update_router_prompt(selected_examples)\n</code></pre> <p>This creates another improvement flywheel: as users interact with the system, it collects data that makes both retrieval and routing better, which leads to higher user satisfaction and more interactions.</p> <p>Feedback Biases</p> <p>Be aware of potential biases in user feedback:</p> <pre><code>1. **Position bias**: Users tend to click on top results regardless of relevance\n2. **Interface bias**: Different interfaces encourage different interaction patterns\n3. **User expertise bias**: Expert users interact differently than novices\n4. **Success bias**: Successful interactions generate more feedback than failures\n\nTo mitigate these biases:\n- Occasionally randomize result ordering for evaluation\n- Analyze feedback separately across user expertise levels\n- Specifically seek feedback on unsuccessful interactions\n- Complement implicit feedback with explicit ratings\n</code></pre>","tags":["performance-metrics","testing","user-interfaces","feedback-loops"]},{"location":"workshops/chapter6-3/#the-combined-success-formula","title":"The Combined Success Formula","text":"<p>Throughout this book, we've focused on a data-driven approach to systematic improvement. In the context of unified architecture, we can express the overall success probability of our system with a simple formula:</p> \\[ P(\\\\text{success}) = P(\\\\text{find right document} \\\\mid \\\\text{right tool}) \\\\times P(\\\\text{right tool}) \\] <p>This formula highlights that our system's performance depends on both:</p> <ol> <li>How well each retriever works when used correctly</li> <li>How often we select the right retriever for the query</li> </ol>","tags":["performance-metrics","testing","user-interfaces","feedback-loops"]},{"location":"workshops/chapter6-3/#a-diagnostic-framework-for-improvement","title":"A Diagnostic Framework for Improvement","text":"<p>This seemingly simple formula provides a powerful diagnostic framework. When your RAG system isn't performing well, it helps pinpoint exactly where the problem lies and what type of solution to pursue:</p> <ul> <li>If tool selection recall is low, focus on improving the routing layer</li> <li>If retrieval recall is low (given the right tool), focus on improving that specific retriever</li> </ul> <p>Example: Imagine users report that when asking about blueprints, they only get satisfactory answers 40% of the time. There are two very different scenarios that could cause this:</p> <p>Scenario 1: The router correctly selects the blueprint search tool 95% of the time, but the blueprint search itself only finds the right blueprints 42% of the time.</p> <ul> <li>P(right tool) = 0.95</li> <li>P(find right document | right tool) = 0.42</li> <li>P(success) = 0.95 \u00d7 0.42 = 0.40 (40%)</li> </ul> <p>Scenario 2: The blueprint search is excellent at finding the right blueprints 80% of the time when used, but the router only selects it 50% of the time (often choosing document search instead).</p> <ul> <li>P(right tool) = 0.50</li> <li>P(find right document | right tool) = 0.80</li> <li>P(success) = 0.50 \u00d7 0.80 = 0.40 (40%)</li> </ul> <p>Same 40% success rate, but completely different problems requiring different solution strategies:</p> <p>For Scenario 1 (retrieval problem):</p> <ul> <li>Generate synthetic data to improve the blueprint search capability</li> <li>Fine-tune embedding models specifically for blueprint content</li> <li>Improve the extraction and structuring of blueprint metadata</li> <li>Experiment with different chunking strategies for blueprints</li> </ul> <p>For Scenario 2 (routing problem):</p> <ul> <li>Add more few-shot examples showing when to use the blueprint tool</li> <li>Improve the blueprint tool description to make it more distinctive</li> <li>Add user feedback from successful interactions into your examples</li> <li>Consider UI changes to help users explicitly request blueprints</li> </ul>","tags":["performance-metrics","testing","user-interfaces","feedback-loops"]},{"location":"workshops/chapter6-3/#measuring-components-independently","title":"Measuring Components Independently","text":"<p>To apply this framework effectively, you need to measure both components independently:</p> <ol> <li>Per-tool recall: How often each retriever finds the right information when used</li> <li>Tool selection accuracy: How often the router selects the right tool(s) for each query</li> </ol> <p>A simple dashboard showing these metrics gives you immediate insight into where to focus your improvement efforts.</p>","tags":["performance-metrics","testing","user-interfaces","feedback-loops"]},{"location":"workshops/chapter6-3/#from-metrics-to-roadmap","title":"From Metrics to Roadmap","text":"<p>This formula provides a clear framework for planning both product and research efforts:</p> P(success | right tool) P(right tool | query) Strategy High High These are strengths to highlight in your product Low High Research focus needed on specific retrievers High Low Focus on improving router or exposing tools directly Low Low Consider whether this query type is worth supporting <p>By systematically measuring and improving these components, you create a continuous improvement flywheel for your unified RAG architecture.</p>","tags":["performance-metrics","testing","user-interfaces","feedback-loops"]},{"location":"workshops/chapter6-3/#conclusion-the-end-of-the-beginning","title":"Conclusion: The End of the Beginning","text":"<p>Throughout this book, we've explored how to systematically improve RAG applications by treating them as continuously evolving products rather than static implementations. We've covered:</p> <ol> <li>Starting the flywheel with synthetic data generation for evaluation</li> <li>Converting evaluations into training data for improvement</li> <li>Building feedback collection mechanisms through user experience design</li> <li>Understanding users through segmentation and capability analysis</li> <li>Creating specialized retrieval capabilities for different content types</li> <li>Unifying these capabilities into a cohesive architecture with intelligent routing</li> </ol> <p>This unified architecture approach represents the culmination of our improvement flywheel\u2014a system that not only retrieves the right information but knows which specialized capability to use for each user need.</p> <p>Framework Development Perspective</p> <p>\"The fundamental building blocks of creating good and successful machine learning products are synthetic data and customer feedback. This is the bedrock\u2014everything else is implementation details that will change as technology evolves.\"</p>","tags":["performance-metrics","testing","user-interfaces","feedback-loops"]},{"location":"workshops/chapter6-3/#the-systematic-improvement-process","title":"The Systematic Improvement Process","text":"<p>The core insight that runs through every chapter of this book is that RAG improvement follows a repeatable pattern:</p> <ol> <li>Measure current performance with precise metrics that separate different system components</li> <li>Identify limiting factors by distinguishing between router accuracy and retriever quality</li> <li>Generate synthetic data to test hypotheses and establish baselines</li> <li>Implement targeted improvements to the specific components that need enhancement</li> <li>Collect user feedback that serves as training data for the next iteration</li> <li>Repeat with increasingly sophisticated capabilities</li> </ol> <p>This process applies equally well whether you're building your first RAG application or enhancing your tenth specialized retriever. The tools and models will change, but the systematic approach remains constant.</p>","tags":["performance-metrics","testing","user-interfaces","feedback-loops"]}]}